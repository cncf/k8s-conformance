I0613 02:15:22.014632      18 e2e.go:116] Starting e2e run "be733e5a-f772-479e-b2cf-3373e56afc42" on Ginkgo node 1
Jun 13 02:15:22.033: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1686622521 - will randomize all specs

Will run 360 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jun 13 02:15:22.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:15:22.194: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0613 02:15:22.201632      18 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0613 02:15:22.201632      18 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jun 13 02:15:22.240: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 13 02:15:22.717: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 13 02:15:22.717: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jun 13 02:15:22.717: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 13 02:15:22.735: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 13 02:15:22.735: INFO: e2e test version: v1.25.9
Jun 13 02:15:22.738: INFO: kube-apiserver version: v1.25.9
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jun 13 02:15:22.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:15:22.757: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.568 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jun 13 02:15:22.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:15:22.194: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0613 02:15:22.201632      18 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Jun 13 02:15:22.240: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jun 13 02:15:22.717: INFO: 24 / 24 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jun 13 02:15:22.717: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
    Jun 13 02:15:22.717: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jun 13 02:15:22.735: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jun 13 02:15:22.735: INFO: e2e test version: v1.25.9
    Jun 13 02:15:22.738: INFO: kube-apiserver version: v1.25.9
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jun 13 02:15:22.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:15:22.757: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:15:22.78
Jun 13 02:15:22.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename containers 06/13/23 02:15:22.781
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:15:22.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:15:22.821
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 06/13/23 02:15:22.826
Jun 13 02:15:22.844: INFO: Waiting up to 5m0s for pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d" in namespace "containers-702" to be "Succeeded or Failed"
Jun 13 02:15:22.849: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.4924ms
Jun 13 02:15:24.879: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035324732s
Jun 13 02:15:26.857: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013687127s
Jun 13 02:15:28.887: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043624032s
Jun 13 02:15:30.945: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101799123s
Jun 13 02:15:32.856: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011998999s
Jun 13 02:15:34.890: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.04639892s
Jun 13 02:15:36.859: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Running", Reason="", readiness=false. Elapsed: 14.015350985s
Jun 13 02:15:38.857: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.013407182s
STEP: Saw pod success 06/13/23 02:15:38.857
Jun 13 02:15:38.857: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d" satisfied condition "Succeeded or Failed"
Jun 13 02:15:38.864: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d container agnhost-container: <nil>
STEP: delete the pod 06/13/23 02:15:38.904
Jun 13 02:15:38.965: INFO: Waiting for pod client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d to disappear
Jun 13 02:15:38.982: INFO: Pod client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jun 13 02:15:38.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-702" for this suite. 06/13/23 02:15:38.999
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":1,"skipped":3,"failed":0}
------------------------------
• [SLOW TEST] [16.244 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:15:22.78
    Jun 13 02:15:22.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename containers 06/13/23 02:15:22.781
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:15:22.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:15:22.821
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 06/13/23 02:15:22.826
    Jun 13 02:15:22.844: INFO: Waiting up to 5m0s for pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d" in namespace "containers-702" to be "Succeeded or Failed"
    Jun 13 02:15:22.849: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.4924ms
    Jun 13 02:15:24.879: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035324732s
    Jun 13 02:15:26.857: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013687127s
    Jun 13 02:15:28.887: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043624032s
    Jun 13 02:15:30.945: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101799123s
    Jun 13 02:15:32.856: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.011998999s
    Jun 13 02:15:34.890: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.04639892s
    Jun 13 02:15:36.859: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Running", Reason="", readiness=false. Elapsed: 14.015350985s
    Jun 13 02:15:38.857: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.013407182s
    STEP: Saw pod success 06/13/23 02:15:38.857
    Jun 13 02:15:38.857: INFO: Pod "client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d" satisfied condition "Succeeded or Failed"
    Jun 13 02:15:38.864: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 02:15:38.904
    Jun 13 02:15:38.965: INFO: Waiting for pod client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d to disappear
    Jun 13 02:15:38.982: INFO: Pod client-containers-f32a7b53-ac49-48aa-a77a-2b7dfe554e3d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jun 13 02:15:38.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-702" for this suite. 06/13/23 02:15:38.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:15:39.028
Jun 13 02:15:39.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename sysctl 06/13/23 02:15:39.029
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:15:39.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:15:39.084
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 06/13/23 02:15:39.099
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 13 02:15:39.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-896" for this suite. 06/13/23 02:15:39.154
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":2,"skipped":67,"failed":0}
------------------------------
• [0.157 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:15:39.028
    Jun 13 02:15:39.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename sysctl 06/13/23 02:15:39.029
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:15:39.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:15:39.084
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 06/13/23 02:15:39.099
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 13 02:15:39.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-896" for this suite. 06/13/23 02:15:39.154
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:15:39.185
Jun 13 02:15:39.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:15:39.187
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:15:39.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:15:39.293
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 06/13/23 02:15:39.305
Jun 13 02:15:39.357: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6" in namespace "projected-3688" to be "Succeeded or Failed"
Jun 13 02:15:39.391: INFO: Pod "downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6": Phase="Pending", Reason="", readiness=false. Elapsed: 34.790152ms
Jun 13 02:15:41.401: INFO: Pod "downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044378146s
Jun 13 02:15:43.600: INFO: Pod "downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.243265943s
Jun 13 02:15:45.401: INFO: Pod "downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044023453s
STEP: Saw pod success 06/13/23 02:15:45.401
Jun 13 02:15:45.401: INFO: Pod "downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6" satisfied condition "Succeeded or Failed"
Jun 13 02:15:45.410: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6 container client-container: <nil>
STEP: delete the pod 06/13/23 02:15:45.426
Jun 13 02:15:45.470: INFO: Waiting for pod downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6 to disappear
Jun 13 02:15:45.477: INFO: Pod downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 13 02:15:45.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3688" for this suite. 06/13/23 02:15:45.496
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":3,"skipped":71,"failed":0}
------------------------------
• [SLOW TEST] [6.345 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:15:39.185
    Jun 13 02:15:39.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:15:39.187
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:15:39.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:15:39.293
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 06/13/23 02:15:39.305
    Jun 13 02:15:39.357: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6" in namespace "projected-3688" to be "Succeeded or Failed"
    Jun 13 02:15:39.391: INFO: Pod "downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6": Phase="Pending", Reason="", readiness=false. Elapsed: 34.790152ms
    Jun 13 02:15:41.401: INFO: Pod "downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044378146s
    Jun 13 02:15:43.600: INFO: Pod "downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.243265943s
    Jun 13 02:15:45.401: INFO: Pod "downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044023453s
    STEP: Saw pod success 06/13/23 02:15:45.401
    Jun 13 02:15:45.401: INFO: Pod "downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6" satisfied condition "Succeeded or Failed"
    Jun 13 02:15:45.410: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6 container client-container: <nil>
    STEP: delete the pod 06/13/23 02:15:45.426
    Jun 13 02:15:45.470: INFO: Waiting for pod downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6 to disappear
    Jun 13 02:15:45.477: INFO: Pod downwardapi-volume-ab51ac05-696e-4718-a4bf-eed79e9494f6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 13 02:15:45.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3688" for this suite. 06/13/23 02:15:45.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:15:45.531
Jun 13 02:15:45.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename gc 06/13/23 02:15:45.533
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:15:45.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:15:45.591
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 06/13/23 02:15:45.629
STEP: create the rc2 06/13/23 02:15:45.642
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 06/13/23 02:15:50.792
STEP: delete the rc simpletest-rc-to-be-deleted 06/13/23 02:15:53.467
STEP: wait for the rc to be deleted 06/13/23 02:15:53.547
Jun 13 02:15:58.766: INFO: 65 pods remaining
Jun 13 02:15:58.766: INFO: 65 pods has nil DeletionTimestamp
Jun 13 02:15:58.766: INFO: 
STEP: Gathering metrics 06/13/23 02:16:03.685
Jun 13 02:16:03.917: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" in namespace "kube-system" to be "running and ready"
Jun 13 02:16:03.993: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq": Phase="Running", Reason="", readiness=true. Elapsed: 76.157508ms
Jun 13 02:16:03.993: INFO: The phase of Pod kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq is Running (Ready = true)
Jun 13 02:16:03.993: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" satisfied condition "running and ready"
Jun 13 02:16:04.186: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 13 02:16:04.186: INFO: Deleting pod "simpletest-rc-to-be-deleted-24kd4" in namespace "gc-2586"
Jun 13 02:16:04.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lcqv" in namespace "gc-2586"
Jun 13 02:16:04.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-42df8" in namespace "gc-2586"
Jun 13 02:16:04.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cxjb" in namespace "gc-2586"
Jun 13 02:16:04.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-4f8z2" in namespace "gc-2586"
Jun 13 02:16:04.674: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p45s" in namespace "gc-2586"
Jun 13 02:16:04.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v8pf" in namespace "gc-2586"
Jun 13 02:16:04.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-55zjz" in namespace "gc-2586"
Jun 13 02:16:04.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jsdx" in namespace "gc-2586"
Jun 13 02:16:04.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-6457k" in namespace "gc-2586"
Jun 13 02:16:04.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-69n2n" in namespace "gc-2586"
Jun 13 02:16:05.058: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bhzp" in namespace "gc-2586"
Jun 13 02:16:05.127: INFO: Deleting pod "simpletest-rc-to-be-deleted-6c9bb" in namespace "gc-2586"
Jun 13 02:16:05.173: INFO: Deleting pod "simpletest-rc-to-be-deleted-72wp6" in namespace "gc-2586"
Jun 13 02:16:05.205: INFO: Deleting pod "simpletest-rc-to-be-deleted-7d9zm" in namespace "gc-2586"
Jun 13 02:16:05.233: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ft9d" in namespace "gc-2586"
Jun 13 02:16:05.300: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qnp4" in namespace "gc-2586"
Jun 13 02:16:05.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sts7" in namespace "gc-2586"
Jun 13 02:16:05.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-84khl" in namespace "gc-2586"
Jun 13 02:16:05.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-86m6c" in namespace "gc-2586"
Jun 13 02:16:05.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pkts" in namespace "gc-2586"
Jun 13 02:16:05.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qksq" in namespace "gc-2586"
Jun 13 02:16:05.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-8thll" in namespace "gc-2586"
Jun 13 02:16:05.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bmcm" in namespace "gc-2586"
Jun 13 02:16:05.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tsqt" in namespace "gc-2586"
Jun 13 02:16:05.962: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vqvs" in namespace "gc-2586"
Jun 13 02:16:06.013: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkbts" in namespace "gc-2586"
Jun 13 02:16:06.117: INFO: Deleting pod "simpletest-rc-to-be-deleted-bn6d9" in namespace "gc-2586"
Jun 13 02:16:06.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq6qx" in namespace "gc-2586"
Jun 13 02:16:06.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx9z9" in namespace "gc-2586"
Jun 13 02:16:06.423: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4hn2" in namespace "gc-2586"
Jun 13 02:16:06.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqm27" in namespace "gc-2586"
Jun 13 02:16:06.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-dttvq" in namespace "gc-2586"
Jun 13 02:16:06.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6nrx" in namespace "gc-2586"
Jun 13 02:16:07.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjrn9" in namespace "gc-2586"
Jun 13 02:16:07.166: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkfqk" in namespace "gc-2586"
Jun 13 02:16:07.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-fn8qm" in namespace "gc-2586"
Jun 13 02:16:07.280: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnf4f" in namespace "gc-2586"
Jun 13 02:16:07.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnqh9" in namespace "gc-2586"
Jun 13 02:16:07.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvp6f" in namespace "gc-2586"
Jun 13 02:16:07.405: INFO: Deleting pod "simpletest-rc-to-be-deleted-g89lr" in namespace "gc-2586"
Jun 13 02:16:07.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8dbl" in namespace "gc-2586"
Jun 13 02:16:07.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-glj77" in namespace "gc-2586"
Jun 13 02:16:07.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-gll4q" in namespace "gc-2586"
Jun 13 02:16:07.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwsrb" in namespace "gc-2586"
Jun 13 02:16:07.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-gx58m" in namespace "gc-2586"
Jun 13 02:16:07.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdxsk" in namespace "gc-2586"
Jun 13 02:16:07.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-hm4mp" in namespace "gc-2586"
Jun 13 02:16:07.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwn8k" in namespace "gc-2586"
Jun 13 02:16:07.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxgsn" in namespace "gc-2586"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 13 02:16:07.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2586" for this suite. 06/13/23 02:16:08.011
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":4,"skipped":80,"failed":0}
------------------------------
• [SLOW TEST] [22.521 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:15:45.531
    Jun 13 02:15:45.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename gc 06/13/23 02:15:45.533
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:15:45.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:15:45.591
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 06/13/23 02:15:45.629
    STEP: create the rc2 06/13/23 02:15:45.642
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 06/13/23 02:15:50.792
    STEP: delete the rc simpletest-rc-to-be-deleted 06/13/23 02:15:53.467
    STEP: wait for the rc to be deleted 06/13/23 02:15:53.547
    Jun 13 02:15:58.766: INFO: 65 pods remaining
    Jun 13 02:15:58.766: INFO: 65 pods has nil DeletionTimestamp
    Jun 13 02:15:58.766: INFO: 
    STEP: Gathering metrics 06/13/23 02:16:03.685
    Jun 13 02:16:03.917: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" in namespace "kube-system" to be "running and ready"
    Jun 13 02:16:03.993: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq": Phase="Running", Reason="", readiness=true. Elapsed: 76.157508ms
    Jun 13 02:16:03.993: INFO: The phase of Pod kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq is Running (Ready = true)
    Jun 13 02:16:03.993: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" satisfied condition "running and ready"
    Jun 13 02:16:04.186: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jun 13 02:16:04.186: INFO: Deleting pod "simpletest-rc-to-be-deleted-24kd4" in namespace "gc-2586"
    Jun 13 02:16:04.446: INFO: Deleting pod "simpletest-rc-to-be-deleted-2lcqv" in namespace "gc-2586"
    Jun 13 02:16:04.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-42df8" in namespace "gc-2586"
    Jun 13 02:16:04.561: INFO: Deleting pod "simpletest-rc-to-be-deleted-4cxjb" in namespace "gc-2586"
    Jun 13 02:16:04.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-4f8z2" in namespace "gc-2586"
    Jun 13 02:16:04.674: INFO: Deleting pod "simpletest-rc-to-be-deleted-4p45s" in namespace "gc-2586"
    Jun 13 02:16:04.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-4v8pf" in namespace "gc-2586"
    Jun 13 02:16:04.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-55zjz" in namespace "gc-2586"
    Jun 13 02:16:04.851: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jsdx" in namespace "gc-2586"
    Jun 13 02:16:04.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-6457k" in namespace "gc-2586"
    Jun 13 02:16:04.985: INFO: Deleting pod "simpletest-rc-to-be-deleted-69n2n" in namespace "gc-2586"
    Jun 13 02:16:05.058: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bhzp" in namespace "gc-2586"
    Jun 13 02:16:05.127: INFO: Deleting pod "simpletest-rc-to-be-deleted-6c9bb" in namespace "gc-2586"
    Jun 13 02:16:05.173: INFO: Deleting pod "simpletest-rc-to-be-deleted-72wp6" in namespace "gc-2586"
    Jun 13 02:16:05.205: INFO: Deleting pod "simpletest-rc-to-be-deleted-7d9zm" in namespace "gc-2586"
    Jun 13 02:16:05.233: INFO: Deleting pod "simpletest-rc-to-be-deleted-7ft9d" in namespace "gc-2586"
    Jun 13 02:16:05.300: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qnp4" in namespace "gc-2586"
    Jun 13 02:16:05.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sts7" in namespace "gc-2586"
    Jun 13 02:16:05.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-84khl" in namespace "gc-2586"
    Jun 13 02:16:05.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-86m6c" in namespace "gc-2586"
    Jun 13 02:16:05.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-8pkts" in namespace "gc-2586"
    Jun 13 02:16:05.644: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qksq" in namespace "gc-2586"
    Jun 13 02:16:05.726: INFO: Deleting pod "simpletest-rc-to-be-deleted-8thll" in namespace "gc-2586"
    Jun 13 02:16:05.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bmcm" in namespace "gc-2586"
    Jun 13 02:16:05.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tsqt" in namespace "gc-2586"
    Jun 13 02:16:05.962: INFO: Deleting pod "simpletest-rc-to-be-deleted-9vqvs" in namespace "gc-2586"
    Jun 13 02:16:06.013: INFO: Deleting pod "simpletest-rc-to-be-deleted-bkbts" in namespace "gc-2586"
    Jun 13 02:16:06.117: INFO: Deleting pod "simpletest-rc-to-be-deleted-bn6d9" in namespace "gc-2586"
    Jun 13 02:16:06.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-bq6qx" in namespace "gc-2586"
    Jun 13 02:16:06.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx9z9" in namespace "gc-2586"
    Jun 13 02:16:06.423: INFO: Deleting pod "simpletest-rc-to-be-deleted-d4hn2" in namespace "gc-2586"
    Jun 13 02:16:06.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-dqm27" in namespace "gc-2586"
    Jun 13 02:16:06.656: INFO: Deleting pod "simpletest-rc-to-be-deleted-dttvq" in namespace "gc-2586"
    Jun 13 02:16:06.714: INFO: Deleting pod "simpletest-rc-to-be-deleted-f6nrx" in namespace "gc-2586"
    Jun 13 02:16:07.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-fjrn9" in namespace "gc-2586"
    Jun 13 02:16:07.166: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkfqk" in namespace "gc-2586"
    Jun 13 02:16:07.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-fn8qm" in namespace "gc-2586"
    Jun 13 02:16:07.280: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnf4f" in namespace "gc-2586"
    Jun 13 02:16:07.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnqh9" in namespace "gc-2586"
    Jun 13 02:16:07.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvp6f" in namespace "gc-2586"
    Jun 13 02:16:07.405: INFO: Deleting pod "simpletest-rc-to-be-deleted-g89lr" in namespace "gc-2586"
    Jun 13 02:16:07.472: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8dbl" in namespace "gc-2586"
    Jun 13 02:16:07.544: INFO: Deleting pod "simpletest-rc-to-be-deleted-glj77" in namespace "gc-2586"
    Jun 13 02:16:07.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-gll4q" in namespace "gc-2586"
    Jun 13 02:16:07.665: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwsrb" in namespace "gc-2586"
    Jun 13 02:16:07.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-gx58m" in namespace "gc-2586"
    Jun 13 02:16:07.777: INFO: Deleting pod "simpletest-rc-to-be-deleted-hdxsk" in namespace "gc-2586"
    Jun 13 02:16:07.815: INFO: Deleting pod "simpletest-rc-to-be-deleted-hm4mp" in namespace "gc-2586"
    Jun 13 02:16:07.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-hwn8k" in namespace "gc-2586"
    Jun 13 02:16:07.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxgsn" in namespace "gc-2586"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 13 02:16:07.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2586" for this suite. 06/13/23 02:16:08.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:16:08.055
Jun 13 02:16:08.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:16:08.066
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:16:08.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:16:08.112
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-8b0848d0-b325-4ec8-b018-c4801e85659c 06/13/23 02:16:08.121
STEP: Creating a pod to test consume secrets 06/13/23 02:16:08.138
Jun 13 02:16:08.166: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab" in namespace "projected-6603" to be "Succeeded or Failed"
Jun 13 02:16:08.176: INFO: Pod "pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.300117ms
Jun 13 02:16:10.187: INFO: Pod "pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021476887s
Jun 13 02:16:12.195: INFO: Pod "pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029589909s
Jun 13 02:16:14.239: INFO: Pod "pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073768789s
STEP: Saw pod success 06/13/23 02:16:14.24
Jun 13 02:16:14.240: INFO: Pod "pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab" satisfied condition "Succeeded or Failed"
Jun 13 02:16:14.250: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab container projected-secret-volume-test: <nil>
STEP: delete the pod 06/13/23 02:16:14.284
Jun 13 02:16:14.332: INFO: Waiting for pod pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab to disappear
Jun 13 02:16:14.372: INFO: Pod pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 13 02:16:14.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6603" for this suite. 06/13/23 02:16:14.382
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":5,"skipped":117,"failed":0}
------------------------------
• [SLOW TEST] [6.358 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:16:08.055
    Jun 13 02:16:08.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:16:08.066
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:16:08.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:16:08.112
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-8b0848d0-b325-4ec8-b018-c4801e85659c 06/13/23 02:16:08.121
    STEP: Creating a pod to test consume secrets 06/13/23 02:16:08.138
    Jun 13 02:16:08.166: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab" in namespace "projected-6603" to be "Succeeded or Failed"
    Jun 13 02:16:08.176: INFO: Pod "pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab": Phase="Pending", Reason="", readiness=false. Elapsed: 10.300117ms
    Jun 13 02:16:10.187: INFO: Pod "pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021476887s
    Jun 13 02:16:12.195: INFO: Pod "pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029589909s
    Jun 13 02:16:14.239: INFO: Pod "pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073768789s
    STEP: Saw pod success 06/13/23 02:16:14.24
    Jun 13 02:16:14.240: INFO: Pod "pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab" satisfied condition "Succeeded or Failed"
    Jun 13 02:16:14.250: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 02:16:14.284
    Jun 13 02:16:14.332: INFO: Waiting for pod pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab to disappear
    Jun 13 02:16:14.372: INFO: Pod pod-projected-secrets-d5c6851a-eeac-495f-bd08-80e3ed78abab no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 13 02:16:14.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6603" for this suite. 06/13/23 02:16:14.382
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:16:14.415
Jun 13 02:16:14.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename security-context-test 06/13/23 02:16:14.417
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:16:14.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:16:14.495
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jun 13 02:16:14.541: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6" in namespace "security-context-test-4971" to be "Succeeded or Failed"
Jun 13 02:16:14.591: INFO: Pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6": Phase="Pending", Reason="", readiness=false. Elapsed: 49.523059ms
Jun 13 02:16:16.614: INFO: Pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072961679s
Jun 13 02:16:18.630: INFO: Pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088647011s
Jun 13 02:16:20.604: INFO: Pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063203164s
Jun 13 02:16:22.605: INFO: Pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.06360191s
Jun 13 02:16:22.605: INFO: Pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 13 02:16:22.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4971" for this suite. 06/13/23 02:16:22.738
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":6,"skipped":120,"failed":0}
------------------------------
• [SLOW TEST] [8.377 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:16:14.415
    Jun 13 02:16:14.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename security-context-test 06/13/23 02:16:14.417
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:16:14.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:16:14.495
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jun 13 02:16:14.541: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6" in namespace "security-context-test-4971" to be "Succeeded or Failed"
    Jun 13 02:16:14.591: INFO: Pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6": Phase="Pending", Reason="", readiness=false. Elapsed: 49.523059ms
    Jun 13 02:16:16.614: INFO: Pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072961679s
    Jun 13 02:16:18.630: INFO: Pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.088647011s
    Jun 13 02:16:20.604: INFO: Pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.063203164s
    Jun 13 02:16:22.605: INFO: Pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.06360191s
    Jun 13 02:16:22.605: INFO: Pod "alpine-nnp-false-29942cf1-4b0b-429f-9035-c49ec2be89f6" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 13 02:16:22.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4971" for this suite. 06/13/23 02:16:22.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:16:22.794
Jun 13 02:16:22.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 02:16:22.795
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:16:22.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:16:22.874
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 02:16:22.936
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 02:16:23.436
STEP: Deploying the webhook pod 06/13/23 02:16:23.493
STEP: Wait for the deployment to be ready 06/13/23 02:16:23.526
Jun 13 02:16:23.548: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/13/23 02:16:25.571
STEP: Verifying the service has paired with the endpoint 06/13/23 02:16:25.622
Jun 13 02:16:26.623: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 06/13/23 02:16:26.817
STEP: Creating a configMap that does not comply to the validation webhook rules 06/13/23 02:16:26.867
STEP: Deleting the collection of validation webhooks 06/13/23 02:16:26.926
STEP: Creating a configMap that does not comply to the validation webhook rules 06/13/23 02:16:27.074
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 02:16:27.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1098" for this suite. 06/13/23 02:16:27.111
STEP: Destroying namespace "webhook-1098-markers" for this suite. 06/13/23 02:16:27.135
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":7,"skipped":133,"failed":0}
------------------------------
• [4.599 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:16:22.794
    Jun 13 02:16:22.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 02:16:22.795
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:16:22.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:16:22.874
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 02:16:22.936
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 02:16:23.436
    STEP: Deploying the webhook pod 06/13/23 02:16:23.493
    STEP: Wait for the deployment to be ready 06/13/23 02:16:23.526
    Jun 13 02:16:23.548: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/13/23 02:16:25.571
    STEP: Verifying the service has paired with the endpoint 06/13/23 02:16:25.622
    Jun 13 02:16:26.623: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 06/13/23 02:16:26.817
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/13/23 02:16:26.867
    STEP: Deleting the collection of validation webhooks 06/13/23 02:16:26.926
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/13/23 02:16:27.074
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 02:16:27.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1098" for this suite. 06/13/23 02:16:27.111
    STEP: Destroying namespace "webhook-1098-markers" for this suite. 06/13/23 02:16:27.135
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:16:27.394
Jun 13 02:16:27.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename podtemplate 06/13/23 02:16:27.395
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:16:27.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:16:27.443
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jun 13 02:16:27.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9785" for this suite. 06/13/23 02:16:27.515
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":8,"skipped":141,"failed":0}
------------------------------
• [0.142 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:16:27.394
    Jun 13 02:16:27.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename podtemplate 06/13/23 02:16:27.395
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:16:27.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:16:27.443
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jun 13 02:16:27.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9785" for this suite. 06/13/23 02:16:27.515
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:16:27.537
Jun 13 02:16:27.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 02:16:27.538
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:16:27.583
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:16:27.588
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 06/13/23 02:16:27.593
Jun 13 02:16:27.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: mark a version not serverd 06/13/23 02:16:38.729
STEP: check the unserved version gets removed 06/13/23 02:16:38.761
STEP: check the other version is not changed 06/13/23 02:16:44.574
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 02:16:56.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6161" for this suite. 06/13/23 02:16:56.038
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":9,"skipped":150,"failed":0}
------------------------------
• [SLOW TEST] [28.517 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:16:27.537
    Jun 13 02:16:27.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 02:16:27.538
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:16:27.583
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:16:27.588
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 06/13/23 02:16:27.593
    Jun 13 02:16:27.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: mark a version not serverd 06/13/23 02:16:38.729
    STEP: check the unserved version gets removed 06/13/23 02:16:38.761
    STEP: check the other version is not changed 06/13/23 02:16:44.574
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 02:16:56.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6161" for this suite. 06/13/23 02:16:56.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:16:56.055
Jun 13 02:16:56.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 02:16:56.056
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:16:56.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:16:56.109
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 06/13/23 02:16:56.116
Jun 13 02:16:56.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: rename a version 06/13/23 02:17:10.646
STEP: check the new version name is served 06/13/23 02:17:10.685
STEP: check the old version name is removed 06/13/23 02:17:16.902
STEP: check the other version is not changed 06/13/23 02:17:19.199
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 02:17:30.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9636" for this suite. 06/13/23 02:17:30.385
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":10,"skipped":159,"failed":0}
------------------------------
• [SLOW TEST] [34.346 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:16:56.055
    Jun 13 02:16:56.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 02:16:56.056
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:16:56.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:16:56.109
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 06/13/23 02:16:56.116
    Jun 13 02:16:56.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: rename a version 06/13/23 02:17:10.646
    STEP: check the new version name is served 06/13/23 02:17:10.685
    STEP: check the old version name is removed 06/13/23 02:17:16.902
    STEP: check the other version is not changed 06/13/23 02:17:19.199
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 02:17:30.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9636" for this suite. 06/13/23 02:17:30.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:17:30.405
Jun 13 02:17:30.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename resourcequota 06/13/23 02:17:30.407
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:17:30.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:17:30.448
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 06/13/23 02:17:30.456
STEP: Creating a ResourceQuota 06/13/23 02:17:35.469
STEP: Ensuring resource quota status is calculated 06/13/23 02:17:35.534
STEP: Creating a Service 06/13/23 02:17:37.563
STEP: Creating a NodePort Service 06/13/23 02:17:37.791
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 06/13/23 02:17:38.05
STEP: Ensuring resource quota status captures service creation 06/13/23 02:17:38.438
STEP: Deleting Services 06/13/23 02:17:40.45
STEP: Ensuring resource quota status released usage 06/13/23 02:17:40.634
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 13 02:17:42.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-274" for this suite. 06/13/23 02:17:42.669
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":11,"skipped":206,"failed":0}
------------------------------
• [SLOW TEST] [12.329 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:17:30.405
    Jun 13 02:17:30.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename resourcequota 06/13/23 02:17:30.407
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:17:30.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:17:30.448
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 06/13/23 02:17:30.456
    STEP: Creating a ResourceQuota 06/13/23 02:17:35.469
    STEP: Ensuring resource quota status is calculated 06/13/23 02:17:35.534
    STEP: Creating a Service 06/13/23 02:17:37.563
    STEP: Creating a NodePort Service 06/13/23 02:17:37.791
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 06/13/23 02:17:38.05
    STEP: Ensuring resource quota status captures service creation 06/13/23 02:17:38.438
    STEP: Deleting Services 06/13/23 02:17:40.45
    STEP: Ensuring resource quota status released usage 06/13/23 02:17:40.634
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 13 02:17:42.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-274" for this suite. 06/13/23 02:17:42.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:17:42.735
Jun 13 02:17:42.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:17:42.737
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:17:43.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:17:43.045
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-1f9a89a4-b0e0-454c-a8d5-517833e48703 06/13/23 02:17:43.063
STEP: Creating a pod to test consume configMaps 06/13/23 02:17:43.085
Jun 13 02:17:43.122: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080" in namespace "projected-1152" to be "Succeeded or Failed"
Jun 13 02:17:43.142: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020067ms
Jun 13 02:17:45.164: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042108103s
Jun 13 02:17:47.173: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051318311s
Jun 13 02:17:49.154: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031985732s
Jun 13 02:17:51.155: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033423161s
Jun 13 02:17:53.155: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Running", Reason="", readiness=false. Elapsed: 10.033501834s
Jun 13 02:17:55.152: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.030349357s
STEP: Saw pod success 06/13/23 02:17:55.152
Jun 13 02:17:55.153: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080" satisfied condition "Succeeded or Failed"
Jun 13 02:17:55.167: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080 container agnhost-container: <nil>
STEP: delete the pod 06/13/23 02:17:55.2
Jun 13 02:17:55.239: INFO: Waiting for pod pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080 to disappear
Jun 13 02:17:55.247: INFO: Pod pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 13 02:17:55.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1152" for this suite. 06/13/23 02:17:55.259
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":12,"skipped":237,"failed":0}
------------------------------
• [SLOW TEST] [12.541 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:17:42.735
    Jun 13 02:17:42.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:17:42.737
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:17:43.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:17:43.045
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-1f9a89a4-b0e0-454c-a8d5-517833e48703 06/13/23 02:17:43.063
    STEP: Creating a pod to test consume configMaps 06/13/23 02:17:43.085
    Jun 13 02:17:43.122: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080" in namespace "projected-1152" to be "Succeeded or Failed"
    Jun 13 02:17:43.142: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020067ms
    Jun 13 02:17:45.164: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042108103s
    Jun 13 02:17:47.173: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051318311s
    Jun 13 02:17:49.154: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Pending", Reason="", readiness=false. Elapsed: 6.031985732s
    Jun 13 02:17:51.155: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Pending", Reason="", readiness=false. Elapsed: 8.033423161s
    Jun 13 02:17:53.155: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Running", Reason="", readiness=false. Elapsed: 10.033501834s
    Jun 13 02:17:55.152: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.030349357s
    STEP: Saw pod success 06/13/23 02:17:55.152
    Jun 13 02:17:55.153: INFO: Pod "pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080" satisfied condition "Succeeded or Failed"
    Jun 13 02:17:55.167: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080 container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 02:17:55.2
    Jun 13 02:17:55.239: INFO: Waiting for pod pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080 to disappear
    Jun 13 02:17:55.247: INFO: Pod pod-projected-configmaps-73a8e89f-d0fa-4512-b115-79b352879080 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 13 02:17:55.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1152" for this suite. 06/13/23 02:17:55.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:17:55.279
Jun 13 02:17:55.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename resourcequota 06/13/23 02:17:55.281
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:17:55.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:17:55.344
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 06/13/23 02:17:55.358
STEP: Creating a ResourceQuota 06/13/23 02:18:00.381
STEP: Ensuring resource quota status is calculated 06/13/23 02:18:00.405
STEP: Creating a ReplicaSet 06/13/23 02:18:02.415
STEP: Ensuring resource quota status captures replicaset creation 06/13/23 02:18:02.48
STEP: Deleting a ReplicaSet 06/13/23 02:18:04.489
STEP: Ensuring resource quota status released usage 06/13/23 02:18:04.507
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 13 02:18:06.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8210" for this suite. 06/13/23 02:18:06.536
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":13,"skipped":252,"failed":0}
------------------------------
• [SLOW TEST] [11.273 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:17:55.279
    Jun 13 02:17:55.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename resourcequota 06/13/23 02:17:55.281
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:17:55.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:17:55.344
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 06/13/23 02:17:55.358
    STEP: Creating a ResourceQuota 06/13/23 02:18:00.381
    STEP: Ensuring resource quota status is calculated 06/13/23 02:18:00.405
    STEP: Creating a ReplicaSet 06/13/23 02:18:02.415
    STEP: Ensuring resource quota status captures replicaset creation 06/13/23 02:18:02.48
    STEP: Deleting a ReplicaSet 06/13/23 02:18:04.489
    STEP: Ensuring resource quota status released usage 06/13/23 02:18:04.507
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 13 02:18:06.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8210" for this suite. 06/13/23 02:18:06.536
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:18:06.553
Jun 13 02:18:06.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:18:06.554
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:18:06.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:18:06.621
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-e7b27e45-9b49-40de-9b5c-f28a494f1731 06/13/23 02:18:06.64
STEP: Creating the pod 06/13/23 02:18:06.678
Jun 13 02:18:06.714: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-727532e7-8463-47cf-b009-da047cb93174" in namespace "projected-6782" to be "running and ready"
Jun 13 02:18:06.724: INFO: Pod "pod-projected-configmaps-727532e7-8463-47cf-b009-da047cb93174": Phase="Pending", Reason="", readiness=false. Elapsed: 9.739287ms
Jun 13 02:18:06.724: INFO: The phase of Pod pod-projected-configmaps-727532e7-8463-47cf-b009-da047cb93174 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:18:08.732: INFO: Pod "pod-projected-configmaps-727532e7-8463-47cf-b009-da047cb93174": Phase="Running", Reason="", readiness=true. Elapsed: 2.018194772s
Jun 13 02:18:08.733: INFO: The phase of Pod pod-projected-configmaps-727532e7-8463-47cf-b009-da047cb93174 is Running (Ready = true)
Jun 13 02:18:08.733: INFO: Pod "pod-projected-configmaps-727532e7-8463-47cf-b009-da047cb93174" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-e7b27e45-9b49-40de-9b5c-f28a494f1731 06/13/23 02:18:08.765
STEP: waiting to observe update in volume 06/13/23 02:18:08.778
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 13 02:18:10.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6782" for this suite. 06/13/23 02:18:10.818
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":14,"skipped":253,"failed":0}
------------------------------
• [4.281 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:18:06.553
    Jun 13 02:18:06.553: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:18:06.554
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:18:06.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:18:06.621
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-e7b27e45-9b49-40de-9b5c-f28a494f1731 06/13/23 02:18:06.64
    STEP: Creating the pod 06/13/23 02:18:06.678
    Jun 13 02:18:06.714: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-727532e7-8463-47cf-b009-da047cb93174" in namespace "projected-6782" to be "running and ready"
    Jun 13 02:18:06.724: INFO: Pod "pod-projected-configmaps-727532e7-8463-47cf-b009-da047cb93174": Phase="Pending", Reason="", readiness=false. Elapsed: 9.739287ms
    Jun 13 02:18:06.724: INFO: The phase of Pod pod-projected-configmaps-727532e7-8463-47cf-b009-da047cb93174 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:18:08.732: INFO: Pod "pod-projected-configmaps-727532e7-8463-47cf-b009-da047cb93174": Phase="Running", Reason="", readiness=true. Elapsed: 2.018194772s
    Jun 13 02:18:08.733: INFO: The phase of Pod pod-projected-configmaps-727532e7-8463-47cf-b009-da047cb93174 is Running (Ready = true)
    Jun 13 02:18:08.733: INFO: Pod "pod-projected-configmaps-727532e7-8463-47cf-b009-da047cb93174" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-e7b27e45-9b49-40de-9b5c-f28a494f1731 06/13/23 02:18:08.765
    STEP: waiting to observe update in volume 06/13/23 02:18:08.778
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 13 02:18:10.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6782" for this suite. 06/13/23 02:18:10.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:18:10.835
Jun 13 02:18:10.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubelet-test 06/13/23 02:18:10.837
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:18:10.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:18:10.868
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jun 13 02:18:10.893: INFO: Waiting up to 5m0s for pod "busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb" in namespace "kubelet-test-71" to be "running and ready"
Jun 13 02:18:10.906: INFO: Pod "busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.818736ms
Jun 13 02:18:10.906: INFO: The phase of Pod busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:18:12.914: INFO: Pod "busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020933432s
Jun 13 02:18:12.914: INFO: The phase of Pod busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:18:14.915: INFO: Pod "busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb": Phase="Running", Reason="", readiness=true. Elapsed: 4.021457171s
Jun 13 02:18:14.915: INFO: The phase of Pod busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb is Running (Ready = true)
Jun 13 02:18:14.915: INFO: Pod "busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 13 02:18:14.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-71" for this suite. 06/13/23 02:18:14.963
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":15,"skipped":259,"failed":0}
------------------------------
• [4.148 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:18:10.835
    Jun 13 02:18:10.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubelet-test 06/13/23 02:18:10.837
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:18:10.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:18:10.868
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jun 13 02:18:10.893: INFO: Waiting up to 5m0s for pod "busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb" in namespace "kubelet-test-71" to be "running and ready"
    Jun 13 02:18:10.906: INFO: Pod "busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.818736ms
    Jun 13 02:18:10.906: INFO: The phase of Pod busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:18:12.914: INFO: Pod "busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020933432s
    Jun 13 02:18:12.914: INFO: The phase of Pod busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:18:14.915: INFO: Pod "busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb": Phase="Running", Reason="", readiness=true. Elapsed: 4.021457171s
    Jun 13 02:18:14.915: INFO: The phase of Pod busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb is Running (Ready = true)
    Jun 13 02:18:14.915: INFO: Pod "busybox-scheduling-93a5922f-9f73-4f6c-9f4e-f38d118283eb" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 13 02:18:14.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-71" for this suite. 06/13/23 02:18:14.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:18:14.988
Jun 13 02:18:14.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 02:18:14.989
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:18:15.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:18:15.034
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-753/configmap-test-745170b0-4c38-4774-ac09-91e3ff68c7be 06/13/23 02:18:15.041
STEP: Creating a pod to test consume configMaps 06/13/23 02:18:15.052
Jun 13 02:18:15.078: INFO: Waiting up to 5m0s for pod "pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76" in namespace "configmap-753" to be "Succeeded or Failed"
Jun 13 02:18:15.091: INFO: Pod "pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76": Phase="Pending", Reason="", readiness=false. Elapsed: 12.476535ms
Jun 13 02:18:17.102: INFO: Pod "pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023512404s
Jun 13 02:18:19.104: INFO: Pod "pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025948592s
STEP: Saw pod success 06/13/23 02:18:19.104
Jun 13 02:18:19.104: INFO: Pod "pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76" satisfied condition "Succeeded or Failed"
Jun 13 02:18:19.117: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76 container env-test: <nil>
STEP: delete the pod 06/13/23 02:18:19.135
Jun 13 02:18:19.157: INFO: Waiting for pod pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76 to disappear
Jun 13 02:18:19.164: INFO: Pod pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 02:18:19.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-753" for this suite. 06/13/23 02:18:19.184
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":16,"skipped":333,"failed":0}
------------------------------
• [4.212 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:18:14.988
    Jun 13 02:18:14.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 02:18:14.989
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:18:15.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:18:15.034
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-753/configmap-test-745170b0-4c38-4774-ac09-91e3ff68c7be 06/13/23 02:18:15.041
    STEP: Creating a pod to test consume configMaps 06/13/23 02:18:15.052
    Jun 13 02:18:15.078: INFO: Waiting up to 5m0s for pod "pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76" in namespace "configmap-753" to be "Succeeded or Failed"
    Jun 13 02:18:15.091: INFO: Pod "pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76": Phase="Pending", Reason="", readiness=false. Elapsed: 12.476535ms
    Jun 13 02:18:17.102: INFO: Pod "pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023512404s
    Jun 13 02:18:19.104: INFO: Pod "pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025948592s
    STEP: Saw pod success 06/13/23 02:18:19.104
    Jun 13 02:18:19.104: INFO: Pod "pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76" satisfied condition "Succeeded or Failed"
    Jun 13 02:18:19.117: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76 container env-test: <nil>
    STEP: delete the pod 06/13/23 02:18:19.135
    Jun 13 02:18:19.157: INFO: Waiting for pod pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76 to disappear
    Jun 13 02:18:19.164: INFO: Pod pod-configmaps-c9eb732e-3aaa-4b24-ada1-7fdd5c2cef76 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 02:18:19.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-753" for this suite. 06/13/23 02:18:19.184
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:18:19.2
Jun 13 02:18:19.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename statefulset 06/13/23 02:18:19.202
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:18:19.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:18:19.243
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1673 06/13/23 02:18:19.251
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jun 13 02:18:19.288: INFO: Found 0 stateful pods, waiting for 1
Jun 13 02:18:29.298: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
Jun 13 02:18:39.300: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 06/13/23 02:18:39.32
W0613 02:18:39.339674      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun 13 02:18:39.356: INFO: Found 1 stateful pods, waiting for 2
Jun 13 02:18:49.370: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 02:18:49.370: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Jun 13 02:18:59.373: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 02:18:59.374: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 06/13/23 02:18:59.392
STEP: Delete all of the StatefulSets 06/13/23 02:18:59.412
STEP: Verify that StatefulSets have been deleted 06/13/23 02:18:59.43
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 13 02:18:59.443: INFO: Deleting all statefulset in ns statefulset-1673
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 13 02:18:59.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1673" for this suite. 06/13/23 02:18:59.48
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":17,"skipped":333,"failed":0}
------------------------------
• [SLOW TEST] [40.299 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:18:19.2
    Jun 13 02:18:19.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename statefulset 06/13/23 02:18:19.202
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:18:19.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:18:19.243
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1673 06/13/23 02:18:19.251
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jun 13 02:18:19.288: INFO: Found 0 stateful pods, waiting for 1
    Jun 13 02:18:29.298: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Pending - Ready=false
    Jun 13 02:18:39.300: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 06/13/23 02:18:39.32
    W0613 02:18:39.339674      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun 13 02:18:39.356: INFO: Found 1 stateful pods, waiting for 2
    Jun 13 02:18:49.370: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 02:18:49.370: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Jun 13 02:18:59.373: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 02:18:59.374: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 06/13/23 02:18:59.392
    STEP: Delete all of the StatefulSets 06/13/23 02:18:59.412
    STEP: Verify that StatefulSets have been deleted 06/13/23 02:18:59.43
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 13 02:18:59.443: INFO: Deleting all statefulset in ns statefulset-1673
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 13 02:18:59.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1673" for this suite. 06/13/23 02:18:59.48
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:18:59.502
Jun 13 02:18:59.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-probe 06/13/23 02:18:59.504
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:18:59.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:18:59.547
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c in namespace container-probe-7453 06/13/23 02:18:59.555
Jun 13 02:18:59.579: INFO: Waiting up to 5m0s for pod "busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c" in namespace "container-probe-7453" to be "not pending"
Jun 13 02:18:59.589: INFO: Pod "busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.960803ms
Jun 13 02:19:01.664: INFO: Pod "busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c": Phase="Running", Reason="", readiness=true. Elapsed: 2.084880576s
Jun 13 02:19:01.664: INFO: Pod "busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c" satisfied condition "not pending"
Jun 13 02:19:01.664: INFO: Started pod busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c in namespace container-probe-7453
STEP: checking the pod's current state and verifying that restartCount is present 06/13/23 02:19:01.664
Jun 13 02:19:01.680: INFO: Initial restart count of pod busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c is 0
Jun 13 02:19:51.969: INFO: Restart count of pod container-probe-7453/busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c is now 1 (50.289001287s elapsed)
STEP: deleting the pod 06/13/23 02:19:51.969
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 13 02:19:51.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7453" for this suite. 06/13/23 02:19:52.004
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":18,"skipped":333,"failed":0}
------------------------------
• [SLOW TEST] [52.519 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:18:59.502
    Jun 13 02:18:59.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-probe 06/13/23 02:18:59.504
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:18:59.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:18:59.547
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c in namespace container-probe-7453 06/13/23 02:18:59.555
    Jun 13 02:18:59.579: INFO: Waiting up to 5m0s for pod "busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c" in namespace "container-probe-7453" to be "not pending"
    Jun 13 02:18:59.589: INFO: Pod "busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.960803ms
    Jun 13 02:19:01.664: INFO: Pod "busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c": Phase="Running", Reason="", readiness=true. Elapsed: 2.084880576s
    Jun 13 02:19:01.664: INFO: Pod "busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c" satisfied condition "not pending"
    Jun 13 02:19:01.664: INFO: Started pod busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c in namespace container-probe-7453
    STEP: checking the pod's current state and verifying that restartCount is present 06/13/23 02:19:01.664
    Jun 13 02:19:01.680: INFO: Initial restart count of pod busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c is 0
    Jun 13 02:19:51.969: INFO: Restart count of pod container-probe-7453/busybox-2be6c58e-bd0c-4a13-96cb-d6abf79e433c is now 1 (50.289001287s elapsed)
    STEP: deleting the pod 06/13/23 02:19:51.969
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 13 02:19:51.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7453" for this suite. 06/13/23 02:19:52.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:19:52.023
Jun 13 02:19:52.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 02:19:52.025
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:19:52.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:19:52.212
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 06/13/23 02:19:52.221
Jun 13 02:19:52.240: INFO: Waiting up to 5m0s for pod "pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27" in namespace "emptydir-1237" to be "Succeeded or Failed"
Jun 13 02:19:52.253: INFO: Pod "pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27": Phase="Pending", Reason="", readiness=false. Elapsed: 12.847317ms
Jun 13 02:19:54.264: INFO: Pod "pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023315809s
Jun 13 02:19:56.264: INFO: Pod "pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023361503s
STEP: Saw pod success 06/13/23 02:19:56.264
Jun 13 02:19:56.264: INFO: Pod "pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27" satisfied condition "Succeeded or Failed"
Jun 13 02:19:56.273: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27 container test-container: <nil>
STEP: delete the pod 06/13/23 02:19:56.298
Jun 13 02:19:56.328: INFO: Waiting for pod pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27 to disappear
Jun 13 02:19:56.335: INFO: Pod pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 02:19:56.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1237" for this suite. 06/13/23 02:19:56.344
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":19,"skipped":360,"failed":0}
------------------------------
• [4.335 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:19:52.023
    Jun 13 02:19:52.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 02:19:52.025
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:19:52.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:19:52.212
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 06/13/23 02:19:52.221
    Jun 13 02:19:52.240: INFO: Waiting up to 5m0s for pod "pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27" in namespace "emptydir-1237" to be "Succeeded or Failed"
    Jun 13 02:19:52.253: INFO: Pod "pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27": Phase="Pending", Reason="", readiness=false. Elapsed: 12.847317ms
    Jun 13 02:19:54.264: INFO: Pod "pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023315809s
    Jun 13 02:19:56.264: INFO: Pod "pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023361503s
    STEP: Saw pod success 06/13/23 02:19:56.264
    Jun 13 02:19:56.264: INFO: Pod "pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27" satisfied condition "Succeeded or Failed"
    Jun 13 02:19:56.273: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27 container test-container: <nil>
    STEP: delete the pod 06/13/23 02:19:56.298
    Jun 13 02:19:56.328: INFO: Waiting for pod pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27 to disappear
    Jun 13 02:19:56.335: INFO: Pod pod-6ff761bb-e5cf-4aff-82b0-13843a5c3c27 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 02:19:56.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1237" for this suite. 06/13/23 02:19:56.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:19:56.359
Jun 13 02:19:56.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:19:56.36
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:19:56.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:19:56.392
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-157f127a-5acc-4deb-893b-eb1391751606 06/13/23 02:19:56.407
STEP: Creating configMap with name cm-test-opt-upd-356698b1-56da-4f12-9274-a977189c8ece 06/13/23 02:19:56.42
STEP: Creating the pod 06/13/23 02:19:56.433
Jun 13 02:19:56.453: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38" in namespace "projected-2427" to be "running and ready"
Jun 13 02:19:56.468: INFO: Pod "pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38": Phase="Pending", Reason="", readiness=false. Elapsed: 15.082158ms
Jun 13 02:19:56.468: INFO: The phase of Pod pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:19:58.480: INFO: Pod "pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027361961s
Jun 13 02:19:58.480: INFO: The phase of Pod pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:20:00.485: INFO: Pod "pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38": Phase="Running", Reason="", readiness=true. Elapsed: 4.031991312s
Jun 13 02:20:00.485: INFO: The phase of Pod pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38 is Running (Ready = true)
Jun 13 02:20:00.485: INFO: Pod "pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-157f127a-5acc-4deb-893b-eb1391751606 06/13/23 02:20:00.545
STEP: Updating configmap cm-test-opt-upd-356698b1-56da-4f12-9274-a977189c8ece 06/13/23 02:20:00.616
STEP: Creating configMap with name cm-test-opt-create-3f11eb60-d0a4-4092-b2bd-a60c6b8ee4a7 06/13/23 02:20:00.7
STEP: waiting to observe update in volume 06/13/23 02:20:00.74
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 13 02:21:33.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2427" for this suite. 06/13/23 02:21:33.673
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":20,"skipped":384,"failed":0}
------------------------------
• [SLOW TEST] [97.352 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:19:56.359
    Jun 13 02:19:56.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:19:56.36
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:19:56.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:19:56.392
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-157f127a-5acc-4deb-893b-eb1391751606 06/13/23 02:19:56.407
    STEP: Creating configMap with name cm-test-opt-upd-356698b1-56da-4f12-9274-a977189c8ece 06/13/23 02:19:56.42
    STEP: Creating the pod 06/13/23 02:19:56.433
    Jun 13 02:19:56.453: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38" in namespace "projected-2427" to be "running and ready"
    Jun 13 02:19:56.468: INFO: Pod "pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38": Phase="Pending", Reason="", readiness=false. Elapsed: 15.082158ms
    Jun 13 02:19:56.468: INFO: The phase of Pod pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:19:58.480: INFO: Pod "pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027361961s
    Jun 13 02:19:58.480: INFO: The phase of Pod pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:20:00.485: INFO: Pod "pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38": Phase="Running", Reason="", readiness=true. Elapsed: 4.031991312s
    Jun 13 02:20:00.485: INFO: The phase of Pod pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38 is Running (Ready = true)
    Jun 13 02:20:00.485: INFO: Pod "pod-projected-configmaps-d775405d-1ba3-4e19-b266-dff20ba6cc38" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-157f127a-5acc-4deb-893b-eb1391751606 06/13/23 02:20:00.545
    STEP: Updating configmap cm-test-opt-upd-356698b1-56da-4f12-9274-a977189c8ece 06/13/23 02:20:00.616
    STEP: Creating configMap with name cm-test-opt-create-3f11eb60-d0a4-4092-b2bd-a60c6b8ee4a7 06/13/23 02:20:00.7
    STEP: waiting to observe update in volume 06/13/23 02:20:00.74
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 13 02:21:33.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2427" for this suite. 06/13/23 02:21:33.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:21:33.715
Jun 13 02:21:33.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 02:21:33.717
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:21:33.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:21:33.767
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 06/13/23 02:21:33.779
Jun 13 02:21:33.813: INFO: Waiting up to 5m0s for pod "pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1" in namespace "emptydir-4125" to be "Succeeded or Failed"
Jun 13 02:21:33.829: INFO: Pod "pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.308187ms
Jun 13 02:21:35.837: INFO: Pod "pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023796828s
Jun 13 02:21:37.841: INFO: Pod "pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027635745s
STEP: Saw pod success 06/13/23 02:21:37.841
Jun 13 02:21:37.841: INFO: Pod "pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1" satisfied condition "Succeeded or Failed"
Jun 13 02:21:37.866: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1 container test-container: <nil>
STEP: delete the pod 06/13/23 02:21:37.895
Jun 13 02:21:37.929: INFO: Waiting for pod pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1 to disappear
Jun 13 02:21:37.935: INFO: Pod pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 02:21:37.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4125" for this suite. 06/13/23 02:21:37.948
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":21,"skipped":418,"failed":0}
------------------------------
• [4.251 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:21:33.715
    Jun 13 02:21:33.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 02:21:33.717
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:21:33.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:21:33.767
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 06/13/23 02:21:33.779
    Jun 13 02:21:33.813: INFO: Waiting up to 5m0s for pod "pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1" in namespace "emptydir-4125" to be "Succeeded or Failed"
    Jun 13 02:21:33.829: INFO: Pod "pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.308187ms
    Jun 13 02:21:35.837: INFO: Pod "pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023796828s
    Jun 13 02:21:37.841: INFO: Pod "pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027635745s
    STEP: Saw pod success 06/13/23 02:21:37.841
    Jun 13 02:21:37.841: INFO: Pod "pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1" satisfied condition "Succeeded or Failed"
    Jun 13 02:21:37.866: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1 container test-container: <nil>
    STEP: delete the pod 06/13/23 02:21:37.895
    Jun 13 02:21:37.929: INFO: Waiting for pod pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1 to disappear
    Jun 13 02:21:37.935: INFO: Pod pod-dc5447ac-9622-4c87-a803-c0eeb62fa7e1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 02:21:37.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4125" for this suite. 06/13/23 02:21:37.948
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:21:37.968
Jun 13 02:21:37.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 02:21:37.969
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:21:38.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:21:38.016
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 06/13/23 02:21:38.022
Jun 13 02:21:38.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034" in namespace "downward-api-6528" to be "Succeeded or Failed"
Jun 13 02:21:38.049: INFO: Pod "downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034": Phase="Pending", Reason="", readiness=false. Elapsed: 9.31188ms
Jun 13 02:21:40.060: INFO: Pod "downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034": Phase="Running", Reason="", readiness=true. Elapsed: 2.020780388s
Jun 13 02:21:42.057: INFO: Pod "downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034": Phase="Running", Reason="", readiness=false. Elapsed: 4.017609997s
Jun 13 02:21:44.062: INFO: Pod "downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022789207s
STEP: Saw pod success 06/13/23 02:21:44.062
Jun 13 02:21:44.062: INFO: Pod "downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034" satisfied condition "Succeeded or Failed"
Jun 13 02:21:44.070: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034 container client-container: <nil>
STEP: delete the pod 06/13/23 02:21:44.085
Jun 13 02:21:44.127: INFO: Waiting for pod downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034 to disappear
Jun 13 02:21:44.137: INFO: Pod downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 13 02:21:44.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6528" for this suite. 06/13/23 02:21:44.151
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":22,"skipped":419,"failed":0}
------------------------------
• [SLOW TEST] [6.202 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:21:37.968
    Jun 13 02:21:37.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 02:21:37.969
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:21:38.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:21:38.016
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 06/13/23 02:21:38.022
    Jun 13 02:21:38.039: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034" in namespace "downward-api-6528" to be "Succeeded or Failed"
    Jun 13 02:21:38.049: INFO: Pod "downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034": Phase="Pending", Reason="", readiness=false. Elapsed: 9.31188ms
    Jun 13 02:21:40.060: INFO: Pod "downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034": Phase="Running", Reason="", readiness=true. Elapsed: 2.020780388s
    Jun 13 02:21:42.057: INFO: Pod "downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034": Phase="Running", Reason="", readiness=false. Elapsed: 4.017609997s
    Jun 13 02:21:44.062: INFO: Pod "downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022789207s
    STEP: Saw pod success 06/13/23 02:21:44.062
    Jun 13 02:21:44.062: INFO: Pod "downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034" satisfied condition "Succeeded or Failed"
    Jun 13 02:21:44.070: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034 container client-container: <nil>
    STEP: delete the pod 06/13/23 02:21:44.085
    Jun 13 02:21:44.127: INFO: Waiting for pod downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034 to disappear
    Jun 13 02:21:44.137: INFO: Pod downwardapi-volume-0b1b82a1-896e-4907-8d8c-7be0e7db7034 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 13 02:21:44.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6528" for this suite. 06/13/23 02:21:44.151
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:21:44.169
Jun 13 02:21:44.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename endpointslice 06/13/23 02:21:44.171
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:21:44.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:21:44.214
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jun 13 02:21:44.253: INFO: Endpoints addresses: [10.255.64.105 10.255.64.106 10.255.64.107] , ports: [6443]
Jun 13 02:21:44.253: INFO: EndpointSlices addresses: [10.255.64.105 10.255.64.106 10.255.64.107] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jun 13 02:21:44.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7395" for this suite. 06/13/23 02:21:44.264
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":23,"skipped":420,"failed":0}
------------------------------
• [0.115 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:21:44.169
    Jun 13 02:21:44.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename endpointslice 06/13/23 02:21:44.171
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:21:44.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:21:44.214
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jun 13 02:21:44.253: INFO: Endpoints addresses: [10.255.64.105 10.255.64.106 10.255.64.107] , ports: [6443]
    Jun 13 02:21:44.253: INFO: EndpointSlices addresses: [10.255.64.105 10.255.64.106 10.255.64.107] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jun 13 02:21:44.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7395" for this suite. 06/13/23 02:21:44.264
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:21:44.285
Jun 13 02:21:44.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:21:44.287
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:21:44.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:21:44.33
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 06/13/23 02:21:44.335
Jun 13 02:21:44.359: INFO: Waiting up to 5m0s for pod "downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176" in namespace "projected-6050" to be "Succeeded or Failed"
Jun 13 02:21:44.388: INFO: Pod "downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176": Phase="Pending", Reason="", readiness=false. Elapsed: 28.492111ms
Jun 13 02:21:46.400: INFO: Pod "downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041104104s
Jun 13 02:21:48.398: INFO: Pod "downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039135674s
STEP: Saw pod success 06/13/23 02:21:48.398
Jun 13 02:21:48.399: INFO: Pod "downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176" satisfied condition "Succeeded or Failed"
Jun 13 02:21:48.413: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176 container client-container: <nil>
STEP: delete the pod 06/13/23 02:21:48.438
Jun 13 02:21:48.590: INFO: Waiting for pod downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176 to disappear
Jun 13 02:21:48.604: INFO: Pod downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 13 02:21:48.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6050" for this suite. 06/13/23 02:21:48.617
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":24,"skipped":424,"failed":0}
------------------------------
• [4.361 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:21:44.285
    Jun 13 02:21:44.286: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:21:44.287
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:21:44.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:21:44.33
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 06/13/23 02:21:44.335
    Jun 13 02:21:44.359: INFO: Waiting up to 5m0s for pod "downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176" in namespace "projected-6050" to be "Succeeded or Failed"
    Jun 13 02:21:44.388: INFO: Pod "downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176": Phase="Pending", Reason="", readiness=false. Elapsed: 28.492111ms
    Jun 13 02:21:46.400: INFO: Pod "downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041104104s
    Jun 13 02:21:48.398: INFO: Pod "downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039135674s
    STEP: Saw pod success 06/13/23 02:21:48.398
    Jun 13 02:21:48.399: INFO: Pod "downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176" satisfied condition "Succeeded or Failed"
    Jun 13 02:21:48.413: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176 container client-container: <nil>
    STEP: delete the pod 06/13/23 02:21:48.438
    Jun 13 02:21:48.590: INFO: Waiting for pod downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176 to disappear
    Jun 13 02:21:48.604: INFO: Pod downwardapi-volume-067701d1-42cf-46a5-a442-5bede6996176 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 13 02:21:48.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6050" for this suite. 06/13/23 02:21:48.617
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:21:48.646
Jun 13 02:21:48.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename replicaset 06/13/23 02:21:48.648
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:21:48.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:21:48.712
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 06/13/23 02:21:48.726
STEP: Verify that the required pods have come up 06/13/23 02:21:48.788
Jun 13 02:21:48.820: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 06/13/23 02:21:48.82
Jun 13 02:21:48.820: INFO: Waiting up to 5m0s for pod "test-rs-vqm5q" in namespace "replicaset-8382" to be "running"
Jun 13 02:21:48.821: INFO: Waiting up to 5m0s for pod "test-rs-68b2l" in namespace "replicaset-8382" to be "running"
Jun 13 02:21:48.821: INFO: Waiting up to 5m0s for pod "test-rs-ltb85" in namespace "replicaset-8382" to be "running"
Jun 13 02:21:48.852: INFO: Pod "test-rs-vqm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 31.24977ms
Jun 13 02:21:48.880: INFO: Pod "test-rs-68b2l": Phase="Pending", Reason="", readiness=false. Elapsed: 59.033047ms
Jun 13 02:21:48.880: INFO: Pod "test-rs-ltb85": Phase="Pending", Reason="", readiness=false. Elapsed: 59.315679ms
Jun 13 02:21:50.862: INFO: Pod "test-rs-vqm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041582427s
Jun 13 02:21:50.894: INFO: Pod "test-rs-ltb85": Phase="Running", Reason="", readiness=true. Elapsed: 2.072800818s
Jun 13 02:21:50.894: INFO: Pod "test-rs-ltb85" satisfied condition "running"
Jun 13 02:21:50.899: INFO: Pod "test-rs-68b2l": Phase="Running", Reason="", readiness=true. Elapsed: 2.078161084s
Jun 13 02:21:50.899: INFO: Pod "test-rs-68b2l" satisfied condition "running"
Jun 13 02:21:52.862: INFO: Pod "test-rs-vqm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042000482s
Jun 13 02:21:54.871: INFO: Pod "test-rs-vqm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050937757s
Jun 13 02:21:56.863: INFO: Pod "test-rs-vqm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042357483s
Jun 13 02:21:58.861: INFO: Pod "test-rs-vqm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 10.040144866s
Jun 13 02:22:00.861: INFO: Pod "test-rs-vqm5q": Phase="Running", Reason="", readiness=true. Elapsed: 12.040946476s
Jun 13 02:22:00.861: INFO: Pod "test-rs-vqm5q" satisfied condition "running"
Jun 13 02:22:00.869: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 06/13/23 02:22:00.869
STEP: DeleteCollection of the ReplicaSets 06/13/23 02:22:00.878
STEP: After DeleteCollection verify that ReplicaSets have been deleted 06/13/23 02:22:00.898
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 13 02:22:00.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8382" for this suite. 06/13/23 02:22:00.928
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":25,"skipped":425,"failed":0}
------------------------------
• [SLOW TEST] [12.300 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:21:48.646
    Jun 13 02:21:48.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename replicaset 06/13/23 02:21:48.648
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:21:48.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:21:48.712
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 06/13/23 02:21:48.726
    STEP: Verify that the required pods have come up 06/13/23 02:21:48.788
    Jun 13 02:21:48.820: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 06/13/23 02:21:48.82
    Jun 13 02:21:48.820: INFO: Waiting up to 5m0s for pod "test-rs-vqm5q" in namespace "replicaset-8382" to be "running"
    Jun 13 02:21:48.821: INFO: Waiting up to 5m0s for pod "test-rs-68b2l" in namespace "replicaset-8382" to be "running"
    Jun 13 02:21:48.821: INFO: Waiting up to 5m0s for pod "test-rs-ltb85" in namespace "replicaset-8382" to be "running"
    Jun 13 02:21:48.852: INFO: Pod "test-rs-vqm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 31.24977ms
    Jun 13 02:21:48.880: INFO: Pod "test-rs-68b2l": Phase="Pending", Reason="", readiness=false. Elapsed: 59.033047ms
    Jun 13 02:21:48.880: INFO: Pod "test-rs-ltb85": Phase="Pending", Reason="", readiness=false. Elapsed: 59.315679ms
    Jun 13 02:21:50.862: INFO: Pod "test-rs-vqm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041582427s
    Jun 13 02:21:50.894: INFO: Pod "test-rs-ltb85": Phase="Running", Reason="", readiness=true. Elapsed: 2.072800818s
    Jun 13 02:21:50.894: INFO: Pod "test-rs-ltb85" satisfied condition "running"
    Jun 13 02:21:50.899: INFO: Pod "test-rs-68b2l": Phase="Running", Reason="", readiness=true. Elapsed: 2.078161084s
    Jun 13 02:21:50.899: INFO: Pod "test-rs-68b2l" satisfied condition "running"
    Jun 13 02:21:52.862: INFO: Pod "test-rs-vqm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042000482s
    Jun 13 02:21:54.871: INFO: Pod "test-rs-vqm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.050937757s
    Jun 13 02:21:56.863: INFO: Pod "test-rs-vqm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042357483s
    Jun 13 02:21:58.861: INFO: Pod "test-rs-vqm5q": Phase="Pending", Reason="", readiness=false. Elapsed: 10.040144866s
    Jun 13 02:22:00.861: INFO: Pod "test-rs-vqm5q": Phase="Running", Reason="", readiness=true. Elapsed: 12.040946476s
    Jun 13 02:22:00.861: INFO: Pod "test-rs-vqm5q" satisfied condition "running"
    Jun 13 02:22:00.869: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 06/13/23 02:22:00.869
    STEP: DeleteCollection of the ReplicaSets 06/13/23 02:22:00.878
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 06/13/23 02:22:00.898
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 13 02:22:00.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8382" for this suite. 06/13/23 02:22:00.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:22:00.95
Jun 13 02:22:00.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename custom-resource-definition 06/13/23 02:22:00.952
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:22:01.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:22:01.048
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jun 13 02:22:01.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 02:22:04.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-235" for this suite. 06/13/23 02:22:04.299
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":26,"skipped":486,"failed":0}
------------------------------
• [3.377 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:22:00.95
    Jun 13 02:22:00.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename custom-resource-definition 06/13/23 02:22:00.952
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:22:01.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:22:01.048
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jun 13 02:22:01.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 02:22:04.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-235" for this suite. 06/13/23 02:22:04.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:22:04.328
Jun 13 02:22:04.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 02:22:04.329
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:22:04.37
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:22:04.377
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-c3186717-5ea3-44a1-b1a1-1b10cbcb8bf0 06/13/23 02:22:04.435
STEP: Creating a pod to test consume secrets 06/13/23 02:22:04.447
Jun 13 02:22:04.465: INFO: Waiting up to 5m0s for pod "pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81" in namespace "secrets-7766" to be "Succeeded or Failed"
Jun 13 02:22:04.474: INFO: Pod "pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81": Phase="Pending", Reason="", readiness=false. Elapsed: 8.684609ms
Jun 13 02:22:06.495: INFO: Pod "pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029682511s
Jun 13 02:22:08.498: INFO: Pod "pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033229302s
STEP: Saw pod success 06/13/23 02:22:08.498
Jun 13 02:22:08.499: INFO: Pod "pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81" satisfied condition "Succeeded or Failed"
Jun 13 02:22:08.508: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81 container secret-volume-test: <nil>
STEP: delete the pod 06/13/23 02:22:08.537
Jun 13 02:22:08.588: INFO: Waiting for pod pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81 to disappear
Jun 13 02:22:08.601: INFO: Pod pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 13 02:22:08.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7766" for this suite. 06/13/23 02:22:08.625
STEP: Destroying namespace "secret-namespace-6858" for this suite. 06/13/23 02:22:08.659
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":27,"skipped":497,"failed":0}
------------------------------
• [4.364 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:22:04.328
    Jun 13 02:22:04.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 02:22:04.329
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:22:04.37
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:22:04.377
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-c3186717-5ea3-44a1-b1a1-1b10cbcb8bf0 06/13/23 02:22:04.435
    STEP: Creating a pod to test consume secrets 06/13/23 02:22:04.447
    Jun 13 02:22:04.465: INFO: Waiting up to 5m0s for pod "pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81" in namespace "secrets-7766" to be "Succeeded or Failed"
    Jun 13 02:22:04.474: INFO: Pod "pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81": Phase="Pending", Reason="", readiness=false. Elapsed: 8.684609ms
    Jun 13 02:22:06.495: INFO: Pod "pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029682511s
    Jun 13 02:22:08.498: INFO: Pod "pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033229302s
    STEP: Saw pod success 06/13/23 02:22:08.498
    Jun 13 02:22:08.499: INFO: Pod "pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81" satisfied condition "Succeeded or Failed"
    Jun 13 02:22:08.508: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81 container secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 02:22:08.537
    Jun 13 02:22:08.588: INFO: Waiting for pod pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81 to disappear
    Jun 13 02:22:08.601: INFO: Pod pod-secrets-9584362a-ceb0-4660-a716-3a6ee2c3bf81 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 02:22:08.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7766" for this suite. 06/13/23 02:22:08.625
    STEP: Destroying namespace "secret-namespace-6858" for this suite. 06/13/23 02:22:08.659
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:22:08.692
Jun 13 02:22:08.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename cronjob 06/13/23 02:22:08.694
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:22:08.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:22:08.755
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 06/13/23 02:22:08.769
STEP: Ensuring a job is scheduled 06/13/23 02:22:08.785
STEP: Ensuring exactly one is scheduled 06/13/23 02:23:00.796
STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/13/23 02:23:00.811
STEP: Ensuring no more jobs are scheduled 06/13/23 02:23:00.821
STEP: Removing cronjob 06/13/23 02:28:00.838
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 13 02:28:00.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-445" for this suite. 06/13/23 02:28:00.872
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":28,"skipped":499,"failed":0}
------------------------------
• [SLOW TEST] [352.196 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:22:08.692
    Jun 13 02:22:08.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename cronjob 06/13/23 02:22:08.694
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:22:08.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:22:08.755
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 06/13/23 02:22:08.769
    STEP: Ensuring a job is scheduled 06/13/23 02:22:08.785
    STEP: Ensuring exactly one is scheduled 06/13/23 02:23:00.796
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/13/23 02:23:00.811
    STEP: Ensuring no more jobs are scheduled 06/13/23 02:23:00.821
    STEP: Removing cronjob 06/13/23 02:28:00.838
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 13 02:28:00.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-445" for this suite. 06/13/23 02:28:00.872
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:28:00.888
Jun 13 02:28:00.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:28:00.89
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:00.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:00.947
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-f79b2268-a4ad-4fb7-be8f-782e75288e20 06/13/23 02:28:00.96
STEP: Creating a pod to test consume configMaps 06/13/23 02:28:00.978
Jun 13 02:28:01.001: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14" in namespace "projected-364" to be "Succeeded or Failed"
Jun 13 02:28:01.011: INFO: Pod "pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14": Phase="Pending", Reason="", readiness=false. Elapsed: 9.602731ms
Jun 13 02:28:03.021: INFO: Pod "pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020395273s
Jun 13 02:28:05.025: INFO: Pod "pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024023445s
STEP: Saw pod success 06/13/23 02:28:05.025
Jun 13 02:28:05.025: INFO: Pod "pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14" satisfied condition "Succeeded or Failed"
Jun 13 02:28:05.033: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14 container agnhost-container: <nil>
STEP: delete the pod 06/13/23 02:28:05.069
Jun 13 02:28:05.104: INFO: Waiting for pod pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14 to disappear
Jun 13 02:28:05.116: INFO: Pod pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 13 02:28:05.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-364" for this suite. 06/13/23 02:28:05.135
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":29,"skipped":500,"failed":0}
------------------------------
• [4.265 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:28:00.888
    Jun 13 02:28:00.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:28:00.89
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:00.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:00.947
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-f79b2268-a4ad-4fb7-be8f-782e75288e20 06/13/23 02:28:00.96
    STEP: Creating a pod to test consume configMaps 06/13/23 02:28:00.978
    Jun 13 02:28:01.001: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14" in namespace "projected-364" to be "Succeeded or Failed"
    Jun 13 02:28:01.011: INFO: Pod "pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14": Phase="Pending", Reason="", readiness=false. Elapsed: 9.602731ms
    Jun 13 02:28:03.021: INFO: Pod "pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020395273s
    Jun 13 02:28:05.025: INFO: Pod "pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024023445s
    STEP: Saw pod success 06/13/23 02:28:05.025
    Jun 13 02:28:05.025: INFO: Pod "pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14" satisfied condition "Succeeded or Failed"
    Jun 13 02:28:05.033: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14 container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 02:28:05.069
    Jun 13 02:28:05.104: INFO: Waiting for pod pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14 to disappear
    Jun 13 02:28:05.116: INFO: Pod pod-projected-configmaps-26473a0a-97c3-4d05-a683-c329127b5b14 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 13 02:28:05.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-364" for this suite. 06/13/23 02:28:05.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:28:05.155
Jun 13 02:28:05.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename watch 06/13/23 02:28:05.157
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:05.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:05.226
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 06/13/23 02:28:05.232
STEP: starting a background goroutine to produce watch events 06/13/23 02:28:05.248
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 06/13/23 02:28:05.248
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 13 02:28:08.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7795" for this suite. 06/13/23 02:28:08.185
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":30,"skipped":517,"failed":0}
------------------------------
• [3.078 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:28:05.155
    Jun 13 02:28:05.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename watch 06/13/23 02:28:05.157
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:05.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:05.226
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 06/13/23 02:28:05.232
    STEP: starting a background goroutine to produce watch events 06/13/23 02:28:05.248
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 06/13/23 02:28:05.248
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 13 02:28:08.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7795" for this suite. 06/13/23 02:28:08.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:28:08.234
Jun 13 02:28:08.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename svcaccounts 06/13/23 02:28:08.235
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:08.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:08.351
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jun 13 02:28:08.372: INFO: Got root ca configmap in namespace "svcaccounts-5399"
Jun 13 02:28:08.419: INFO: Deleted root ca configmap in namespace "svcaccounts-5399"
STEP: waiting for a new root ca configmap created 06/13/23 02:28:08.92
Jun 13 02:28:08.939: INFO: Recreated root ca configmap in namespace "svcaccounts-5399"
Jun 13 02:28:08.956: INFO: Updated root ca configmap in namespace "svcaccounts-5399"
STEP: waiting for the root ca configmap reconciled 06/13/23 02:28:09.457
Jun 13 02:28:09.468: INFO: Reconciled root ca configmap in namespace "svcaccounts-5399"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 13 02:28:09.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5399" for this suite. 06/13/23 02:28:09.483
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":31,"skipped":529,"failed":0}
------------------------------
• [1.270 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:28:08.234
    Jun 13 02:28:08.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename svcaccounts 06/13/23 02:28:08.235
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:08.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:08.351
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jun 13 02:28:08.372: INFO: Got root ca configmap in namespace "svcaccounts-5399"
    Jun 13 02:28:08.419: INFO: Deleted root ca configmap in namespace "svcaccounts-5399"
    STEP: waiting for a new root ca configmap created 06/13/23 02:28:08.92
    Jun 13 02:28:08.939: INFO: Recreated root ca configmap in namespace "svcaccounts-5399"
    Jun 13 02:28:08.956: INFO: Updated root ca configmap in namespace "svcaccounts-5399"
    STEP: waiting for the root ca configmap reconciled 06/13/23 02:28:09.457
    Jun 13 02:28:09.468: INFO: Reconciled root ca configmap in namespace "svcaccounts-5399"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 13 02:28:09.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5399" for this suite. 06/13/23 02:28:09.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:28:09.506
Jun 13 02:28:09.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pods 06/13/23 02:28:09.508
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:09.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:09.562
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jun 13 02:28:09.610: INFO: Waiting up to 5m0s for pod "server-envvars-cc2f06b8-e566-4474-97d3-5efdce86055d" in namespace "pods-7306" to be "running and ready"
Jun 13 02:28:09.623: INFO: Pod "server-envvars-cc2f06b8-e566-4474-97d3-5efdce86055d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.032215ms
Jun 13 02:28:09.623: INFO: The phase of Pod server-envvars-cc2f06b8-e566-4474-97d3-5efdce86055d is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:28:11.632: INFO: Pod "server-envvars-cc2f06b8-e566-4474-97d3-5efdce86055d": Phase="Running", Reason="", readiness=true. Elapsed: 2.022748019s
Jun 13 02:28:11.633: INFO: The phase of Pod server-envvars-cc2f06b8-e566-4474-97d3-5efdce86055d is Running (Ready = true)
Jun 13 02:28:11.633: INFO: Pod "server-envvars-cc2f06b8-e566-4474-97d3-5efdce86055d" satisfied condition "running and ready"
Jun 13 02:28:11.784: INFO: Waiting up to 5m0s for pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c" in namespace "pods-7306" to be "Succeeded or Failed"
Jun 13 02:28:11.872: INFO: Pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c": Phase="Pending", Reason="", readiness=false. Elapsed: 87.906828ms
Jun 13 02:28:13.894: INFO: Pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109869194s
Jun 13 02:28:15.890: INFO: Pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.105283242s
Jun 13 02:28:17.906: INFO: Pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122230923s
Jun 13 02:28:19.890: INFO: Pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.105303011s
STEP: Saw pod success 06/13/23 02:28:19.89
Jun 13 02:28:19.890: INFO: Pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c" satisfied condition "Succeeded or Failed"
Jun 13 02:28:19.900: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c container env3cont: <nil>
STEP: delete the pod 06/13/23 02:28:19.937
Jun 13 02:28:20.039: INFO: Waiting for pod client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c to disappear
Jun 13 02:28:20.050: INFO: Pod client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 13 02:28:20.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7306" for this suite. 06/13/23 02:28:20.067
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":32,"skipped":559,"failed":0}
------------------------------
• [SLOW TEST] [10.606 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:28:09.506
    Jun 13 02:28:09.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pods 06/13/23 02:28:09.508
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:09.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:09.562
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jun 13 02:28:09.610: INFO: Waiting up to 5m0s for pod "server-envvars-cc2f06b8-e566-4474-97d3-5efdce86055d" in namespace "pods-7306" to be "running and ready"
    Jun 13 02:28:09.623: INFO: Pod "server-envvars-cc2f06b8-e566-4474-97d3-5efdce86055d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.032215ms
    Jun 13 02:28:09.623: INFO: The phase of Pod server-envvars-cc2f06b8-e566-4474-97d3-5efdce86055d is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:28:11.632: INFO: Pod "server-envvars-cc2f06b8-e566-4474-97d3-5efdce86055d": Phase="Running", Reason="", readiness=true. Elapsed: 2.022748019s
    Jun 13 02:28:11.633: INFO: The phase of Pod server-envvars-cc2f06b8-e566-4474-97d3-5efdce86055d is Running (Ready = true)
    Jun 13 02:28:11.633: INFO: Pod "server-envvars-cc2f06b8-e566-4474-97d3-5efdce86055d" satisfied condition "running and ready"
    Jun 13 02:28:11.784: INFO: Waiting up to 5m0s for pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c" in namespace "pods-7306" to be "Succeeded or Failed"
    Jun 13 02:28:11.872: INFO: Pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c": Phase="Pending", Reason="", readiness=false. Elapsed: 87.906828ms
    Jun 13 02:28:13.894: INFO: Pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109869194s
    Jun 13 02:28:15.890: INFO: Pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.105283242s
    Jun 13 02:28:17.906: INFO: Pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122230923s
    Jun 13 02:28:19.890: INFO: Pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.105303011s
    STEP: Saw pod success 06/13/23 02:28:19.89
    Jun 13 02:28:19.890: INFO: Pod "client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c" satisfied condition "Succeeded or Failed"
    Jun 13 02:28:19.900: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c container env3cont: <nil>
    STEP: delete the pod 06/13/23 02:28:19.937
    Jun 13 02:28:20.039: INFO: Waiting for pod client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c to disappear
    Jun 13 02:28:20.050: INFO: Pod client-envvars-4ee455aa-784e-4cb6-9413-dfea00ccf67c no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 13 02:28:20.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7306" for this suite. 06/13/23 02:28:20.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:28:20.113
Jun 13 02:28:20.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 02:28:20.115
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:20.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:20.221
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 06/13/23 02:28:20.228
STEP: listing secrets in all namespaces to ensure that there are more than zero 06/13/23 02:28:20.25
STEP: patching the secret 06/13/23 02:28:20.266
STEP: deleting the secret using a LabelSelector 06/13/23 02:28:20.299
STEP: listing secrets in all namespaces, searching for label name and value in patch 06/13/23 02:28:20.329
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jun 13 02:28:20.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-231" for this suite. 06/13/23 02:28:20.356
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":33,"skipped":566,"failed":0}
------------------------------
• [0.265 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:28:20.113
    Jun 13 02:28:20.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 02:28:20.115
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:20.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:20.221
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 06/13/23 02:28:20.228
    STEP: listing secrets in all namespaces to ensure that there are more than zero 06/13/23 02:28:20.25
    STEP: patching the secret 06/13/23 02:28:20.266
    STEP: deleting the secret using a LabelSelector 06/13/23 02:28:20.299
    STEP: listing secrets in all namespaces, searching for label name and value in patch 06/13/23 02:28:20.329
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 02:28:20.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-231" for this suite. 06/13/23 02:28:20.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:28:20.379
Jun 13 02:28:20.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename svcaccounts 06/13/23 02:28:20.382
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:20.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:20.447
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jun 13 02:28:20.496: INFO: created pod
Jun 13 02:28:20.496: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8864" to be "Succeeded or Failed"
Jun 13 02:28:20.513: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.49694ms
Jun 13 02:28:22.524: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028165994s
Jun 13 02:28:24.531: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035005954s
Jun 13 02:28:26.525: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028621719s
STEP: Saw pod success 06/13/23 02:28:26.525
Jun 13 02:28:26.525: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jun 13 02:28:56.527: INFO: polling logs
Jun 13 02:28:56.554: INFO: Pod logs: 
I0613 02:28:21.583239       1 log.go:195] OK: Got token
I0613 02:28:21.583307       1 log.go:195] validating with in-cluster discovery
I0613 02:28:21.583806       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0613 02:28:21.583839       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8864:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686623900, NotBefore:1686623300, IssuedAt:1686623300, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8864", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f26f80f3-82a1-4edd-9642-4d13473cb787"}}}
I0613 02:28:21.607117       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0613 02:28:21.617684       1 log.go:195] OK: Validated signature on JWT
I0613 02:28:21.617823       1 log.go:195] OK: Got valid claims from token!
I0613 02:28:21.617853       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8864:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686623900, NotBefore:1686623300, IssuedAt:1686623300, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8864", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f26f80f3-82a1-4edd-9642-4d13473cb787"}}}

Jun 13 02:28:56.554: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 13 02:28:56.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8864" for this suite. 06/13/23 02:28:56.592
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":34,"skipped":573,"failed":0}
------------------------------
• [SLOW TEST] [36.244 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:28:20.379
    Jun 13 02:28:20.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename svcaccounts 06/13/23 02:28:20.382
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:20.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:20.447
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jun 13 02:28:20.496: INFO: created pod
    Jun 13 02:28:20.496: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8864" to be "Succeeded or Failed"
    Jun 13 02:28:20.513: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.49694ms
    Jun 13 02:28:22.524: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028165994s
    Jun 13 02:28:24.531: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035005954s
    Jun 13 02:28:26.525: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028621719s
    STEP: Saw pod success 06/13/23 02:28:26.525
    Jun 13 02:28:26.525: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jun 13 02:28:56.527: INFO: polling logs
    Jun 13 02:28:56.554: INFO: Pod logs: 
    I0613 02:28:21.583239       1 log.go:195] OK: Got token
    I0613 02:28:21.583307       1 log.go:195] validating with in-cluster discovery
    I0613 02:28:21.583806       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0613 02:28:21.583839       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8864:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686623900, NotBefore:1686623300, IssuedAt:1686623300, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8864", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f26f80f3-82a1-4edd-9642-4d13473cb787"}}}
    I0613 02:28:21.607117       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0613 02:28:21.617684       1 log.go:195] OK: Validated signature on JWT
    I0613 02:28:21.617823       1 log.go:195] OK: Got valid claims from token!
    I0613 02:28:21.617853       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8864:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1686623900, NotBefore:1686623300, IssuedAt:1686623300, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8864", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f26f80f3-82a1-4edd-9642-4d13473cb787"}}}

    Jun 13 02:28:56.554: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 13 02:28:56.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-8864" for this suite. 06/13/23 02:28:56.592
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:28:56.624
Jun 13 02:28:56.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 02:28:56.626
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:56.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:56.698
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-df3e23ab-bad2-41d3-988f-876ef571a911 06/13/23 02:28:56.721
STEP: Creating the pod 06/13/23 02:28:56.736
Jun 13 02:28:56.765: INFO: Waiting up to 5m0s for pod "pod-configmaps-5dd82b82-3da9-45da-9af3-eb859932c877" in namespace "configmap-5367" to be "running and ready"
Jun 13 02:28:56.779: INFO: Pod "pod-configmaps-5dd82b82-3da9-45da-9af3-eb859932c877": Phase="Pending", Reason="", readiness=false. Elapsed: 13.760692ms
Jun 13 02:28:56.779: INFO: The phase of Pod pod-configmaps-5dd82b82-3da9-45da-9af3-eb859932c877 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:28:58.796: INFO: Pod "pod-configmaps-5dd82b82-3da9-45da-9af3-eb859932c877": Phase="Running", Reason="", readiness=true. Elapsed: 2.030899048s
Jun 13 02:28:58.796: INFO: The phase of Pod pod-configmaps-5dd82b82-3da9-45da-9af3-eb859932c877 is Running (Ready = true)
Jun 13 02:28:58.796: INFO: Pod "pod-configmaps-5dd82b82-3da9-45da-9af3-eb859932c877" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-df3e23ab-bad2-41d3-988f-876ef571a911 06/13/23 02:28:58.845
STEP: waiting to observe update in volume 06/13/23 02:28:58.866
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 02:29:00.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5367" for this suite. 06/13/23 02:29:00.942
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":35,"skipped":576,"failed":0}
------------------------------
• [4.362 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:28:56.624
    Jun 13 02:28:56.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 02:28:56.626
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:28:56.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:28:56.698
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-df3e23ab-bad2-41d3-988f-876ef571a911 06/13/23 02:28:56.721
    STEP: Creating the pod 06/13/23 02:28:56.736
    Jun 13 02:28:56.765: INFO: Waiting up to 5m0s for pod "pod-configmaps-5dd82b82-3da9-45da-9af3-eb859932c877" in namespace "configmap-5367" to be "running and ready"
    Jun 13 02:28:56.779: INFO: Pod "pod-configmaps-5dd82b82-3da9-45da-9af3-eb859932c877": Phase="Pending", Reason="", readiness=false. Elapsed: 13.760692ms
    Jun 13 02:28:56.779: INFO: The phase of Pod pod-configmaps-5dd82b82-3da9-45da-9af3-eb859932c877 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:28:58.796: INFO: Pod "pod-configmaps-5dd82b82-3da9-45da-9af3-eb859932c877": Phase="Running", Reason="", readiness=true. Elapsed: 2.030899048s
    Jun 13 02:28:58.796: INFO: The phase of Pod pod-configmaps-5dd82b82-3da9-45da-9af3-eb859932c877 is Running (Ready = true)
    Jun 13 02:28:58.796: INFO: Pod "pod-configmaps-5dd82b82-3da9-45da-9af3-eb859932c877" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-df3e23ab-bad2-41d3-988f-876ef571a911 06/13/23 02:28:58.845
    STEP: waiting to observe update in volume 06/13/23 02:28:58.866
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 02:29:00.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5367" for this suite. 06/13/23 02:29:00.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:29:00.988
Jun 13 02:29:00.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename resourcequota 06/13/23 02:29:00.99
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:29:01.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:29:01.028
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 06/13/23 02:29:01.035
STEP: Counting existing ResourceQuota 06/13/23 02:29:06.103
STEP: Creating a ResourceQuota 06/13/23 02:29:11.128
STEP: Ensuring resource quota status is calculated 06/13/23 02:29:11.162
STEP: Creating a Secret 06/13/23 02:29:13.17
STEP: Ensuring resource quota status captures secret creation 06/13/23 02:29:13.198
STEP: Deleting a secret 06/13/23 02:29:15.208
STEP: Ensuring resource quota status released usage 06/13/23 02:29:15.251
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 13 02:29:17.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9971" for this suite. 06/13/23 02:29:17.275
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":36,"skipped":617,"failed":0}
------------------------------
• [SLOW TEST] [16.304 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:29:00.988
    Jun 13 02:29:00.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename resourcequota 06/13/23 02:29:00.99
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:29:01.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:29:01.028
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 06/13/23 02:29:01.035
    STEP: Counting existing ResourceQuota 06/13/23 02:29:06.103
    STEP: Creating a ResourceQuota 06/13/23 02:29:11.128
    STEP: Ensuring resource quota status is calculated 06/13/23 02:29:11.162
    STEP: Creating a Secret 06/13/23 02:29:13.17
    STEP: Ensuring resource quota status captures secret creation 06/13/23 02:29:13.198
    STEP: Deleting a secret 06/13/23 02:29:15.208
    STEP: Ensuring resource quota status released usage 06/13/23 02:29:15.251
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 13 02:29:17.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9971" for this suite. 06/13/23 02:29:17.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:29:17.293
Jun 13 02:29:17.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename init-container 06/13/23 02:29:17.295
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:29:17.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:29:17.347
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 06/13/23 02:29:17.357
Jun 13 02:29:17.357: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 13 02:29:23.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1387" for this suite. 06/13/23 02:29:23.844
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":37,"skipped":628,"failed":0}
------------------------------
• [SLOW TEST] [6.570 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:29:17.293
    Jun 13 02:29:17.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename init-container 06/13/23 02:29:17.295
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:29:17.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:29:17.347
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 06/13/23 02:29:17.357
    Jun 13 02:29:17.357: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 13 02:29:23.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1387" for this suite. 06/13/23 02:29:23.844
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:29:23.864
Jun 13 02:29:23.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename svcaccounts 06/13/23 02:29:23.866
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:29:23.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:29:23.912
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  06/13/23 02:29:23.918
Jun 13 02:29:23.940: INFO: Waiting up to 5m0s for pod "test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3" in namespace "svcaccounts-4227" to be "Succeeded or Failed"
Jun 13 02:29:23.955: INFO: Pod "test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.405356ms
Jun 13 02:29:25.967: INFO: Pod "test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026705195s
Jun 13 02:29:27.966: INFO: Pod "test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025288372s
STEP: Saw pod success 06/13/23 02:29:27.966
Jun 13 02:29:27.966: INFO: Pod "test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3" satisfied condition "Succeeded or Failed"
Jun 13 02:29:27.974: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3 container agnhost-container: <nil>
STEP: delete the pod 06/13/23 02:29:28.021
Jun 13 02:29:28.249: INFO: Waiting for pod test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3 to disappear
Jun 13 02:29:28.281: INFO: Pod test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 13 02:29:28.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4227" for this suite. 06/13/23 02:29:28.3
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":38,"skipped":630,"failed":0}
------------------------------
• [4.466 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:29:23.864
    Jun 13 02:29:23.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename svcaccounts 06/13/23 02:29:23.866
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:29:23.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:29:23.912
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  06/13/23 02:29:23.918
    Jun 13 02:29:23.940: INFO: Waiting up to 5m0s for pod "test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3" in namespace "svcaccounts-4227" to be "Succeeded or Failed"
    Jun 13 02:29:23.955: INFO: Pod "test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.405356ms
    Jun 13 02:29:25.967: INFO: Pod "test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026705195s
    Jun 13 02:29:27.966: INFO: Pod "test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025288372s
    STEP: Saw pod success 06/13/23 02:29:27.966
    Jun 13 02:29:27.966: INFO: Pod "test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3" satisfied condition "Succeeded or Failed"
    Jun 13 02:29:27.974: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3 container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 02:29:28.021
    Jun 13 02:29:28.249: INFO: Waiting for pod test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3 to disappear
    Jun 13 02:29:28.281: INFO: Pod test-pod-053a1652-8498-4c3e-80ea-59f5b1934ab3 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 13 02:29:28.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-4227" for this suite. 06/13/23 02:29:28.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:29:28.331
Jun 13 02:29:28.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 02:29:28.332
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:29:28.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:29:28.385
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194
STEP: creating service in namespace services-2030 06/13/23 02:29:28.394
STEP: creating service affinity-nodeport in namespace services-2030 06/13/23 02:29:28.394
STEP: creating replication controller affinity-nodeport in namespace services-2030 06/13/23 02:29:28.482
I0613 02:29:28.507079      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2030, replica count: 3
I0613 02:29:31.565238      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0613 02:29:34.565482      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0613 02:29:37.566224      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0613 02:29:40.566972      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0613 02:29:43.567735      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 13 02:29:43.601: INFO: Creating new exec pod
Jun 13 02:29:43.628: INFO: Waiting up to 5m0s for pod "execpod-affinitylrlzw" in namespace "services-2030" to be "running"
Jun 13 02:29:43.639: INFO: Pod "execpod-affinitylrlzw": Phase="Pending", Reason="", readiness=false. Elapsed: 10.933701ms
Jun 13 02:29:45.649: INFO: Pod "execpod-affinitylrlzw": Phase="Running", Reason="", readiness=true. Elapsed: 2.020235147s
Jun 13 02:29:45.649: INFO: Pod "execpod-affinitylrlzw" satisfied condition "running"
Jun 13 02:29:46.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2030 exec execpod-affinitylrlzw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jun 13 02:29:46.939: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun 13 02:29:46.939: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 02:29:46.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2030 exec execpod-affinitylrlzw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.159.71 80'
Jun 13 02:29:47.138: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.159.71 80\nConnection to 10.96.159.71 80 port [tcp/http] succeeded!\n"
Jun 13 02:29:47.138: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 02:29:47.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2030 exec execpod-affinitylrlzw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 31795'
Jun 13 02:29:47.328: INFO: stderr: "+ + nc -v -t -w 2 10.255.64.102 31795\necho hostName\nConnection to 10.255.64.102 31795 port [tcp/*] succeeded!\n"
Jun 13 02:29:47.328: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 02:29:47.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2030 exec execpod-affinitylrlzw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.104 31795'
Jun 13 02:29:47.543: INFO: stderr: "+ nc -v -t -w 2 10.255.64.104 31795\n+ echo hostName\nConnection to 10.255.64.104 31795 port [tcp/*] succeeded!\n"
Jun 13 02:29:47.543: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 02:29:47.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2030 exec execpod-affinitylrlzw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.64.103:31795/ ; done'
Jun 13 02:29:48.026: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n"
Jun 13 02:29:48.026: INFO: stdout: "\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs"
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
Jun 13 02:29:48.026: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2030, will wait for the garbage collector to delete the pods 06/13/23 02:29:48.059
Jun 13 02:29:48.145: INFO: Deleting ReplicationController affinity-nodeport took: 21.87948ms
Jun 13 02:29:48.246: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.558699ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 02:29:50.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2030" for this suite. 06/13/23 02:29:50.879
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":39,"skipped":640,"failed":0}
------------------------------
• [SLOW TEST] [22.567 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:29:28.331
    Jun 13 02:29:28.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 02:29:28.332
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:29:28.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:29:28.385
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2194
    STEP: creating service in namespace services-2030 06/13/23 02:29:28.394
    STEP: creating service affinity-nodeport in namespace services-2030 06/13/23 02:29:28.394
    STEP: creating replication controller affinity-nodeport in namespace services-2030 06/13/23 02:29:28.482
    I0613 02:29:28.507079      18 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2030, replica count: 3
    I0613 02:29:31.565238      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0613 02:29:34.565482      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0613 02:29:37.566224      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0613 02:29:40.566972      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0613 02:29:43.567735      18 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 13 02:29:43.601: INFO: Creating new exec pod
    Jun 13 02:29:43.628: INFO: Waiting up to 5m0s for pod "execpod-affinitylrlzw" in namespace "services-2030" to be "running"
    Jun 13 02:29:43.639: INFO: Pod "execpod-affinitylrlzw": Phase="Pending", Reason="", readiness=false. Elapsed: 10.933701ms
    Jun 13 02:29:45.649: INFO: Pod "execpod-affinitylrlzw": Phase="Running", Reason="", readiness=true. Elapsed: 2.020235147s
    Jun 13 02:29:45.649: INFO: Pod "execpod-affinitylrlzw" satisfied condition "running"
    Jun 13 02:29:46.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2030 exec execpod-affinitylrlzw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jun 13 02:29:46.939: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jun 13 02:29:46.939: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 02:29:46.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2030 exec execpod-affinitylrlzw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.159.71 80'
    Jun 13 02:29:47.138: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.159.71 80\nConnection to 10.96.159.71 80 port [tcp/http] succeeded!\n"
    Jun 13 02:29:47.138: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 02:29:47.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2030 exec execpod-affinitylrlzw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 31795'
    Jun 13 02:29:47.328: INFO: stderr: "+ + nc -v -t -w 2 10.255.64.102 31795\necho hostName\nConnection to 10.255.64.102 31795 port [tcp/*] succeeded!\n"
    Jun 13 02:29:47.328: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 02:29:47.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2030 exec execpod-affinitylrlzw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.104 31795'
    Jun 13 02:29:47.543: INFO: stderr: "+ nc -v -t -w 2 10.255.64.104 31795\n+ echo hostName\nConnection to 10.255.64.104 31795 port [tcp/*] succeeded!\n"
    Jun 13 02:29:47.543: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 02:29:47.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2030 exec execpod-affinitylrlzw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.64.103:31795/ ; done'
    Jun 13 02:29:48.026: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:31795/\n"
    Jun 13 02:29:48.026: INFO: stdout: "\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs\naffinity-nodeport-tn6fs"
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Received response from host: affinity-nodeport-tn6fs
    Jun 13 02:29:48.026: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-2030, will wait for the garbage collector to delete the pods 06/13/23 02:29:48.059
    Jun 13 02:29:48.145: INFO: Deleting ReplicationController affinity-nodeport took: 21.87948ms
    Jun 13 02:29:48.246: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.558699ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 02:29:50.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2030" for this suite. 06/13/23 02:29:50.879
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:29:50.902
Jun 13 02:29:50.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename gc 06/13/23 02:29:50.904
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:29:50.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:29:50.942
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jun 13 02:29:51.036: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ff303450-b3ea-485f-94da-025785a72eae", Controller:(*bool)(0xc003b5fdbe), BlockOwnerDeletion:(*bool)(0xc003b5fdbf)}}
Jun 13 02:29:51.056: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a2ac9577-f124-40b4-8441-bd10449c186f", Controller:(*bool)(0xc00115bcae), BlockOwnerDeletion:(*bool)(0xc00115bcaf)}}
Jun 13 02:29:51.092: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7f757170-49b1-425f-9dc0-5bc570e867e2", Controller:(*bool)(0xc000fc804e), BlockOwnerDeletion:(*bool)(0xc000fc804f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 13 02:29:56.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2468" for this suite. 06/13/23 02:29:56.143
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":40,"skipped":696,"failed":0}
------------------------------
• [SLOW TEST] [5.258 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:29:50.902
    Jun 13 02:29:50.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename gc 06/13/23 02:29:50.904
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:29:50.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:29:50.942
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jun 13 02:29:51.036: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"ff303450-b3ea-485f-94da-025785a72eae", Controller:(*bool)(0xc003b5fdbe), BlockOwnerDeletion:(*bool)(0xc003b5fdbf)}}
    Jun 13 02:29:51.056: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"a2ac9577-f124-40b4-8441-bd10449c186f", Controller:(*bool)(0xc00115bcae), BlockOwnerDeletion:(*bool)(0xc00115bcaf)}}
    Jun 13 02:29:51.092: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7f757170-49b1-425f-9dc0-5bc570e867e2", Controller:(*bool)(0xc000fc804e), BlockOwnerDeletion:(*bool)(0xc000fc804f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 13 02:29:56.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2468" for this suite. 06/13/23 02:29:56.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:29:56.161
Jun 13 02:29:56.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename daemonsets 06/13/23 02:29:56.163
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:29:56.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:29:56.24
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 06/13/23 02:29:56.328
STEP: Check that daemon pods launch on every node of the cluster. 06/13/23 02:29:56.363
Jun 13 02:29:56.375: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:56.376: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:56.376: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:56.432: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 02:29:56.432: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 02:29:57.444: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:57.444: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:57.444: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:57.453: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 02:29:57.453: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 02:29:58.449: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:58.449: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:58.449: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:58.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 13 02:29:58.535: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
Jun 13 02:29:59.451: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:59.451: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:59.451: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:59.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 13 02:29:59.460: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 06/13/23 02:29:59.468
Jun 13 02:29:59.506: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:59.506: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:59.506: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:29:59.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 13 02:29:59.514: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
Jun 13 02:30:00.541: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:00.542: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:00.542: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:00.564: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 13 02:30:00.564: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
Jun 13 02:30:01.537: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:01.537: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:01.537: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:01.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 13 02:30:01.551: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
Jun 13 02:30:02.538: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:02.538: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:02.538: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:02.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 13 02:30:02.562: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
Jun 13 02:30:03.555: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:03.555: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:03.555: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:03.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 13 02:30:03.577: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
Jun 13 02:30:04.531: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:04.531: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:04.531: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:30:04.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 13 02:30:04.543: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/13/23 02:30:04.554
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-529, will wait for the garbage collector to delete the pods 06/13/23 02:30:04.554
Jun 13 02:30:04.654: INFO: Deleting DaemonSet.extensions daemon-set took: 33.054128ms
Jun 13 02:30:04.754: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.139208ms
Jun 13 02:30:06.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 02:30:06.963: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 13 02:30:06.972: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12237"},"items":null}

Jun 13 02:30:06.980: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12237"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 13 02:30:07.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-529" for this suite. 06/13/23 02:30:07.047
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":41,"skipped":711,"failed":0}
------------------------------
• [SLOW TEST] [10.937 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:29:56.161
    Jun 13 02:29:56.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename daemonsets 06/13/23 02:29:56.163
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:29:56.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:29:56.24
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 06/13/23 02:29:56.328
    STEP: Check that daemon pods launch on every node of the cluster. 06/13/23 02:29:56.363
    Jun 13 02:29:56.375: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:56.376: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:56.376: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:56.432: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 02:29:56.432: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 02:29:57.444: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:57.444: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:57.444: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:57.453: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 02:29:57.453: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 02:29:58.449: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:58.449: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:58.449: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:58.535: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 13 02:29:58.535: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
    Jun 13 02:29:59.451: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:59.451: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:59.451: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:59.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 13 02:29:59.460: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 06/13/23 02:29:59.468
    Jun 13 02:29:59.506: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:59.506: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:59.506: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:29:59.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 13 02:29:59.514: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
    Jun 13 02:30:00.541: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:00.542: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:00.542: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:00.564: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 13 02:30:00.564: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
    Jun 13 02:30:01.537: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:01.537: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:01.537: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:01.551: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 13 02:30:01.551: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
    Jun 13 02:30:02.538: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:02.538: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:02.538: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:02.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 13 02:30:02.562: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
    Jun 13 02:30:03.555: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:03.555: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:03.555: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:03.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 13 02:30:03.577: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
    Jun 13 02:30:04.531: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:04.531: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:04.531: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:30:04.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 13 02:30:04.543: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/13/23 02:30:04.554
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-529, will wait for the garbage collector to delete the pods 06/13/23 02:30:04.554
    Jun 13 02:30:04.654: INFO: Deleting DaemonSet.extensions daemon-set took: 33.054128ms
    Jun 13 02:30:04.754: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.139208ms
    Jun 13 02:30:06.963: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 02:30:06.963: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 13 02:30:06.972: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12237"},"items":null}

    Jun 13 02:30:06.980: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12237"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 02:30:07.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-529" for this suite. 06/13/23 02:30:07.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:30:07.103
Jun 13 02:30:07.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 02:30:07.104
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:07.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:07.254
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-4ff229e1-55cc-4a33-aea6-65f96c1cc3a0 06/13/23 02:30:07.26
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jun 13 02:30:07.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3715" for this suite. 06/13/23 02:30:07.276
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":42,"skipped":770,"failed":0}
------------------------------
• [0.198 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:30:07.103
    Jun 13 02:30:07.103: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 02:30:07.104
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:07.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:07.254
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-4ff229e1-55cc-4a33-aea6-65f96c1cc3a0 06/13/23 02:30:07.26
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 02:30:07.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3715" for this suite. 06/13/23 02:30:07.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:30:07.301
Jun 13 02:30:07.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pods 06/13/23 02:30:07.303
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:07.33
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:07.335
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 06/13/23 02:30:07.341
STEP: setting up watch 06/13/23 02:30:07.342
STEP: submitting the pod to kubernetes 06/13/23 02:30:07.472
STEP: verifying the pod is in kubernetes 06/13/23 02:30:07.51
STEP: verifying pod creation was observed 06/13/23 02:30:07.537
Jun 13 02:30:07.537: INFO: Waiting up to 5m0s for pod "pod-submit-remove-5f30e17f-3d11-44c9-9b56-2ebd7675c69f" in namespace "pods-2664" to be "running"
Jun 13 02:30:07.547: INFO: Pod "pod-submit-remove-5f30e17f-3d11-44c9-9b56-2ebd7675c69f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.848708ms
Jun 13 02:30:09.556: INFO: Pod "pod-submit-remove-5f30e17f-3d11-44c9-9b56-2ebd7675c69f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019322671s
Jun 13 02:30:11.560: INFO: Pod "pod-submit-remove-5f30e17f-3d11-44c9-9b56-2ebd7675c69f": Phase="Running", Reason="", readiness=true. Elapsed: 4.02286782s
Jun 13 02:30:11.560: INFO: Pod "pod-submit-remove-5f30e17f-3d11-44c9-9b56-2ebd7675c69f" satisfied condition "running"
STEP: deleting the pod gracefully 06/13/23 02:30:11.586
STEP: verifying pod deletion was observed 06/13/23 02:30:11.624
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 13 02:30:13.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2664" for this suite. 06/13/23 02:30:13.079
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":43,"skipped":782,"failed":0}
------------------------------
• [SLOW TEST] [5.812 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:30:07.301
    Jun 13 02:30:07.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pods 06/13/23 02:30:07.303
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:07.33
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:07.335
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 06/13/23 02:30:07.341
    STEP: setting up watch 06/13/23 02:30:07.342
    STEP: submitting the pod to kubernetes 06/13/23 02:30:07.472
    STEP: verifying the pod is in kubernetes 06/13/23 02:30:07.51
    STEP: verifying pod creation was observed 06/13/23 02:30:07.537
    Jun 13 02:30:07.537: INFO: Waiting up to 5m0s for pod "pod-submit-remove-5f30e17f-3d11-44c9-9b56-2ebd7675c69f" in namespace "pods-2664" to be "running"
    Jun 13 02:30:07.547: INFO: Pod "pod-submit-remove-5f30e17f-3d11-44c9-9b56-2ebd7675c69f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.848708ms
    Jun 13 02:30:09.556: INFO: Pod "pod-submit-remove-5f30e17f-3d11-44c9-9b56-2ebd7675c69f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019322671s
    Jun 13 02:30:11.560: INFO: Pod "pod-submit-remove-5f30e17f-3d11-44c9-9b56-2ebd7675c69f": Phase="Running", Reason="", readiness=true. Elapsed: 4.02286782s
    Jun 13 02:30:11.560: INFO: Pod "pod-submit-remove-5f30e17f-3d11-44c9-9b56-2ebd7675c69f" satisfied condition "running"
    STEP: deleting the pod gracefully 06/13/23 02:30:11.586
    STEP: verifying pod deletion was observed 06/13/23 02:30:11.624
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 13 02:30:13.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2664" for this suite. 06/13/23 02:30:13.079
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:30:13.114
Jun 13 02:30:13.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename namespaces 06/13/23 02:30:13.117
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:13.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:13.172
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 06/13/23 02:30:13.178
STEP: patching the Namespace 06/13/23 02:30:13.236
STEP: get the Namespace and ensuring it has the label 06/13/23 02:30:13.253
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jun 13 02:30:13.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4614" for this suite. 06/13/23 02:30:13.301
STEP: Destroying namespace "nspatchtest-82354319-1abc-4b54-b5c5-9b3e2a0f8e02-4131" for this suite. 06/13/23 02:30:13.321
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":44,"skipped":785,"failed":0}
------------------------------
• [0.224 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:30:13.114
    Jun 13 02:30:13.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename namespaces 06/13/23 02:30:13.117
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:13.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:13.172
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 06/13/23 02:30:13.178
    STEP: patching the Namespace 06/13/23 02:30:13.236
    STEP: get the Namespace and ensuring it has the label 06/13/23 02:30:13.253
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 02:30:13.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4614" for this suite. 06/13/23 02:30:13.301
    STEP: Destroying namespace "nspatchtest-82354319-1abc-4b54-b5c5-9b3e2a0f8e02-4131" for this suite. 06/13/23 02:30:13.321
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:30:13.339
Jun 13 02:30:13.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename resourcequota 06/13/23 02:30:13.341
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:13.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:13.403
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 06/13/23 02:30:13.409
STEP: Creating a ResourceQuota 06/13/23 02:30:18.424
STEP: Ensuring resource quota status is calculated 06/13/23 02:30:18.444
STEP: Creating a Pod that fits quota 06/13/23 02:30:20.455
STEP: Ensuring ResourceQuota status captures the pod usage 06/13/23 02:30:20.492
STEP: Not allowing a pod to be created that exceeds remaining quota 06/13/23 02:30:22.503
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 06/13/23 02:30:22.509
STEP: Ensuring a pod cannot update its resource requirements 06/13/23 02:30:22.516
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 06/13/23 02:30:22.532
STEP: Deleting the pod 06/13/23 02:30:24.54
STEP: Ensuring resource quota status released the pod usage 06/13/23 02:30:24.615
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 13 02:30:26.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6325" for this suite. 06/13/23 02:30:26.653
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":45,"skipped":785,"failed":0}
------------------------------
• [SLOW TEST] [13.330 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:30:13.339
    Jun 13 02:30:13.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename resourcequota 06/13/23 02:30:13.341
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:13.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:13.403
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 06/13/23 02:30:13.409
    STEP: Creating a ResourceQuota 06/13/23 02:30:18.424
    STEP: Ensuring resource quota status is calculated 06/13/23 02:30:18.444
    STEP: Creating a Pod that fits quota 06/13/23 02:30:20.455
    STEP: Ensuring ResourceQuota status captures the pod usage 06/13/23 02:30:20.492
    STEP: Not allowing a pod to be created that exceeds remaining quota 06/13/23 02:30:22.503
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 06/13/23 02:30:22.509
    STEP: Ensuring a pod cannot update its resource requirements 06/13/23 02:30:22.516
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 06/13/23 02:30:22.532
    STEP: Deleting the pod 06/13/23 02:30:24.54
    STEP: Ensuring resource quota status released the pod usage 06/13/23 02:30:24.615
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 13 02:30:26.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6325" for this suite. 06/13/23 02:30:26.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:30:26.67
Jun 13 02:30:26.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pods 06/13/23 02:30:26.672
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:26.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:26.747
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 06/13/23 02:30:26.757
STEP: submitting the pod to kubernetes 06/13/23 02:30:26.757
STEP: verifying QOS class is set on the pod 06/13/23 02:30:26.779
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jun 13 02:30:26.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5578" for this suite. 06/13/23 02:30:26.864
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":46,"skipped":791,"failed":0}
------------------------------
• [0.309 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:30:26.67
    Jun 13 02:30:26.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pods 06/13/23 02:30:26.672
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:26.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:26.747
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 06/13/23 02:30:26.757
    STEP: submitting the pod to kubernetes 06/13/23 02:30:26.757
    STEP: verifying QOS class is set on the pod 06/13/23 02:30:26.779
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jun 13 02:30:26.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5578" for this suite. 06/13/23 02:30:26.864
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:30:26.979
Jun 13 02:30:26.979: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 02:30:26.981
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:27.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:27.122
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 06/13/23 02:30:27.131
Jun 13 02:30:27.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8754 api-versions'
Jun 13 02:30:27.240: INFO: stderr: ""
Jun 13 02:30:27.240: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndata.packaging.carvel.dev/v1alpha1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninternal.packaging.carvel.dev/v1alpha1\nkappctrl.k14s.io/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperator.tigera.io/v1\npackaging.carvel.dev/v1alpha1\npolicy/v1\nprojectcalico.org/v3\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 02:30:27.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8754" for this suite. 06/13/23 02:30:27.252
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":47,"skipped":793,"failed":0}
------------------------------
• [0.297 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:30:26.979
    Jun 13 02:30:26.979: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 02:30:26.981
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:27.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:27.122
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 06/13/23 02:30:27.131
    Jun 13 02:30:27.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8754 api-versions'
    Jun 13 02:30:27.240: INFO: stderr: ""
    Jun 13 02:30:27.240: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndata.packaging.carvel.dev/v1alpha1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\ninternal.packaging.carvel.dev/v1alpha1\nkappctrl.k14s.io/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noperator.tigera.io/v1\npackaging.carvel.dev/v1alpha1\npolicy/v1\nprojectcalico.org/v3\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 02:30:27.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8754" for this suite. 06/13/23 02:30:27.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:30:27.278
Jun 13 02:30:27.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename sysctl 06/13/23 02:30:27.28
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:27.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:27.377
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 06/13/23 02:30:27.384
STEP: Watching for error events or started pod 06/13/23 02:30:27.446
STEP: Waiting for pod completion 06/13/23 02:30:29.456
Jun 13 02:30:29.457: INFO: Waiting up to 3m0s for pod "sysctl-250d5c28-22a3-4911-abc6-388eca09c4fd" in namespace "sysctl-7551" to be "completed"
Jun 13 02:30:29.465: INFO: Pod "sysctl-250d5c28-22a3-4911-abc6-388eca09c4fd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464149ms
Jun 13 02:30:31.478: INFO: Pod "sysctl-250d5c28-22a3-4911-abc6-388eca09c4fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021669402s
Jun 13 02:30:31.479: INFO: Pod "sysctl-250d5c28-22a3-4911-abc6-388eca09c4fd" satisfied condition "completed"
STEP: Checking that the pod succeeded 06/13/23 02:30:31.504
STEP: Getting logs from the pod 06/13/23 02:30:31.504
STEP: Checking that the sysctl is actually updated 06/13/23 02:30:31.53
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 13 02:30:31.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7551" for this suite. 06/13/23 02:30:31.547
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":48,"skipped":806,"failed":0}
------------------------------
• [4.289 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:30:27.278
    Jun 13 02:30:27.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename sysctl 06/13/23 02:30:27.28
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:27.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:27.377
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 06/13/23 02:30:27.384
    STEP: Watching for error events or started pod 06/13/23 02:30:27.446
    STEP: Waiting for pod completion 06/13/23 02:30:29.456
    Jun 13 02:30:29.457: INFO: Waiting up to 3m0s for pod "sysctl-250d5c28-22a3-4911-abc6-388eca09c4fd" in namespace "sysctl-7551" to be "completed"
    Jun 13 02:30:29.465: INFO: Pod "sysctl-250d5c28-22a3-4911-abc6-388eca09c4fd": Phase="Pending", Reason="", readiness=false. Elapsed: 8.464149ms
    Jun 13 02:30:31.478: INFO: Pod "sysctl-250d5c28-22a3-4911-abc6-388eca09c4fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021669402s
    Jun 13 02:30:31.479: INFO: Pod "sysctl-250d5c28-22a3-4911-abc6-388eca09c4fd" satisfied condition "completed"
    STEP: Checking that the pod succeeded 06/13/23 02:30:31.504
    STEP: Getting logs from the pod 06/13/23 02:30:31.504
    STEP: Checking that the sysctl is actually updated 06/13/23 02:30:31.53
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 13 02:30:31.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-7551" for this suite. 06/13/23 02:30:31.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:30:31.571
Jun 13 02:30:31.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 02:30:31.572
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:31.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:31.64
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/13/23 02:30:31.649
Jun 13 02:30:31.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-19 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun 13 02:30:31.765: INFO: stderr: ""
Jun 13 02:30:31.765: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 06/13/23 02:30:31.765
Jun 13 02:30:31.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-19 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jun 13 02:30:33.612: INFO: stderr: ""
Jun 13 02:30:33.612: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/13/23 02:30:33.612
Jun 13 02:30:33.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-19 delete pods e2e-test-httpd-pod'
Jun 13 02:30:35.851: INFO: stderr: ""
Jun 13 02:30:35.851: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 02:30:35.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-19" for this suite. 06/13/23 02:30:35.871
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":49,"skipped":865,"failed":0}
------------------------------
• [4.324 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:30:31.571
    Jun 13 02:30:31.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 02:30:31.572
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:31.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:31.64
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/13/23 02:30:31.649
    Jun 13 02:30:31.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-19 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jun 13 02:30:31.765: INFO: stderr: ""
    Jun 13 02:30:31.765: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 06/13/23 02:30:31.765
    Jun 13 02:30:31.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-19 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jun 13 02:30:33.612: INFO: stderr: ""
    Jun 13 02:30:33.612: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/13/23 02:30:33.612
    Jun 13 02:30:33.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-19 delete pods e2e-test-httpd-pod'
    Jun 13 02:30:35.851: INFO: stderr: ""
    Jun 13 02:30:35.851: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 02:30:35.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-19" for this suite. 06/13/23 02:30:35.871
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:30:35.896
Jun 13 02:30:35.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 02:30:35.898
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:35.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:35.944
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jun 13 02:30:35.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 create -f -'
Jun 13 02:30:37.756: INFO: stderr: ""
Jun 13 02:30:37.756: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun 13 02:30:37.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 create -f -'
Jun 13 02:30:39.592: INFO: stderr: ""
Jun 13 02:30:39.592: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/13/23 02:30:39.592
Jun 13 02:30:40.603: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 13 02:30:40.603: INFO: Found 1 / 1
Jun 13 02:30:40.603: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 13 02:30:40.611: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 13 02:30:40.611: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 13 02:30:40.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 describe pod agnhost-primary-lddpr'
Jun 13 02:30:40.735: INFO: stderr: ""
Jun 13 02:30:40.735: INFO: stdout: "Name:             agnhost-primary-lddpr\nNamespace:        kubectl-8282\nPriority:         0\nService Account:  default\nNode:             sks-test-v1-25-9-workergroup-469fm/10.255.64.102\nStart Time:       Tue, 13 Jun 2023 02:30:37 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: af74f3f45a2f570d4b11a2d073a5e027c6a4595b28817d40188df690ae154ac3\n                  cni.projectcalico.org/podIP: 172.30.77.183/32\n                  cni.projectcalico.org/podIPs: 172.30.77.183/32\nStatus:           Running\nIP:               172.30.77.183\nIPs:\n  IP:           172.30.77.183\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://9d8df42cffea5993a4b7cebef96e96ad8161a561c9ece9dbebd5cd6065010806\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 13 Jun 2023 02:30:39 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bqvjg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bqvjg:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-8282/agnhost-primary-lddpr to sks-test-v1-25-9-workergroup-469fm\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jun 13 02:30:40.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 describe rc agnhost-primary'
Jun 13 02:30:40.898: INFO: stderr: ""
Jun 13 02:30:40.898: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8282\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-lddpr\n"
Jun 13 02:30:40.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 describe service agnhost-primary'
Jun 13 02:30:41.041: INFO: stderr: ""
Jun 13 02:30:41.041: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8282\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.105.63.81\nIPs:               10.105.63.81\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.77.183:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 13 02:30:41.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 describe node sks-test-v1-25-9-controlplane-48kt5'
Jun 13 02:30:41.240: INFO: stderr: ""
Jun 13 02:30:41.240: INFO: stdout: "Name:               sks-test-v1-25-9-controlplane-48kt5\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cape.infrastructure.cluster.x-k8s.io/host-server-id=clb291e3200fg0958s6r9tf79\n                    cape.infrastructure.cluster.x-k8s.io/host-server-name=node20-218\n                    cape.infrastructure.cluster.x-k8s.io/node-group=controlplane\n                    cape.infrastructure.cluster.x-k8s.io/tower-vm-id=clitn1cjm96qe0958r9xwns7x\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=sks-test-v1-25-9-controlplane-48kt5\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        cluster.x-k8s.io/cluster-name: sks-test-v1-25-9\n                    cluster.x-k8s.io/cluster-namespace: default\n                    cluster.x-k8s.io/labels-from-machine: \n                    cluster.x-k8s.io/machine: sks-test-v1-25-9-controlplane-8njc2\n                    cluster.x-k8s.io/owner-kind: KubeadmControlPlane\n                    cluster.x-k8s.io/owner-name: sks-test-v1-25-9-controlplane\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"com.smartx.elf-csi-driver\":\"sks-test-v1-25-9-controlplane-48kt5\",\"csi.tigera.io\":\"sks-test-v1-25-9-controlplane-48kt5\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.255.64.106/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.28.174.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 13 Jun 2023 02:03:59 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  sks-test-v1-25-9-controlplane-48kt5\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 13 Jun 2023 02:30:34 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 13 Jun 2023 02:04:56 +0000   Tue, 13 Jun 2023 02:04:56 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 13 Jun 2023 02:26:05 +0000   Tue, 13 Jun 2023 02:03:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 13 Jun 2023 02:26:05 +0000   Tue, 13 Jun 2023 02:03:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 13 Jun 2023 02:26:05 +0000   Tue, 13 Jun 2023 02:03:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 13 Jun 2023 02:26:05 +0000   Tue, 13 Jun 2023 02:04:50 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.255.64.106\n  Hostname:    sks-test-v1-25-9-controlplane-48kt5\nCapacity:\n  cpu:                8\n  ephemeral-storage:  64198016Ki\n  hugepages-2Mi:      0\n  memory:             7888372Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  59164891448\n  hugepages-2Mi:      0\n  memory:             7785972Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 9caed80b60cc41b3af30818615ec474f\n  System UUID:                9caed80b-60cc-41b3-af30-818615ec474f\n  Boot ID:                    39b1d439-0e97-4b37-b769-767b886a8a23\n  Kernel Version:             4.18.0-372.9.1.el8.x86_64\n  OS Image:                   Rocky Linux 8.6 (Green Obsidian)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.4\n  Kubelet Version:            v1.25.9\n  Kube-Proxy Version:         v1.25.9\nPodCIDR:                      172.16.4.0/24\nPodCIDRs:                     172.16.4.0/24\nProviderID:                   elf://2b4d1514-2c18-49d5-adff-086f74bf074e\nNon-terminated Pods:          (16 in total)\n  Namespace                   Name                                                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                           ------------  ----------  ---------------  -------------  ---\n  calico-apiserver            calico-apiserver-684587d46-b6smw                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  calico-system               calico-kube-controllers-cd6899b-6tz96                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  calico-system               calico-node-pdv4v                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  calico-system               calico-typha-79dfdd7d65-66gp9                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  calico-system               csi-node-driver-xxd6c                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  kube-system                 coredns-688f47b68c-8xzvc                                       100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     29m\n  kube-system                 coredns-688f47b68c-rx4t2                                       100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     29m\n  kube-system                 etcd-sks-test-v1-25-9-controlplane-48kt5                       100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         26m\n  kube-system                 kube-apiserver-sks-test-v1-25-9-controlplane-48kt5             250m (3%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-controller-manager-sks-test-v1-25-9-controlplane-48kt5    200m (2%)     0 (0%)      0 (0%)           0 (0%)         25m\n  kube-system                 kube-proxy-26p4d                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-scheduler-sks-test-v1-25-9-controlplane-48kt5             100m (1%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-vip-sks-test-v1-25-9-controlplane-48kt5                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  sks-system                  smtx-elf-csi-driver-controller-plugin-576c68bb6-hw7b9          510m (6%)     600m (7%)   450Mi (5%)       1050Mi (13%)   25m\n  sks-system                  smtx-elf-csi-driver-node-plugin-82nzd                          210m (2%)     300m (3%)   200Mi (2%)       550Mi (7%)     25m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-2fpgc        0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1570m (19%)  900m (11%)\n  memory             890Mi (11%)  1940Mi (25%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:\n  Type    Reason          Age   From             Message\n  ----    ------          ----  ----             -------\n  Normal  Starting        26m   kube-proxy       \n  Normal  RegisteredNode  26m   node-controller  Node sks-test-v1-25-9-controlplane-48kt5 event: Registered Node sks-test-v1-25-9-controlplane-48kt5 in Controller\n"
Jun 13 02:30:41.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 describe namespace kubectl-8282'
Jun 13 02:30:41.419: INFO: stderr: ""
Jun 13 02:30:41.419: INFO: stdout: "Name:         kubectl-8282\nLabels:       e2e-framework=kubectl\n              e2e-run=be733e5a-f772-479e-b2cf-3373e56afc42\n              kubernetes.io/metadata.name=kubectl-8282\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 02:30:41.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8282" for this suite. 06/13/23 02:30:41.437
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":50,"skipped":893,"failed":0}
------------------------------
• [SLOW TEST] [5.567 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:30:35.896
    Jun 13 02:30:35.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 02:30:35.898
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:35.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:35.944
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jun 13 02:30:35.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 create -f -'
    Jun 13 02:30:37.756: INFO: stderr: ""
    Jun 13 02:30:37.756: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jun 13 02:30:37.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 create -f -'
    Jun 13 02:30:39.592: INFO: stderr: ""
    Jun 13 02:30:39.592: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/13/23 02:30:39.592
    Jun 13 02:30:40.603: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 13 02:30:40.603: INFO: Found 1 / 1
    Jun 13 02:30:40.603: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jun 13 02:30:40.611: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 13 02:30:40.611: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun 13 02:30:40.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 describe pod agnhost-primary-lddpr'
    Jun 13 02:30:40.735: INFO: stderr: ""
    Jun 13 02:30:40.735: INFO: stdout: "Name:             agnhost-primary-lddpr\nNamespace:        kubectl-8282\nPriority:         0\nService Account:  default\nNode:             sks-test-v1-25-9-workergroup-469fm/10.255.64.102\nStart Time:       Tue, 13 Jun 2023 02:30:37 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: af74f3f45a2f570d4b11a2d073a5e027c6a4595b28817d40188df690ae154ac3\n                  cni.projectcalico.org/podIP: 172.30.77.183/32\n                  cni.projectcalico.org/podIPs: 172.30.77.183/32\nStatus:           Running\nIP:               172.30.77.183\nIPs:\n  IP:           172.30.77.183\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://9d8df42cffea5993a4b7cebef96e96ad8161a561c9ece9dbebd5cd6065010806\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 13 Jun 2023 02:30:39 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bqvjg (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bqvjg:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-8282/agnhost-primary-lddpr to sks-test-v1-25-9-workergroup-469fm\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Jun 13 02:30:40.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 describe rc agnhost-primary'
    Jun 13 02:30:40.898: INFO: stderr: ""
    Jun 13 02:30:40.898: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-8282\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-lddpr\n"
    Jun 13 02:30:40.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 describe service agnhost-primary'
    Jun 13 02:30:41.041: INFO: stderr: ""
    Jun 13 02:30:41.041: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-8282\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.105.63.81\nIPs:               10.105.63.81\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.77.183:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jun 13 02:30:41.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 describe node sks-test-v1-25-9-controlplane-48kt5'
    Jun 13 02:30:41.240: INFO: stderr: ""
    Jun 13 02:30:41.240: INFO: stdout: "Name:               sks-test-v1-25-9-controlplane-48kt5\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cape.infrastructure.cluster.x-k8s.io/host-server-id=clb291e3200fg0958s6r9tf79\n                    cape.infrastructure.cluster.x-k8s.io/host-server-name=node20-218\n                    cape.infrastructure.cluster.x-k8s.io/node-group=controlplane\n                    cape.infrastructure.cluster.x-k8s.io/tower-vm-id=clitn1cjm96qe0958r9xwns7x\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=sks-test-v1-25-9-controlplane-48kt5\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        cluster.x-k8s.io/cluster-name: sks-test-v1-25-9\n                    cluster.x-k8s.io/cluster-namespace: default\n                    cluster.x-k8s.io/labels-from-machine: \n                    cluster.x-k8s.io/machine: sks-test-v1-25-9-controlplane-8njc2\n                    cluster.x-k8s.io/owner-kind: KubeadmControlPlane\n                    cluster.x-k8s.io/owner-name: sks-test-v1-25-9-controlplane\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"com.smartx.elf-csi-driver\":\"sks-test-v1-25-9-controlplane-48kt5\",\"csi.tigera.io\":\"sks-test-v1-25-9-controlplane-48kt5\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.255.64.106/16\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.28.174.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 13 Jun 2023 02:03:59 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  sks-test-v1-25-9-controlplane-48kt5\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 13 Jun 2023 02:30:34 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 13 Jun 2023 02:04:56 +0000   Tue, 13 Jun 2023 02:04:56 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 13 Jun 2023 02:26:05 +0000   Tue, 13 Jun 2023 02:03:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 13 Jun 2023 02:26:05 +0000   Tue, 13 Jun 2023 02:03:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 13 Jun 2023 02:26:05 +0000   Tue, 13 Jun 2023 02:03:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 13 Jun 2023 02:26:05 +0000   Tue, 13 Jun 2023 02:04:50 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.255.64.106\n  Hostname:    sks-test-v1-25-9-controlplane-48kt5\nCapacity:\n  cpu:                8\n  ephemeral-storage:  64198016Ki\n  hugepages-2Mi:      0\n  memory:             7888372Ki\n  pods:               110\nAllocatable:\n  cpu:                8\n  ephemeral-storage:  59164891448\n  hugepages-2Mi:      0\n  memory:             7785972Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 9caed80b60cc41b3af30818615ec474f\n  System UUID:                9caed80b-60cc-41b3-af30-818615ec474f\n  Boot ID:                    39b1d439-0e97-4b37-b769-767b886a8a23\n  Kernel Version:             4.18.0-372.9.1.el8.x86_64\n  OS Image:                   Rocky Linux 8.6 (Green Obsidian)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.4\n  Kubelet Version:            v1.25.9\n  Kube-Proxy Version:         v1.25.9\nPodCIDR:                      172.16.4.0/24\nPodCIDRs:                     172.16.4.0/24\nProviderID:                   elf://2b4d1514-2c18-49d5-adff-086f74bf074e\nNon-terminated Pods:          (16 in total)\n  Namespace                   Name                                                           CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                           ------------  ----------  ---------------  -------------  ---\n  calico-apiserver            calico-apiserver-684587d46-b6smw                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  calico-system               calico-kube-controllers-cd6899b-6tz96                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  calico-system               calico-node-pdv4v                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  calico-system               calico-typha-79dfdd7d65-66gp9                                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  calico-system               csi-node-driver-xxd6c                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  kube-system                 coredns-688f47b68c-8xzvc                                       100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     29m\n  kube-system                 coredns-688f47b68c-rx4t2                                       100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     29m\n  kube-system                 etcd-sks-test-v1-25-9-controlplane-48kt5                       100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         26m\n  kube-system                 kube-apiserver-sks-test-v1-25-9-controlplane-48kt5             250m (3%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-controller-manager-sks-test-v1-25-9-controlplane-48kt5    200m (2%)     0 (0%)      0 (0%)           0 (0%)         25m\n  kube-system                 kube-proxy-26p4d                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-scheduler-sks-test-v1-25-9-controlplane-48kt5             100m (1%)     0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 kube-vip-sks-test-v1-25-9-controlplane-48kt5                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m\n  sks-system                  smtx-elf-csi-driver-controller-plugin-576c68bb6-hw7b9          510m (6%)     600m (7%)   450Mi (5%)       1050Mi (13%)   25m\n  sks-system                  smtx-elf-csi-driver-node-plugin-82nzd                          210m (2%)     300m (3%)   200Mi (2%)       550Mi (7%)     25m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-2fpgc        0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1570m (19%)  900m (11%)\n  memory             890Mi (11%)  1940Mi (25%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:\n  Type    Reason          Age   From             Message\n  ----    ------          ----  ----             -------\n  Normal  Starting        26m   kube-proxy       \n  Normal  RegisteredNode  26m   node-controller  Node sks-test-v1-25-9-controlplane-48kt5 event: Registered Node sks-test-v1-25-9-controlplane-48kt5 in Controller\n"
    Jun 13 02:30:41.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-8282 describe namespace kubectl-8282'
    Jun 13 02:30:41.419: INFO: stderr: ""
    Jun 13 02:30:41.419: INFO: stdout: "Name:         kubectl-8282\nLabels:       e2e-framework=kubectl\n              e2e-run=be733e5a-f772-479e-b2cf-3373e56afc42\n              kubernetes.io/metadata.name=kubectl-8282\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 02:30:41.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8282" for this suite. 06/13/23 02:30:41.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:30:41.465
Jun 13 02:30:41.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename subpath 06/13/23 02:30:41.467
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:41.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:41.57
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/13/23 02:30:41.58
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-mwjh 06/13/23 02:30:41.63
STEP: Creating a pod to test atomic-volume-subpath 06/13/23 02:30:41.63
Jun 13 02:30:41.666: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-mwjh" in namespace "subpath-4754" to be "Succeeded or Failed"
Jun 13 02:30:41.683: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Pending", Reason="", readiness=false. Elapsed: 17.697072ms
Jun 13 02:30:43.692: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 2.026457059s
Jun 13 02:30:45.695: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 4.02947139s
Jun 13 02:30:47.695: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 6.029484114s
Jun 13 02:30:49.694: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 8.028714627s
Jun 13 02:30:51.700: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 10.034393484s
Jun 13 02:30:53.696: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 12.030197582s
Jun 13 02:30:55.697: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 14.031735741s
Jun 13 02:30:57.699: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 16.033064828s
Jun 13 02:30:59.693: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 18.027047027s
Jun 13 02:31:01.702: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 20.036098185s
Jun 13 02:31:03.693: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=false. Elapsed: 22.027523856s
Jun 13 02:31:05.694: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.028100115s
STEP: Saw pod success 06/13/23 02:31:05.694
Jun 13 02:31:05.694: INFO: Pod "pod-subpath-test-secret-mwjh" satisfied condition "Succeeded or Failed"
Jun 13 02:31:05.702: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-subpath-test-secret-mwjh container test-container-subpath-secret-mwjh: <nil>
STEP: delete the pod 06/13/23 02:31:05.74
Jun 13 02:31:05.882: INFO: Waiting for pod pod-subpath-test-secret-mwjh to disappear
Jun 13 02:31:05.904: INFO: Pod pod-subpath-test-secret-mwjh no longer exists
STEP: Deleting pod pod-subpath-test-secret-mwjh 06/13/23 02:31:05.904
Jun 13 02:31:05.904: INFO: Deleting pod "pod-subpath-test-secret-mwjh" in namespace "subpath-4754"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 13 02:31:05.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4754" for this suite. 06/13/23 02:31:05.938
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":51,"skipped":931,"failed":0}
------------------------------
• [SLOW TEST] [24.503 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:30:41.465
    Jun 13 02:30:41.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename subpath 06/13/23 02:30:41.467
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:30:41.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:30:41.57
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/13/23 02:30:41.58
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-mwjh 06/13/23 02:30:41.63
    STEP: Creating a pod to test atomic-volume-subpath 06/13/23 02:30:41.63
    Jun 13 02:30:41.666: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-mwjh" in namespace "subpath-4754" to be "Succeeded or Failed"
    Jun 13 02:30:41.683: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Pending", Reason="", readiness=false. Elapsed: 17.697072ms
    Jun 13 02:30:43.692: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 2.026457059s
    Jun 13 02:30:45.695: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 4.02947139s
    Jun 13 02:30:47.695: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 6.029484114s
    Jun 13 02:30:49.694: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 8.028714627s
    Jun 13 02:30:51.700: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 10.034393484s
    Jun 13 02:30:53.696: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 12.030197582s
    Jun 13 02:30:55.697: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 14.031735741s
    Jun 13 02:30:57.699: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 16.033064828s
    Jun 13 02:30:59.693: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 18.027047027s
    Jun 13 02:31:01.702: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=true. Elapsed: 20.036098185s
    Jun 13 02:31:03.693: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Running", Reason="", readiness=false. Elapsed: 22.027523856s
    Jun 13 02:31:05.694: INFO: Pod "pod-subpath-test-secret-mwjh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.028100115s
    STEP: Saw pod success 06/13/23 02:31:05.694
    Jun 13 02:31:05.694: INFO: Pod "pod-subpath-test-secret-mwjh" satisfied condition "Succeeded or Failed"
    Jun 13 02:31:05.702: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-subpath-test-secret-mwjh container test-container-subpath-secret-mwjh: <nil>
    STEP: delete the pod 06/13/23 02:31:05.74
    Jun 13 02:31:05.882: INFO: Waiting for pod pod-subpath-test-secret-mwjh to disappear
    Jun 13 02:31:05.904: INFO: Pod pod-subpath-test-secret-mwjh no longer exists
    STEP: Deleting pod pod-subpath-test-secret-mwjh 06/13/23 02:31:05.904
    Jun 13 02:31:05.904: INFO: Deleting pod "pod-subpath-test-secret-mwjh" in namespace "subpath-4754"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 13 02:31:05.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-4754" for this suite. 06/13/23 02:31:05.938
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:31:05.97
Jun 13 02:31:05.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 02:31:05.972
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:31:06.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:31:06.038
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-5410/secret-test-11e7293b-678c-415c-b7d4-f9e40024a608 06/13/23 02:31:06.049
STEP: Creating a pod to test consume secrets 06/13/23 02:31:06.065
Jun 13 02:31:06.093: INFO: Waiting up to 5m0s for pod "pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2" in namespace "secrets-5410" to be "Succeeded or Failed"
Jun 13 02:31:06.104: INFO: Pod "pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.82522ms
Jun 13 02:31:08.115: INFO: Pod "pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021882953s
Jun 13 02:31:10.115: INFO: Pod "pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021961761s
STEP: Saw pod success 06/13/23 02:31:10.115
Jun 13 02:31:10.115: INFO: Pod "pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2" satisfied condition "Succeeded or Failed"
Jun 13 02:31:10.122: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2 container env-test: <nil>
STEP: delete the pod 06/13/23 02:31:10.137
Jun 13 02:31:10.167: INFO: Waiting for pod pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2 to disappear
Jun 13 02:31:10.175: INFO: Pod pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jun 13 02:31:10.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5410" for this suite. 06/13/23 02:31:10.195
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":52,"skipped":958,"failed":0}
------------------------------
• [4.249 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:31:05.97
    Jun 13 02:31:05.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 02:31:05.972
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:31:06.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:31:06.038
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-5410/secret-test-11e7293b-678c-415c-b7d4-f9e40024a608 06/13/23 02:31:06.049
    STEP: Creating a pod to test consume secrets 06/13/23 02:31:06.065
    Jun 13 02:31:06.093: INFO: Waiting up to 5m0s for pod "pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2" in namespace "secrets-5410" to be "Succeeded or Failed"
    Jun 13 02:31:06.104: INFO: Pod "pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.82522ms
    Jun 13 02:31:08.115: INFO: Pod "pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021882953s
    Jun 13 02:31:10.115: INFO: Pod "pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021961761s
    STEP: Saw pod success 06/13/23 02:31:10.115
    Jun 13 02:31:10.115: INFO: Pod "pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2" satisfied condition "Succeeded or Failed"
    Jun 13 02:31:10.122: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2 container env-test: <nil>
    STEP: delete the pod 06/13/23 02:31:10.137
    Jun 13 02:31:10.167: INFO: Waiting for pod pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2 to disappear
    Jun 13 02:31:10.175: INFO: Pod pod-configmaps-27177281-d9b6-4fbf-a350-19fbe1e0fbe2 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 02:31:10.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5410" for this suite. 06/13/23 02:31:10.195
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:31:10.22
Jun 13 02:31:10.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubelet-test 06/13/23 02:31:10.221
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:31:10.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:31:10.265
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 06/13/23 02:31:10.291
Jun 13 02:31:10.291: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases356d92e7-7bb1-441c-b5c6-f0ce3163b9ce" in namespace "kubelet-test-4912" to be "completed"
Jun 13 02:31:10.331: INFO: Pod "agnhost-host-aliases356d92e7-7bb1-441c-b5c6-f0ce3163b9ce": Phase="Pending", Reason="", readiness=false. Elapsed: 39.890011ms
Jun 13 02:31:12.342: INFO: Pod "agnhost-host-aliases356d92e7-7bb1-441c-b5c6-f0ce3163b9ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050435421s
Jun 13 02:31:14.349: INFO: Pod "agnhost-host-aliases356d92e7-7bb1-441c-b5c6-f0ce3163b9ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057505314s
Jun 13 02:31:14.349: INFO: Pod "agnhost-host-aliases356d92e7-7bb1-441c-b5c6-f0ce3163b9ce" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 13 02:31:14.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4912" for this suite. 06/13/23 02:31:14.381
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":53,"skipped":961,"failed":0}
------------------------------
• [4.189 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:31:10.22
    Jun 13 02:31:10.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubelet-test 06/13/23 02:31:10.221
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:31:10.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:31:10.265
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 06/13/23 02:31:10.291
    Jun 13 02:31:10.291: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases356d92e7-7bb1-441c-b5c6-f0ce3163b9ce" in namespace "kubelet-test-4912" to be "completed"
    Jun 13 02:31:10.331: INFO: Pod "agnhost-host-aliases356d92e7-7bb1-441c-b5c6-f0ce3163b9ce": Phase="Pending", Reason="", readiness=false. Elapsed: 39.890011ms
    Jun 13 02:31:12.342: INFO: Pod "agnhost-host-aliases356d92e7-7bb1-441c-b5c6-f0ce3163b9ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050435421s
    Jun 13 02:31:14.349: INFO: Pod "agnhost-host-aliases356d92e7-7bb1-441c-b5c6-f0ce3163b9ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.057505314s
    Jun 13 02:31:14.349: INFO: Pod "agnhost-host-aliases356d92e7-7bb1-441c-b5c6-f0ce3163b9ce" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 13 02:31:14.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4912" for this suite. 06/13/23 02:31:14.381
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:31:14.409
Jun 13 02:31:14.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 02:31:14.411
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:31:14.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:31:14.456
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 06/13/23 02:31:14.464
Jun 13 02:31:14.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4132 create -f -'
Jun 13 02:31:14.926: INFO: stderr: ""
Jun 13 02:31:14.926: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 06/13/23 02:31:14.926
Jun 13 02:31:14.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4132 diff -f -'
Jun 13 02:31:15.369: INFO: rc: 1
Jun 13 02:31:15.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4132 delete -f -'
Jun 13 02:31:15.551: INFO: stderr: ""
Jun 13 02:31:15.551: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 02:31:15.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4132" for this suite. 06/13/23 02:31:15.573
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":54,"skipped":964,"failed":0}
------------------------------
• [1.186 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:31:14.409
    Jun 13 02:31:14.410: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 02:31:14.411
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:31:14.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:31:14.456
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 06/13/23 02:31:14.464
    Jun 13 02:31:14.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4132 create -f -'
    Jun 13 02:31:14.926: INFO: stderr: ""
    Jun 13 02:31:14.926: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 06/13/23 02:31:14.926
    Jun 13 02:31:14.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4132 diff -f -'
    Jun 13 02:31:15.369: INFO: rc: 1
    Jun 13 02:31:15.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4132 delete -f -'
    Jun 13 02:31:15.551: INFO: stderr: ""
    Jun 13 02:31:15.551: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 02:31:15.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4132" for this suite. 06/13/23 02:31:15.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:31:15.599
Jun 13 02:31:15.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 02:31:15.602
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:31:15.648
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:31:15.667
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-82ff5248-e18c-44d4-b56f-dd7dc30e8021 06/13/23 02:31:15.694
STEP: Creating secret with name s-test-opt-upd-dc3a14f6-02d3-49e4-b412-bb52c9cb1b9b 06/13/23 02:31:15.709
STEP: Creating the pod 06/13/23 02:31:15.726
Jun 13 02:31:15.768: INFO: Waiting up to 5m0s for pod "pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a" in namespace "secrets-9777" to be "running and ready"
Jun 13 02:31:15.780: INFO: Pod "pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.761391ms
Jun 13 02:31:15.780: INFO: The phase of Pod pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:31:17.799: INFO: Pod "pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030672017s
Jun 13 02:31:17.799: INFO: The phase of Pod pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:31:19.790: INFO: Pod "pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a": Phase="Running", Reason="", readiness=true. Elapsed: 4.021599444s
Jun 13 02:31:19.790: INFO: The phase of Pod pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a is Running (Ready = true)
Jun 13 02:31:19.790: INFO: Pod "pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-82ff5248-e18c-44d4-b56f-dd7dc30e8021 06/13/23 02:31:19.9
STEP: Updating secret s-test-opt-upd-dc3a14f6-02d3-49e4-b412-bb52c9cb1b9b 06/13/23 02:31:19.934
STEP: Creating secret with name s-test-opt-create-6b1e0c98-2070-44ba-8990-273400dca807 06/13/23 02:31:19.961
STEP: waiting to observe update in volume 06/13/23 02:31:19.982
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 13 02:32:34.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9777" for this suite. 06/13/23 02:32:34.929
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":55,"skipped":1014,"failed":0}
------------------------------
• [SLOW TEST] [79.355 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:31:15.599
    Jun 13 02:31:15.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 02:31:15.602
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:31:15.648
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:31:15.667
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-82ff5248-e18c-44d4-b56f-dd7dc30e8021 06/13/23 02:31:15.694
    STEP: Creating secret with name s-test-opt-upd-dc3a14f6-02d3-49e4-b412-bb52c9cb1b9b 06/13/23 02:31:15.709
    STEP: Creating the pod 06/13/23 02:31:15.726
    Jun 13 02:31:15.768: INFO: Waiting up to 5m0s for pod "pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a" in namespace "secrets-9777" to be "running and ready"
    Jun 13 02:31:15.780: INFO: Pod "pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.761391ms
    Jun 13 02:31:15.780: INFO: The phase of Pod pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:31:17.799: INFO: Pod "pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030672017s
    Jun 13 02:31:17.799: INFO: The phase of Pod pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:31:19.790: INFO: Pod "pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a": Phase="Running", Reason="", readiness=true. Elapsed: 4.021599444s
    Jun 13 02:31:19.790: INFO: The phase of Pod pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a is Running (Ready = true)
    Jun 13 02:31:19.790: INFO: Pod "pod-secrets-f0be7e93-7d7c-429f-a880-51e44da32d3a" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-82ff5248-e18c-44d4-b56f-dd7dc30e8021 06/13/23 02:31:19.9
    STEP: Updating secret s-test-opt-upd-dc3a14f6-02d3-49e4-b412-bb52c9cb1b9b 06/13/23 02:31:19.934
    STEP: Creating secret with name s-test-opt-create-6b1e0c98-2070-44ba-8990-273400dca807 06/13/23 02:31:19.961
    STEP: waiting to observe update in volume 06/13/23 02:31:19.982
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 02:32:34.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9777" for this suite. 06/13/23 02:32:34.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:32:34.955
Jun 13 02:32:34.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 02:32:34.958
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:32:34.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:32:35.001
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 06/13/23 02:32:35.013
Jun 13 02:32:35.036: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564" in namespace "downward-api-8507" to be "Succeeded or Failed"
Jun 13 02:32:35.072: INFO: Pod "downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564": Phase="Pending", Reason="", readiness=false. Elapsed: 35.914656ms
Jun 13 02:32:37.083: INFO: Pod "downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564": Phase="Running", Reason="", readiness=true. Elapsed: 2.046633933s
Jun 13 02:32:39.083: INFO: Pod "downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564": Phase="Running", Reason="", readiness=false. Elapsed: 4.046747279s
Jun 13 02:32:41.249: INFO: Pod "downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.212897966s
STEP: Saw pod success 06/13/23 02:32:41.249
Jun 13 02:32:41.249: INFO: Pod "downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564" satisfied condition "Succeeded or Failed"
Jun 13 02:32:41.263: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564 container client-container: <nil>
STEP: delete the pod 06/13/23 02:32:41.289
Jun 13 02:32:41.312: INFO: Waiting for pod downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564 to disappear
Jun 13 02:32:41.332: INFO: Pod downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 13 02:32:41.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8507" for this suite. 06/13/23 02:32:41.344
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":56,"skipped":1023,"failed":0}
------------------------------
• [SLOW TEST] [6.406 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:32:34.955
    Jun 13 02:32:34.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 02:32:34.958
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:32:34.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:32:35.001
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 06/13/23 02:32:35.013
    Jun 13 02:32:35.036: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564" in namespace "downward-api-8507" to be "Succeeded or Failed"
    Jun 13 02:32:35.072: INFO: Pod "downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564": Phase="Pending", Reason="", readiness=false. Elapsed: 35.914656ms
    Jun 13 02:32:37.083: INFO: Pod "downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564": Phase="Running", Reason="", readiness=true. Elapsed: 2.046633933s
    Jun 13 02:32:39.083: INFO: Pod "downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564": Phase="Running", Reason="", readiness=false. Elapsed: 4.046747279s
    Jun 13 02:32:41.249: INFO: Pod "downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.212897966s
    STEP: Saw pod success 06/13/23 02:32:41.249
    Jun 13 02:32:41.249: INFO: Pod "downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564" satisfied condition "Succeeded or Failed"
    Jun 13 02:32:41.263: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564 container client-container: <nil>
    STEP: delete the pod 06/13/23 02:32:41.289
    Jun 13 02:32:41.312: INFO: Waiting for pod downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564 to disappear
    Jun 13 02:32:41.332: INFO: Pod downwardapi-volume-d3225ca4-87ce-4d37-b631-549c4db5b564 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 13 02:32:41.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8507" for this suite. 06/13/23 02:32:41.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:32:41.362
Jun 13 02:32:41.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename disruption 06/13/23 02:32:41.364
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:32:41.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:32:41.434
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 06/13/23 02:32:41.457
STEP: Waiting for all pods to be running 06/13/23 02:32:43.549
Jun 13 02:32:43.566: INFO: running pods: 0 < 3
Jun 13 02:32:45.580: INFO: running pods: 1 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 13 02:32:47.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-5707" for this suite. 06/13/23 02:32:47.593
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":57,"skipped":1045,"failed":0}
------------------------------
• [SLOW TEST] [6.250 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:32:41.362
    Jun 13 02:32:41.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename disruption 06/13/23 02:32:41.364
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:32:41.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:32:41.434
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 06/13/23 02:32:41.457
    STEP: Waiting for all pods to be running 06/13/23 02:32:43.549
    Jun 13 02:32:43.566: INFO: running pods: 0 < 3
    Jun 13 02:32:45.580: INFO: running pods: 1 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 13 02:32:47.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-5707" for this suite. 06/13/23 02:32:47.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:32:47.613
Jun 13 02:32:47.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 02:32:47.614
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:32:47.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:32:47.648
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-3266 06/13/23 02:32:47.656
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3266 to expose endpoints map[] 06/13/23 02:32:47.723
Jun 13 02:32:47.750: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jun 13 02:32:48.771: INFO: successfully validated that service endpoint-test2 in namespace services-3266 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-3266 06/13/23 02:32:48.771
Jun 13 02:32:48.829: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3266" to be "running and ready"
Jun 13 02:32:48.846: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.355534ms
Jun 13 02:32:48.847: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:32:50.954: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.124871704s
Jun 13 02:32:50.954: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:32:52.859: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.03033276s
Jun 13 02:32:52.860: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun 13 02:32:52.860: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3266 to expose endpoints map[pod1:[80]] 06/13/23 02:32:52.869
Jun 13 02:32:52.908: INFO: successfully validated that service endpoint-test2 in namespace services-3266 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 06/13/23 02:32:52.908
Jun 13 02:32:52.908: INFO: Creating new exec pod
Jun 13 02:32:52.920: INFO: Waiting up to 5m0s for pod "execpodllb4f" in namespace "services-3266" to be "running"
Jun 13 02:32:52.942: INFO: Pod "execpodllb4f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.778789ms
Jun 13 02:32:54.953: INFO: Pod "execpodllb4f": Phase="Running", Reason="", readiness=true. Elapsed: 2.033071717s
Jun 13 02:32:54.953: INFO: Pod "execpodllb4f" satisfied condition "running"
Jun 13 02:32:55.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-3266 exec execpodllb4f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 13 02:32:56.228: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 13 02:32:56.229: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 02:32:56.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-3266 exec execpodllb4f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.144.188 80'
Jun 13 02:32:56.462: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.144.188 80\nConnection to 10.105.144.188 80 port [tcp/http] succeeded!\n"
Jun 13 02:32:56.462: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-3266 06/13/23 02:32:56.462
Jun 13 02:32:56.474: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3266" to be "running and ready"
Jun 13 02:32:56.484: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.642408ms
Jun 13 02:32:56.484: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:32:58.497: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.023310886s
Jun 13 02:32:58.497: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun 13 02:32:58.497: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3266 to expose endpoints map[pod1:[80] pod2:[80]] 06/13/23 02:32:58.512
Jun 13 02:32:58.559: INFO: successfully validated that service endpoint-test2 in namespace services-3266 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 06/13/23 02:32:58.559
Jun 13 02:32:59.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-3266 exec execpodllb4f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 13 02:32:59.808: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 13 02:32:59.808: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 02:32:59.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-3266 exec execpodllb4f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.144.188 80'
Jun 13 02:33:00.129: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.144.188 80\nConnection to 10.105.144.188 80 port [tcp/http] succeeded!\n"
Jun 13 02:33:00.129: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-3266 06/13/23 02:33:00.129
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3266 to expose endpoints map[pod2:[80]] 06/13/23 02:33:00.229
Jun 13 02:33:00.567: INFO: successfully validated that service endpoint-test2 in namespace services-3266 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 06/13/23 02:33:00.567
Jun 13 02:33:01.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-3266 exec execpodllb4f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 13 02:33:01.825: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 13 02:33:01.825: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 02:33:01.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-3266 exec execpodllb4f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.144.188 80'
Jun 13 02:33:02.073: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.144.188 80\nConnection to 10.105.144.188 80 port [tcp/http] succeeded!\n"
Jun 13 02:33:02.073: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-3266 06/13/23 02:33:02.073
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3266 to expose endpoints map[] 06/13/23 02:33:02.133
Jun 13 02:33:02.161: INFO: successfully validated that service endpoint-test2 in namespace services-3266 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 02:33:02.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3266" for this suite. 06/13/23 02:33:02.246
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":58,"skipped":1054,"failed":0}
------------------------------
• [SLOW TEST] [14.653 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:32:47.613
    Jun 13 02:32:47.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 02:32:47.614
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:32:47.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:32:47.648
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-3266 06/13/23 02:32:47.656
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3266 to expose endpoints map[] 06/13/23 02:32:47.723
    Jun 13 02:32:47.750: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jun 13 02:32:48.771: INFO: successfully validated that service endpoint-test2 in namespace services-3266 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-3266 06/13/23 02:32:48.771
    Jun 13 02:32:48.829: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-3266" to be "running and ready"
    Jun 13 02:32:48.846: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.355534ms
    Jun 13 02:32:48.847: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:32:50.954: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.124871704s
    Jun 13 02:32:50.954: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:32:52.859: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.03033276s
    Jun 13 02:32:52.860: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun 13 02:32:52.860: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3266 to expose endpoints map[pod1:[80]] 06/13/23 02:32:52.869
    Jun 13 02:32:52.908: INFO: successfully validated that service endpoint-test2 in namespace services-3266 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 06/13/23 02:32:52.908
    Jun 13 02:32:52.908: INFO: Creating new exec pod
    Jun 13 02:32:52.920: INFO: Waiting up to 5m0s for pod "execpodllb4f" in namespace "services-3266" to be "running"
    Jun 13 02:32:52.942: INFO: Pod "execpodllb4f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.778789ms
    Jun 13 02:32:54.953: INFO: Pod "execpodllb4f": Phase="Running", Reason="", readiness=true. Elapsed: 2.033071717s
    Jun 13 02:32:54.953: INFO: Pod "execpodllb4f" satisfied condition "running"
    Jun 13 02:32:55.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-3266 exec execpodllb4f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jun 13 02:32:56.228: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun 13 02:32:56.229: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 02:32:56.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-3266 exec execpodllb4f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.144.188 80'
    Jun 13 02:32:56.462: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.144.188 80\nConnection to 10.105.144.188 80 port [tcp/http] succeeded!\n"
    Jun 13 02:32:56.462: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-3266 06/13/23 02:32:56.462
    Jun 13 02:32:56.474: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-3266" to be "running and ready"
    Jun 13 02:32:56.484: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.642408ms
    Jun 13 02:32:56.484: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:32:58.497: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.023310886s
    Jun 13 02:32:58.497: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun 13 02:32:58.497: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3266 to expose endpoints map[pod1:[80] pod2:[80]] 06/13/23 02:32:58.512
    Jun 13 02:32:58.559: INFO: successfully validated that service endpoint-test2 in namespace services-3266 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 06/13/23 02:32:58.559
    Jun 13 02:32:59.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-3266 exec execpodllb4f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jun 13 02:32:59.808: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun 13 02:32:59.808: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 02:32:59.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-3266 exec execpodllb4f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.144.188 80'
    Jun 13 02:33:00.129: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.144.188 80\nConnection to 10.105.144.188 80 port [tcp/http] succeeded!\n"
    Jun 13 02:33:00.129: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-3266 06/13/23 02:33:00.129
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3266 to expose endpoints map[pod2:[80]] 06/13/23 02:33:00.229
    Jun 13 02:33:00.567: INFO: successfully validated that service endpoint-test2 in namespace services-3266 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 06/13/23 02:33:00.567
    Jun 13 02:33:01.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-3266 exec execpodllb4f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jun 13 02:33:01.825: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun 13 02:33:01.825: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 02:33:01.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-3266 exec execpodllb4f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.144.188 80'
    Jun 13 02:33:02.073: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.144.188 80\nConnection to 10.105.144.188 80 port [tcp/http] succeeded!\n"
    Jun 13 02:33:02.073: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-3266 06/13/23 02:33:02.073
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3266 to expose endpoints map[] 06/13/23 02:33:02.133
    Jun 13 02:33:02.161: INFO: successfully validated that service endpoint-test2 in namespace services-3266 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 02:33:02.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3266" for this suite. 06/13/23 02:33:02.246
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:33:02.267
Jun 13 02:33:02.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename statefulset 06/13/23 02:33:02.271
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:33:02.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:33:02.33
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2655 06/13/23 02:33:02.336
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-2655 06/13/23 02:33:02.354
Jun 13 02:33:02.391: INFO: Found 0 stateful pods, waiting for 1
Jun 13 02:33:12.405: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 06/13/23 02:33:12.436
STEP: updating a scale subresource 06/13/23 02:33:12.446
STEP: verifying the statefulset Spec.Replicas was modified 06/13/23 02:33:12.463
STEP: Patch a scale subresource 06/13/23 02:33:12.498
STEP: verifying the statefulset Spec.Replicas was modified 06/13/23 02:33:12.533
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 13 02:33:12.544: INFO: Deleting all statefulset in ns statefulset-2655
Jun 13 02:33:12.555: INFO: Scaling statefulset ss to 0
Jun 13 02:33:22.619: INFO: Waiting for statefulset status.replicas updated to 0
Jun 13 02:33:22.626: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 13 02:33:22.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2655" for this suite. 06/13/23 02:33:22.694
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":59,"skipped":1078,"failed":0}
------------------------------
• [SLOW TEST] [20.450 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:33:02.267
    Jun 13 02:33:02.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename statefulset 06/13/23 02:33:02.271
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:33:02.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:33:02.33
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2655 06/13/23 02:33:02.336
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-2655 06/13/23 02:33:02.354
    Jun 13 02:33:02.391: INFO: Found 0 stateful pods, waiting for 1
    Jun 13 02:33:12.405: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 06/13/23 02:33:12.436
    STEP: updating a scale subresource 06/13/23 02:33:12.446
    STEP: verifying the statefulset Spec.Replicas was modified 06/13/23 02:33:12.463
    STEP: Patch a scale subresource 06/13/23 02:33:12.498
    STEP: verifying the statefulset Spec.Replicas was modified 06/13/23 02:33:12.533
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 13 02:33:12.544: INFO: Deleting all statefulset in ns statefulset-2655
    Jun 13 02:33:12.555: INFO: Scaling statefulset ss to 0
    Jun 13 02:33:22.619: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 13 02:33:22.626: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 13 02:33:22.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2655" for this suite. 06/13/23 02:33:22.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:33:22.72
Jun 13 02:33:22.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename sched-preemption 06/13/23 02:33:22.721
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:33:22.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:33:22.76
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 13 02:33:22.802: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 13 02:34:22.894: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 06/13/23 02:34:22.905
Jun 13 02:34:22.962: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun 13 02:34:22.985: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun 13 02:34:23.023: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun 13 02:34:23.041: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun 13 02:34:23.078: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun 13 02:34:23.100: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 06/13/23 02:34:23.1
Jun 13 02:34:23.100: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1360" to be "running"
Jun 13 02:34:23.124: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 23.783943ms
Jun 13 02:34:25.136: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035159378s
Jun 13 02:34:27.136: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036149709s
Jun 13 02:34:29.136: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036153522s
Jun 13 02:34:31.136: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.035272207s
Jun 13 02:34:31.136: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jun 13 02:34:31.136: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1360" to be "running"
Jun 13 02:34:31.143: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.108594ms
Jun 13 02:34:31.143: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 13 02:34:31.143: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1360" to be "running"
Jun 13 02:34:31.149: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.929389ms
Jun 13 02:34:33.177: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034524518s
Jun 13 02:34:35.171: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.028127829s
Jun 13 02:34:35.171: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 13 02:34:35.171: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1360" to be "running"
Jun 13 02:34:35.181: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.480396ms
Jun 13 02:34:35.181: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 13 02:34:35.181: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1360" to be "running"
Jun 13 02:34:35.198: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.406212ms
Jun 13 02:34:35.198: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 13 02:34:35.198: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1360" to be "running"
Jun 13 02:34:35.216: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.515739ms
Jun 13 02:34:35.216: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 06/13/23 02:34:35.216
Jun 13 02:34:35.306: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jun 13 02:34:35.315: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.936117ms
Jun 13 02:34:37.324: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017906598s
Jun 13 02:34:39.323: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016826777s
Jun 13 02:34:41.328: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.021691545s
Jun 13 02:34:41.328: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jun 13 02:34:41.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1360" for this suite. 06/13/23 02:34:41.576
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":60,"skipped":1110,"failed":0}
------------------------------
• [SLOW TEST] [79.069 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:33:22.72
    Jun 13 02:33:22.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename sched-preemption 06/13/23 02:33:22.721
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:33:22.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:33:22.76
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jun 13 02:33:22.802: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 13 02:34:22.894: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 06/13/23 02:34:22.905
    Jun 13 02:34:22.962: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jun 13 02:34:22.985: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jun 13 02:34:23.023: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jun 13 02:34:23.041: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jun 13 02:34:23.078: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jun 13 02:34:23.100: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 06/13/23 02:34:23.1
    Jun 13 02:34:23.100: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-1360" to be "running"
    Jun 13 02:34:23.124: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 23.783943ms
    Jun 13 02:34:25.136: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035159378s
    Jun 13 02:34:27.136: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036149709s
    Jun 13 02:34:29.136: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036153522s
    Jun 13 02:34:31.136: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.035272207s
    Jun 13 02:34:31.136: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jun 13 02:34:31.136: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-1360" to be "running"
    Jun 13 02:34:31.143: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.108594ms
    Jun 13 02:34:31.143: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 13 02:34:31.143: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-1360" to be "running"
    Jun 13 02:34:31.149: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.929389ms
    Jun 13 02:34:33.177: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034524518s
    Jun 13 02:34:35.171: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.028127829s
    Jun 13 02:34:35.171: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 13 02:34:35.171: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-1360" to be "running"
    Jun 13 02:34:35.181: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 9.480396ms
    Jun 13 02:34:35.181: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 13 02:34:35.181: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-1360" to be "running"
    Jun 13 02:34:35.198: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.406212ms
    Jun 13 02:34:35.198: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 13 02:34:35.198: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-1360" to be "running"
    Jun 13 02:34:35.216: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.515739ms
    Jun 13 02:34:35.216: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 06/13/23 02:34:35.216
    Jun 13 02:34:35.306: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jun 13 02:34:35.315: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.936117ms
    Jun 13 02:34:37.324: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017906598s
    Jun 13 02:34:39.323: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016826777s
    Jun 13 02:34:41.328: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.021691545s
    Jun 13 02:34:41.328: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 02:34:41.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-1360" for this suite. 06/13/23 02:34:41.576
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:34:41.79
Jun 13 02:34:41.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename resourcequota 06/13/23 02:34:41.792
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:34:41.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:34:41.855
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 06/13/23 02:34:41.863
STEP: Creating a ResourceQuota 06/13/23 02:34:46.892
STEP: Ensuring resource quota status is calculated 06/13/23 02:34:46.908
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 13 02:34:48.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2100" for this suite. 06/13/23 02:34:49.001
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":61,"skipped":1136,"failed":0}
------------------------------
• [SLOW TEST] [7.230 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:34:41.79
    Jun 13 02:34:41.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename resourcequota 06/13/23 02:34:41.792
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:34:41.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:34:41.855
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 06/13/23 02:34:41.863
    STEP: Creating a ResourceQuota 06/13/23 02:34:46.892
    STEP: Ensuring resource quota status is calculated 06/13/23 02:34:46.908
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 13 02:34:48.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2100" for this suite. 06/13/23 02:34:49.001
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:34:49.021
Jun 13 02:34:49.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename var-expansion 06/13/23 02:34:49.023
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:34:49.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:34:49.075
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 06/13/23 02:34:49.087
Jun 13 02:34:49.111: INFO: Waiting up to 5m0s for pod "var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235" in namespace "var-expansion-1741" to be "Succeeded or Failed"
Jun 13 02:34:49.130: INFO: Pod "var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235": Phase="Pending", Reason="", readiness=false. Elapsed: 19.862093ms
Jun 13 02:34:51.139: INFO: Pod "var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028323126s
Jun 13 02:34:53.139: INFO: Pod "var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028604194s
STEP: Saw pod success 06/13/23 02:34:53.139
Jun 13 02:34:53.140: INFO: Pod "var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235" satisfied condition "Succeeded or Failed"
Jun 13 02:34:53.155: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235 container dapi-container: <nil>
STEP: delete the pod 06/13/23 02:34:53.196
Jun 13 02:34:53.341: INFO: Waiting for pod var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235 to disappear
Jun 13 02:34:53.363: INFO: Pod var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 13 02:34:53.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1741" for this suite. 06/13/23 02:34:53.381
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":62,"skipped":1153,"failed":0}
------------------------------
• [4.379 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:34:49.021
    Jun 13 02:34:49.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename var-expansion 06/13/23 02:34:49.023
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:34:49.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:34:49.075
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 06/13/23 02:34:49.087
    Jun 13 02:34:49.111: INFO: Waiting up to 5m0s for pod "var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235" in namespace "var-expansion-1741" to be "Succeeded or Failed"
    Jun 13 02:34:49.130: INFO: Pod "var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235": Phase="Pending", Reason="", readiness=false. Elapsed: 19.862093ms
    Jun 13 02:34:51.139: INFO: Pod "var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028323126s
    Jun 13 02:34:53.139: INFO: Pod "var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028604194s
    STEP: Saw pod success 06/13/23 02:34:53.139
    Jun 13 02:34:53.140: INFO: Pod "var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235" satisfied condition "Succeeded or Failed"
    Jun 13 02:34:53.155: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235 container dapi-container: <nil>
    STEP: delete the pod 06/13/23 02:34:53.196
    Jun 13 02:34:53.341: INFO: Waiting for pod var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235 to disappear
    Jun 13 02:34:53.363: INFO: Pod var-expansion-2e4c7adc-33ef-45d6-9a9f-45c04cc52235 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 13 02:34:53.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1741" for this suite. 06/13/23 02:34:53.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:34:53.403
Jun 13 02:34:53.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename resourcequota 06/13/23 02:34:53.404
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:34:53.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:34:53.501
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 06/13/23 02:34:53.51
STEP: Getting a ResourceQuota 06/13/23 02:34:53.545
STEP: Listing all ResourceQuotas with LabelSelector 06/13/23 02:34:53.553
STEP: Patching the ResourceQuota 06/13/23 02:34:53.562
STEP: Deleting a Collection of ResourceQuotas 06/13/23 02:34:53.583
STEP: Verifying the deleted ResourceQuota 06/13/23 02:34:53.611
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 13 02:34:53.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6077" for this suite. 06/13/23 02:34:53.641
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":63,"skipped":1220,"failed":0}
------------------------------
• [0.260 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:34:53.403
    Jun 13 02:34:53.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename resourcequota 06/13/23 02:34:53.404
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:34:53.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:34:53.501
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 06/13/23 02:34:53.51
    STEP: Getting a ResourceQuota 06/13/23 02:34:53.545
    STEP: Listing all ResourceQuotas with LabelSelector 06/13/23 02:34:53.553
    STEP: Patching the ResourceQuota 06/13/23 02:34:53.562
    STEP: Deleting a Collection of ResourceQuotas 06/13/23 02:34:53.583
    STEP: Verifying the deleted ResourceQuota 06/13/23 02:34:53.611
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 13 02:34:53.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6077" for this suite. 06/13/23 02:34:53.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:34:53.666
Jun 13 02:34:53.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename var-expansion 06/13/23 02:34:53.668
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:34:53.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:34:53.724
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 06/13/23 02:34:53.733
STEP: waiting for pod running 06/13/23 02:34:53.784
Jun 13 02:34:53.784: INFO: Waiting up to 2m0s for pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07" in namespace "var-expansion-9257" to be "running"
Jun 13 02:34:53.802: INFO: Pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07": Phase="Pending", Reason="", readiness=false. Elapsed: 18.32561ms
Jun 13 02:34:55.813: INFO: Pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07": Phase="Running", Reason="", readiness=true. Elapsed: 2.028898154s
Jun 13 02:34:55.813: INFO: Pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07" satisfied condition "running"
STEP: creating a file in subpath 06/13/23 02:34:55.813
Jun 13 02:34:55.822: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9257 PodName:var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 02:34:55.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:34:55.823: INFO: ExecWithOptions: Clientset creation
Jun 13 02:34:55.823: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9257/pods/var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 06/13/23 02:34:55.933
Jun 13 02:34:55.947: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9257 PodName:var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 02:34:55.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:34:55.948: INFO: ExecWithOptions: Clientset creation
Jun 13 02:34:55.948: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9257/pods/var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 06/13/23 02:34:56.071
Jun 13 02:34:56.610: INFO: Successfully updated pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07"
STEP: waiting for annotated pod running 06/13/23 02:34:56.61
Jun 13 02:34:56.610: INFO: Waiting up to 2m0s for pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07" in namespace "var-expansion-9257" to be "running"
Jun 13 02:34:56.622: INFO: Pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07": Phase="Running", Reason="", readiness=true. Elapsed: 11.322112ms
Jun 13 02:34:56.622: INFO: Pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07" satisfied condition "running"
STEP: deleting the pod gracefully 06/13/23 02:34:56.622
Jun 13 02:34:56.622: INFO: Deleting pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07" in namespace "var-expansion-9257"
Jun 13 02:34:56.716: INFO: Wait up to 5m0s for pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 13 02:35:30.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9257" for this suite. 06/13/23 02:35:30.762
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":64,"skipped":1250,"failed":0}
------------------------------
• [SLOW TEST] [37.119 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:34:53.666
    Jun 13 02:34:53.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename var-expansion 06/13/23 02:34:53.668
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:34:53.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:34:53.724
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 06/13/23 02:34:53.733
    STEP: waiting for pod running 06/13/23 02:34:53.784
    Jun 13 02:34:53.784: INFO: Waiting up to 2m0s for pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07" in namespace "var-expansion-9257" to be "running"
    Jun 13 02:34:53.802: INFO: Pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07": Phase="Pending", Reason="", readiness=false. Elapsed: 18.32561ms
    Jun 13 02:34:55.813: INFO: Pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07": Phase="Running", Reason="", readiness=true. Elapsed: 2.028898154s
    Jun 13 02:34:55.813: INFO: Pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07" satisfied condition "running"
    STEP: creating a file in subpath 06/13/23 02:34:55.813
    Jun 13 02:34:55.822: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9257 PodName:var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 02:34:55.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:34:55.823: INFO: ExecWithOptions: Clientset creation
    Jun 13 02:34:55.823: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9257/pods/var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 06/13/23 02:34:55.933
    Jun 13 02:34:55.947: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9257 PodName:var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 02:34:55.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:34:55.948: INFO: ExecWithOptions: Clientset creation
    Jun 13 02:34:55.948: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-9257/pods/var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 06/13/23 02:34:56.071
    Jun 13 02:34:56.610: INFO: Successfully updated pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07"
    STEP: waiting for annotated pod running 06/13/23 02:34:56.61
    Jun 13 02:34:56.610: INFO: Waiting up to 2m0s for pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07" in namespace "var-expansion-9257" to be "running"
    Jun 13 02:34:56.622: INFO: Pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07": Phase="Running", Reason="", readiness=true. Elapsed: 11.322112ms
    Jun 13 02:34:56.622: INFO: Pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07" satisfied condition "running"
    STEP: deleting the pod gracefully 06/13/23 02:34:56.622
    Jun 13 02:34:56.622: INFO: Deleting pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07" in namespace "var-expansion-9257"
    Jun 13 02:34:56.716: INFO: Wait up to 5m0s for pod "var-expansion-793d1c6a-9c10-4846-be68-70f2717a0b07" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 13 02:35:30.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-9257" for this suite. 06/13/23 02:35:30.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:35:30.786
Jun 13 02:35:30.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 02:35:30.788
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:30.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:30.883
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jun 13 02:35:30.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2508 version'
Jun 13 02:35:30.995: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jun 13 02:35:30.995: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.9\", GitCommit:\"a1a87a0a2bcd605820920c6b0e618a8ab7d117d4\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:16:51Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.9\", GitCommit:\"a1a87a0a2bcd605820920c6b0e618a8ab7d117d4\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:08:36Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 02:35:30.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2508" for this suite. 06/13/23 02:35:31.013
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":65,"skipped":1268,"failed":0}
------------------------------
• [0.243 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:35:30.786
    Jun 13 02:35:30.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 02:35:30.788
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:30.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:30.883
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jun 13 02:35:30.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2508 version'
    Jun 13 02:35:30.995: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jun 13 02:35:30.995: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.9\", GitCommit:\"a1a87a0a2bcd605820920c6b0e618a8ab7d117d4\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:16:51Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.9\", GitCommit:\"a1a87a0a2bcd605820920c6b0e618a8ab7d117d4\", GitTreeState:\"clean\", BuildDate:\"2023-04-12T12:08:36Z\", GoVersion:\"go1.19.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 02:35:30.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2508" for this suite. 06/13/23 02:35:31.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:35:31.03
Jun 13 02:35:31.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:35:31.031
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:31.075
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:31.082
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 06/13/23 02:35:31.089
Jun 13 02:35:31.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68" in namespace "projected-8018" to be "Succeeded or Failed"
Jun 13 02:35:31.116: INFO: Pod "downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68": Phase="Pending", Reason="", readiness=false. Elapsed: 6.273895ms
Jun 13 02:35:33.131: INFO: Pod "downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021274449s
Jun 13 02:35:35.126: INFO: Pod "downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015726634s
STEP: Saw pod success 06/13/23 02:35:35.126
Jun 13 02:35:35.126: INFO: Pod "downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68" satisfied condition "Succeeded or Failed"
Jun 13 02:35:35.133: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68 container client-container: <nil>
STEP: delete the pod 06/13/23 02:35:35.172
Jun 13 02:35:35.207: INFO: Waiting for pod downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68 to disappear
Jun 13 02:35:35.217: INFO: Pod downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 13 02:35:35.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8018" for this suite. 06/13/23 02:35:35.235
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":66,"skipped":1279,"failed":0}
------------------------------
• [4.237 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:35:31.03
    Jun 13 02:35:31.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:35:31.031
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:31.075
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:31.082
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 06/13/23 02:35:31.089
    Jun 13 02:35:31.110: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68" in namespace "projected-8018" to be "Succeeded or Failed"
    Jun 13 02:35:31.116: INFO: Pod "downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68": Phase="Pending", Reason="", readiness=false. Elapsed: 6.273895ms
    Jun 13 02:35:33.131: INFO: Pod "downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021274449s
    Jun 13 02:35:35.126: INFO: Pod "downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015726634s
    STEP: Saw pod success 06/13/23 02:35:35.126
    Jun 13 02:35:35.126: INFO: Pod "downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68" satisfied condition "Succeeded or Failed"
    Jun 13 02:35:35.133: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68 container client-container: <nil>
    STEP: delete the pod 06/13/23 02:35:35.172
    Jun 13 02:35:35.207: INFO: Waiting for pod downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68 to disappear
    Jun 13 02:35:35.217: INFO: Pod downwardapi-volume-ee43ae30-3bfa-4d5d-9bce-abed64b76c68 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 13 02:35:35.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8018" for this suite. 06/13/23 02:35:35.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:35:35.268
Jun 13 02:35:35.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:35:35.269
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:35.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:35.329
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 06/13/23 02:35:35.339
Jun 13 02:35:35.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a" in namespace "projected-2742" to be "Succeeded or Failed"
Jun 13 02:35:35.379: INFO: Pod "downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.298147ms
Jun 13 02:35:37.402: INFO: Pod "downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036753836s
Jun 13 02:35:39.388: INFO: Pod "downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022986682s
STEP: Saw pod success 06/13/23 02:35:39.388
Jun 13 02:35:39.389: INFO: Pod "downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a" satisfied condition "Succeeded or Failed"
Jun 13 02:35:39.397: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a container client-container: <nil>
STEP: delete the pod 06/13/23 02:35:39.417
Jun 13 02:35:39.442: INFO: Waiting for pod downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a to disappear
Jun 13 02:35:39.454: INFO: Pod downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 13 02:35:39.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2742" for this suite. 06/13/23 02:35:39.468
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":67,"skipped":1287,"failed":0}
------------------------------
• [4.227 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:35:35.268
    Jun 13 02:35:35.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:35:35.269
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:35.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:35.329
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 06/13/23 02:35:35.339
    Jun 13 02:35:35.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a" in namespace "projected-2742" to be "Succeeded or Failed"
    Jun 13 02:35:35.379: INFO: Pod "downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.298147ms
    Jun 13 02:35:37.402: INFO: Pod "downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036753836s
    Jun 13 02:35:39.388: INFO: Pod "downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022986682s
    STEP: Saw pod success 06/13/23 02:35:39.388
    Jun 13 02:35:39.389: INFO: Pod "downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a" satisfied condition "Succeeded or Failed"
    Jun 13 02:35:39.397: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a container client-container: <nil>
    STEP: delete the pod 06/13/23 02:35:39.417
    Jun 13 02:35:39.442: INFO: Waiting for pod downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a to disappear
    Jun 13 02:35:39.454: INFO: Pod downwardapi-volume-6297bc71-0833-4795-a42e-1b33ce0c092a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 13 02:35:39.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2742" for this suite. 06/13/23 02:35:39.468
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:35:39.495
Jun 13 02:35:39.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename certificates 06/13/23 02:35:39.497
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:39.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:39.586
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 06/13/23 02:35:40.169
STEP: getting /apis/certificates.k8s.io 06/13/23 02:35:40.176
STEP: getting /apis/certificates.k8s.io/v1 06/13/23 02:35:40.183
STEP: creating 06/13/23 02:35:40.186
STEP: getting 06/13/23 02:35:40.229
STEP: listing 06/13/23 02:35:40.238
STEP: watching 06/13/23 02:35:40.246
Jun 13 02:35:40.246: INFO: starting watch
STEP: patching 06/13/23 02:35:40.249
STEP: updating 06/13/23 02:35:40.263
Jun 13 02:35:40.272: INFO: waiting for watch events with expected annotations
Jun 13 02:35:40.272: INFO: saw patched and updated annotations
STEP: getting /approval 06/13/23 02:35:40.272
STEP: patching /approval 06/13/23 02:35:40.282
STEP: updating /approval 06/13/23 02:35:40.295
STEP: getting /status 06/13/23 02:35:40.307
STEP: patching /status 06/13/23 02:35:40.32
STEP: updating /status 06/13/23 02:35:40.34
STEP: deleting 06/13/23 02:35:40.357
STEP: deleting a collection 06/13/23 02:35:40.384
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 02:35:40.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-8764" for this suite. 06/13/23 02:35:40.444
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":68,"skipped":1287,"failed":0}
------------------------------
• [0.966 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:35:39.495
    Jun 13 02:35:39.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename certificates 06/13/23 02:35:39.497
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:39.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:39.586
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 06/13/23 02:35:40.169
    STEP: getting /apis/certificates.k8s.io 06/13/23 02:35:40.176
    STEP: getting /apis/certificates.k8s.io/v1 06/13/23 02:35:40.183
    STEP: creating 06/13/23 02:35:40.186
    STEP: getting 06/13/23 02:35:40.229
    STEP: listing 06/13/23 02:35:40.238
    STEP: watching 06/13/23 02:35:40.246
    Jun 13 02:35:40.246: INFO: starting watch
    STEP: patching 06/13/23 02:35:40.249
    STEP: updating 06/13/23 02:35:40.263
    Jun 13 02:35:40.272: INFO: waiting for watch events with expected annotations
    Jun 13 02:35:40.272: INFO: saw patched and updated annotations
    STEP: getting /approval 06/13/23 02:35:40.272
    STEP: patching /approval 06/13/23 02:35:40.282
    STEP: updating /approval 06/13/23 02:35:40.295
    STEP: getting /status 06/13/23 02:35:40.307
    STEP: patching /status 06/13/23 02:35:40.32
    STEP: updating /status 06/13/23 02:35:40.34
    STEP: deleting 06/13/23 02:35:40.357
    STEP: deleting a collection 06/13/23 02:35:40.384
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 02:35:40.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-8764" for this suite. 06/13/23 02:35:40.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:35:40.463
Jun 13 02:35:40.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 02:35:40.464
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:40.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:40.55
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185
STEP: fetching services 06/13/23 02:35:40.558
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 02:35:40.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7405" for this suite. 06/13/23 02:35:40.575
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":69,"skipped":1315,"failed":0}
------------------------------
• [0.132 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:35:40.463
    Jun 13 02:35:40.463: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 02:35:40.464
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:40.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:40.55
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3185
    STEP: fetching services 06/13/23 02:35:40.558
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 02:35:40.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7405" for this suite. 06/13/23 02:35:40.575
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:35:40.596
Jun 13 02:35:40.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 02:35:40.597
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:40.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:40.643
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 06/13/23 02:35:40.65
Jun 13 02:35:40.670: INFO: Waiting up to 5m0s for pod "pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491" in namespace "emptydir-9500" to be "Succeeded or Failed"
Jun 13 02:35:40.684: INFO: Pod "pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491": Phase="Pending", Reason="", readiness=false. Elapsed: 13.723921ms
Jun 13 02:35:42.697: INFO: Pod "pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026482068s
Jun 13 02:35:44.691: INFO: Pod "pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02123592s
STEP: Saw pod success 06/13/23 02:35:44.691
Jun 13 02:35:44.692: INFO: Pod "pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491" satisfied condition "Succeeded or Failed"
Jun 13 02:35:44.700: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491 container test-container: <nil>
STEP: delete the pod 06/13/23 02:35:44.72
Jun 13 02:35:44.744: INFO: Waiting for pod pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491 to disappear
Jun 13 02:35:44.758: INFO: Pod pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 02:35:44.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9500" for this suite. 06/13/23 02:35:44.768
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":70,"skipped":1333,"failed":0}
------------------------------
• [4.190 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:35:40.596
    Jun 13 02:35:40.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 02:35:40.597
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:40.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:40.643
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 06/13/23 02:35:40.65
    Jun 13 02:35:40.670: INFO: Waiting up to 5m0s for pod "pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491" in namespace "emptydir-9500" to be "Succeeded or Failed"
    Jun 13 02:35:40.684: INFO: Pod "pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491": Phase="Pending", Reason="", readiness=false. Elapsed: 13.723921ms
    Jun 13 02:35:42.697: INFO: Pod "pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026482068s
    Jun 13 02:35:44.691: INFO: Pod "pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02123592s
    STEP: Saw pod success 06/13/23 02:35:44.691
    Jun 13 02:35:44.692: INFO: Pod "pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491" satisfied condition "Succeeded or Failed"
    Jun 13 02:35:44.700: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491 container test-container: <nil>
    STEP: delete the pod 06/13/23 02:35:44.72
    Jun 13 02:35:44.744: INFO: Waiting for pod pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491 to disappear
    Jun 13 02:35:44.758: INFO: Pod pod-f8a1130f-d241-4df8-8ed9-4e5d135d0491 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 02:35:44.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9500" for this suite. 06/13/23 02:35:44.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:35:44.787
Jun 13 02:35:44.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 02:35:44.789
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:44.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:44.836
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-83386c80-66b9-4267-b68a-06929a5bf5e5 06/13/23 02:35:44.844
STEP: Creating a pod to test consume configMaps 06/13/23 02:35:44.852
Jun 13 02:35:44.869: INFO: Waiting up to 5m0s for pod "pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6" in namespace "configmap-7975" to be "Succeeded or Failed"
Jun 13 02:35:44.882: INFO: Pod "pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.632557ms
Jun 13 02:35:46.893: INFO: Pod "pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.023896281s
Jun 13 02:35:48.898: INFO: Pod "pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6": Phase="Running", Reason="", readiness=false. Elapsed: 4.028400542s
Jun 13 02:35:50.901: INFO: Pod "pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031204966s
STEP: Saw pod success 06/13/23 02:35:50.901
Jun 13 02:35:50.901: INFO: Pod "pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6" satisfied condition "Succeeded or Failed"
Jun 13 02:35:50.915: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6 container agnhost-container: <nil>
STEP: delete the pod 06/13/23 02:35:50.943
Jun 13 02:35:50.977: INFO: Waiting for pod pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6 to disappear
Jun 13 02:35:50.987: INFO: Pod pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 02:35:50.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7975" for this suite. 06/13/23 02:35:51.002
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":71,"skipped":1341,"failed":0}
------------------------------
• [SLOW TEST] [6.234 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:35:44.787
    Jun 13 02:35:44.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 02:35:44.789
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:44.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:44.836
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-83386c80-66b9-4267-b68a-06929a5bf5e5 06/13/23 02:35:44.844
    STEP: Creating a pod to test consume configMaps 06/13/23 02:35:44.852
    Jun 13 02:35:44.869: INFO: Waiting up to 5m0s for pod "pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6" in namespace "configmap-7975" to be "Succeeded or Failed"
    Jun 13 02:35:44.882: INFO: Pod "pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.632557ms
    Jun 13 02:35:46.893: INFO: Pod "pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.023896281s
    Jun 13 02:35:48.898: INFO: Pod "pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6": Phase="Running", Reason="", readiness=false. Elapsed: 4.028400542s
    Jun 13 02:35:50.901: INFO: Pod "pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031204966s
    STEP: Saw pod success 06/13/23 02:35:50.901
    Jun 13 02:35:50.901: INFO: Pod "pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6" satisfied condition "Succeeded or Failed"
    Jun 13 02:35:50.915: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6 container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 02:35:50.943
    Jun 13 02:35:50.977: INFO: Waiting for pod pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6 to disappear
    Jun 13 02:35:50.987: INFO: Pod pod-configmaps-a6800b35-2c79-468b-8393-864226a009b6 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 02:35:50.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7975" for this suite. 06/13/23 02:35:51.002
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:35:51.022
Jun 13 02:35:51.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename disruption 06/13/23 02:35:51.023
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:51.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:51.063
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 06/13/23 02:35:51.072
STEP: Waiting for the pdb to be processed 06/13/23 02:35:51.086
STEP: updating the pdb 06/13/23 02:35:51.1
STEP: Waiting for the pdb to be processed 06/13/23 02:35:51.136
STEP: patching the pdb 06/13/23 02:35:51.146
STEP: Waiting for the pdb to be processed 06/13/23 02:35:51.17
STEP: Waiting for the pdb to be deleted 06/13/23 02:35:51.198
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 13 02:35:51.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-388" for this suite. 06/13/23 02:35:51.225
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":72,"skipped":1345,"failed":0}
------------------------------
• [0.229 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:35:51.022
    Jun 13 02:35:51.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename disruption 06/13/23 02:35:51.023
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:51.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:51.063
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 06/13/23 02:35:51.072
    STEP: Waiting for the pdb to be processed 06/13/23 02:35:51.086
    STEP: updating the pdb 06/13/23 02:35:51.1
    STEP: Waiting for the pdb to be processed 06/13/23 02:35:51.136
    STEP: patching the pdb 06/13/23 02:35:51.146
    STEP: Waiting for the pdb to be processed 06/13/23 02:35:51.17
    STEP: Waiting for the pdb to be deleted 06/13/23 02:35:51.198
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 13 02:35:51.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-388" for this suite. 06/13/23 02:35:51.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:35:51.251
Jun 13 02:35:51.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:35:51.252
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:51.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:51.3
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-7d719701-35b3-4ba8-9991-31419a61f536 06/13/23 02:35:51.308
STEP: Creating a pod to test consume configMaps 06/13/23 02:35:51.321
Jun 13 02:35:51.349: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24" in namespace "projected-2311" to be "Succeeded or Failed"
Jun 13 02:35:51.361: INFO: Pod "pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24": Phase="Pending", Reason="", readiness=false. Elapsed: 12.48513ms
Jun 13 02:35:53.374: INFO: Pod "pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024865602s
Jun 13 02:35:55.370: INFO: Pod "pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021383637s
STEP: Saw pod success 06/13/23 02:35:55.37
Jun 13 02:35:55.370: INFO: Pod "pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24" satisfied condition "Succeeded or Failed"
Jun 13 02:35:55.380: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24 container agnhost-container: <nil>
STEP: delete the pod 06/13/23 02:35:55.393
Jun 13 02:35:55.478: INFO: Waiting for pod pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24 to disappear
Jun 13 02:35:55.487: INFO: Pod pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 13 02:35:55.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2311" for this suite. 06/13/23 02:35:55.498
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":73,"skipped":1355,"failed":0}
------------------------------
• [4.269 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:35:51.251
    Jun 13 02:35:51.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:35:51.252
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:51.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:51.3
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-7d719701-35b3-4ba8-9991-31419a61f536 06/13/23 02:35:51.308
    STEP: Creating a pod to test consume configMaps 06/13/23 02:35:51.321
    Jun 13 02:35:51.349: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24" in namespace "projected-2311" to be "Succeeded or Failed"
    Jun 13 02:35:51.361: INFO: Pod "pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24": Phase="Pending", Reason="", readiness=false. Elapsed: 12.48513ms
    Jun 13 02:35:53.374: INFO: Pod "pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024865602s
    Jun 13 02:35:55.370: INFO: Pod "pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021383637s
    STEP: Saw pod success 06/13/23 02:35:55.37
    Jun 13 02:35:55.370: INFO: Pod "pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24" satisfied condition "Succeeded or Failed"
    Jun 13 02:35:55.380: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24 container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 02:35:55.393
    Jun 13 02:35:55.478: INFO: Waiting for pod pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24 to disappear
    Jun 13 02:35:55.487: INFO: Pod pod-projected-configmaps-2a3b44e5-0505-4eb6-a0ae-4319fcea9e24 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 13 02:35:55.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2311" for this suite. 06/13/23 02:35:55.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:35:55.523
Jun 13 02:35:55.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename daemonsets 06/13/23 02:35:55.525
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:55.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:55.573
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jun 13 02:35:55.634: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 06/13/23 02:35:55.654
Jun 13 02:35:55.669: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:55.669: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:55.669: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:55.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 02:35:55.684: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 02:35:56.703: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:56.703: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:56.703: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:56.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 02:35:56.713: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 02:35:57.700: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:57.700: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:57.700: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:57.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 13 02:35:57.711: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 06/13/23 02:35:57.748
STEP: Check that daemon pods images are updated. 06/13/23 02:35:57.795
Jun 13 02:35:57.857: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:35:57.857: INFO: Wrong image for pod: daemon-set-ljltk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:35:57.877: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:57.877: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:57.877: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:58.888: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:35:58.888: INFO: Wrong image for pod: daemon-set-ljltk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:35:58.900: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:58.900: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:58.900: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:59.887: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:35:59.887: INFO: Wrong image for pod: daemon-set-ljltk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:35:59.896: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:59.896: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:35:59.896: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:00.890: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:36:00.890: INFO: Pod daemon-set-lfpxn is not available
Jun 13 02:36:00.890: INFO: Wrong image for pod: daemon-set-ljltk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:36:00.903: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:00.903: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:00.903: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:01.886: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:36:01.886: INFO: Pod daemon-set-lfpxn is not available
Jun 13 02:36:01.886: INFO: Wrong image for pod: daemon-set-ljltk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:36:01.895: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:01.895: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:01.895: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:02.890: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:36:02.899: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:02.899: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:02.899: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:03.886: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:36:03.886: INFO: Pod daemon-set-gdwjf is not available
Jun 13 02:36:03.902: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:03.902: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:03.903: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:04.887: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 13 02:36:04.887: INFO: Pod daemon-set-gdwjf is not available
Jun 13 02:36:04.909: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:04.910: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:04.910: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:05.899: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:05.899: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:05.899: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:06.890: INFO: Pod daemon-set-7hpvv is not available
Jun 13 02:36:06.929: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:06.929: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:06.929: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 06/13/23 02:36:06.929
Jun 13 02:36:06.939: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:06.939: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:06.939: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:06.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 13 02:36:06.947: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
Jun 13 02:36:07.958: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:07.958: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:07.958: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 02:36:07.966: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 13 02:36:07.966: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/13/23 02:36:08.007
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8512, will wait for the garbage collector to delete the pods 06/13/23 02:36:08.007
Jun 13 02:36:08.079: INFO: Deleting DaemonSet.extensions daemon-set took: 13.862631ms
Jun 13 02:36:08.180: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.398881ms
Jun 13 02:36:10.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 02:36:10.991: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 13 02:36:10.998: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15084"},"items":null}

Jun 13 02:36:11.005: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15084"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 13 02:36:11.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8512" for this suite. 06/13/23 02:36:11.045
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":74,"skipped":1409,"failed":0}
------------------------------
• [SLOW TEST] [15.532 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:35:55.523
    Jun 13 02:35:55.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename daemonsets 06/13/23 02:35:55.525
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:35:55.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:35:55.573
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jun 13 02:35:55.634: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 06/13/23 02:35:55.654
    Jun 13 02:35:55.669: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:55.669: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:55.669: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:55.684: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 02:35:55.684: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 02:35:56.703: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:56.703: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:56.703: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:56.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 02:35:56.713: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 02:35:57.700: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:57.700: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:57.700: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:57.711: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 13 02:35:57.711: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 06/13/23 02:35:57.748
    STEP: Check that daemon pods images are updated. 06/13/23 02:35:57.795
    Jun 13 02:35:57.857: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:35:57.857: INFO: Wrong image for pod: daemon-set-ljltk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:35:57.877: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:57.877: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:57.877: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:58.888: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:35:58.888: INFO: Wrong image for pod: daemon-set-ljltk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:35:58.900: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:58.900: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:58.900: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:59.887: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:35:59.887: INFO: Wrong image for pod: daemon-set-ljltk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:35:59.896: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:59.896: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:35:59.896: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:00.890: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:36:00.890: INFO: Pod daemon-set-lfpxn is not available
    Jun 13 02:36:00.890: INFO: Wrong image for pod: daemon-set-ljltk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:36:00.903: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:00.903: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:00.903: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:01.886: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:36:01.886: INFO: Pod daemon-set-lfpxn is not available
    Jun 13 02:36:01.886: INFO: Wrong image for pod: daemon-set-ljltk. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:36:01.895: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:01.895: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:01.895: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:02.890: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:36:02.899: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:02.899: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:02.899: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:03.886: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:36:03.886: INFO: Pod daemon-set-gdwjf is not available
    Jun 13 02:36:03.902: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:03.902: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:03.903: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:04.887: INFO: Wrong image for pod: daemon-set-5r5rv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 13 02:36:04.887: INFO: Pod daemon-set-gdwjf is not available
    Jun 13 02:36:04.909: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:04.910: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:04.910: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:05.899: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:05.899: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:05.899: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:06.890: INFO: Pod daemon-set-7hpvv is not available
    Jun 13 02:36:06.929: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:06.929: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:06.929: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 06/13/23 02:36:06.929
    Jun 13 02:36:06.939: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:06.939: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:06.939: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:06.947: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 13 02:36:06.947: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
    Jun 13 02:36:07.958: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:07.958: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:07.958: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 02:36:07.966: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 13 02:36:07.966: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/13/23 02:36:08.007
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8512, will wait for the garbage collector to delete the pods 06/13/23 02:36:08.007
    Jun 13 02:36:08.079: INFO: Deleting DaemonSet.extensions daemon-set took: 13.862631ms
    Jun 13 02:36:08.180: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.398881ms
    Jun 13 02:36:10.991: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 02:36:10.991: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 13 02:36:10.998: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15084"},"items":null}

    Jun 13 02:36:11.005: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15084"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 02:36:11.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8512" for this suite. 06/13/23 02:36:11.045
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:36:11.057
Jun 13 02:36:11.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename events 06/13/23 02:36:11.059
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:36:11.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:36:11.092
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 06/13/23 02:36:11.097
STEP: listing events in all namespaces 06/13/23 02:36:11.11
STEP: listing events in test namespace 06/13/23 02:36:11.127
STEP: listing events with field selection filtering on source 06/13/23 02:36:11.134
STEP: listing events with field selection filtering on reportingController 06/13/23 02:36:11.14
STEP: getting the test event 06/13/23 02:36:11.146
STEP: patching the test event 06/13/23 02:36:11.154
STEP: getting the test event 06/13/23 02:36:11.176
STEP: updating the test event 06/13/23 02:36:11.183
STEP: getting the test event 06/13/23 02:36:11.2
STEP: deleting the test event 06/13/23 02:36:11.209
STEP: listing events in all namespaces 06/13/23 02:36:11.229
STEP: listing events in test namespace 06/13/23 02:36:11.249
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jun 13 02:36:11.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5107" for this suite. 06/13/23 02:36:11.264
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":75,"skipped":1439,"failed":0}
------------------------------
• [0.219 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:36:11.057
    Jun 13 02:36:11.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename events 06/13/23 02:36:11.059
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:36:11.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:36:11.092
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 06/13/23 02:36:11.097
    STEP: listing events in all namespaces 06/13/23 02:36:11.11
    STEP: listing events in test namespace 06/13/23 02:36:11.127
    STEP: listing events with field selection filtering on source 06/13/23 02:36:11.134
    STEP: listing events with field selection filtering on reportingController 06/13/23 02:36:11.14
    STEP: getting the test event 06/13/23 02:36:11.146
    STEP: patching the test event 06/13/23 02:36:11.154
    STEP: getting the test event 06/13/23 02:36:11.176
    STEP: updating the test event 06/13/23 02:36:11.183
    STEP: getting the test event 06/13/23 02:36:11.2
    STEP: deleting the test event 06/13/23 02:36:11.209
    STEP: listing events in all namespaces 06/13/23 02:36:11.229
    STEP: listing events in test namespace 06/13/23 02:36:11.249
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jun 13 02:36:11.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-5107" for this suite. 06/13/23 02:36:11.264
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:36:11.276
Jun 13 02:36:11.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pods 06/13/23 02:36:11.278
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:36:11.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:36:11.315
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 06/13/23 02:36:11.32
STEP: submitting the pod to kubernetes 06/13/23 02:36:11.321
Jun 13 02:36:11.335: INFO: Waiting up to 5m0s for pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518" in namespace "pods-8806" to be "running and ready"
Jun 13 02:36:11.344: INFO: Pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518": Phase="Pending", Reason="", readiness=false. Elapsed: 8.905414ms
Jun 13 02:36:11.344: INFO: The phase of Pod pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:36:13.418: INFO: Pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518": Phase="Running", Reason="", readiness=true. Elapsed: 2.08343137s
Jun 13 02:36:13.418: INFO: The phase of Pod pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518 is Running (Ready = true)
Jun 13 02:36:13.418: INFO: Pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 06/13/23 02:36:13.428
STEP: updating the pod 06/13/23 02:36:13.438
Jun 13 02:36:13.965: INFO: Successfully updated pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518"
Jun 13 02:36:13.965: INFO: Waiting up to 5m0s for pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518" in namespace "pods-8806" to be "running"
Jun 13 02:36:13.983: INFO: Pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518": Phase="Running", Reason="", readiness=true. Elapsed: 17.329207ms
Jun 13 02:36:13.983: INFO: Pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 06/13/23 02:36:13.983
Jun 13 02:36:14.003: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 13 02:36:14.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8806" for this suite. 06/13/23 02:36:14.014
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":76,"skipped":1443,"failed":0}
------------------------------
• [2.768 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:36:11.276
    Jun 13 02:36:11.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pods 06/13/23 02:36:11.278
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:36:11.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:36:11.315
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 06/13/23 02:36:11.32
    STEP: submitting the pod to kubernetes 06/13/23 02:36:11.321
    Jun 13 02:36:11.335: INFO: Waiting up to 5m0s for pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518" in namespace "pods-8806" to be "running and ready"
    Jun 13 02:36:11.344: INFO: Pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518": Phase="Pending", Reason="", readiness=false. Elapsed: 8.905414ms
    Jun 13 02:36:11.344: INFO: The phase of Pod pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:36:13.418: INFO: Pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518": Phase="Running", Reason="", readiness=true. Elapsed: 2.08343137s
    Jun 13 02:36:13.418: INFO: The phase of Pod pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518 is Running (Ready = true)
    Jun 13 02:36:13.418: INFO: Pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 06/13/23 02:36:13.428
    STEP: updating the pod 06/13/23 02:36:13.438
    Jun 13 02:36:13.965: INFO: Successfully updated pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518"
    Jun 13 02:36:13.965: INFO: Waiting up to 5m0s for pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518" in namespace "pods-8806" to be "running"
    Jun 13 02:36:13.983: INFO: Pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518": Phase="Running", Reason="", readiness=true. Elapsed: 17.329207ms
    Jun 13 02:36:13.983: INFO: Pod "pod-update-9604ed05-801c-4536-9c0f-9c0a52af2518" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 06/13/23 02:36:13.983
    Jun 13 02:36:14.003: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 13 02:36:14.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8806" for this suite. 06/13/23 02:36:14.014
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:36:14.045
Jun 13 02:36:14.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 02:36:14.047
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:36:14.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:36:14.108
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 02:36:14.172
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 02:36:15.017
STEP: Deploying the webhook pod 06/13/23 02:36:15.035
STEP: Wait for the deployment to be ready 06/13/23 02:36:15.066
Jun 13 02:36:15.094: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 13 02:36:17.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 2, 36, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 2, 36, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 2, 36, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 2, 36, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/13/23 02:36:19.133
STEP: Verifying the service has paired with the endpoint 06/13/23 02:36:19.212
Jun 13 02:36:20.212: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jun 13 02:36:20.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Registering the custom resource webhook via the AdmissionRegistration API 06/13/23 02:36:20.749
STEP: Creating a custom resource that should be denied by the webhook 06/13/23 02:36:20.808
STEP: Creating a custom resource whose deletion would be denied by the webhook 06/13/23 02:36:22.857
STEP: Updating the custom resource with disallowed data should be denied 06/13/23 02:36:22.875
STEP: Deleting the custom resource should be denied 06/13/23 02:36:22.896
STEP: Remove the offending key and value from the custom resource data 06/13/23 02:36:22.914
STEP: Deleting the updated custom resource should be successful 06/13/23 02:36:22.938
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 02:36:23.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3384" for this suite. 06/13/23 02:36:23.539
STEP: Destroying namespace "webhook-3384-markers" for this suite. 06/13/23 02:36:23.554
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":77,"skipped":1447,"failed":0}
------------------------------
• [SLOW TEST] [9.789 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:36:14.045
    Jun 13 02:36:14.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 02:36:14.047
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:36:14.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:36:14.108
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 02:36:14.172
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 02:36:15.017
    STEP: Deploying the webhook pod 06/13/23 02:36:15.035
    STEP: Wait for the deployment to be ready 06/13/23 02:36:15.066
    Jun 13 02:36:15.094: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 13 02:36:17.119: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 2, 36, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 2, 36, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 2, 36, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 2, 36, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/13/23 02:36:19.133
    STEP: Verifying the service has paired with the endpoint 06/13/23 02:36:19.212
    Jun 13 02:36:20.212: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jun 13 02:36:20.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 06/13/23 02:36:20.749
    STEP: Creating a custom resource that should be denied by the webhook 06/13/23 02:36:20.808
    STEP: Creating a custom resource whose deletion would be denied by the webhook 06/13/23 02:36:22.857
    STEP: Updating the custom resource with disallowed data should be denied 06/13/23 02:36:22.875
    STEP: Deleting the custom resource should be denied 06/13/23 02:36:22.896
    STEP: Remove the offending key and value from the custom resource data 06/13/23 02:36:22.914
    STEP: Deleting the updated custom resource should be successful 06/13/23 02:36:22.938
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 02:36:23.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3384" for this suite. 06/13/23 02:36:23.539
    STEP: Destroying namespace "webhook-3384-markers" for this suite. 06/13/23 02:36:23.554
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:36:23.836
Jun 13 02:36:23.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:36:23.838
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:36:23.882
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:36:23.889
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 06/13/23 02:36:23.899
Jun 13 02:36:23.921: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965" in namespace "projected-3926" to be "Succeeded or Failed"
Jun 13 02:36:23.937: INFO: Pod "downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965": Phase="Pending", Reason="", readiness=false. Elapsed: 16.269375ms
Jun 13 02:36:25.951: INFO: Pod "downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030345717s
Jun 13 02:36:27.947: INFO: Pod "downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026297738s
Jun 13 02:36:29.988: INFO: Pod "downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066961428s
STEP: Saw pod success 06/13/23 02:36:29.988
Jun 13 02:36:29.988: INFO: Pod "downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965" satisfied condition "Succeeded or Failed"
Jun 13 02:36:29.996: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965 container client-container: <nil>
STEP: delete the pod 06/13/23 02:36:30.01
Jun 13 02:36:30.055: INFO: Waiting for pod downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965 to disappear
Jun 13 02:36:30.072: INFO: Pod downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 13 02:36:30.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3926" for this suite. 06/13/23 02:36:30.087
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":78,"skipped":1460,"failed":0}
------------------------------
• [SLOW TEST] [6.304 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:36:23.836
    Jun 13 02:36:23.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:36:23.838
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:36:23.882
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:36:23.889
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 06/13/23 02:36:23.899
    Jun 13 02:36:23.921: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965" in namespace "projected-3926" to be "Succeeded or Failed"
    Jun 13 02:36:23.937: INFO: Pod "downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965": Phase="Pending", Reason="", readiness=false. Elapsed: 16.269375ms
    Jun 13 02:36:25.951: INFO: Pod "downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030345717s
    Jun 13 02:36:27.947: INFO: Pod "downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026297738s
    Jun 13 02:36:29.988: INFO: Pod "downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.066961428s
    STEP: Saw pod success 06/13/23 02:36:29.988
    Jun 13 02:36:29.988: INFO: Pod "downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965" satisfied condition "Succeeded or Failed"
    Jun 13 02:36:29.996: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965 container client-container: <nil>
    STEP: delete the pod 06/13/23 02:36:30.01
    Jun 13 02:36:30.055: INFO: Waiting for pod downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965 to disappear
    Jun 13 02:36:30.072: INFO: Pod downwardapi-volume-9a72712d-f095-41c1-a33e-eb76e2c6c965 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 13 02:36:30.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3926" for this suite. 06/13/23 02:36:30.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:36:30.142
Jun 13 02:36:30.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename dns 06/13/23 02:36:30.143
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:36:30.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:36:30.191
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 06/13/23 02:36:30.201
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1286.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1286.svc.cluster.local;sleep 1; done
 06/13/23 02:36:30.224
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1286.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1286.svc.cluster.local;sleep 1; done
 06/13/23 02:36:30.224
STEP: creating a pod to probe DNS 06/13/23 02:36:30.224
STEP: submitting the pod to kubernetes 06/13/23 02:36:30.224
Jun 13 02:36:30.258: INFO: Waiting up to 15m0s for pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f" in namespace "dns-1286" to be "running"
Jun 13 02:36:30.280: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.775227ms
Jun 13 02:36:32.294: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036150036s
Jun 13 02:36:34.291: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03296194s
Jun 13 02:36:36.307: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04929578s
Jun 13 02:36:38.295: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036736983s
Jun 13 02:36:40.291: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033128547s
Jun 13 02:36:42.288: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.030463469s
Jun 13 02:36:44.294: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.035961205s
Jun 13 02:36:46.291: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.032958116s
Jun 13 02:36:48.295: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.036945371s
Jun 13 02:36:50.290: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.031622254s
Jun 13 02:36:52.290: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.031708581s
Jun 13 02:36:54.288: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Running", Reason="", readiness=true. Elapsed: 24.030551638s
Jun 13 02:36:54.288: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f" satisfied condition "running"
STEP: retrieving the pod 06/13/23 02:36:54.288
STEP: looking for the results for each expected name from probers 06/13/23 02:36:54.297
Jun 13 02:36:54.309: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:54.318: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:54.328: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:54.340: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:54.356: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:54.366: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:54.375: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:54.384: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:54.384: INFO: Lookups using dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1286.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1286.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local jessie_udp@dns-test-service-2.dns-1286.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1286.svc.cluster.local]

Jun 13 02:36:59.397: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:59.415: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:59.427: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:59.435: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:59.445: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:59.457: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:59.471: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:59.481: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
Jun 13 02:36:59.481: INFO: Lookups using dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1286.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1286.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local jessie_udp@dns-test-service-2.dns-1286.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1286.svc.cluster.local]

Jun 13 02:37:04.461: INFO: DNS probes using dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f succeeded

STEP: deleting the pod 06/13/23 02:37:04.461
STEP: deleting the test headless service 06/13/23 02:37:04.531
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 13 02:37:04.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1286" for this suite. 06/13/23 02:37:04.717
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":79,"skipped":1470,"failed":0}
------------------------------
• [SLOW TEST] [34.644 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:36:30.142
    Jun 13 02:36:30.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename dns 06/13/23 02:36:30.143
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:36:30.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:36:30.191
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 06/13/23 02:36:30.201
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1286.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1286.svc.cluster.local;sleep 1; done
     06/13/23 02:36:30.224
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1286.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1286.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1286.svc.cluster.local;sleep 1; done
     06/13/23 02:36:30.224
    STEP: creating a pod to probe DNS 06/13/23 02:36:30.224
    STEP: submitting the pod to kubernetes 06/13/23 02:36:30.224
    Jun 13 02:36:30.258: INFO: Waiting up to 15m0s for pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f" in namespace "dns-1286" to be "running"
    Jun 13 02:36:30.280: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.775227ms
    Jun 13 02:36:32.294: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036150036s
    Jun 13 02:36:34.291: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03296194s
    Jun 13 02:36:36.307: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.04929578s
    Jun 13 02:36:38.295: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.036736983s
    Jun 13 02:36:40.291: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033128547s
    Jun 13 02:36:42.288: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.030463469s
    Jun 13 02:36:44.294: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.035961205s
    Jun 13 02:36:46.291: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.032958116s
    Jun 13 02:36:48.295: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 18.036945371s
    Jun 13 02:36:50.290: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.031622254s
    Jun 13 02:36:52.290: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Pending", Reason="", readiness=false. Elapsed: 22.031708581s
    Jun 13 02:36:54.288: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f": Phase="Running", Reason="", readiness=true. Elapsed: 24.030551638s
    Jun 13 02:36:54.288: INFO: Pod "dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f" satisfied condition "running"
    STEP: retrieving the pod 06/13/23 02:36:54.288
    STEP: looking for the results for each expected name from probers 06/13/23 02:36:54.297
    Jun 13 02:36:54.309: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:54.318: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:54.328: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:54.340: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:54.356: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:54.366: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:54.375: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:54.384: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:54.384: INFO: Lookups using dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1286.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1286.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local jessie_udp@dns-test-service-2.dns-1286.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1286.svc.cluster.local]

    Jun 13 02:36:59.397: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:59.415: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:59.427: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:59.435: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:59.445: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:59.457: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:59.471: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:59.481: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1286.svc.cluster.local from pod dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f: the server could not find the requested resource (get pods dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f)
    Jun 13 02:36:59.481: INFO: Lookups using dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1286.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1286.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1286.svc.cluster.local jessie_udp@dns-test-service-2.dns-1286.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1286.svc.cluster.local]

    Jun 13 02:37:04.461: INFO: DNS probes using dns-1286/dns-test-5e99917b-fcac-4dfa-9cb0-52e2f1c73e5f succeeded

    STEP: deleting the pod 06/13/23 02:37:04.461
    STEP: deleting the test headless service 06/13/23 02:37:04.531
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 13 02:37:04.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1286" for this suite. 06/13/23 02:37:04.717
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:37:04.786
Jun 13 02:37:04.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 02:37:04.788
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:04.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:04.878
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-c79e1abd-f3ce-4be4-9c0c-0a7cf4e1203f 06/13/23 02:37:04.886
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 02:37:04.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7508" for this suite. 06/13/23 02:37:04.903
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":80,"skipped":1472,"failed":0}
------------------------------
• [0.133 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:37:04.786
    Jun 13 02:37:04.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 02:37:04.788
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:04.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:04.878
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-c79e1abd-f3ce-4be4-9c0c-0a7cf4e1203f 06/13/23 02:37:04.886
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 02:37:04.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7508" for this suite. 06/13/23 02:37:04.903
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:37:04.919
Jun 13 02:37:04.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename init-container 06/13/23 02:37:04.922
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:04.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:05.001
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 06/13/23 02:37:05.008
Jun 13 02:37:05.009: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 13 02:37:10.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9502" for this suite. 06/13/23 02:37:10.455
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":81,"skipped":1473,"failed":0}
------------------------------
• [SLOW TEST] [5.557 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:37:04.919
    Jun 13 02:37:04.920: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename init-container 06/13/23 02:37:04.922
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:04.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:05.001
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 06/13/23 02:37:05.008
    Jun 13 02:37:05.009: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 13 02:37:10.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9502" for this suite. 06/13/23 02:37:10.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:37:10.477
Jun 13 02:37:10.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 02:37:10.478
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:10.525
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:10.531
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-def30780-ee2b-4793-9c6b-ac20beba12bf 06/13/23 02:37:10.538
STEP: Creating a pod to test consume configMaps 06/13/23 02:37:10.555
Jun 13 02:37:10.572: INFO: Waiting up to 5m0s for pod "pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3" in namespace "configmap-8277" to be "Succeeded or Failed"
Jun 13 02:37:10.595: INFO: Pod "pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.603301ms
Jun 13 02:37:12.604: INFO: Pod "pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032013495s
Jun 13 02:37:14.606: INFO: Pod "pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033528687s
STEP: Saw pod success 06/13/23 02:37:14.606
Jun 13 02:37:14.606: INFO: Pod "pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3" satisfied condition "Succeeded or Failed"
Jun 13 02:37:14.614: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3 container agnhost-container: <nil>
STEP: delete the pod 06/13/23 02:37:14.628
Jun 13 02:37:14.651: INFO: Waiting for pod pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3 to disappear
Jun 13 02:37:14.658: INFO: Pod pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 02:37:14.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8277" for this suite. 06/13/23 02:37:14.67
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":82,"skipped":1479,"failed":0}
------------------------------
• [4.210 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:37:10.477
    Jun 13 02:37:10.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 02:37:10.478
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:10.525
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:10.531
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-def30780-ee2b-4793-9c6b-ac20beba12bf 06/13/23 02:37:10.538
    STEP: Creating a pod to test consume configMaps 06/13/23 02:37:10.555
    Jun 13 02:37:10.572: INFO: Waiting up to 5m0s for pod "pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3" in namespace "configmap-8277" to be "Succeeded or Failed"
    Jun 13 02:37:10.595: INFO: Pod "pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 22.603301ms
    Jun 13 02:37:12.604: INFO: Pod "pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032013495s
    Jun 13 02:37:14.606: INFO: Pod "pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033528687s
    STEP: Saw pod success 06/13/23 02:37:14.606
    Jun 13 02:37:14.606: INFO: Pod "pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3" satisfied condition "Succeeded or Failed"
    Jun 13 02:37:14.614: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3 container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 02:37:14.628
    Jun 13 02:37:14.651: INFO: Waiting for pod pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3 to disappear
    Jun 13 02:37:14.658: INFO: Pod pod-configmaps-b1490b8f-7021-4162-a495-ac87e6b6a9a3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 02:37:14.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8277" for this suite. 06/13/23 02:37:14.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:37:14.689
Jun 13 02:37:14.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename runtimeclass 06/13/23 02:37:14.69
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:14.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:14.736
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jun 13 02:37:14.774: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7761 to be scheduled
Jun 13 02:37:14.785: INFO: 1 pods are not scheduled: [runtimeclass-7761/test-runtimeclass-runtimeclass-7761-preconfigured-handler-gcdl7(746169af-8b17-4061-9034-81565be3a471)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 13 02:37:16.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7761" for this suite. 06/13/23 02:37:16.826
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":83,"skipped":1509,"failed":0}
------------------------------
• [2.155 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:37:14.689
    Jun 13 02:37:14.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename runtimeclass 06/13/23 02:37:14.69
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:14.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:14.736
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jun 13 02:37:14.774: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7761 to be scheduled
    Jun 13 02:37:14.785: INFO: 1 pods are not scheduled: [runtimeclass-7761/test-runtimeclass-runtimeclass-7761-preconfigured-handler-gcdl7(746169af-8b17-4061-9034-81565be3a471)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 13 02:37:16.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7761" for this suite. 06/13/23 02:37:16.826
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:37:16.845
Jun 13 02:37:16.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 02:37:16.846
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:16.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:16.887
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-bf082382-417a-414b-a147-de51af96e933 06/13/23 02:37:16.902
STEP: Creating the pod 06/13/23 02:37:16.913
Jun 13 02:37:16.930: INFO: Waiting up to 5m0s for pod "pod-configmaps-179c2d05-989b-4053-a331-b60ce10c933b" in namespace "configmap-1194" to be "running"
Jun 13 02:37:16.948: INFO: Pod "pod-configmaps-179c2d05-989b-4053-a331-b60ce10c933b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.10134ms
Jun 13 02:37:18.963: INFO: Pod "pod-configmaps-179c2d05-989b-4053-a331-b60ce10c933b": Phase="Running", Reason="", readiness=false. Elapsed: 2.03246271s
Jun 13 02:37:18.963: INFO: Pod "pod-configmaps-179c2d05-989b-4053-a331-b60ce10c933b" satisfied condition "running"
STEP: Waiting for pod with text data 06/13/23 02:37:18.963
STEP: Waiting for pod with binary data 06/13/23 02:37:18.983
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 02:37:18.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1194" for this suite. 06/13/23 02:37:19.017
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":84,"skipped":1515,"failed":0}
------------------------------
• [2.206 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:37:16.845
    Jun 13 02:37:16.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 02:37:16.846
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:16.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:16.887
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-bf082382-417a-414b-a147-de51af96e933 06/13/23 02:37:16.902
    STEP: Creating the pod 06/13/23 02:37:16.913
    Jun 13 02:37:16.930: INFO: Waiting up to 5m0s for pod "pod-configmaps-179c2d05-989b-4053-a331-b60ce10c933b" in namespace "configmap-1194" to be "running"
    Jun 13 02:37:16.948: INFO: Pod "pod-configmaps-179c2d05-989b-4053-a331-b60ce10c933b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.10134ms
    Jun 13 02:37:18.963: INFO: Pod "pod-configmaps-179c2d05-989b-4053-a331-b60ce10c933b": Phase="Running", Reason="", readiness=false. Elapsed: 2.03246271s
    Jun 13 02:37:18.963: INFO: Pod "pod-configmaps-179c2d05-989b-4053-a331-b60ce10c933b" satisfied condition "running"
    STEP: Waiting for pod with text data 06/13/23 02:37:18.963
    STEP: Waiting for pod with binary data 06/13/23 02:37:18.983
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 02:37:18.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1194" for this suite. 06/13/23 02:37:19.017
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:37:19.054
Jun 13 02:37:19.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename endpointslicemirroring 06/13/23 02:37:19.056
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:19.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:19.106
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 06/13/23 02:37:19.194
Jun 13 02:37:19.224: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 06/13/23 02:37:21.241
STEP: mirroring deletion of a custom Endpoint 06/13/23 02:37:21.269
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jun 13 02:37:21.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-4474" for this suite. 06/13/23 02:37:21.323
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":85,"skipped":1531,"failed":0}
------------------------------
• [2.289 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:37:19.054
    Jun 13 02:37:19.054: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename endpointslicemirroring 06/13/23 02:37:19.056
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:19.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:19.106
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 06/13/23 02:37:19.194
    Jun 13 02:37:19.224: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 06/13/23 02:37:21.241
    STEP: mirroring deletion of a custom Endpoint 06/13/23 02:37:21.269
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jun 13 02:37:21.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-4474" for this suite. 06/13/23 02:37:21.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:37:21.344
Jun 13 02:37:21.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 02:37:21.345
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:21.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:21.394
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 06/13/23 02:37:21.404
Jun 13 02:37:21.405: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-6810 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 06/13/23 02:37:21.498
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 02:37:21.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6810" for this suite. 06/13/23 02:37:21.531
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":86,"skipped":1538,"failed":0}
------------------------------
• [0.209 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:37:21.344
    Jun 13 02:37:21.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 02:37:21.345
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:21.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:21.394
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 06/13/23 02:37:21.404
    Jun 13 02:37:21.405: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-6810 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 06/13/23 02:37:21.498
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 02:37:21.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6810" for this suite. 06/13/23 02:37:21.531
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:37:21.554
Jun 13 02:37:21.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 02:37:21.555
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:21.589
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:21.594
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 06/13/23 02:37:21.602
Jun 13 02:37:21.621: INFO: Waiting up to 5m0s for pod "pod-0e164f59-2064-4b51-ae64-fab01fcd069a" in namespace "emptydir-7487" to be "Succeeded or Failed"
Jun 13 02:37:21.638: INFO: Pod "pod-0e164f59-2064-4b51-ae64-fab01fcd069a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.03102ms
Jun 13 02:37:23.647: INFO: Pod "pod-0e164f59-2064-4b51-ae64-fab01fcd069a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025900361s
Jun 13 02:37:25.648: INFO: Pod "pod-0e164f59-2064-4b51-ae64-fab01fcd069a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026418898s
STEP: Saw pod success 06/13/23 02:37:25.648
Jun 13 02:37:25.648: INFO: Pod "pod-0e164f59-2064-4b51-ae64-fab01fcd069a" satisfied condition "Succeeded or Failed"
Jun 13 02:37:25.662: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-l5gcd pod pod-0e164f59-2064-4b51-ae64-fab01fcd069a container test-container: <nil>
STEP: delete the pod 06/13/23 02:37:25.733
Jun 13 02:37:25.782: INFO: Waiting for pod pod-0e164f59-2064-4b51-ae64-fab01fcd069a to disappear
Jun 13 02:37:25.789: INFO: Pod pod-0e164f59-2064-4b51-ae64-fab01fcd069a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 02:37:25.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7487" for this suite. 06/13/23 02:37:25.841
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":87,"skipped":1539,"failed":0}
------------------------------
• [4.303 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:37:21.554
    Jun 13 02:37:21.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 02:37:21.555
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:21.589
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:21.594
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 06/13/23 02:37:21.602
    Jun 13 02:37:21.621: INFO: Waiting up to 5m0s for pod "pod-0e164f59-2064-4b51-ae64-fab01fcd069a" in namespace "emptydir-7487" to be "Succeeded or Failed"
    Jun 13 02:37:21.638: INFO: Pod "pod-0e164f59-2064-4b51-ae64-fab01fcd069a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.03102ms
    Jun 13 02:37:23.647: INFO: Pod "pod-0e164f59-2064-4b51-ae64-fab01fcd069a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025900361s
    Jun 13 02:37:25.648: INFO: Pod "pod-0e164f59-2064-4b51-ae64-fab01fcd069a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026418898s
    STEP: Saw pod success 06/13/23 02:37:25.648
    Jun 13 02:37:25.648: INFO: Pod "pod-0e164f59-2064-4b51-ae64-fab01fcd069a" satisfied condition "Succeeded or Failed"
    Jun 13 02:37:25.662: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-l5gcd pod pod-0e164f59-2064-4b51-ae64-fab01fcd069a container test-container: <nil>
    STEP: delete the pod 06/13/23 02:37:25.733
    Jun 13 02:37:25.782: INFO: Waiting for pod pod-0e164f59-2064-4b51-ae64-fab01fcd069a to disappear
    Jun 13 02:37:25.789: INFO: Pod pod-0e164f59-2064-4b51-ae64-fab01fcd069a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 02:37:25.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7487" for this suite. 06/13/23 02:37:25.841
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:37:25.858
Jun 13 02:37:25.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pod-network-test 06/13/23 02:37:25.859
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:25.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:25.961
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-9588 06/13/23 02:37:25.969
STEP: creating a selector 06/13/23 02:37:25.969
STEP: Creating the service pods in kubernetes 06/13/23 02:37:25.969
Jun 13 02:37:25.969: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 13 02:37:26.088: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9588" to be "running and ready"
Jun 13 02:37:26.109: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.587173ms
Jun 13 02:37:26.109: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:37:28.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.02795286s
Jun 13 02:37:28.116: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:37:30.119: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.031080826s
Jun 13 02:37:30.119: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:37:32.122: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.03398178s
Jun 13 02:37:32.122: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:37:34.145: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.056632644s
Jun 13 02:37:34.145: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:37:36.118: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.030077627s
Jun 13 02:37:36.118: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:37:38.119: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.030959034s
Jun 13 02:37:38.119: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:37:40.118: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.03020061s
Jun 13 02:37:40.118: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:37:42.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.031641252s
Jun 13 02:37:42.120: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:37:44.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.028323102s
Jun 13 02:37:44.116: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:37:46.122: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.03424201s
Jun 13 02:37:46.122: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:37:48.124: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.036426958s
Jun 13 02:37:48.125: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 13 02:37:48.125: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 13 02:37:48.133: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9588" to be "running and ready"
Jun 13 02:37:48.141: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.701622ms
Jun 13 02:37:48.142: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 13 02:37:48.142: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 13 02:37:48.151: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9588" to be "running and ready"
Jun 13 02:37:48.160: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.232325ms
Jun 13 02:37:48.160: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 13 02:37:48.160: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/13/23 02:37:48.171
Jun 13 02:37:48.191: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9588" to be "running"
Jun 13 02:37:48.199: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.894255ms
Jun 13 02:37:50.212: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020840802s
Jun 13 02:37:50.212: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 13 02:37:50.224: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 13 02:37:50.224: INFO: Breadth first check of 172.16.172.27 on host 10.255.64.103...
Jun 13 02:37:50.234: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.77.138:9080/dial?request=hostname&protocol=udp&host=172.16.172.27&port=8081&tries=1'] Namespace:pod-network-test-9588 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 02:37:50.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:37:50.235: INFO: ExecWithOptions: Clientset creation
Jun 13 02:37:50.235: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9588/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.77.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.172.27%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 13 02:37:50.354: INFO: Waiting for responses: map[]
Jun 13 02:37:50.354: INFO: reached 172.16.172.27 after 0/1 tries
Jun 13 02:37:50.354: INFO: Breadth first check of 172.30.77.135 on host 10.255.64.102...
Jun 13 02:37:50.366: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.77.138:9080/dial?request=hostname&protocol=udp&host=172.30.77.135&port=8081&tries=1'] Namespace:pod-network-test-9588 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 02:37:50.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:37:50.367: INFO: ExecWithOptions: Clientset creation
Jun 13 02:37:50.367: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9588/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.77.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.77.135%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 13 02:37:50.510: INFO: Waiting for responses: map[]
Jun 13 02:37:50.510: INFO: reached 172.30.77.135 after 0/1 tries
Jun 13 02:37:50.510: INFO: Breadth first check of 172.28.156.240 on host 10.255.64.104...
Jun 13 02:37:50.525: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.77.138:9080/dial?request=hostname&protocol=udp&host=172.28.156.240&port=8081&tries=1'] Namespace:pod-network-test-9588 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 02:37:50.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:37:50.526: INFO: ExecWithOptions: Clientset creation
Jun 13 02:37:50.526: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9588/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.77.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.28.156.240%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 13 02:37:50.635: INFO: Waiting for responses: map[]
Jun 13 02:37:50.635: INFO: reached 172.28.156.240 after 0/1 tries
Jun 13 02:37:50.635: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jun 13 02:37:50.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9588" for this suite. 06/13/23 02:37:50.649
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":88,"skipped":1547,"failed":0}
------------------------------
• [SLOW TEST] [24.807 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:37:25.858
    Jun 13 02:37:25.858: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pod-network-test 06/13/23 02:37:25.859
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:25.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:25.961
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-9588 06/13/23 02:37:25.969
    STEP: creating a selector 06/13/23 02:37:25.969
    STEP: Creating the service pods in kubernetes 06/13/23 02:37:25.969
    Jun 13 02:37:25.969: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun 13 02:37:26.088: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9588" to be "running and ready"
    Jun 13 02:37:26.109: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 20.587173ms
    Jun 13 02:37:26.109: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:37:28.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.02795286s
    Jun 13 02:37:28.116: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:37:30.119: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.031080826s
    Jun 13 02:37:30.119: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:37:32.122: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.03398178s
    Jun 13 02:37:32.122: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:37:34.145: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.056632644s
    Jun 13 02:37:34.145: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:37:36.118: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.030077627s
    Jun 13 02:37:36.118: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:37:38.119: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.030959034s
    Jun 13 02:37:38.119: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:37:40.118: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.03020061s
    Jun 13 02:37:40.118: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:37:42.120: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.031641252s
    Jun 13 02:37:42.120: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:37:44.116: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.028323102s
    Jun 13 02:37:44.116: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:37:46.122: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.03424201s
    Jun 13 02:37:46.122: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:37:48.124: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.036426958s
    Jun 13 02:37:48.125: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 13 02:37:48.125: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 13 02:37:48.133: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9588" to be "running and ready"
    Jun 13 02:37:48.141: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.701622ms
    Jun 13 02:37:48.142: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 13 02:37:48.142: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 13 02:37:48.151: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9588" to be "running and ready"
    Jun 13 02:37:48.160: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.232325ms
    Jun 13 02:37:48.160: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 13 02:37:48.160: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/13/23 02:37:48.171
    Jun 13 02:37:48.191: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9588" to be "running"
    Jun 13 02:37:48.199: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.894255ms
    Jun 13 02:37:50.212: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.020840802s
    Jun 13 02:37:50.212: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 13 02:37:50.224: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 13 02:37:50.224: INFO: Breadth first check of 172.16.172.27 on host 10.255.64.103...
    Jun 13 02:37:50.234: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.77.138:9080/dial?request=hostname&protocol=udp&host=172.16.172.27&port=8081&tries=1'] Namespace:pod-network-test-9588 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 02:37:50.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:37:50.235: INFO: ExecWithOptions: Clientset creation
    Jun 13 02:37:50.235: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9588/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.77.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.172.27%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 13 02:37:50.354: INFO: Waiting for responses: map[]
    Jun 13 02:37:50.354: INFO: reached 172.16.172.27 after 0/1 tries
    Jun 13 02:37:50.354: INFO: Breadth first check of 172.30.77.135 on host 10.255.64.102...
    Jun 13 02:37:50.366: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.77.138:9080/dial?request=hostname&protocol=udp&host=172.30.77.135&port=8081&tries=1'] Namespace:pod-network-test-9588 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 02:37:50.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:37:50.367: INFO: ExecWithOptions: Clientset creation
    Jun 13 02:37:50.367: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9588/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.77.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.77.135%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 13 02:37:50.510: INFO: Waiting for responses: map[]
    Jun 13 02:37:50.510: INFO: reached 172.30.77.135 after 0/1 tries
    Jun 13 02:37:50.510: INFO: Breadth first check of 172.28.156.240 on host 10.255.64.104...
    Jun 13 02:37:50.525: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.77.138:9080/dial?request=hostname&protocol=udp&host=172.28.156.240&port=8081&tries=1'] Namespace:pod-network-test-9588 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 02:37:50.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:37:50.526: INFO: ExecWithOptions: Clientset creation
    Jun 13 02:37:50.526: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9588/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.77.138%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.28.156.240%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 13 02:37:50.635: INFO: Waiting for responses: map[]
    Jun 13 02:37:50.635: INFO: reached 172.28.156.240 after 0/1 tries
    Jun 13 02:37:50.635: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jun 13 02:37:50.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-9588" for this suite. 06/13/23 02:37:50.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:37:50.665
Jun 13 02:37:50.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename containers 06/13/23 02:37:50.667
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:50.704
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:50.709
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jun 13 02:37:50.734: INFO: Waiting up to 5m0s for pod "client-containers-896721db-d099-412f-894c-92e835ccf639" in namespace "containers-1297" to be "running"
Jun 13 02:37:50.744: INFO: Pod "client-containers-896721db-d099-412f-894c-92e835ccf639": Phase="Pending", Reason="", readiness=false. Elapsed: 9.634314ms
Jun 13 02:37:52.754: INFO: Pod "client-containers-896721db-d099-412f-894c-92e835ccf639": Phase="Running", Reason="", readiness=true. Elapsed: 2.019978567s
Jun 13 02:37:52.754: INFO: Pod "client-containers-896721db-d099-412f-894c-92e835ccf639" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jun 13 02:37:52.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1297" for this suite. 06/13/23 02:37:52.776
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":89,"skipped":1554,"failed":0}
------------------------------
• [2.137 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:37:50.665
    Jun 13 02:37:50.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename containers 06/13/23 02:37:50.667
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:50.704
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:50.709
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jun 13 02:37:50.734: INFO: Waiting up to 5m0s for pod "client-containers-896721db-d099-412f-894c-92e835ccf639" in namespace "containers-1297" to be "running"
    Jun 13 02:37:50.744: INFO: Pod "client-containers-896721db-d099-412f-894c-92e835ccf639": Phase="Pending", Reason="", readiness=false. Elapsed: 9.634314ms
    Jun 13 02:37:52.754: INFO: Pod "client-containers-896721db-d099-412f-894c-92e835ccf639": Phase="Running", Reason="", readiness=true. Elapsed: 2.019978567s
    Jun 13 02:37:52.754: INFO: Pod "client-containers-896721db-d099-412f-894c-92e835ccf639" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jun 13 02:37:52.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1297" for this suite. 06/13/23 02:37:52.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:37:52.805
Jun 13 02:37:52.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 02:37:52.807
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:52.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:52.859
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-488094d8-91c5-48f3-ad98-97bf1229ed72 06/13/23 02:37:52.867
STEP: Creating a pod to test consume secrets 06/13/23 02:37:52.878
Jun 13 02:37:52.906: INFO: Waiting up to 5m0s for pod "pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce" in namespace "secrets-4421" to be "Succeeded or Failed"
Jun 13 02:37:52.915: INFO: Pod "pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.311147ms
Jun 13 02:37:54.923: INFO: Pod "pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016716237s
Jun 13 02:37:56.925: INFO: Pod "pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018298703s
STEP: Saw pod success 06/13/23 02:37:56.925
Jun 13 02:37:56.925: INFO: Pod "pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce" satisfied condition "Succeeded or Failed"
Jun 13 02:37:56.934: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-l5gcd pod pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce container secret-volume-test: <nil>
STEP: delete the pod 06/13/23 02:37:56.951
Jun 13 02:37:56.986: INFO: Waiting for pod pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce to disappear
Jun 13 02:37:57.001: INFO: Pod pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 13 02:37:57.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4421" for this suite. 06/13/23 02:37:57.018
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":90,"skipped":1598,"failed":0}
------------------------------
• [4.240 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:37:52.805
    Jun 13 02:37:52.805: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 02:37:52.807
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:52.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:52.859
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-488094d8-91c5-48f3-ad98-97bf1229ed72 06/13/23 02:37:52.867
    STEP: Creating a pod to test consume secrets 06/13/23 02:37:52.878
    Jun 13 02:37:52.906: INFO: Waiting up to 5m0s for pod "pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce" in namespace "secrets-4421" to be "Succeeded or Failed"
    Jun 13 02:37:52.915: INFO: Pod "pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce": Phase="Pending", Reason="", readiness=false. Elapsed: 8.311147ms
    Jun 13 02:37:54.923: INFO: Pod "pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016716237s
    Jun 13 02:37:56.925: INFO: Pod "pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018298703s
    STEP: Saw pod success 06/13/23 02:37:56.925
    Jun 13 02:37:56.925: INFO: Pod "pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce" satisfied condition "Succeeded or Failed"
    Jun 13 02:37:56.934: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-l5gcd pod pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce container secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 02:37:56.951
    Jun 13 02:37:56.986: INFO: Waiting for pod pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce to disappear
    Jun 13 02:37:57.001: INFO: Pod pod-secrets-a79f8ffd-898e-448e-be91-048dd8606fce no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 02:37:57.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4421" for this suite. 06/13/23 02:37:57.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:37:57.046
Jun 13 02:37:57.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:37:57.048
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:57.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:57.108
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 06/13/23 02:37:57.116
Jun 13 02:37:57.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c" in namespace "projected-3305" to be "Succeeded or Failed"
Jun 13 02:37:57.145: INFO: Pod "downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.175047ms
Jun 13 02:37:59.153: INFO: Pod "downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015168224s
Jun 13 02:38:01.154: INFO: Pod "downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016675677s
STEP: Saw pod success 06/13/23 02:38:01.154
Jun 13 02:38:01.155: INFO: Pod "downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c" satisfied condition "Succeeded or Failed"
Jun 13 02:38:01.166: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c container client-container: <nil>
STEP: delete the pod 06/13/23 02:38:01.186
Jun 13 02:38:01.212: INFO: Waiting for pod downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c to disappear
Jun 13 02:38:01.221: INFO: Pod downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 13 02:38:01.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3305" for this suite. 06/13/23 02:38:01.248
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":91,"skipped":1605,"failed":0}
------------------------------
• [4.226 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:37:57.046
    Jun 13 02:37:57.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:37:57.048
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:37:57.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:37:57.108
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 06/13/23 02:37:57.116
    Jun 13 02:37:57.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c" in namespace "projected-3305" to be "Succeeded or Failed"
    Jun 13 02:37:57.145: INFO: Pod "downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.175047ms
    Jun 13 02:37:59.153: INFO: Pod "downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015168224s
    Jun 13 02:38:01.154: INFO: Pod "downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016675677s
    STEP: Saw pod success 06/13/23 02:38:01.154
    Jun 13 02:38:01.155: INFO: Pod "downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c" satisfied condition "Succeeded or Failed"
    Jun 13 02:38:01.166: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c container client-container: <nil>
    STEP: delete the pod 06/13/23 02:38:01.186
    Jun 13 02:38:01.212: INFO: Waiting for pod downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c to disappear
    Jun 13 02:38:01.221: INFO: Pod downwardapi-volume-ede8f581-84bc-4593-9f69-74633fd0904c no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 13 02:38:01.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3305" for this suite. 06/13/23 02:38:01.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:38:01.273
Jun 13 02:38:01.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename replicaset 06/13/23 02:38:01.275
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:38:01.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:38:01.331
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jun 13 02:38:01.371: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 13 02:38:06.389: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/13/23 02:38:06.389
STEP: Scaling up "test-rs" replicaset  06/13/23 02:38:06.39
Jun 13 02:38:06.434: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 06/13/23 02:38:06.434
W0613 02:38:06.507166      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun 13 02:38:06.512: INFO: observed ReplicaSet test-rs in namespace replicaset-3574 with ReadyReplicas 1, AvailableReplicas 1
Jun 13 02:38:06.624: INFO: observed ReplicaSet test-rs in namespace replicaset-3574 with ReadyReplicas 1, AvailableReplicas 1
Jun 13 02:38:06.898: INFO: observed ReplicaSet test-rs in namespace replicaset-3574 with ReadyReplicas 1, AvailableReplicas 1
Jun 13 02:38:06.980: INFO: observed ReplicaSet test-rs in namespace replicaset-3574 with ReadyReplicas 1, AvailableReplicas 1
Jun 13 02:38:08.299: INFO: observed ReplicaSet test-rs in namespace replicaset-3574 with ReadyReplicas 2, AvailableReplicas 2
Jun 13 02:38:08.746: INFO: observed Replicaset test-rs in namespace replicaset-3574 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 13 02:38:08.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3574" for this suite. 06/13/23 02:38:08.768
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":92,"skipped":1613,"failed":0}
------------------------------
• [SLOW TEST] [7.517 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:38:01.273
    Jun 13 02:38:01.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename replicaset 06/13/23 02:38:01.275
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:38:01.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:38:01.331
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jun 13 02:38:01.371: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 13 02:38:06.389: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/13/23 02:38:06.389
    STEP: Scaling up "test-rs" replicaset  06/13/23 02:38:06.39
    Jun 13 02:38:06.434: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 06/13/23 02:38:06.434
    W0613 02:38:06.507166      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun 13 02:38:06.512: INFO: observed ReplicaSet test-rs in namespace replicaset-3574 with ReadyReplicas 1, AvailableReplicas 1
    Jun 13 02:38:06.624: INFO: observed ReplicaSet test-rs in namespace replicaset-3574 with ReadyReplicas 1, AvailableReplicas 1
    Jun 13 02:38:06.898: INFO: observed ReplicaSet test-rs in namespace replicaset-3574 with ReadyReplicas 1, AvailableReplicas 1
    Jun 13 02:38:06.980: INFO: observed ReplicaSet test-rs in namespace replicaset-3574 with ReadyReplicas 1, AvailableReplicas 1
    Jun 13 02:38:08.299: INFO: observed ReplicaSet test-rs in namespace replicaset-3574 with ReadyReplicas 2, AvailableReplicas 2
    Jun 13 02:38:08.746: INFO: observed Replicaset test-rs in namespace replicaset-3574 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 13 02:38:08.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3574" for this suite. 06/13/23 02:38:08.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:38:08.794
Jun 13 02:38:08.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 02:38:08.795
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:38:08.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:38:08.838
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 02:38:08.924
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 02:38:09.439
STEP: Deploying the webhook pod 06/13/23 02:38:09.466
STEP: Wait for the deployment to be ready 06/13/23 02:38:09.499
Jun 13 02:38:09.528: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 13 02:38:11.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 2, 38, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 2, 38, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 2, 38, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 2, 38, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/13/23 02:38:13.568
STEP: Verifying the service has paired with the endpoint 06/13/23 02:38:13.725
Jun 13 02:38:14.726: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 06/13/23 02:38:14.736
STEP: Creating a configMap that does not comply to the validation webhook rules 06/13/23 02:38:14.766
STEP: Updating a validating webhook configuration's rules to not include the create operation 06/13/23 02:38:14.79
STEP: Creating a configMap that does not comply to the validation webhook rules 06/13/23 02:38:14.824
STEP: Patching a validating webhook configuration's rules to include the create operation 06/13/23 02:38:14.857
STEP: Creating a configMap that does not comply to the validation webhook rules 06/13/23 02:38:14.874
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 02:38:14.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6652" for this suite. 06/13/23 02:38:14.911
STEP: Destroying namespace "webhook-6652-markers" for this suite. 06/13/23 02:38:14.931
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":93,"skipped":1668,"failed":0}
------------------------------
• [SLOW TEST] [6.284 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:38:08.794
    Jun 13 02:38:08.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 02:38:08.795
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:38:08.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:38:08.838
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 02:38:08.924
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 02:38:09.439
    STEP: Deploying the webhook pod 06/13/23 02:38:09.466
    STEP: Wait for the deployment to be ready 06/13/23 02:38:09.499
    Jun 13 02:38:09.528: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 13 02:38:11.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 2, 38, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 2, 38, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 2, 38, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 2, 38, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/13/23 02:38:13.568
    STEP: Verifying the service has paired with the endpoint 06/13/23 02:38:13.725
    Jun 13 02:38:14.726: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 06/13/23 02:38:14.736
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/13/23 02:38:14.766
    STEP: Updating a validating webhook configuration's rules to not include the create operation 06/13/23 02:38:14.79
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/13/23 02:38:14.824
    STEP: Patching a validating webhook configuration's rules to include the create operation 06/13/23 02:38:14.857
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/13/23 02:38:14.874
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 02:38:14.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6652" for this suite. 06/13/23 02:38:14.911
    STEP: Destroying namespace "webhook-6652-markers" for this suite. 06/13/23 02:38:14.931
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:38:15.08
Jun 13 02:38:15.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pods 06/13/23 02:38:15.082
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:38:15.135
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:38:15.141
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 06/13/23 02:38:15.172
STEP: watching for Pod to be ready 06/13/23 02:38:15.189
Jun 13 02:38:15.192: INFO: observed Pod pod-test in namespace pods-8794 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jun 13 02:38:15.220: INFO: observed Pod pod-test in namespace pods-8794 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  }]
Jun 13 02:38:15.246: INFO: observed Pod pod-test in namespace pods-8794 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  }]
Jun 13 02:38:16.051: INFO: observed Pod pod-test in namespace pods-8794 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  }]
Jun 13 02:38:17.838: INFO: Found Pod pod-test in namespace pods-8794 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 06/13/23 02:38:17.854
STEP: getting the Pod and ensuring that it's patched 06/13/23 02:38:17.879
STEP: replacing the Pod's status Ready condition to False 06/13/23 02:38:17.895
STEP: check the Pod again to ensure its Ready conditions are False 06/13/23 02:38:17.928
STEP: deleting the Pod via a Collection with a LabelSelector 06/13/23 02:38:17.928
STEP: watching for the Pod to be deleted 06/13/23 02:38:17.96
Jun 13 02:38:17.965: INFO: observed event type MODIFIED
Jun 13 02:38:19.826: INFO: observed event type MODIFIED
Jun 13 02:38:20.101: INFO: observed event type MODIFIED
Jun 13 02:38:20.845: INFO: observed event type MODIFIED
Jun 13 02:38:20.861: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 13 02:38:20.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8794" for this suite. 06/13/23 02:38:20.907
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":94,"skipped":1693,"failed":0}
------------------------------
• [SLOW TEST] [5.863 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:38:15.08
    Jun 13 02:38:15.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pods 06/13/23 02:38:15.082
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:38:15.135
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:38:15.141
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 06/13/23 02:38:15.172
    STEP: watching for Pod to be ready 06/13/23 02:38:15.189
    Jun 13 02:38:15.192: INFO: observed Pod pod-test in namespace pods-8794 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jun 13 02:38:15.220: INFO: observed Pod pod-test in namespace pods-8794 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  }]
    Jun 13 02:38:15.246: INFO: observed Pod pod-test in namespace pods-8794 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  }]
    Jun 13 02:38:16.051: INFO: observed Pod pod-test in namespace pods-8794 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  }]
    Jun 13 02:38:17.838: INFO: Found Pod pod-test in namespace pods-8794 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 02:38:15 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 06/13/23 02:38:17.854
    STEP: getting the Pod and ensuring that it's patched 06/13/23 02:38:17.879
    STEP: replacing the Pod's status Ready condition to False 06/13/23 02:38:17.895
    STEP: check the Pod again to ensure its Ready conditions are False 06/13/23 02:38:17.928
    STEP: deleting the Pod via a Collection with a LabelSelector 06/13/23 02:38:17.928
    STEP: watching for the Pod to be deleted 06/13/23 02:38:17.96
    Jun 13 02:38:17.965: INFO: observed event type MODIFIED
    Jun 13 02:38:19.826: INFO: observed event type MODIFIED
    Jun 13 02:38:20.101: INFO: observed event type MODIFIED
    Jun 13 02:38:20.845: INFO: observed event type MODIFIED
    Jun 13 02:38:20.861: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 13 02:38:20.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8794" for this suite. 06/13/23 02:38:20.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:38:20.949
Jun 13 02:38:20.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename dns 06/13/23 02:38:20.95
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:38:21.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:38:21.037
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 06/13/23 02:38:21.045
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7735 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7735;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7735 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7735;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7735.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7735.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7735.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7735.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7735.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7735.svc;check="$$(dig +notcp +noall +answer +search 67.176.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.176.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.176.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.176.67_tcp@PTR;sleep 1; done
 06/13/23 02:38:21.209
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7735 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7735;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7735 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7735;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7735.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7735.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7735.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7735.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7735.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7735.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7735.svc;check="$$(dig +notcp +noall +answer +search 67.176.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.176.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.176.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.176.67_tcp@PTR;sleep 1; done
 06/13/23 02:38:21.21
STEP: creating a pod to probe DNS 06/13/23 02:38:21.21
STEP: submitting the pod to kubernetes 06/13/23 02:38:21.21
Jun 13 02:38:21.272: INFO: Waiting up to 15m0s for pod "dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255" in namespace "dns-7735" to be "running"
Jun 13 02:38:21.297: INFO: Pod "dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255": Phase="Pending", Reason="", readiness=false. Elapsed: 25.319801ms
Jun 13 02:38:23.307: INFO: Pod "dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255": Phase="Running", Reason="", readiness=true. Elapsed: 2.035638607s
Jun 13 02:38:23.307: INFO: Pod "dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255" satisfied condition "running"
STEP: retrieving the pod 06/13/23 02:38:23.307
STEP: looking for the results for each expected name from probers 06/13/23 02:38:23.315
Jun 13 02:38:23.334: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.348: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.361: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.371: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.381: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.396: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.408: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.418: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.478: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.491: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.502: INFO: Unable to read jessie_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.511: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.520: INFO: Unable to read jessie_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.531: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.539: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.547: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:23.594: INFO: Lookups using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7735 wheezy_tcp@dns-test-service.dns-7735 wheezy_udp@dns-test-service.dns-7735.svc wheezy_tcp@dns-test-service.dns-7735.svc wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7735 jessie_tcp@dns-test-service.dns-7735 jessie_udp@dns-test-service.dns-7735.svc jessie_tcp@dns-test-service.dns-7735.svc jessie_udp@_http._tcp.dns-test-service.dns-7735.svc jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc]

Jun 13 02:38:28.608: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.617: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.627: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.636: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.646: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.656: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.666: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.677: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.734: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.746: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.756: INFO: Unable to read jessie_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.767: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.776: INFO: Unable to read jessie_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.786: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.801: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.815: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:28.893: INFO: Lookups using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7735 wheezy_tcp@dns-test-service.dns-7735 wheezy_udp@dns-test-service.dns-7735.svc wheezy_tcp@dns-test-service.dns-7735.svc wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7735 jessie_tcp@dns-test-service.dns-7735 jessie_udp@dns-test-service.dns-7735.svc jessie_tcp@dns-test-service.dns-7735.svc jessie_udp@_http._tcp.dns-test-service.dns-7735.svc jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc]

Jun 13 02:38:33.608: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.625: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.647: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.658: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.667: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.678: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.692: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.752: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.766: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.778: INFO: Unable to read jessie_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.794: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.807: INFO: Unable to read jessie_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.818: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.828: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.865: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:33.984: INFO: Lookups using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7735 wheezy_tcp@dns-test-service.dns-7735 wheezy_udp@dns-test-service.dns-7735.svc wheezy_tcp@dns-test-service.dns-7735.svc wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7735 jessie_tcp@dns-test-service.dns-7735 jessie_udp@dns-test-service.dns-7735.svc jessie_tcp@dns-test-service.dns-7735.svc jessie_udp@_http._tcp.dns-test-service.dns-7735.svc jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc]

Jun 13 02:38:38.607: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.619: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.629: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.640: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.650: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.664: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.674: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.683: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.733: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.741: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.750: INFO: Unable to read jessie_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.774: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.785: INFO: Unable to read jessie_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.796: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.805: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.815: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:38.852: INFO: Lookups using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7735 wheezy_tcp@dns-test-service.dns-7735 wheezy_udp@dns-test-service.dns-7735.svc wheezy_tcp@dns-test-service.dns-7735.svc wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7735 jessie_tcp@dns-test-service.dns-7735 jessie_udp@dns-test-service.dns-7735.svc jessie_tcp@dns-test-service.dns-7735.svc jessie_udp@_http._tcp.dns-test-service.dns-7735.svc jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc]

Jun 13 02:38:43.612: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.622: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.632: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.645: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.653: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.665: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.677: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.686: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.745: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.754: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.762: INFO: Unable to read jessie_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.774: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.795: INFO: Unable to read jessie_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.804: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.818: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:43.898: INFO: Lookups using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7735 wheezy_tcp@dns-test-service.dns-7735 wheezy_udp@dns-test-service.dns-7735.svc wheezy_tcp@dns-test-service.dns-7735.svc wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7735 jessie_tcp@dns-test-service.dns-7735 jessie_udp@dns-test-service.dns-7735.svc jessie_tcp@dns-test-service.dns-7735.svc jessie_udp@_http._tcp.dns-test-service.dns-7735.svc jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc]

Jun 13 02:38:48.607: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.617: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.633: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.647: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.675: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.694: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.714: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.795: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.805: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.821: INFO: Unable to read jessie_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.839: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.851: INFO: Unable to read jessie_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.866: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.883: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.894: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
Jun 13 02:38:48.951: INFO: Lookups using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7735 wheezy_tcp@dns-test-service.dns-7735 wheezy_udp@dns-test-service.dns-7735.svc wheezy_tcp@dns-test-service.dns-7735.svc wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7735 jessie_tcp@dns-test-service.dns-7735 jessie_udp@dns-test-service.dns-7735.svc jessie_tcp@dns-test-service.dns-7735.svc jessie_udp@_http._tcp.dns-test-service.dns-7735.svc jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc]

Jun 13 02:38:53.887: INFO: DNS probes using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 succeeded

STEP: deleting the pod 06/13/23 02:38:53.887
STEP: deleting the test service 06/13/23 02:38:53.943
STEP: deleting the test headless service 06/13/23 02:38:54.135
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 13 02:38:54.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7735" for this suite. 06/13/23 02:38:54.22
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":95,"skipped":1726,"failed":0}
------------------------------
• [SLOW TEST] [33.291 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:38:20.949
    Jun 13 02:38:20.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename dns 06/13/23 02:38:20.95
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:38:21.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:38:21.037
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 06/13/23 02:38:21.045
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7735 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7735;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7735 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7735;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7735.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7735.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7735.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7735.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7735.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7735.svc;check="$$(dig +notcp +noall +answer +search 67.176.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.176.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.176.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.176.67_tcp@PTR;sleep 1; done
     06/13/23 02:38:21.209
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7735 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7735;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7735 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7735;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7735.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7735.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7735.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7735.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7735.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7735.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7735.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7735.svc;check="$$(dig +notcp +noall +answer +search 67.176.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.176.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.176.104.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.104.176.67_tcp@PTR;sleep 1; done
     06/13/23 02:38:21.21
    STEP: creating a pod to probe DNS 06/13/23 02:38:21.21
    STEP: submitting the pod to kubernetes 06/13/23 02:38:21.21
    Jun 13 02:38:21.272: INFO: Waiting up to 15m0s for pod "dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255" in namespace "dns-7735" to be "running"
    Jun 13 02:38:21.297: INFO: Pod "dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255": Phase="Pending", Reason="", readiness=false. Elapsed: 25.319801ms
    Jun 13 02:38:23.307: INFO: Pod "dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255": Phase="Running", Reason="", readiness=true. Elapsed: 2.035638607s
    Jun 13 02:38:23.307: INFO: Pod "dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255" satisfied condition "running"
    STEP: retrieving the pod 06/13/23 02:38:23.307
    STEP: looking for the results for each expected name from probers 06/13/23 02:38:23.315
    Jun 13 02:38:23.334: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.348: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.361: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.371: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.381: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.396: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.408: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.418: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.478: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.491: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.502: INFO: Unable to read jessie_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.511: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.520: INFO: Unable to read jessie_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.531: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.539: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.547: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:23.594: INFO: Lookups using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7735 wheezy_tcp@dns-test-service.dns-7735 wheezy_udp@dns-test-service.dns-7735.svc wheezy_tcp@dns-test-service.dns-7735.svc wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7735 jessie_tcp@dns-test-service.dns-7735 jessie_udp@dns-test-service.dns-7735.svc jessie_tcp@dns-test-service.dns-7735.svc jessie_udp@_http._tcp.dns-test-service.dns-7735.svc jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc]

    Jun 13 02:38:28.608: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.617: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.627: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.636: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.646: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.656: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.666: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.677: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.734: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.746: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.756: INFO: Unable to read jessie_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.767: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.776: INFO: Unable to read jessie_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.786: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.801: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.815: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:28.893: INFO: Lookups using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7735 wheezy_tcp@dns-test-service.dns-7735 wheezy_udp@dns-test-service.dns-7735.svc wheezy_tcp@dns-test-service.dns-7735.svc wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7735 jessie_tcp@dns-test-service.dns-7735 jessie_udp@dns-test-service.dns-7735.svc jessie_tcp@dns-test-service.dns-7735.svc jessie_udp@_http._tcp.dns-test-service.dns-7735.svc jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc]

    Jun 13 02:38:33.608: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.625: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.639: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.647: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.658: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.667: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.678: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.692: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.752: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.766: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.778: INFO: Unable to read jessie_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.794: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.807: INFO: Unable to read jessie_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.818: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.828: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.865: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:33.984: INFO: Lookups using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7735 wheezy_tcp@dns-test-service.dns-7735 wheezy_udp@dns-test-service.dns-7735.svc wheezy_tcp@dns-test-service.dns-7735.svc wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7735 jessie_tcp@dns-test-service.dns-7735 jessie_udp@dns-test-service.dns-7735.svc jessie_tcp@dns-test-service.dns-7735.svc jessie_udp@_http._tcp.dns-test-service.dns-7735.svc jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc]

    Jun 13 02:38:38.607: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.619: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.629: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.640: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.650: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.664: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.674: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.683: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.733: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.741: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.750: INFO: Unable to read jessie_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.774: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.785: INFO: Unable to read jessie_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.796: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.805: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.815: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:38.852: INFO: Lookups using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7735 wheezy_tcp@dns-test-service.dns-7735 wheezy_udp@dns-test-service.dns-7735.svc wheezy_tcp@dns-test-service.dns-7735.svc wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7735 jessie_tcp@dns-test-service.dns-7735 jessie_udp@dns-test-service.dns-7735.svc jessie_tcp@dns-test-service.dns-7735.svc jessie_udp@_http._tcp.dns-test-service.dns-7735.svc jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc]

    Jun 13 02:38:43.612: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.622: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.632: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.645: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.653: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.665: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.677: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.686: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.745: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.754: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.762: INFO: Unable to read jessie_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.774: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.795: INFO: Unable to read jessie_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.804: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.818: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:43.898: INFO: Lookups using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7735 wheezy_tcp@dns-test-service.dns-7735 wheezy_udp@dns-test-service.dns-7735.svc wheezy_tcp@dns-test-service.dns-7735.svc wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7735 jessie_tcp@dns-test-service.dns-7735 jessie_udp@dns-test-service.dns-7735.svc jessie_tcp@dns-test-service.dns-7735.svc jessie_udp@_http._tcp.dns-test-service.dns-7735.svc jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc]

    Jun 13 02:38:48.607: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.617: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.633: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.647: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.662: INFO: Unable to read wheezy_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.675: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.694: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.714: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.795: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.805: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.821: INFO: Unable to read jessie_udp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.839: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735 from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.851: INFO: Unable to read jessie_udp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.866: INFO: Unable to read jessie_tcp@dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.883: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.894: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc from pod dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255: the server could not find the requested resource (get pods dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255)
    Jun 13 02:38:48.951: INFO: Lookups using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7735 wheezy_tcp@dns-test-service.dns-7735 wheezy_udp@dns-test-service.dns-7735.svc wheezy_tcp@dns-test-service.dns-7735.svc wheezy_udp@_http._tcp.dns-test-service.dns-7735.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7735.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7735 jessie_tcp@dns-test-service.dns-7735 jessie_udp@dns-test-service.dns-7735.svc jessie_tcp@dns-test-service.dns-7735.svc jessie_udp@_http._tcp.dns-test-service.dns-7735.svc jessie_tcp@_http._tcp.dns-test-service.dns-7735.svc]

    Jun 13 02:38:53.887: INFO: DNS probes using dns-7735/dns-test-b73515dc-e8ce-415f-bf3b-e27abff36255 succeeded

    STEP: deleting the pod 06/13/23 02:38:53.887
    STEP: deleting the test service 06/13/23 02:38:53.943
    STEP: deleting the test headless service 06/13/23 02:38:54.135
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 13 02:38:54.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7735" for this suite. 06/13/23 02:38:54.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:38:54.241
Jun 13 02:38:54.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir-wrapper 06/13/23 02:38:54.244
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:38:54.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:38:54.31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 06/13/23 02:38:54.331
STEP: Creating RC which spawns configmap-volume pods 06/13/23 02:38:55.141
Jun 13 02:38:55.166: INFO: Pod name wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4: Found 0 pods out of 5
Jun 13 02:39:00.183: INFO: Pod name wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/13/23 02:39:00.183
Jun 13 02:39:00.183: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:00.196: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.998072ms
Jun 13 02:39:02.220: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036676668s
Jun 13 02:39:04.271: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087480634s
Jun 13 02:39:06.205: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021753854s
Jun 13 02:39:08.206: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022834976s
Jun 13 02:39:10.211: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9": Phase="Running", Reason="", readiness=true. Elapsed: 10.02766347s
Jun 13 02:39:10.211: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9" satisfied condition "running"
Jun 13 02:39:10.211: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-crz9w" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:10.222: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-crz9w": Phase="Running", Reason="", readiness=true. Elapsed: 10.760571ms
Jun 13 02:39:10.222: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-crz9w" satisfied condition "running"
Jun 13 02:39:10.222: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-kmgmj" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:10.234: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-kmgmj": Phase="Running", Reason="", readiness=true. Elapsed: 11.940345ms
Jun 13 02:39:10.234: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-kmgmj" satisfied condition "running"
Jun 13 02:39:10.234: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-m7pbh" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:10.243: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-m7pbh": Phase="Running", Reason="", readiness=true. Elapsed: 9.465742ms
Jun 13 02:39:10.243: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-m7pbh" satisfied condition "running"
Jun 13 02:39:10.243: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-wtmfh" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:10.257: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-wtmfh": Phase="Running", Reason="", readiness=true. Elapsed: 13.167658ms
Jun 13 02:39:10.257: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-wtmfh" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4 in namespace emptydir-wrapper-7730, will wait for the garbage collector to delete the pods 06/13/23 02:39:10.257
Jun 13 02:39:10.370: INFO: Deleting ReplicationController wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4 took: 34.48297ms
Jun 13 02:39:10.873: INFO: Terminating ReplicationController wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4 pods took: 503.163513ms
STEP: Creating RC which spawns configmap-volume pods 06/13/23 02:39:14.684
Jun 13 02:39:14.720: INFO: Pod name wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703: Found 1 pods out of 5
Jun 13 02:39:19.818: INFO: Pod name wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/13/23 02:39:19.818
Jun 13 02:39:19.818: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:19.829: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml": Phase="Pending", Reason="", readiness=false. Elapsed: 10.952189ms
Jun 13 02:39:21.845: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026854835s
Jun 13 02:39:23.841: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023566682s
Jun 13 02:39:25.984: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml": Phase="Pending", Reason="", readiness=false. Elapsed: 6.166448879s
Jun 13 02:39:27.893: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml": Phase="Pending", Reason="", readiness=false. Elapsed: 8.074972785s
Jun 13 02:39:29.839: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml": Phase="Running", Reason="", readiness=true. Elapsed: 10.021410406s
Jun 13 02:39:29.839: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml" satisfied condition "running"
Jun 13 02:39:29.839: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f5r2q" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:29.873: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f5r2q": Phase="Running", Reason="", readiness=true. Elapsed: 33.096656ms
Jun 13 02:39:29.873: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f5r2q" satisfied condition "running"
Jun 13 02:39:29.873: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f824g" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:29.891: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f824g": Phase="Pending", Reason="", readiness=false. Elapsed: 18.697947ms
Jun 13 02:39:31.903: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f824g": Phase="Running", Reason="", readiness=true. Elapsed: 2.030371269s
Jun 13 02:39:31.903: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f824g" satisfied condition "running"
Jun 13 02:39:31.903: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-mgzjk" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:31.913: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-mgzjk": Phase="Running", Reason="", readiness=true. Elapsed: 9.942496ms
Jun 13 02:39:31.913: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-mgzjk" satisfied condition "running"
Jun 13 02:39:31.913: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-zk7wd" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:31.928: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-zk7wd": Phase="Running", Reason="", readiness=true. Elapsed: 14.76663ms
Jun 13 02:39:31.928: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-zk7wd" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703 in namespace emptydir-wrapper-7730, will wait for the garbage collector to delete the pods 06/13/23 02:39:31.928
Jun 13 02:39:32.017: INFO: Deleting ReplicationController wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703 took: 23.798472ms
Jun 13 02:39:32.121: INFO: Terminating ReplicationController wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703 pods took: 104.462407ms
STEP: Creating RC which spawns configmap-volume pods 06/13/23 02:39:35.849
Jun 13 02:39:35.888: INFO: Pod name wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc: Found 0 pods out of 5
Jun 13 02:39:40.902: INFO: Pod name wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/13/23 02:39:40.902
Jun 13 02:39:40.903: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:40.913: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p": Phase="Pending", Reason="", readiness=false. Elapsed: 10.3168ms
Jun 13 02:39:42.931: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028863787s
Jun 13 02:39:44.924: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021294237s
Jun 13 02:39:46.931: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028792415s
Jun 13 02:39:48.980: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.076968012s
Jun 13 02:39:50.924: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p": Phase="Running", Reason="", readiness=true. Elapsed: 10.020970245s
Jun 13 02:39:50.924: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p" satisfied condition "running"
Jun 13 02:39:50.924: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-ltrwh" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:50.936: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-ltrwh": Phase="Running", Reason="", readiness=true. Elapsed: 12.478968ms
Jun 13 02:39:50.936: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-ltrwh" satisfied condition "running"
Jun 13 02:39:50.936: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-m2vtk" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:50.953: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-m2vtk": Phase="Running", Reason="", readiness=true. Elapsed: 17.102856ms
Jun 13 02:39:50.953: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-m2vtk" satisfied condition "running"
Jun 13 02:39:50.953: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-pmgmt" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:50.965: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-pmgmt": Phase="Running", Reason="", readiness=true. Elapsed: 11.880611ms
Jun 13 02:39:50.965: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-pmgmt" satisfied condition "running"
Jun 13 02:39:50.965: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-sx7fk" in namespace "emptydir-wrapper-7730" to be "running"
Jun 13 02:39:50.978: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-sx7fk": Phase="Running", Reason="", readiness=true. Elapsed: 13.155411ms
Jun 13 02:39:50.979: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-sx7fk" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc in namespace emptydir-wrapper-7730, will wait for the garbage collector to delete the pods 06/13/23 02:39:50.979
Jun 13 02:39:51.060: INFO: Deleting ReplicationController wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc took: 18.500133ms
Jun 13 02:39:51.261: INFO: Terminating ReplicationController wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc pods took: 201.015062ms
STEP: Cleaning up the configMaps 06/13/23 02:39:55.362
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jun 13 02:39:56.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7730" for this suite. 06/13/23 02:39:56.667
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":96,"skipped":1751,"failed":0}
------------------------------
• [SLOW TEST] [62.462 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:38:54.241
    Jun 13 02:38:54.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir-wrapper 06/13/23 02:38:54.244
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:38:54.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:38:54.31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 06/13/23 02:38:54.331
    STEP: Creating RC which spawns configmap-volume pods 06/13/23 02:38:55.141
    Jun 13 02:38:55.166: INFO: Pod name wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4: Found 0 pods out of 5
    Jun 13 02:39:00.183: INFO: Pod name wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/13/23 02:39:00.183
    Jun 13 02:39:00.183: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:00.196: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.998072ms
    Jun 13 02:39:02.220: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036676668s
    Jun 13 02:39:04.271: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087480634s
    Jun 13 02:39:06.205: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021753854s
    Jun 13 02:39:08.206: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022834976s
    Jun 13 02:39:10.211: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9": Phase="Running", Reason="", readiness=true. Elapsed: 10.02766347s
    Jun 13 02:39:10.211: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-4zbb9" satisfied condition "running"
    Jun 13 02:39:10.211: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-crz9w" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:10.222: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-crz9w": Phase="Running", Reason="", readiness=true. Elapsed: 10.760571ms
    Jun 13 02:39:10.222: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-crz9w" satisfied condition "running"
    Jun 13 02:39:10.222: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-kmgmj" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:10.234: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-kmgmj": Phase="Running", Reason="", readiness=true. Elapsed: 11.940345ms
    Jun 13 02:39:10.234: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-kmgmj" satisfied condition "running"
    Jun 13 02:39:10.234: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-m7pbh" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:10.243: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-m7pbh": Phase="Running", Reason="", readiness=true. Elapsed: 9.465742ms
    Jun 13 02:39:10.243: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-m7pbh" satisfied condition "running"
    Jun 13 02:39:10.243: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-wtmfh" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:10.257: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-wtmfh": Phase="Running", Reason="", readiness=true. Elapsed: 13.167658ms
    Jun 13 02:39:10.257: INFO: Pod "wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4-wtmfh" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4 in namespace emptydir-wrapper-7730, will wait for the garbage collector to delete the pods 06/13/23 02:39:10.257
    Jun 13 02:39:10.370: INFO: Deleting ReplicationController wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4 took: 34.48297ms
    Jun 13 02:39:10.873: INFO: Terminating ReplicationController wrapped-volume-race-af5f4686-2a22-4fa1-b166-4e75174021e4 pods took: 503.163513ms
    STEP: Creating RC which spawns configmap-volume pods 06/13/23 02:39:14.684
    Jun 13 02:39:14.720: INFO: Pod name wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703: Found 1 pods out of 5
    Jun 13 02:39:19.818: INFO: Pod name wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/13/23 02:39:19.818
    Jun 13 02:39:19.818: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:19.829: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml": Phase="Pending", Reason="", readiness=false. Elapsed: 10.952189ms
    Jun 13 02:39:21.845: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026854835s
    Jun 13 02:39:23.841: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023566682s
    Jun 13 02:39:25.984: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml": Phase="Pending", Reason="", readiness=false. Elapsed: 6.166448879s
    Jun 13 02:39:27.893: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml": Phase="Pending", Reason="", readiness=false. Elapsed: 8.074972785s
    Jun 13 02:39:29.839: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml": Phase="Running", Reason="", readiness=true. Elapsed: 10.021410406s
    Jun 13 02:39:29.839: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-6fxml" satisfied condition "running"
    Jun 13 02:39:29.839: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f5r2q" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:29.873: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f5r2q": Phase="Running", Reason="", readiness=true. Elapsed: 33.096656ms
    Jun 13 02:39:29.873: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f5r2q" satisfied condition "running"
    Jun 13 02:39:29.873: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f824g" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:29.891: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f824g": Phase="Pending", Reason="", readiness=false. Elapsed: 18.697947ms
    Jun 13 02:39:31.903: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f824g": Phase="Running", Reason="", readiness=true. Elapsed: 2.030371269s
    Jun 13 02:39:31.903: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-f824g" satisfied condition "running"
    Jun 13 02:39:31.903: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-mgzjk" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:31.913: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-mgzjk": Phase="Running", Reason="", readiness=true. Elapsed: 9.942496ms
    Jun 13 02:39:31.913: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-mgzjk" satisfied condition "running"
    Jun 13 02:39:31.913: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-zk7wd" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:31.928: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-zk7wd": Phase="Running", Reason="", readiness=true. Elapsed: 14.76663ms
    Jun 13 02:39:31.928: INFO: Pod "wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703-zk7wd" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703 in namespace emptydir-wrapper-7730, will wait for the garbage collector to delete the pods 06/13/23 02:39:31.928
    Jun 13 02:39:32.017: INFO: Deleting ReplicationController wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703 took: 23.798472ms
    Jun 13 02:39:32.121: INFO: Terminating ReplicationController wrapped-volume-race-0e67007f-772e-4dde-ac6f-9abccdc06703 pods took: 104.462407ms
    STEP: Creating RC which spawns configmap-volume pods 06/13/23 02:39:35.849
    Jun 13 02:39:35.888: INFO: Pod name wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc: Found 0 pods out of 5
    Jun 13 02:39:40.902: INFO: Pod name wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/13/23 02:39:40.902
    Jun 13 02:39:40.903: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:40.913: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p": Phase="Pending", Reason="", readiness=false. Elapsed: 10.3168ms
    Jun 13 02:39:42.931: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028863787s
    Jun 13 02:39:44.924: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021294237s
    Jun 13 02:39:46.931: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028792415s
    Jun 13 02:39:48.980: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p": Phase="Pending", Reason="", readiness=false. Elapsed: 8.076968012s
    Jun 13 02:39:50.924: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p": Phase="Running", Reason="", readiness=true. Elapsed: 10.020970245s
    Jun 13 02:39:50.924: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-7m29p" satisfied condition "running"
    Jun 13 02:39:50.924: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-ltrwh" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:50.936: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-ltrwh": Phase="Running", Reason="", readiness=true. Elapsed: 12.478968ms
    Jun 13 02:39:50.936: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-ltrwh" satisfied condition "running"
    Jun 13 02:39:50.936: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-m2vtk" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:50.953: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-m2vtk": Phase="Running", Reason="", readiness=true. Elapsed: 17.102856ms
    Jun 13 02:39:50.953: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-m2vtk" satisfied condition "running"
    Jun 13 02:39:50.953: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-pmgmt" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:50.965: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-pmgmt": Phase="Running", Reason="", readiness=true. Elapsed: 11.880611ms
    Jun 13 02:39:50.965: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-pmgmt" satisfied condition "running"
    Jun 13 02:39:50.965: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-sx7fk" in namespace "emptydir-wrapper-7730" to be "running"
    Jun 13 02:39:50.978: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-sx7fk": Phase="Running", Reason="", readiness=true. Elapsed: 13.155411ms
    Jun 13 02:39:50.979: INFO: Pod "wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc-sx7fk" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc in namespace emptydir-wrapper-7730, will wait for the garbage collector to delete the pods 06/13/23 02:39:50.979
    Jun 13 02:39:51.060: INFO: Deleting ReplicationController wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc took: 18.500133ms
    Jun 13 02:39:51.261: INFO: Terminating ReplicationController wrapped-volume-race-2b206a1e-bd5c-4dc1-b998-4e72facdf3bc pods took: 201.015062ms
    STEP: Cleaning up the configMaps 06/13/23 02:39:55.362
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jun 13 02:39:56.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-7730" for this suite. 06/13/23 02:39:56.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:39:56.707
Jun 13 02:39:56.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 02:39:56.71
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:39:56.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:39:56.769
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 02:39:56.822
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 02:39:57.178
STEP: Deploying the webhook pod 06/13/23 02:39:57.206
STEP: Wait for the deployment to be ready 06/13/23 02:39:57.23
Jun 13 02:39:57.245: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 13 02:39:59.276: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 2, 39, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 2, 39, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 2, 39, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 2, 39, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/13/23 02:40:01.29
STEP: Verifying the service has paired with the endpoint 06/13/23 02:40:01.343
Jun 13 02:40:02.344: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 06/13/23 02:40:02.366
STEP: create a pod that should be updated by the webhook 06/13/23 02:40:02.452
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 02:40:02.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2589" for this suite. 06/13/23 02:40:02.578
STEP: Destroying namespace "webhook-2589-markers" for this suite. 06/13/23 02:40:02.609
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":97,"skipped":1807,"failed":0}
------------------------------
• [SLOW TEST] [6.244 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:39:56.707
    Jun 13 02:39:56.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 02:39:56.71
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:39:56.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:39:56.769
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 02:39:56.822
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 02:39:57.178
    STEP: Deploying the webhook pod 06/13/23 02:39:57.206
    STEP: Wait for the deployment to be ready 06/13/23 02:39:57.23
    Jun 13 02:39:57.245: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 13 02:39:59.276: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 2, 39, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 2, 39, 57, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 2, 39, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 2, 39, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/13/23 02:40:01.29
    STEP: Verifying the service has paired with the endpoint 06/13/23 02:40:01.343
    Jun 13 02:40:02.344: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 06/13/23 02:40:02.366
    STEP: create a pod that should be updated by the webhook 06/13/23 02:40:02.452
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 02:40:02.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2589" for this suite. 06/13/23 02:40:02.578
    STEP: Destroying namespace "webhook-2589-markers" for this suite. 06/13/23 02:40:02.609
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:40:02.951
Jun 13 02:40:02.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename var-expansion 06/13/23 02:40:02.953
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:02.998
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:03.005
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 06/13/23 02:40:03.015
Jun 13 02:40:03.065: INFO: Waiting up to 5m0s for pod "var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20" in namespace "var-expansion-5771" to be "Succeeded or Failed"
Jun 13 02:40:03.077: INFO: Pod "var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20": Phase="Pending", Reason="", readiness=false. Elapsed: 12.119771ms
Jun 13 02:40:05.091: INFO: Pod "var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025721034s
Jun 13 02:40:07.101: INFO: Pod "var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035626714s
STEP: Saw pod success 06/13/23 02:40:07.101
Jun 13 02:40:07.101: INFO: Pod "var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20" satisfied condition "Succeeded or Failed"
Jun 13 02:40:07.117: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20 container dapi-container: <nil>
STEP: delete the pod 06/13/23 02:40:07.171
Jun 13 02:40:07.208: INFO: Waiting for pod var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20 to disappear
Jun 13 02:40:07.223: INFO: Pod var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 13 02:40:07.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5771" for this suite. 06/13/23 02:40:07.236
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":98,"skipped":1810,"failed":0}
------------------------------
• [4.300 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:40:02.951
    Jun 13 02:40:02.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename var-expansion 06/13/23 02:40:02.953
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:02.998
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:03.005
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 06/13/23 02:40:03.015
    Jun 13 02:40:03.065: INFO: Waiting up to 5m0s for pod "var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20" in namespace "var-expansion-5771" to be "Succeeded or Failed"
    Jun 13 02:40:03.077: INFO: Pod "var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20": Phase="Pending", Reason="", readiness=false. Elapsed: 12.119771ms
    Jun 13 02:40:05.091: INFO: Pod "var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025721034s
    Jun 13 02:40:07.101: INFO: Pod "var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035626714s
    STEP: Saw pod success 06/13/23 02:40:07.101
    Jun 13 02:40:07.101: INFO: Pod "var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20" satisfied condition "Succeeded or Failed"
    Jun 13 02:40:07.117: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20 container dapi-container: <nil>
    STEP: delete the pod 06/13/23 02:40:07.171
    Jun 13 02:40:07.208: INFO: Waiting for pod var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20 to disappear
    Jun 13 02:40:07.223: INFO: Pod var-expansion-fe982e5b-796d-4d0f-b8ad-eedb2f26de20 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 13 02:40:07.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5771" for this suite. 06/13/23 02:40:07.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:40:07.251
Jun 13 02:40:07.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pod-network-test 06/13/23 02:40:07.253
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:07.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:07.309
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-671 06/13/23 02:40:07.316
STEP: creating a selector 06/13/23 02:40:07.317
STEP: Creating the service pods in kubernetes 06/13/23 02:40:07.317
Jun 13 02:40:07.317: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 13 02:40:07.424: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-671" to be "running and ready"
Jun 13 02:40:07.446: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.619743ms
Jun 13 02:40:07.446: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:40:09.460: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.036369563s
Jun 13 02:40:09.460: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:40:11.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.033049924s
Jun 13 02:40:11.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:40:13.460: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.035996911s
Jun 13 02:40:13.460: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:40:15.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.034473046s
Jun 13 02:40:15.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:40:17.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.033206374s
Jun 13 02:40:17.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:40:19.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.03258807s
Jun 13 02:40:19.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:40:21.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.032911764s
Jun 13 02:40:21.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:40:23.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.031714358s
Jun 13 02:40:23.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:40:25.460: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.036585404s
Jun 13 02:40:25.461: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:40:27.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.033863801s
Jun 13 02:40:27.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 02:40:29.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.041553136s
Jun 13 02:40:29.466: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 13 02:40:29.466: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 13 02:40:29.539: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-671" to be "running and ready"
Jun 13 02:40:29.546: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.561335ms
Jun 13 02:40:29.546: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 13 02:40:29.546: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 13 02:40:29.556: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-671" to be "running and ready"
Jun 13 02:40:29.570: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 14.097238ms
Jun 13 02:40:29.571: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 13 02:40:29.571: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/13/23 02:40:29.597
Jun 13 02:40:29.669: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-671" to be "running"
Jun 13 02:40:29.683: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.881701ms
Jun 13 02:40:31.690: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021075778s
Jun 13 02:40:31.690: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 13 02:40:31.697: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-671" to be "running"
Jun 13 02:40:31.710: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 12.219954ms
Jun 13 02:40:31.710: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jun 13 02:40:31.719: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 13 02:40:31.719: INFO: Going to poll 172.16.172.7 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 13 02:40:31.728: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.172.7 8081 | grep -v '^\s*$'] Namespace:pod-network-test-671 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 02:40:31.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:40:31.729: INFO: ExecWithOptions: Clientset creation
Jun 13 02:40:31.729: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-671/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.172.7+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 13 02:40:32.833: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 13 02:40:32.833: INFO: Going to poll 172.30.77.156 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 13 02:40:32.840: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.77.156 8081 | grep -v '^\s*$'] Namespace:pod-network-test-671 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 02:40:32.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:40:32.841: INFO: ExecWithOptions: Clientset creation
Jun 13 02:40:32.841: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-671/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.77.156+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 13 02:40:33.942: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 13 02:40:33.942: INFO: Going to poll 172.28.156.253 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 13 02:40:33.950: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.28.156.253 8081 | grep -v '^\s*$'] Namespace:pod-network-test-671 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 02:40:33.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:40:33.951: INFO: ExecWithOptions: Clientset creation
Jun 13 02:40:33.951: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-671/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.28.156.253+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 13 02:40:35.047: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jun 13 02:40:35.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-671" for this suite. 06/13/23 02:40:35.071
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":99,"skipped":1819,"failed":0}
------------------------------
• [SLOW TEST] [27.916 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:40:07.251
    Jun 13 02:40:07.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pod-network-test 06/13/23 02:40:07.253
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:07.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:07.309
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-671 06/13/23 02:40:07.316
    STEP: creating a selector 06/13/23 02:40:07.317
    STEP: Creating the service pods in kubernetes 06/13/23 02:40:07.317
    Jun 13 02:40:07.317: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun 13 02:40:07.424: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-671" to be "running and ready"
    Jun 13 02:40:07.446: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 21.619743ms
    Jun 13 02:40:07.446: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:40:09.460: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.036369563s
    Jun 13 02:40:09.460: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:40:11.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.033049924s
    Jun 13 02:40:11.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:40:13.460: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.035996911s
    Jun 13 02:40:13.460: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:40:15.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.034473046s
    Jun 13 02:40:15.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:40:17.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.033206374s
    Jun 13 02:40:17.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:40:19.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.03258807s
    Jun 13 02:40:19.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:40:21.457: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.032911764s
    Jun 13 02:40:21.457: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:40:23.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.031714358s
    Jun 13 02:40:23.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:40:25.460: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.036585404s
    Jun 13 02:40:25.461: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:40:27.458: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.033863801s
    Jun 13 02:40:27.458: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 02:40:29.465: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.041553136s
    Jun 13 02:40:29.466: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 13 02:40:29.466: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 13 02:40:29.539: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-671" to be "running and ready"
    Jun 13 02:40:29.546: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 7.561335ms
    Jun 13 02:40:29.546: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 13 02:40:29.546: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 13 02:40:29.556: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-671" to be "running and ready"
    Jun 13 02:40:29.570: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 14.097238ms
    Jun 13 02:40:29.571: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 13 02:40:29.571: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/13/23 02:40:29.597
    Jun 13 02:40:29.669: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-671" to be "running"
    Jun 13 02:40:29.683: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.881701ms
    Jun 13 02:40:31.690: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021075778s
    Jun 13 02:40:31.690: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 13 02:40:31.697: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-671" to be "running"
    Jun 13 02:40:31.710: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 12.219954ms
    Jun 13 02:40:31.710: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jun 13 02:40:31.719: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 13 02:40:31.719: INFO: Going to poll 172.16.172.7 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun 13 02:40:31.728: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.172.7 8081 | grep -v '^\s*$'] Namespace:pod-network-test-671 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 02:40:31.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:40:31.729: INFO: ExecWithOptions: Clientset creation
    Jun 13 02:40:31.729: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-671/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.172.7+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 13 02:40:32.833: INFO: Found all 1 expected endpoints: [netserver-0]
    Jun 13 02:40:32.833: INFO: Going to poll 172.30.77.156 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun 13 02:40:32.840: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.77.156 8081 | grep -v '^\s*$'] Namespace:pod-network-test-671 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 02:40:32.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:40:32.841: INFO: ExecWithOptions: Clientset creation
    Jun 13 02:40:32.841: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-671/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.77.156+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 13 02:40:33.942: INFO: Found all 1 expected endpoints: [netserver-1]
    Jun 13 02:40:33.942: INFO: Going to poll 172.28.156.253 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun 13 02:40:33.950: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.28.156.253 8081 | grep -v '^\s*$'] Namespace:pod-network-test-671 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 02:40:33.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:40:33.951: INFO: ExecWithOptions: Clientset creation
    Jun 13 02:40:33.951: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-671/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.28.156.253+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 13 02:40:35.047: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jun 13 02:40:35.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-671" for this suite. 06/13/23 02:40:35.071
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:40:35.168
Jun 13 02:40:35.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename dns 06/13/23 02:40:35.17
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:35.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:35.249
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 06/13/23 02:40:35.257
Jun 13 02:40:35.275: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2799  b6e6a410-6887-4a7d-953a-a5efd9b6af74 18151 0 2023-06-13 02:40:35 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-13 02:40:35 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zs86d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zs86d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 02:40:35.276: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2799" to be "running and ready"
Jun 13 02:40:35.288: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 11.819747ms
Jun 13 02:40:35.288: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:40:37.308: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.032146076s
Jun 13 02:40:37.308: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jun 13 02:40:37.308: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 06/13/23 02:40:37.308
Jun 13 02:40:37.310: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2799 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 02:40:37.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:40:37.312: INFO: ExecWithOptions: Clientset creation
Jun 13 02:40:37.312: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2799/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 06/13/23 02:40:37.435
Jun 13 02:40:37.435: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2799 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 02:40:37.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:40:37.436: INFO: ExecWithOptions: Clientset creation
Jun 13 02:40:37.436: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2799/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 13 02:40:37.562: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 13 02:40:37.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2799" for this suite. 06/13/23 02:40:37.612
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":100,"skipped":1820,"failed":0}
------------------------------
• [2.461 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:40:35.168
    Jun 13 02:40:35.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename dns 06/13/23 02:40:35.17
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:35.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:35.249
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 06/13/23 02:40:35.257
    Jun 13 02:40:35.275: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-2799  b6e6a410-6887-4a7d-953a-a5efd9b6af74 18151 0 2023-06-13 02:40:35 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-06-13 02:40:35 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zs86d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zs86d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 02:40:35.276: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-2799" to be "running and ready"
    Jun 13 02:40:35.288: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 11.819747ms
    Jun 13 02:40:35.288: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:40:37.308: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.032146076s
    Jun 13 02:40:37.308: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jun 13 02:40:37.308: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 06/13/23 02:40:37.308
    Jun 13 02:40:37.310: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-2799 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 02:40:37.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:40:37.312: INFO: ExecWithOptions: Clientset creation
    Jun 13 02:40:37.312: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2799/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 06/13/23 02:40:37.435
    Jun 13 02:40:37.435: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-2799 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 02:40:37.435: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:40:37.436: INFO: ExecWithOptions: Clientset creation
    Jun 13 02:40:37.436: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-2799/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 13 02:40:37.562: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 13 02:40:37.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2799" for this suite. 06/13/23 02:40:37.612
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:40:37.63
Jun 13 02:40:37.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename podtemplate 06/13/23 02:40:37.631
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:37.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:37.69
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 06/13/23 02:40:37.698
STEP: Replace a pod template 06/13/23 02:40:37.71
Jun 13 02:40:37.729: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jun 13 02:40:37.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-8618" for this suite. 06/13/23 02:40:37.738
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":101,"skipped":1830,"failed":0}
------------------------------
• [0.130 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:40:37.63
    Jun 13 02:40:37.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename podtemplate 06/13/23 02:40:37.631
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:37.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:37.69
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 06/13/23 02:40:37.698
    STEP: Replace a pod template 06/13/23 02:40:37.71
    Jun 13 02:40:37.729: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jun 13 02:40:37.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-8618" for this suite. 06/13/23 02:40:37.738
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:40:37.762
Jun 13 02:40:37.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:40:37.763
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:37.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:37.808
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 06/13/23 02:40:37.815
Jun 13 02:40:37.847: INFO: Waiting up to 5m0s for pod "labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf" in namespace "projected-8120" to be "running and ready"
Jun 13 02:40:37.859: INFO: Pod "labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.940528ms
Jun 13 02:40:37.859: INFO: The phase of Pod labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:40:39.868: INFO: Pod "labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.020476294s
Jun 13 02:40:39.868: INFO: The phase of Pod labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf is Running (Ready = true)
Jun 13 02:40:39.868: INFO: Pod "labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf" satisfied condition "running and ready"
Jun 13 02:40:40.447: INFO: Successfully updated pod "labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 13 02:40:44.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8120" for this suite. 06/13/23 02:40:44.538
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":102,"skipped":1837,"failed":0}
------------------------------
• [SLOW TEST] [6.813 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:40:37.762
    Jun 13 02:40:37.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:40:37.763
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:37.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:37.808
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 06/13/23 02:40:37.815
    Jun 13 02:40:37.847: INFO: Waiting up to 5m0s for pod "labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf" in namespace "projected-8120" to be "running and ready"
    Jun 13 02:40:37.859: INFO: Pod "labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.940528ms
    Jun 13 02:40:37.859: INFO: The phase of Pod labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:40:39.868: INFO: Pod "labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.020476294s
    Jun 13 02:40:39.868: INFO: The phase of Pod labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf is Running (Ready = true)
    Jun 13 02:40:39.868: INFO: Pod "labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf" satisfied condition "running and ready"
    Jun 13 02:40:40.447: INFO: Successfully updated pod "labelsupdatea877c609-1bb6-4eb6-aac1-43abf9d264bf"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 13 02:40:44.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8120" for this suite. 06/13/23 02:40:44.538
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:40:44.575
Jun 13 02:40:44.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 02:40:44.578
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:44.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:44.681
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 06/13/23 02:40:44.704
Jun 13 02:40:44.735: INFO: Waiting up to 5m0s for pod "downward-api-9be539fb-e878-4b60-92f4-444af4e62078" in namespace "downward-api-2124" to be "Succeeded or Failed"
Jun 13 02:40:44.752: INFO: Pod "downward-api-9be539fb-e878-4b60-92f4-444af4e62078": Phase="Pending", Reason="", readiness=false. Elapsed: 16.973551ms
Jun 13 02:40:46.764: INFO: Pod "downward-api-9be539fb-e878-4b60-92f4-444af4e62078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029850184s
Jun 13 02:40:48.762: INFO: Pod "downward-api-9be539fb-e878-4b60-92f4-444af4e62078": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027294387s
STEP: Saw pod success 06/13/23 02:40:48.762
Jun 13 02:40:48.762: INFO: Pod "downward-api-9be539fb-e878-4b60-92f4-444af4e62078" satisfied condition "Succeeded or Failed"
Jun 13 02:40:48.779: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downward-api-9be539fb-e878-4b60-92f4-444af4e62078 container dapi-container: <nil>
STEP: delete the pod 06/13/23 02:40:48.798
Jun 13 02:40:48.847: INFO: Waiting for pod downward-api-9be539fb-e878-4b60-92f4-444af4e62078 to disappear
Jun 13 02:40:48.885: INFO: Pod downward-api-9be539fb-e878-4b60-92f4-444af4e62078 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 13 02:40:48.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2124" for this suite. 06/13/23 02:40:48.908
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":103,"skipped":1841,"failed":0}
------------------------------
• [4.388 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:40:44.575
    Jun 13 02:40:44.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 02:40:44.578
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:44.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:44.681
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 06/13/23 02:40:44.704
    Jun 13 02:40:44.735: INFO: Waiting up to 5m0s for pod "downward-api-9be539fb-e878-4b60-92f4-444af4e62078" in namespace "downward-api-2124" to be "Succeeded or Failed"
    Jun 13 02:40:44.752: INFO: Pod "downward-api-9be539fb-e878-4b60-92f4-444af4e62078": Phase="Pending", Reason="", readiness=false. Elapsed: 16.973551ms
    Jun 13 02:40:46.764: INFO: Pod "downward-api-9be539fb-e878-4b60-92f4-444af4e62078": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029850184s
    Jun 13 02:40:48.762: INFO: Pod "downward-api-9be539fb-e878-4b60-92f4-444af4e62078": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027294387s
    STEP: Saw pod success 06/13/23 02:40:48.762
    Jun 13 02:40:48.762: INFO: Pod "downward-api-9be539fb-e878-4b60-92f4-444af4e62078" satisfied condition "Succeeded or Failed"
    Jun 13 02:40:48.779: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downward-api-9be539fb-e878-4b60-92f4-444af4e62078 container dapi-container: <nil>
    STEP: delete the pod 06/13/23 02:40:48.798
    Jun 13 02:40:48.847: INFO: Waiting for pod downward-api-9be539fb-e878-4b60-92f4-444af4e62078 to disappear
    Jun 13 02:40:48.885: INFO: Pod downward-api-9be539fb-e878-4b60-92f4-444af4e62078 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 13 02:40:48.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2124" for this suite. 06/13/23 02:40:48.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:40:48.966
Jun 13 02:40:48.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename init-container 06/13/23 02:40:48.968
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:49.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:49.148
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 06/13/23 02:40:49.17
Jun 13 02:40:49.171: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 13 02:40:53.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5538" for this suite. 06/13/23 02:40:53.184
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":104,"skipped":1897,"failed":0}
------------------------------
• [4.234 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:40:48.966
    Jun 13 02:40:48.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename init-container 06/13/23 02:40:48.968
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:49.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:49.148
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 06/13/23 02:40:49.17
    Jun 13 02:40:49.171: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 13 02:40:53.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5538" for this suite. 06/13/23 02:40:53.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:40:53.204
Jun 13 02:40:53.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename custom-resource-definition 06/13/23 02:40:53.206
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:53.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:53.259
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 06/13/23 02:40:53.264
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 06/13/23 02:40:53.266
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 06/13/23 02:40:53.266
STEP: fetching the /apis/apiextensions.k8s.io discovery document 06/13/23 02:40:53.266
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 06/13/23 02:40:53.269
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 06/13/23 02:40:53.27
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 06/13/23 02:40:53.273
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 02:40:53.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5406" for this suite. 06/13/23 02:40:53.286
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":105,"skipped":1953,"failed":0}
------------------------------
• [0.101 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:40:53.204
    Jun 13 02:40:53.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename custom-resource-definition 06/13/23 02:40:53.206
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:53.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:53.259
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 06/13/23 02:40:53.264
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 06/13/23 02:40:53.266
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 06/13/23 02:40:53.266
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 06/13/23 02:40:53.266
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 06/13/23 02:40:53.269
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 06/13/23 02:40:53.27
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 06/13/23 02:40:53.273
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 02:40:53.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-5406" for this suite. 06/13/23 02:40:53.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:40:53.311
Jun 13 02:40:53.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename subpath 06/13/23 02:40:53.313
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:53.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:53.363
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/13/23 02:40:53.371
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-mcdz 06/13/23 02:40:53.407
STEP: Creating a pod to test atomic-volume-subpath 06/13/23 02:40:53.407
Jun 13 02:40:53.423: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mcdz" in namespace "subpath-7655" to be "Succeeded or Failed"
Jun 13 02:40:53.437: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Pending", Reason="", readiness=false. Elapsed: 13.683994ms
Jun 13 02:40:55.463: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.040115889s
Jun 13 02:40:57.452: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 4.028557045s
Jun 13 02:40:59.447: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 6.024295374s
Jun 13 02:41:01.446: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 8.022744975s
Jun 13 02:41:03.446: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 10.023529323s
Jun 13 02:41:05.461: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 12.03847044s
Jun 13 02:41:07.446: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 14.022945979s
Jun 13 02:41:09.448: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 16.024687534s
Jun 13 02:41:11.444: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 18.021327609s
Jun 13 02:41:13.446: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 20.022694754s
Jun 13 02:41:15.447: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=false. Elapsed: 22.024338303s
Jun 13 02:41:17.449: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.0263993s
STEP: Saw pod success 06/13/23 02:41:17.449
Jun 13 02:41:17.450: INFO: Pod "pod-subpath-test-downwardapi-mcdz" satisfied condition "Succeeded or Failed"
Jun 13 02:41:17.457: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-l5gcd pod pod-subpath-test-downwardapi-mcdz container test-container-subpath-downwardapi-mcdz: <nil>
STEP: delete the pod 06/13/23 02:41:17.476
Jun 13 02:41:17.502: INFO: Waiting for pod pod-subpath-test-downwardapi-mcdz to disappear
Jun 13 02:41:17.512: INFO: Pod pod-subpath-test-downwardapi-mcdz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mcdz 06/13/23 02:41:17.512
Jun 13 02:41:17.512: INFO: Deleting pod "pod-subpath-test-downwardapi-mcdz" in namespace "subpath-7655"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 13 02:41:17.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7655" for this suite. 06/13/23 02:41:17.54
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":106,"skipped":2022,"failed":0}
------------------------------
• [SLOW TEST] [24.250 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:40:53.311
    Jun 13 02:40:53.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename subpath 06/13/23 02:40:53.313
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:40:53.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:40:53.363
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/13/23 02:40:53.371
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-mcdz 06/13/23 02:40:53.407
    STEP: Creating a pod to test atomic-volume-subpath 06/13/23 02:40:53.407
    Jun 13 02:40:53.423: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mcdz" in namespace "subpath-7655" to be "Succeeded or Failed"
    Jun 13 02:40:53.437: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Pending", Reason="", readiness=false. Elapsed: 13.683994ms
    Jun 13 02:40:55.463: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 2.040115889s
    Jun 13 02:40:57.452: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 4.028557045s
    Jun 13 02:40:59.447: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 6.024295374s
    Jun 13 02:41:01.446: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 8.022744975s
    Jun 13 02:41:03.446: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 10.023529323s
    Jun 13 02:41:05.461: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 12.03847044s
    Jun 13 02:41:07.446: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 14.022945979s
    Jun 13 02:41:09.448: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 16.024687534s
    Jun 13 02:41:11.444: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 18.021327609s
    Jun 13 02:41:13.446: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=true. Elapsed: 20.022694754s
    Jun 13 02:41:15.447: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Running", Reason="", readiness=false. Elapsed: 22.024338303s
    Jun 13 02:41:17.449: INFO: Pod "pod-subpath-test-downwardapi-mcdz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.0263993s
    STEP: Saw pod success 06/13/23 02:41:17.449
    Jun 13 02:41:17.450: INFO: Pod "pod-subpath-test-downwardapi-mcdz" satisfied condition "Succeeded or Failed"
    Jun 13 02:41:17.457: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-l5gcd pod pod-subpath-test-downwardapi-mcdz container test-container-subpath-downwardapi-mcdz: <nil>
    STEP: delete the pod 06/13/23 02:41:17.476
    Jun 13 02:41:17.502: INFO: Waiting for pod pod-subpath-test-downwardapi-mcdz to disappear
    Jun 13 02:41:17.512: INFO: Pod pod-subpath-test-downwardapi-mcdz no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-mcdz 06/13/23 02:41:17.512
    Jun 13 02:41:17.512: INFO: Deleting pod "pod-subpath-test-downwardapi-mcdz" in namespace "subpath-7655"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 13 02:41:17.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7655" for this suite. 06/13/23 02:41:17.54
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:41:17.563
Jun 13 02:41:17.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-lifecycle-hook 06/13/23 02:41:17.565
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:17.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:17.611
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 06/13/23 02:41:17.647
Jun 13 02:41:17.670: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6371" to be "running and ready"
Jun 13 02:41:17.684: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.093544ms
Jun 13 02:41:17.684: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:41:19.702: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.031815144s
Jun 13 02:41:19.702: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 13 02:41:19.702: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 06/13/23 02:41:19.712
Jun 13 02:41:19.833: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6371" to be "running and ready"
Jun 13 02:41:19.872: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 39.16283ms
Jun 13 02:41:19.872: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:41:21.888: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055251285s
Jun 13 02:41:21.888: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:41:23.896: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.062659992s
Jun 13 02:41:23.896: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jun 13 02:41:23.896: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 06/13/23 02:41:23.931
STEP: delete the pod with lifecycle hook 06/13/23 02:41:23.96
Jun 13 02:41:23.998: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 13 02:41:24.007: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 13 02:41:26.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 13 02:41:26.016: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jun 13 02:41:26.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6371" for this suite. 06/13/23 02:41:26.042
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":107,"skipped":2036,"failed":0}
------------------------------
• [SLOW TEST] [8.518 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:41:17.563
    Jun 13 02:41:17.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/13/23 02:41:17.565
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:17.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:17.611
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 06/13/23 02:41:17.647
    Jun 13 02:41:17.670: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6371" to be "running and ready"
    Jun 13 02:41:17.684: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 14.093544ms
    Jun 13 02:41:17.684: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:41:19.702: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.031815144s
    Jun 13 02:41:19.702: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 13 02:41:19.702: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 06/13/23 02:41:19.712
    Jun 13 02:41:19.833: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-6371" to be "running and ready"
    Jun 13 02:41:19.872: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 39.16283ms
    Jun 13 02:41:19.872: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:41:21.888: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055251285s
    Jun 13 02:41:21.888: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:41:23.896: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.062659992s
    Jun 13 02:41:23.896: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jun 13 02:41:23.896: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 06/13/23 02:41:23.931
    STEP: delete the pod with lifecycle hook 06/13/23 02:41:23.96
    Jun 13 02:41:23.998: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun 13 02:41:24.007: INFO: Pod pod-with-poststart-exec-hook still exists
    Jun 13 02:41:26.008: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun 13 02:41:26.016: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jun 13 02:41:26.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6371" for this suite. 06/13/23 02:41:26.042
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:41:26.082
Jun 13 02:41:26.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename cronjob 06/13/23 02:41:26.084
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:26.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:26.229
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 06/13/23 02:41:26.237
STEP: creating 06/13/23 02:41:26.238
STEP: getting 06/13/23 02:41:26.257
STEP: listing 06/13/23 02:41:26.268
STEP: watching 06/13/23 02:41:26.277
Jun 13 02:41:26.277: INFO: starting watch
STEP: cluster-wide listing 06/13/23 02:41:26.281
STEP: cluster-wide watching 06/13/23 02:41:26.29
Jun 13 02:41:26.290: INFO: starting watch
STEP: patching 06/13/23 02:41:26.293
STEP: updating 06/13/23 02:41:26.307
Jun 13 02:41:26.340: INFO: waiting for watch events with expected annotations
Jun 13 02:41:26.340: INFO: saw patched and updated annotations
STEP: patching /status 06/13/23 02:41:26.341
STEP: updating /status 06/13/23 02:41:26.358
STEP: get /status 06/13/23 02:41:26.388
STEP: deleting 06/13/23 02:41:26.398
STEP: deleting a collection 06/13/23 02:41:26.454
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 13 02:41:26.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-850" for this suite. 06/13/23 02:41:26.499
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":108,"skipped":2076,"failed":0}
------------------------------
• [0.433 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:41:26.082
    Jun 13 02:41:26.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename cronjob 06/13/23 02:41:26.084
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:26.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:26.229
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 06/13/23 02:41:26.237
    STEP: creating 06/13/23 02:41:26.238
    STEP: getting 06/13/23 02:41:26.257
    STEP: listing 06/13/23 02:41:26.268
    STEP: watching 06/13/23 02:41:26.277
    Jun 13 02:41:26.277: INFO: starting watch
    STEP: cluster-wide listing 06/13/23 02:41:26.281
    STEP: cluster-wide watching 06/13/23 02:41:26.29
    Jun 13 02:41:26.290: INFO: starting watch
    STEP: patching 06/13/23 02:41:26.293
    STEP: updating 06/13/23 02:41:26.307
    Jun 13 02:41:26.340: INFO: waiting for watch events with expected annotations
    Jun 13 02:41:26.340: INFO: saw patched and updated annotations
    STEP: patching /status 06/13/23 02:41:26.341
    STEP: updating /status 06/13/23 02:41:26.358
    STEP: get /status 06/13/23 02:41:26.388
    STEP: deleting 06/13/23 02:41:26.398
    STEP: deleting a collection 06/13/23 02:41:26.454
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 13 02:41:26.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-850" for this suite. 06/13/23 02:41:26.499
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:41:26.516
Jun 13 02:41:26.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename deployment 06/13/23 02:41:26.517
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:26.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:26.557
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jun 13 02:41:26.571: INFO: Creating simple deployment test-new-deployment
Jun 13 02:41:26.681: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 06/13/23 02:41:28.717
STEP: updating a scale subresource 06/13/23 02:41:28.725
STEP: verifying the deployment Spec.Replicas was modified 06/13/23 02:41:28.752
STEP: Patch a scale subresource 06/13/23 02:41:28.768
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 13 02:41:28.844: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-6857  5fdfaaee-a4ee-4f1e-a546-4362917e76d3 18697 3 2023-06-13 02:41:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-13 02:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0013d5fa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-06-13 02:41:28 +0000 UTC,LastTransitionTime:2023-06-13 02:41:26 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-13 02:41:28 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 13 02:41:28.860: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-6857  d5bb73e8-2fe5-4ff9-90d5-05af4f050613 18698 3 2023-06-13 02:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 5fdfaaee-a4ee-4f1e-a546-4362917e76d3 0xc003ac83e7 0xc003ac83e8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5fdfaaee-a4ee-4f1e-a546-4362917e76d3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ac8478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 13 02:41:28.873: INFO: Pod "test-new-deployment-845c8977d9-86qw6" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-86qw6 test-new-deployment-845c8977d9- deployment-6857  1519b196-1f1d-4b06-9bc2-27f8009d2979 18704 0 2023-06-13 02:41:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 d5bb73e8-2fe5-4ff9-90d5-05af4f050613 0xc003ac8867 0xc003ac8868}] [] [{kube-controller-manager Update v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5bb73e8-2fe5-4ff9-90d5-05af4f050613\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfxqs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfxqs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 02:41:28.874: INFO: Pod "test-new-deployment-845c8977d9-bc22v" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-bc22v test-new-deployment-845c8977d9- deployment-6857  b6bc8a91-47df-41ac-88ee-1c73ceacd663 18703 0 2023-06-13 02:41:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 d5bb73e8-2fe5-4ff9-90d5-05af4f050613 0xc003ac89a7 0xc003ac89a8}] [] [{kube-controller-manager Update v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5bb73e8-2fe5-4ff9-90d5-05af4f050613\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fh5f6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fh5f6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 02:41:28.874: INFO: Pod "test-new-deployment-845c8977d9-fsnnj" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-fsnnj test-new-deployment-845c8977d9- deployment-6857  7f82961a-1270-40ff-a6d6-a3543c8b3660 18695 0 2023-06-13 02:41:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 d5bb73e8-2fe5-4ff9-90d5-05af4f050613 0xc003ac8b00 0xc003ac8b01}] [] [{kube-controller-manager Update v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5bb73e8-2fe5-4ff9-90d5-05af4f050613\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gvgcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gvgcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 02:41:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 02:41:28.874: INFO: Pod "test-new-deployment-845c8977d9-lqsxb" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-lqsxb test-new-deployment-845c8977d9- deployment-6857  c5ca2dc4-1b2c-4a66-a0e2-ba691298827a 18678 0 2023-06-13 02:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d25121508da2b4dc03e133f3ecd5b27b8c801e5028ac294be2b81a47b047d41b cni.projectcalico.org/podIP:172.30.77.160/32 cni.projectcalico.org/podIPs:172.30.77.160/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 d5bb73e8-2fe5-4ff9-90d5-05af4f050613 0xc003ac8cd7 0xc003ac8cd8}] [] [{kube-controller-manager Update v1 2023-06-13 02:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5bb73e8-2fe5-4ff9-90d5-05af4f050613\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 02:41:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkfxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkfxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.77.160,StartTime:2023-06-13 02:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 02:41:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2a2639bb2cbb3d44cb014139cc9642807659b3f56141519c68f3fb003b5426c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.77.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 13 02:41:28.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6857" for this suite. 06/13/23 02:41:28.898
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":109,"skipped":2086,"failed":0}
------------------------------
• [2.396 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:41:26.516
    Jun 13 02:41:26.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename deployment 06/13/23 02:41:26.517
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:26.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:26.557
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jun 13 02:41:26.571: INFO: Creating simple deployment test-new-deployment
    Jun 13 02:41:26.681: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 06/13/23 02:41:28.717
    STEP: updating a scale subresource 06/13/23 02:41:28.725
    STEP: verifying the deployment Spec.Replicas was modified 06/13/23 02:41:28.752
    STEP: Patch a scale subresource 06/13/23 02:41:28.768
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 13 02:41:28.844: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-6857  5fdfaaee-a4ee-4f1e-a546-4362917e76d3 18697 3 2023-06-13 02:41:26 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-13 02:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0013d5fa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:2,UpdatedReplicas:2,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-06-13 02:41:28 +0000 UTC,LastTransitionTime:2023-06-13 02:41:26 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-13 02:41:28 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 13 02:41:28.860: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-6857  d5bb73e8-2fe5-4ff9-90d5-05af4f050613 18698 3 2023-06-13 02:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 5fdfaaee-a4ee-4f1e-a546-4362917e76d3 0xc003ac83e7 0xc003ac83e8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5fdfaaee-a4ee-4f1e-a546-4362917e76d3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ac8478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 02:41:28.873: INFO: Pod "test-new-deployment-845c8977d9-86qw6" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-86qw6 test-new-deployment-845c8977d9- deployment-6857  1519b196-1f1d-4b06-9bc2-27f8009d2979 18704 0 2023-06-13 02:41:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 d5bb73e8-2fe5-4ff9-90d5-05af4f050613 0xc003ac8867 0xc003ac8868}] [] [{kube-controller-manager Update v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5bb73e8-2fe5-4ff9-90d5-05af4f050613\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zfxqs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zfxqs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 02:41:28.874: INFO: Pod "test-new-deployment-845c8977d9-bc22v" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-bc22v test-new-deployment-845c8977d9- deployment-6857  b6bc8a91-47df-41ac-88ee-1c73ceacd663 18703 0 2023-06-13 02:41:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 d5bb73e8-2fe5-4ff9-90d5-05af4f050613 0xc003ac89a7 0xc003ac89a8}] [] [{kube-controller-manager Update v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5bb73e8-2fe5-4ff9-90d5-05af4f050613\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fh5f6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fh5f6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 02:41:28.874: INFO: Pod "test-new-deployment-845c8977d9-fsnnj" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-fsnnj test-new-deployment-845c8977d9- deployment-6857  7f82961a-1270-40ff-a6d6-a3543c8b3660 18695 0 2023-06-13 02:41:28 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 d5bb73e8-2fe5-4ff9-90d5-05af4f050613 0xc003ac8b00 0xc003ac8b01}] [] [{kube-controller-manager Update v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5bb73e8-2fe5-4ff9-90d5-05af4f050613\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gvgcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gvgcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 02:41:28 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 02:41:28.874: INFO: Pod "test-new-deployment-845c8977d9-lqsxb" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-lqsxb test-new-deployment-845c8977d9- deployment-6857  c5ca2dc4-1b2c-4a66-a0e2-ba691298827a 18678 0 2023-06-13 02:41:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d25121508da2b4dc03e133f3ecd5b27b8c801e5028ac294be2b81a47b047d41b cni.projectcalico.org/podIP:172.30.77.160/32 cni.projectcalico.org/podIPs:172.30.77.160/32] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 d5bb73e8-2fe5-4ff9-90d5-05af4f050613 0xc003ac8cd7 0xc003ac8cd8}] [] [{kube-controller-manager Update v1 2023-06-13 02:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5bb73e8-2fe5-4ff9-90d5-05af4f050613\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 02:41:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 02:41:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkfxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkfxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 02:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.77.160,StartTime:2023-06-13 02:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 02:41:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2a2639bb2cbb3d44cb014139cc9642807659b3f56141519c68f3fb003b5426c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.77.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 13 02:41:28.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6857" for this suite. 06/13/23 02:41:28.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:41:28.915
Jun 13 02:41:28.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pods 06/13/23 02:41:28.916
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:28.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:28.954
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 06/13/23 02:41:28.96
Jun 13 02:41:28.975: INFO: Waiting up to 5m0s for pod "pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7" in namespace "pods-9750" to be "running and ready"
Jun 13 02:41:28.981: INFO: Pod "pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027704ms
Jun 13 02:41:28.981: INFO: The phase of Pod pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:41:30.990: INFO: Pod "pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.015143152s
Jun 13 02:41:30.990: INFO: The phase of Pod pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7 is Running (Ready = true)
Jun 13 02:41:30.990: INFO: Pod "pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7" satisfied condition "running and ready"
Jun 13 02:41:31.007: INFO: Pod pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7 has hostIP: 10.255.64.103
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 13 02:41:31.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9750" for this suite. 06/13/23 02:41:31.019
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":110,"skipped":2130,"failed":0}
------------------------------
• [2.121 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:41:28.915
    Jun 13 02:41:28.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pods 06/13/23 02:41:28.916
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:28.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:28.954
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 06/13/23 02:41:28.96
    Jun 13 02:41:28.975: INFO: Waiting up to 5m0s for pod "pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7" in namespace "pods-9750" to be "running and ready"
    Jun 13 02:41:28.981: INFO: Pod "pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027704ms
    Jun 13 02:41:28.981: INFO: The phase of Pod pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:41:30.990: INFO: Pod "pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7": Phase="Running", Reason="", readiness=true. Elapsed: 2.015143152s
    Jun 13 02:41:30.990: INFO: The phase of Pod pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7 is Running (Ready = true)
    Jun 13 02:41:30.990: INFO: Pod "pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7" satisfied condition "running and ready"
    Jun 13 02:41:31.007: INFO: Pod pod-hostip-9b4edfa5-86e1-4e9e-b3ce-9c9e6fc12bd7 has hostIP: 10.255.64.103
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 13 02:41:31.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9750" for this suite. 06/13/23 02:41:31.019
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:41:31.036
Jun 13 02:41:31.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubelet-test 06/13/23 02:41:31.038
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:31.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:31.09
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jun 13 02:41:31.122: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160" in namespace "kubelet-test-7932" to be "running and ready"
Jun 13 02:41:31.133: INFO: Pod "busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160": Phase="Pending", Reason="", readiness=false. Elapsed: 10.51673ms
Jun 13 02:41:31.133: INFO: The phase of Pod busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:41:33.146: INFO: Pod "busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023883179s
Jun 13 02:41:33.146: INFO: The phase of Pod busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:41:35.144: INFO: Pod "busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021578739s
Jun 13 02:41:35.144: INFO: The phase of Pod busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 02:41:37.152: INFO: Pod "busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160": Phase="Running", Reason="", readiness=true. Elapsed: 6.029505297s
Jun 13 02:41:37.152: INFO: The phase of Pod busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160 is Running (Ready = true)
Jun 13 02:41:37.152: INFO: Pod "busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 13 02:41:37.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7932" for this suite. 06/13/23 02:41:37.201
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":111,"skipped":2133,"failed":0}
------------------------------
• [SLOW TEST] [6.184 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:41:31.036
    Jun 13 02:41:31.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubelet-test 06/13/23 02:41:31.038
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:31.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:31.09
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jun 13 02:41:31.122: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160" in namespace "kubelet-test-7932" to be "running and ready"
    Jun 13 02:41:31.133: INFO: Pod "busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160": Phase="Pending", Reason="", readiness=false. Elapsed: 10.51673ms
    Jun 13 02:41:31.133: INFO: The phase of Pod busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:41:33.146: INFO: Pod "busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023883179s
    Jun 13 02:41:33.146: INFO: The phase of Pod busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:41:35.144: INFO: Pod "busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021578739s
    Jun 13 02:41:35.144: INFO: The phase of Pod busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 02:41:37.152: INFO: Pod "busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160": Phase="Running", Reason="", readiness=true. Elapsed: 6.029505297s
    Jun 13 02:41:37.152: INFO: The phase of Pod busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160 is Running (Ready = true)
    Jun 13 02:41:37.152: INFO: Pod "busybox-readonly-fs7dfe675f-90d9-49a2-b663-5718e5f27160" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 13 02:41:37.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7932" for this suite. 06/13/23 02:41:37.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:41:37.221
Jun 13 02:41:37.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 02:41:37.223
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:37.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:37.263
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 06/13/23 02:41:37.27
Jun 13 02:41:37.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2614 create -f -'
Jun 13 02:41:38.802: INFO: stderr: ""
Jun 13 02:41:38.802: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/13/23 02:41:38.802
Jun 13 02:41:39.830: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 13 02:41:39.830: INFO: Found 0 / 1
Jun 13 02:41:40.831: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 13 02:41:40.831: INFO: Found 1 / 1
Jun 13 02:41:40.831: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 06/13/23 02:41:40.831
Jun 13 02:41:40.847: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 13 02:41:40.848: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 13 02:41:40.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2614 patch pod agnhost-primary-wb2hw -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 13 02:41:40.981: INFO: stderr: ""
Jun 13 02:41:40.981: INFO: stdout: "pod/agnhost-primary-wb2hw patched\n"
STEP: checking annotations 06/13/23 02:41:40.981
Jun 13 02:41:41.002: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 13 02:41:41.002: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 02:41:41.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2614" for this suite. 06/13/23 02:41:41.018
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":112,"skipped":2164,"failed":0}
------------------------------
• [3.815 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:41:37.221
    Jun 13 02:41:37.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 02:41:37.223
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:37.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:37.263
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 06/13/23 02:41:37.27
    Jun 13 02:41:37.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2614 create -f -'
    Jun 13 02:41:38.802: INFO: stderr: ""
    Jun 13 02:41:38.802: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/13/23 02:41:38.802
    Jun 13 02:41:39.830: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 13 02:41:39.830: INFO: Found 0 / 1
    Jun 13 02:41:40.831: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 13 02:41:40.831: INFO: Found 1 / 1
    Jun 13 02:41:40.831: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 06/13/23 02:41:40.831
    Jun 13 02:41:40.847: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 13 02:41:40.848: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun 13 02:41:40.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2614 patch pod agnhost-primary-wb2hw -p {"metadata":{"annotations":{"x":"y"}}}'
    Jun 13 02:41:40.981: INFO: stderr: ""
    Jun 13 02:41:40.981: INFO: stdout: "pod/agnhost-primary-wb2hw patched\n"
    STEP: checking annotations 06/13/23 02:41:40.981
    Jun 13 02:41:41.002: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 13 02:41:41.002: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 02:41:41.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2614" for this suite. 06/13/23 02:41:41.018
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:41:41.036
Jun 13 02:41:41.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename runtimeclass 06/13/23 02:41:41.037
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:41.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:41.085
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 06/13/23 02:41:41.092
STEP: getting /apis/node.k8s.io 06/13/23 02:41:41.097
STEP: getting /apis/node.k8s.io/v1 06/13/23 02:41:41.101
STEP: creating 06/13/23 02:41:41.106
STEP: watching 06/13/23 02:41:41.146
Jun 13 02:41:41.146: INFO: starting watch
STEP: getting 06/13/23 02:41:41.16
STEP: listing 06/13/23 02:41:41.178
STEP: patching 06/13/23 02:41:41.187
STEP: updating 06/13/23 02:41:41.2
Jun 13 02:41:41.214: INFO: waiting for watch events with expected annotations
STEP: deleting 06/13/23 02:41:41.214
STEP: deleting a collection 06/13/23 02:41:41.258
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 13 02:41:41.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6483" for this suite. 06/13/23 02:41:41.356
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":113,"skipped":2165,"failed":0}
------------------------------
• [0.408 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:41:41.036
    Jun 13 02:41:41.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename runtimeclass 06/13/23 02:41:41.037
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:41.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:41.085
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 06/13/23 02:41:41.092
    STEP: getting /apis/node.k8s.io 06/13/23 02:41:41.097
    STEP: getting /apis/node.k8s.io/v1 06/13/23 02:41:41.101
    STEP: creating 06/13/23 02:41:41.106
    STEP: watching 06/13/23 02:41:41.146
    Jun 13 02:41:41.146: INFO: starting watch
    STEP: getting 06/13/23 02:41:41.16
    STEP: listing 06/13/23 02:41:41.178
    STEP: patching 06/13/23 02:41:41.187
    STEP: updating 06/13/23 02:41:41.2
    Jun 13 02:41:41.214: INFO: waiting for watch events with expected annotations
    STEP: deleting 06/13/23 02:41:41.214
    STEP: deleting a collection 06/13/23 02:41:41.258
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 13 02:41:41.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6483" for this suite. 06/13/23 02:41:41.356
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:41:41.444
Jun 13 02:41:41.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 02:41:41.446
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:41.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:41.561
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 06/13/23 02:41:41.568
Jun 13 02:41:41.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 02:41:48.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 02:42:09.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1894" for this suite. 06/13/23 02:42:09.18
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":114,"skipped":2167,"failed":0}
------------------------------
• [SLOW TEST] [27.758 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:41:41.444
    Jun 13 02:41:41.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 02:41:41.446
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:41:41.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:41:41.561
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 06/13/23 02:41:41.568
    Jun 13 02:41:41.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 02:41:48.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 02:42:09.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1894" for this suite. 06/13/23 02:42:09.18
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:42:09.204
Jun 13 02:42:09.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename cronjob 06/13/23 02:42:09.206
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:42:09.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:42:09.253
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 06/13/23 02:42:09.259
STEP: Ensuring no jobs are scheduled 06/13/23 02:42:09.272
STEP: Ensuring no job exists by listing jobs explicitly 06/13/23 02:47:09.294
STEP: Removing cronjob 06/13/23 02:47:09.323
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 13 02:47:09.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5348" for this suite. 06/13/23 02:47:09.506
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":115,"skipped":2170,"failed":0}
------------------------------
• [SLOW TEST] [300.334 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:42:09.204
    Jun 13 02:42:09.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename cronjob 06/13/23 02:42:09.206
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:42:09.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:42:09.253
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 06/13/23 02:42:09.259
    STEP: Ensuring no jobs are scheduled 06/13/23 02:42:09.272
    STEP: Ensuring no job exists by listing jobs explicitly 06/13/23 02:47:09.294
    STEP: Removing cronjob 06/13/23 02:47:09.323
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 13 02:47:09.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5348" for this suite. 06/13/23 02:47:09.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:47:09.541
Jun 13 02:47:09.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-probe 06/13/23 02:47:09.544
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:47:09.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:47:09.588
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481 in namespace container-probe-1412 06/13/23 02:47:09.607
Jun 13 02:47:09.628: INFO: Waiting up to 5m0s for pod "liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481" in namespace "container-probe-1412" to be "not pending"
Jun 13 02:47:09.638: INFO: Pod "liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481": Phase="Pending", Reason="", readiness=false. Elapsed: 10.057725ms
Jun 13 02:47:11.654: INFO: Pod "liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026375065s
Jun 13 02:47:13.647: INFO: Pod "liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481": Phase="Running", Reason="", readiness=true. Elapsed: 4.01898343s
Jun 13 02:47:13.647: INFO: Pod "liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481" satisfied condition "not pending"
Jun 13 02:47:13.648: INFO: Started pod liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481 in namespace container-probe-1412
STEP: checking the pod's current state and verifying that restartCount is present 06/13/23 02:47:13.648
Jun 13 02:47:13.656: INFO: Initial restart count of pod liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481 is 0
Jun 13 02:47:31.749: INFO: Restart count of pod container-probe-1412/liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481 is now 1 (18.09289427s elapsed)
STEP: deleting the pod 06/13/23 02:47:31.749
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 13 02:47:31.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1412" for this suite. 06/13/23 02:47:31.794
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":116,"skipped":2215,"failed":0}
------------------------------
• [SLOW TEST] [22.271 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:47:09.541
    Jun 13 02:47:09.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-probe 06/13/23 02:47:09.544
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:47:09.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:47:09.588
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481 in namespace container-probe-1412 06/13/23 02:47:09.607
    Jun 13 02:47:09.628: INFO: Waiting up to 5m0s for pod "liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481" in namespace "container-probe-1412" to be "not pending"
    Jun 13 02:47:09.638: INFO: Pod "liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481": Phase="Pending", Reason="", readiness=false. Elapsed: 10.057725ms
    Jun 13 02:47:11.654: INFO: Pod "liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026375065s
    Jun 13 02:47:13.647: INFO: Pod "liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481": Phase="Running", Reason="", readiness=true. Elapsed: 4.01898343s
    Jun 13 02:47:13.647: INFO: Pod "liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481" satisfied condition "not pending"
    Jun 13 02:47:13.648: INFO: Started pod liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481 in namespace container-probe-1412
    STEP: checking the pod's current state and verifying that restartCount is present 06/13/23 02:47:13.648
    Jun 13 02:47:13.656: INFO: Initial restart count of pod liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481 is 0
    Jun 13 02:47:31.749: INFO: Restart count of pod container-probe-1412/liveness-cfec9ce7-145c-403b-b5a9-004b4fa27481 is now 1 (18.09289427s elapsed)
    STEP: deleting the pod 06/13/23 02:47:31.749
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 13 02:47:31.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1412" for this suite. 06/13/23 02:47:31.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:47:31.813
Jun 13 02:47:31.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename gc 06/13/23 02:47:31.815
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:47:31.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:47:31.863
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 06/13/23 02:47:31.872
STEP: delete the rc 06/13/23 02:47:36.9
STEP: wait for all pods to be garbage collected 06/13/23 02:47:36.935
STEP: Gathering metrics 06/13/23 02:47:41.968
Jun 13 02:47:42.030: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" in namespace "kube-system" to be "running and ready"
Jun 13 02:47:42.038: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq": Phase="Running", Reason="", readiness=true. Elapsed: 7.641748ms
Jun 13 02:47:42.038: INFO: The phase of Pod kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq is Running (Ready = true)
Jun 13 02:47:42.038: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" satisfied condition "running and ready"
Jun 13 02:47:42.156: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 13 02:47:42.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4416" for this suite. 06/13/23 02:47:42.167
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":117,"skipped":2234,"failed":0}
------------------------------
• [SLOW TEST] [10.368 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:47:31.813
    Jun 13 02:47:31.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename gc 06/13/23 02:47:31.815
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:47:31.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:47:31.863
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 06/13/23 02:47:31.872
    STEP: delete the rc 06/13/23 02:47:36.9
    STEP: wait for all pods to be garbage collected 06/13/23 02:47:36.935
    STEP: Gathering metrics 06/13/23 02:47:41.968
    Jun 13 02:47:42.030: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" in namespace "kube-system" to be "running and ready"
    Jun 13 02:47:42.038: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq": Phase="Running", Reason="", readiness=true. Elapsed: 7.641748ms
    Jun 13 02:47:42.038: INFO: The phase of Pod kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq is Running (Ready = true)
    Jun 13 02:47:42.038: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" satisfied condition "running and ready"
    Jun 13 02:47:42.156: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 13 02:47:42.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4416" for this suite. 06/13/23 02:47:42.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:47:42.184
Jun 13 02:47:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename custom-resource-definition 06/13/23 02:47:42.185
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:47:42.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:47:42.219
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jun 13 02:47:42.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 02:47:43.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9174" for this suite. 06/13/23 02:47:43.312
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":118,"skipped":2265,"failed":0}
------------------------------
• [1.140 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:47:42.184
    Jun 13 02:47:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename custom-resource-definition 06/13/23 02:47:42.185
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:47:42.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:47:42.219
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jun 13 02:47:42.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 02:47:43.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9174" for this suite. 06/13/23 02:47:43.312
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:47:43.325
Jun 13 02:47:43.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename dns 06/13/23 02:47:43.326
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:47:43.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:47:43.36
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 06/13/23 02:47:43.379
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2447.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2447.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2447.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2447.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 175.240.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.240.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.240.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.240.175_tcp@PTR;sleep 1; done
 06/13/23 02:47:43.422
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2447.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2447.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2447.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2447.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 175.240.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.240.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.240.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.240.175_tcp@PTR;sleep 1; done
 06/13/23 02:47:43.422
STEP: creating a pod to probe DNS 06/13/23 02:47:43.422
STEP: submitting the pod to kubernetes 06/13/23 02:47:43.422
Jun 13 02:47:43.440: INFO: Waiting up to 15m0s for pod "dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc" in namespace "dns-2447" to be "running"
Jun 13 02:47:43.448: INFO: Pod "dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.913409ms
Jun 13 02:47:45.456: INFO: Pod "dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015963572s
Jun 13 02:47:47.456: INFO: Pod "dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc": Phase="Running", Reason="", readiness=true. Elapsed: 4.015657878s
Jun 13 02:47:47.456: INFO: Pod "dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc" satisfied condition "running"
STEP: retrieving the pod 06/13/23 02:47:47.456
STEP: looking for the results for each expected name from probers 06/13/23 02:47:47.47
Jun 13 02:47:47.482: INFO: Unable to read wheezy_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:47.499: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:47.505: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:47.511: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:47.605: INFO: Unable to read jessie_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:47.613: INFO: Unable to read jessie_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:47.623: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:47.630: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:47.768: INFO: Lookups using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc failed for: [wheezy_udp@dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_udp@dns-test-service.dns-2447.svc.cluster.local jessie_tcp@dns-test-service.dns-2447.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local]

Jun 13 02:47:52.776: INFO: Unable to read wheezy_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:52.784: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:52.793: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:52.802: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:52.841: INFO: Unable to read jessie_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:52.849: INFO: Unable to read jessie_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:52.857: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:52.865: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:52.896: INFO: Lookups using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc failed for: [wheezy_udp@dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_udp@dns-test-service.dns-2447.svc.cluster.local jessie_tcp@dns-test-service.dns-2447.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local]

Jun 13 02:47:57.775: INFO: Unable to read wheezy_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:57.782: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:57.789: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:57.797: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:57.873: INFO: Unable to read jessie_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:57.881: INFO: Unable to read jessie_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:57.887: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:57.897: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:47:57.933: INFO: Lookups using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc failed for: [wheezy_udp@dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_udp@dns-test-service.dns-2447.svc.cluster.local jessie_tcp@dns-test-service.dns-2447.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local]

Jun 13 02:48:02.778: INFO: Unable to read wheezy_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:02.785: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:02.791: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:02.798: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:02.842: INFO: Unable to read jessie_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:02.849: INFO: Unable to read jessie_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:02.857: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:02.863: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:02.893: INFO: Lookups using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc failed for: [wheezy_udp@dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_udp@dns-test-service.dns-2447.svc.cluster.local jessie_tcp@dns-test-service.dns-2447.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local]

Jun 13 02:48:07.775: INFO: Unable to read wheezy_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:07.783: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:07.791: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:07.798: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:07.911: INFO: Unable to read jessie_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:07.917: INFO: Unable to read jessie_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:07.924: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:07.930: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:07.963: INFO: Lookups using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc failed for: [wheezy_udp@dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_udp@dns-test-service.dns-2447.svc.cluster.local jessie_tcp@dns-test-service.dns-2447.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local]

Jun 13 02:48:12.775: INFO: Unable to read wheezy_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:12.783: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:12.788: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:12.794: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:12.823: INFO: Unable to read jessie_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:12.830: INFO: Unable to read jessie_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:12.837: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:12.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
Jun 13 02:48:12.865: INFO: Lookups using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc failed for: [wheezy_udp@dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_udp@dns-test-service.dns-2447.svc.cluster.local jessie_tcp@dns-test-service.dns-2447.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local]

Jun 13 02:48:17.912: INFO: DNS probes using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc succeeded

STEP: deleting the pod 06/13/23 02:48:17.912
STEP: deleting the test service 06/13/23 02:48:17.935
STEP: deleting the test headless service 06/13/23 02:48:18.266
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 13 02:48:18.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2447" for this suite. 06/13/23 02:48:18.31
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":119,"skipped":2269,"failed":0}
------------------------------
• [SLOW TEST] [35.005 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:47:43.325
    Jun 13 02:47:43.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename dns 06/13/23 02:47:43.326
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:47:43.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:47:43.36
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 06/13/23 02:47:43.379
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2447.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2447.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2447.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2447.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 175.240.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.240.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.240.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.240.175_tcp@PTR;sleep 1; done
     06/13/23 02:47:43.422
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2447.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2447.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2447.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2447.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2447.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 175.240.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.240.175_udp@PTR;check="$$(dig +tcp +noall +answer +search 175.240.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.240.175_tcp@PTR;sleep 1; done
     06/13/23 02:47:43.422
    STEP: creating a pod to probe DNS 06/13/23 02:47:43.422
    STEP: submitting the pod to kubernetes 06/13/23 02:47:43.422
    Jun 13 02:47:43.440: INFO: Waiting up to 15m0s for pod "dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc" in namespace "dns-2447" to be "running"
    Jun 13 02:47:43.448: INFO: Pod "dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc": Phase="Pending", Reason="", readiness=false. Elapsed: 7.913409ms
    Jun 13 02:47:45.456: INFO: Pod "dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015963572s
    Jun 13 02:47:47.456: INFO: Pod "dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc": Phase="Running", Reason="", readiness=true. Elapsed: 4.015657878s
    Jun 13 02:47:47.456: INFO: Pod "dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc" satisfied condition "running"
    STEP: retrieving the pod 06/13/23 02:47:47.456
    STEP: looking for the results for each expected name from probers 06/13/23 02:47:47.47
    Jun 13 02:47:47.482: INFO: Unable to read wheezy_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:47.499: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:47.505: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:47.511: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:47.605: INFO: Unable to read jessie_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:47.613: INFO: Unable to read jessie_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:47.623: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:47.630: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:47.768: INFO: Lookups using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc failed for: [wheezy_udp@dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_udp@dns-test-service.dns-2447.svc.cluster.local jessie_tcp@dns-test-service.dns-2447.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local]

    Jun 13 02:47:52.776: INFO: Unable to read wheezy_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:52.784: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:52.793: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:52.802: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:52.841: INFO: Unable to read jessie_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:52.849: INFO: Unable to read jessie_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:52.857: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:52.865: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:52.896: INFO: Lookups using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc failed for: [wheezy_udp@dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_udp@dns-test-service.dns-2447.svc.cluster.local jessie_tcp@dns-test-service.dns-2447.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local]

    Jun 13 02:47:57.775: INFO: Unable to read wheezy_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:57.782: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:57.789: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:57.797: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:57.873: INFO: Unable to read jessie_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:57.881: INFO: Unable to read jessie_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:57.887: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:57.897: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:47:57.933: INFO: Lookups using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc failed for: [wheezy_udp@dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_udp@dns-test-service.dns-2447.svc.cluster.local jessie_tcp@dns-test-service.dns-2447.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local]

    Jun 13 02:48:02.778: INFO: Unable to read wheezy_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:02.785: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:02.791: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:02.798: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:02.842: INFO: Unable to read jessie_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:02.849: INFO: Unable to read jessie_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:02.857: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:02.863: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:02.893: INFO: Lookups using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc failed for: [wheezy_udp@dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_udp@dns-test-service.dns-2447.svc.cluster.local jessie_tcp@dns-test-service.dns-2447.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local]

    Jun 13 02:48:07.775: INFO: Unable to read wheezy_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:07.783: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:07.791: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:07.798: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:07.911: INFO: Unable to read jessie_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:07.917: INFO: Unable to read jessie_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:07.924: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:07.930: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:07.963: INFO: Lookups using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc failed for: [wheezy_udp@dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_udp@dns-test-service.dns-2447.svc.cluster.local jessie_tcp@dns-test-service.dns-2447.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local]

    Jun 13 02:48:12.775: INFO: Unable to read wheezy_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:12.783: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:12.788: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:12.794: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:12.823: INFO: Unable to read jessie_udp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:12.830: INFO: Unable to read jessie_tcp@dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:12.837: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:12.843: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local from pod dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc: the server could not find the requested resource (get pods dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc)
    Jun 13 02:48:12.865: INFO: Lookups using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc failed for: [wheezy_udp@dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@dns-test-service.dns-2447.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_udp@dns-test-service.dns-2447.svc.cluster.local jessie_tcp@dns-test-service.dns-2447.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2447.svc.cluster.local]

    Jun 13 02:48:17.912: INFO: DNS probes using dns-2447/dns-test-e7db0c64-7bbb-4c16-88c6-b3afe6c0d2fc succeeded

    STEP: deleting the pod 06/13/23 02:48:17.912
    STEP: deleting the test service 06/13/23 02:48:17.935
    STEP: deleting the test headless service 06/13/23 02:48:18.266
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 13 02:48:18.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-2447" for this suite. 06/13/23 02:48:18.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:48:18.337
Jun 13 02:48:18.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-probe 06/13/23 02:48:18.339
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:48:18.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:48:18.375
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 in namespace container-probe-6033 06/13/23 02:48:18.383
Jun 13 02:48:18.408: INFO: Waiting up to 5m0s for pod "liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3" in namespace "container-probe-6033" to be "not pending"
Jun 13 02:48:18.425: INFO: Pod "liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.994981ms
Jun 13 02:48:20.434: INFO: Pod "liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3": Phase="Running", Reason="", readiness=true. Elapsed: 2.025909071s
Jun 13 02:48:20.434: INFO: Pod "liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3" satisfied condition "not pending"
Jun 13 02:48:20.434: INFO: Started pod liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 in namespace container-probe-6033
STEP: checking the pod's current state and verifying that restartCount is present 06/13/23 02:48:20.434
Jun 13 02:48:20.442: INFO: Initial restart count of pod liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 is 0
Jun 13 02:48:40.629: INFO: Restart count of pod container-probe-6033/liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 is now 1 (20.186402972s elapsed)
Jun 13 02:49:00.747: INFO: Restart count of pod container-probe-6033/liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 is now 2 (40.304835432s elapsed)
Jun 13 02:49:20.850: INFO: Restart count of pod container-probe-6033/liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 is now 3 (1m0.407575698s elapsed)
Jun 13 02:49:41.082: INFO: Restart count of pod container-probe-6033/liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 is now 4 (1m20.6401679s elapsed)
Jun 13 02:50:53.810: INFO: Restart count of pod container-probe-6033/liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 is now 5 (2m33.367561064s elapsed)
STEP: deleting the pod 06/13/23 02:50:53.81
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 13 02:50:53.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6033" for this suite. 06/13/23 02:50:53.865
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":120,"skipped":2364,"failed":0}
------------------------------
• [SLOW TEST] [155.541 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:48:18.337
    Jun 13 02:48:18.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-probe 06/13/23 02:48:18.339
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:48:18.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:48:18.375
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 in namespace container-probe-6033 06/13/23 02:48:18.383
    Jun 13 02:48:18.408: INFO: Waiting up to 5m0s for pod "liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3" in namespace "container-probe-6033" to be "not pending"
    Jun 13 02:48:18.425: INFO: Pod "liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.994981ms
    Jun 13 02:48:20.434: INFO: Pod "liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3": Phase="Running", Reason="", readiness=true. Elapsed: 2.025909071s
    Jun 13 02:48:20.434: INFO: Pod "liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3" satisfied condition "not pending"
    Jun 13 02:48:20.434: INFO: Started pod liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 in namespace container-probe-6033
    STEP: checking the pod's current state and verifying that restartCount is present 06/13/23 02:48:20.434
    Jun 13 02:48:20.442: INFO: Initial restart count of pod liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 is 0
    Jun 13 02:48:40.629: INFO: Restart count of pod container-probe-6033/liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 is now 1 (20.186402972s elapsed)
    Jun 13 02:49:00.747: INFO: Restart count of pod container-probe-6033/liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 is now 2 (40.304835432s elapsed)
    Jun 13 02:49:20.850: INFO: Restart count of pod container-probe-6033/liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 is now 3 (1m0.407575698s elapsed)
    Jun 13 02:49:41.082: INFO: Restart count of pod container-probe-6033/liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 is now 4 (1m20.6401679s elapsed)
    Jun 13 02:50:53.810: INFO: Restart count of pod container-probe-6033/liveness-4d616e24-80e1-4320-b2ce-31d202cff3f3 is now 5 (2m33.367561064s elapsed)
    STEP: deleting the pod 06/13/23 02:50:53.81
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 13 02:50:53.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-6033" for this suite. 06/13/23 02:50:53.865
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:50:53.879
Jun 13 02:50:53.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 02:50:53.881
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:50:53.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:50:53.951
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 06/13/23 02:50:53.96
Jun 13 02:50:53.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb" in namespace "projected-9442" to be "Succeeded or Failed"
Jun 13 02:50:54.003: INFO: Pod "downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.459656ms
Jun 13 02:50:56.010: INFO: Pod "downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018602387s
Jun 13 02:50:58.016: INFO: Pod "downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024316825s
STEP: Saw pod success 06/13/23 02:50:58.016
Jun 13 02:50:58.016: INFO: Pod "downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb" satisfied condition "Succeeded or Failed"
Jun 13 02:50:58.022: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb container client-container: <nil>
STEP: delete the pod 06/13/23 02:50:58.052
Jun 13 02:50:58.127: INFO: Waiting for pod downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb to disappear
Jun 13 02:50:58.134: INFO: Pod downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 13 02:50:58.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9442" for this suite. 06/13/23 02:50:58.144
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":121,"skipped":2365,"failed":0}
------------------------------
• [4.284 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:50:53.879
    Jun 13 02:50:53.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 02:50:53.881
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:50:53.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:50:53.951
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 06/13/23 02:50:53.96
    Jun 13 02:50:53.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb" in namespace "projected-9442" to be "Succeeded or Failed"
    Jun 13 02:50:54.003: INFO: Pod "downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb": Phase="Pending", Reason="", readiness=false. Elapsed: 11.459656ms
    Jun 13 02:50:56.010: INFO: Pod "downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018602387s
    Jun 13 02:50:58.016: INFO: Pod "downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024316825s
    STEP: Saw pod success 06/13/23 02:50:58.016
    Jun 13 02:50:58.016: INFO: Pod "downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb" satisfied condition "Succeeded or Failed"
    Jun 13 02:50:58.022: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb container client-container: <nil>
    STEP: delete the pod 06/13/23 02:50:58.052
    Jun 13 02:50:58.127: INFO: Waiting for pod downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb to disappear
    Jun 13 02:50:58.134: INFO: Pod downwardapi-volume-536e3f3a-9761-40b5-88fc-a07bdf603bdb no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 13 02:50:58.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9442" for this suite. 06/13/23 02:50:58.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:50:58.163
Jun 13 02:50:58.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename statefulset 06/13/23 02:50:58.164
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:50:58.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:50:58.203
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1542 06/13/23 02:50:58.211
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 06/13/23 02:50:58.222
STEP: Creating stateful set ss in namespace statefulset-1542 06/13/23 02:50:58.232
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1542 06/13/23 02:50:58.245
Jun 13 02:50:58.257: INFO: Found 0 stateful pods, waiting for 1
Jun 13 02:51:08.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 06/13/23 02:51:08.266
Jun 13 02:51:08.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 13 02:51:08.477: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 13 02:51:08.477: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 13 02:51:08.477: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 13 02:51:08.485: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 13 02:51:18.495: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 13 02:51:18.495: INFO: Waiting for statefulset status.replicas updated to 0
Jun 13 02:51:18.534: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999606s
Jun 13 02:51:19.543: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.986589584s
Jun 13 02:51:20.552: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.977830345s
Jun 13 02:51:21.565: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.96700018s
Jun 13 02:51:22.577: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.955024306s
Jun 13 02:51:23.586: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.943629894s
Jun 13 02:51:24.594: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.933870212s
Jun 13 02:51:25.601: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.926542407s
Jun 13 02:51:26.612: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.919517726s
Jun 13 02:51:27.621: INFO: Verifying statefulset ss doesn't scale past 1 for another 908.214619ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1542 06/13/23 02:51:28.621
Jun 13 02:51:28.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 13 02:51:28.824: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 13 02:51:28.824: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 13 02:51:28.824: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 13 02:51:28.833: INFO: Found 1 stateful pods, waiting for 3
Jun 13 02:51:38.842: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 02:51:38.842: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 02:51:38.842: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 06/13/23 02:51:38.842
STEP: Scale down will halt with unhealthy stateful pod 06/13/23 02:51:38.842
Jun 13 02:51:38.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 13 02:51:39.055: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 13 02:51:39.055: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 13 02:51:39.055: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 13 02:51:39.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 13 02:51:39.236: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 13 02:51:39.236: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 13 02:51:39.236: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 13 02:51:39.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 13 02:51:39.439: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 13 02:51:39.439: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 13 02:51:39.439: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 13 02:51:39.439: INFO: Waiting for statefulset status.replicas updated to 0
Jun 13 02:51:39.446: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 13 02:51:49.461: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 13 02:51:49.461: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 13 02:51:49.461: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 13 02:51:49.488: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999437s
Jun 13 02:51:50.503: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991888639s
Jun 13 02:51:51.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977294382s
Jun 13 02:51:52.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.968036644s
Jun 13 02:51:53.548: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.940002314s
Jun 13 02:51:54.555: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.931794205s
Jun 13 02:51:55.567: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.924399583s
Jun 13 02:51:56.573: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.913346336s
Jun 13 02:51:57.582: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.906203764s
Jun 13 02:51:58.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 897.910239ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1542 06/13/23 02:51:59.589
Jun 13 02:51:59.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 13 02:51:59.805: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 13 02:51:59.805: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 13 02:51:59.805: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 13 02:51:59.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 13 02:52:00.039: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 13 02:52:00.039: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 13 02:52:00.039: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 13 02:52:00.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 13 02:52:00.321: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 13 02:52:00.322: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 13 02:52:00.322: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 13 02:52:00.322: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 06/13/23 02:52:10.364
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 13 02:52:10.365: INFO: Deleting all statefulset in ns statefulset-1542
Jun 13 02:52:10.376: INFO: Scaling statefulset ss to 0
Jun 13 02:52:10.407: INFO: Waiting for statefulset status.replicas updated to 0
Jun 13 02:52:10.414: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 13 02:52:10.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1542" for this suite. 06/13/23 02:52:10.459
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":122,"skipped":2370,"failed":0}
------------------------------
• [SLOW TEST] [72.319 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:50:58.163
    Jun 13 02:50:58.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename statefulset 06/13/23 02:50:58.164
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:50:58.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:50:58.203
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1542 06/13/23 02:50:58.211
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 06/13/23 02:50:58.222
    STEP: Creating stateful set ss in namespace statefulset-1542 06/13/23 02:50:58.232
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1542 06/13/23 02:50:58.245
    Jun 13 02:50:58.257: INFO: Found 0 stateful pods, waiting for 1
    Jun 13 02:51:08.266: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 06/13/23 02:51:08.266
    Jun 13 02:51:08.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 13 02:51:08.477: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 13 02:51:08.477: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 13 02:51:08.477: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 13 02:51:08.485: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jun 13 02:51:18.495: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 13 02:51:18.495: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 13 02:51:18.534: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999606s
    Jun 13 02:51:19.543: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.986589584s
    Jun 13 02:51:20.552: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.977830345s
    Jun 13 02:51:21.565: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.96700018s
    Jun 13 02:51:22.577: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.955024306s
    Jun 13 02:51:23.586: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.943629894s
    Jun 13 02:51:24.594: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.933870212s
    Jun 13 02:51:25.601: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.926542407s
    Jun 13 02:51:26.612: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.919517726s
    Jun 13 02:51:27.621: INFO: Verifying statefulset ss doesn't scale past 1 for another 908.214619ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1542 06/13/23 02:51:28.621
    Jun 13 02:51:28.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 13 02:51:28.824: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 13 02:51:28.824: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 13 02:51:28.824: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 13 02:51:28.833: INFO: Found 1 stateful pods, waiting for 3
    Jun 13 02:51:38.842: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 02:51:38.842: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 02:51:38.842: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 06/13/23 02:51:38.842
    STEP: Scale down will halt with unhealthy stateful pod 06/13/23 02:51:38.842
    Jun 13 02:51:38.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 13 02:51:39.055: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 13 02:51:39.055: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 13 02:51:39.055: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 13 02:51:39.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 13 02:51:39.236: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 13 02:51:39.236: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 13 02:51:39.236: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 13 02:51:39.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 13 02:51:39.439: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 13 02:51:39.439: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 13 02:51:39.439: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 13 02:51:39.439: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 13 02:51:39.446: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jun 13 02:51:49.461: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 13 02:51:49.461: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jun 13 02:51:49.461: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jun 13 02:51:49.488: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999437s
    Jun 13 02:51:50.503: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991888639s
    Jun 13 02:51:51.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.977294382s
    Jun 13 02:51:52.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.968036644s
    Jun 13 02:51:53.548: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.940002314s
    Jun 13 02:51:54.555: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.931794205s
    Jun 13 02:51:55.567: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.924399583s
    Jun 13 02:51:56.573: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.913346336s
    Jun 13 02:51:57.582: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.906203764s
    Jun 13 02:51:58.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 897.910239ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1542 06/13/23 02:51:59.589
    Jun 13 02:51:59.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 13 02:51:59.805: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 13 02:51:59.805: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 13 02:51:59.805: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 13 02:51:59.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 13 02:52:00.039: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 13 02:52:00.039: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 13 02:52:00.039: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 13 02:52:00.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-1542 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 13 02:52:00.321: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 13 02:52:00.322: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 13 02:52:00.322: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 13 02:52:00.322: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 06/13/23 02:52:10.364
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 13 02:52:10.365: INFO: Deleting all statefulset in ns statefulset-1542
    Jun 13 02:52:10.376: INFO: Scaling statefulset ss to 0
    Jun 13 02:52:10.407: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 13 02:52:10.414: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 13 02:52:10.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1542" for this suite. 06/13/23 02:52:10.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:52:10.485
Jun 13 02:52:10.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename watch 06/13/23 02:52:10.487
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:52:10.52
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:52:10.532
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 06/13/23 02:52:10.546
STEP: modifying the configmap once 06/13/23 02:52:10.571
STEP: modifying the configmap a second time 06/13/23 02:52:10.605
STEP: deleting the configmap 06/13/23 02:52:10.621
STEP: creating a watch on configmaps from the resource version returned by the first update 06/13/23 02:52:10.634
STEP: Expecting to observe notifications for all changes to the configmap after the first update 06/13/23 02:52:10.639
Jun 13 02:52:10.639: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4214  1c777fc8-8a1e-4d39-baaf-7e831ed06014 21929 0 2023-06-13 02:52:10 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-13 02:52:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 02:52:10.640: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4214  1c777fc8-8a1e-4d39-baaf-7e831ed06014 21930 0 2023-06-13 02:52:10 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-13 02:52:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 13 02:52:10.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4214" for this suite. 06/13/23 02:52:10.655
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":123,"skipped":2415,"failed":0}
------------------------------
• [0.186 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:52:10.485
    Jun 13 02:52:10.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename watch 06/13/23 02:52:10.487
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:52:10.52
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:52:10.532
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 06/13/23 02:52:10.546
    STEP: modifying the configmap once 06/13/23 02:52:10.571
    STEP: modifying the configmap a second time 06/13/23 02:52:10.605
    STEP: deleting the configmap 06/13/23 02:52:10.621
    STEP: creating a watch on configmaps from the resource version returned by the first update 06/13/23 02:52:10.634
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 06/13/23 02:52:10.639
    Jun 13 02:52:10.639: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4214  1c777fc8-8a1e-4d39-baaf-7e831ed06014 21929 0 2023-06-13 02:52:10 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-13 02:52:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 02:52:10.640: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4214  1c777fc8-8a1e-4d39-baaf-7e831ed06014 21930 0 2023-06-13 02:52:10 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-13 02:52:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 13 02:52:10.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4214" for this suite. 06/13/23 02:52:10.655
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:52:10.671
Jun 13 02:52:10.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-probe 06/13/23 02:52:10.672
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:52:10.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:52:10.725
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 13 02:53:10.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5554" for this suite. 06/13/23 02:53:10.774
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":124,"skipped":2419,"failed":0}
------------------------------
• [SLOW TEST] [60.116 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:52:10.671
    Jun 13 02:52:10.671: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-probe 06/13/23 02:52:10.672
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:52:10.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:52:10.725
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 13 02:53:10.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5554" for this suite. 06/13/23 02:53:10.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:53:10.79
Jun 13 02:53:10.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename sched-pred 06/13/23 02:53:10.792
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:53:10.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:53:10.833
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 13 02:53:10.839: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 13 02:53:10.856: INFO: Waiting for terminating namespaces to be deleted...
Jun 13 02:53:10.866: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-2q6k2 before test
Jun 13 02:53:10.886: INFO: calico-node-xb8kf from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
Jun 13 02:53:10.886: INFO: 	Container calico-node ready: true, restart count 0
Jun 13 02:53:10.886: INFO: calico-typha-79dfdd7d65-w95lk from calico-system started at 2023-06-13 02:04:34 +0000 UTC (1 container statuses recorded)
Jun 13 02:53:10.886: INFO: 	Container calico-typha ready: true, restart count 0
Jun 13 02:53:10.886: INFO: csi-node-driver-vkf24 from calico-system started at 2023-06-13 02:05:19 +0000 UTC (2 container statuses recorded)
Jun 13 02:53:10.886: INFO: 	Container calico-csi ready: true, restart count 0
Jun 13 02:53:10.886: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun 13 02:53:10.886: INFO: test-webserver-67be0501-ec94-4e25-a28b-bacc17b5e0ad from container-probe-5554 started at 2023-06-13 02:52:10 +0000 UTC (1 container statuses recorded)
Jun 13 02:53:10.886: INFO: 	Container test-webserver ready: false, restart count 0
Jun 13 02:53:10.886: INFO: kube-proxy-g2qcx from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
Jun 13 02:53:10.886: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 13 02:53:10.886: INFO: smtx-elf-csi-driver-node-plugin-gqbkk from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
Jun 13 02:53:10.886: INFO: 	Container csi-driver ready: true, restart count 0
Jun 13 02:53:10.886: INFO: 	Container driver-registrar ready: true, restart count 0
Jun 13 02:53:10.886: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 13 02:53:10.886: INFO: sonobuoy from sonobuoy started at 2023-06-13 02:14:11 +0000 UTC (1 container statuses recorded)
Jun 13 02:53:10.886: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 13 02:53:10.886: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-f9dsl from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 02:53:10.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 02:53:10.886: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 13 02:53:10.886: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-469fm before test
Jun 13 02:53:10.920: INFO: calico-node-lhmmz from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
Jun 13 02:53:10.920: INFO: 	Container calico-node ready: true, restart count 0
Jun 13 02:53:10.920: INFO: csi-node-driver-jrfzt from calico-system started at 2023-06-13 02:04:55 +0000 UTC (2 container statuses recorded)
Jun 13 02:53:10.920: INFO: 	Container calico-csi ready: true, restart count 0
Jun 13 02:53:10.920: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun 13 02:53:10.920: INFO: kube-proxy-fn2dq from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
Jun 13 02:53:10.920: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 13 02:53:10.920: INFO: snapshot-controller-76c6888c5-9m5td from kube-system started at 2023-06-13 02:04:55 +0000 UTC (1 container statuses recorded)
Jun 13 02:53:10.920: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 13 02:53:10.920: INFO: smtx-elf-csi-driver-node-plugin-h8k7m from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
Jun 13 02:53:10.920: INFO: 	Container csi-driver ready: true, restart count 0
Jun 13 02:53:10.920: INFO: 	Container driver-registrar ready: true, restart count 0
Jun 13 02:53:10.920: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 13 02:53:10.920: INFO: tigera-operator-6f49bd984-r7pkz from sks-system started at 2023-06-13 02:04:29 +0000 UTC (1 container statuses recorded)
Jun 13 02:53:10.920: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 13 02:53:10.920: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-hcvsr from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 02:53:10.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 02:53:10.920: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 13 02:53:10.920: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-l5gcd before test
Jun 13 02:53:10.943: INFO: calico-node-fptww from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
Jun 13 02:53:10.943: INFO: 	Container calico-node ready: true, restart count 0
Jun 13 02:53:10.943: INFO: calico-typha-79dfdd7d65-nrkt5 from calico-system started at 2023-06-13 02:04:43 +0000 UTC (1 container statuses recorded)
Jun 13 02:53:10.943: INFO: 	Container calico-typha ready: true, restart count 0
Jun 13 02:53:10.943: INFO: csi-node-driver-66bp4 from calico-system started at 2023-06-13 02:05:20 +0000 UTC (2 container statuses recorded)
Jun 13 02:53:10.943: INFO: 	Container calico-csi ready: true, restart count 0
Jun 13 02:53:10.943: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun 13 02:53:10.943: INFO: kube-proxy-669th from kube-system started at 2023-06-13 02:03:17 +0000 UTC (1 container statuses recorded)
Jun 13 02:53:10.943: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 13 02:53:10.943: INFO: smtx-elf-csi-driver-node-plugin-dtt2k from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
Jun 13 02:53:10.943: INFO: 	Container csi-driver ready: true, restart count 0
Jun 13 02:53:10.943: INFO: 	Container driver-registrar ready: true, restart count 0
Jun 13 02:53:10.943: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 13 02:53:10.943: INFO: sonobuoy-e2e-job-246c2b6dcb314696 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 02:53:10.943: INFO: 	Container e2e ready: true, restart count 0
Jun 13 02:53:10.943: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 02:53:10.943: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-bvbp2 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 02:53:10.943: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 02:53:10.943: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/13/23 02:53:10.943
Jun 13 02:53:10.960: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4304" to be "running"
Jun 13 02:53:10.972: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.652326ms
Jun 13 02:53:12.980: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.019962932s
Jun 13 02:53:12.980: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/13/23 02:53:12.986
STEP: Trying to apply a random label on the found node. 06/13/23 02:53:13.023
STEP: verifying the node has the label kubernetes.io/e2e-88d69fea-421c-4c8c-bff7-8a3e8a683440 95 06/13/23 02:53:13.038
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 06/13/23 02:53:13.044
Jun 13 02:53:13.055: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4304" to be "not pending"
Jun 13 02:53:13.061: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.680375ms
Jun 13 02:53:15.068: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012821143s
Jun 13 02:53:17.069: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.013394405s
Jun 13 02:53:17.069: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.255.64.102 on the node which pod4 resides and expect not scheduled 06/13/23 02:53:17.069
Jun 13 02:53:17.083: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4304" to be "not pending"
Jun 13 02:53:17.088: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660122ms
Jun 13 02:53:19.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012879287s
Jun 13 02:53:21.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010737762s
Jun 13 02:53:23.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011971643s
Jun 13 02:53:25.104: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020778118s
Jun 13 02:53:27.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012085647s
Jun 13 02:53:29.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011331342s
Jun 13 02:53:31.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011899518s
Jun 13 02:53:33.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011828924s
Jun 13 02:53:35.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012315292s
Jun 13 02:53:37.103: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.019751729s
Jun 13 02:53:39.102: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.019005848s
Jun 13 02:53:41.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.011313321s
Jun 13 02:53:43.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011313712s
Jun 13 02:53:45.101: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01749275s
Jun 13 02:53:47.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012683404s
Jun 13 02:53:49.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.012688604s
Jun 13 02:53:51.102: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.01869696s
Jun 13 02:53:53.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011242253s
Jun 13 02:53:55.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.013558286s
Jun 13 02:53:57.099: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.015848053s
Jun 13 02:53:59.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010732779s
Jun 13 02:54:01.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.011813167s
Jun 13 02:54:03.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.0125845s
Jun 13 02:54:05.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.012270806s
Jun 13 02:54:07.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012666795s
Jun 13 02:54:09.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.013743396s
Jun 13 02:54:11.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.015348785s
Jun 13 02:54:13.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.045313683s
Jun 13 02:54:15.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012823925s
Jun 13 02:54:17.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011719653s
Jun 13 02:54:19.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.012460353s
Jun 13 02:54:21.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.012909381s
Jun 13 02:54:23.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.015021626s
Jun 13 02:54:25.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.013044185s
Jun 13 02:54:27.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012694672s
Jun 13 02:54:29.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01220182s
Jun 13 02:54:31.099: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.01580732s
Jun 13 02:54:33.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.012731354s
Jun 13 02:54:35.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.015311396s
Jun 13 02:54:37.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012027112s
Jun 13 02:54:39.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.013297787s
Jun 13 02:54:41.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011319087s
Jun 13 02:54:43.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012210127s
Jun 13 02:54:45.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.011447345s
Jun 13 02:54:47.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.011672556s
Jun 13 02:54:49.186: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.102480965s
Jun 13 02:54:51.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011294432s
Jun 13 02:54:53.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.013557424s
Jun 13 02:54:55.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011242019s
Jun 13 02:54:57.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.012841825s
Jun 13 02:54:59.106: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.022594365s
Jun 13 02:55:01.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010838692s
Jun 13 02:55:03.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.012372686s
Jun 13 02:55:05.099: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.015457158s
Jun 13 02:55:07.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.012755623s
Jun 13 02:55:09.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011742262s
Jun 13 02:55:11.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01186342s
Jun 13 02:55:13.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010795863s
Jun 13 02:55:15.100: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.017216292s
Jun 13 02:55:17.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014318162s
Jun 13 02:55:19.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.012244415s
Jun 13 02:55:21.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.011344784s
Jun 13 02:55:23.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.012766613s
Jun 13 02:55:25.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.013542546s
Jun 13 02:55:27.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.013915871s
Jun 13 02:55:29.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.011389525s
Jun 13 02:55:31.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.042045439s
Jun 13 02:55:33.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.011972678s
Jun 13 02:55:35.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.012327663s
Jun 13 02:55:37.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.012870783s
Jun 13 02:55:39.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.011963926s
Jun 13 02:55:41.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.01386232s
Jun 13 02:55:43.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.012375395s
Jun 13 02:55:45.106: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.022464956s
Jun 13 02:55:47.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.011547182s
Jun 13 02:55:49.106: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.022693494s
Jun 13 02:55:51.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.014599017s
Jun 13 02:55:53.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.011643993s
Jun 13 02:55:55.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.011813103s
Jun 13 02:55:57.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.012359679s
Jun 13 02:55:59.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.012215423s
Jun 13 02:56:01.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.011547722s
Jun 13 02:56:03.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.011635419s
Jun 13 02:56:05.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.012494157s
Jun 13 02:56:07.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.014830615s
Jun 13 02:56:09.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.013769185s
Jun 13 02:56:11.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.010781297s
Jun 13 02:56:13.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.011775257s
Jun 13 02:56:15.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.012909347s
Jun 13 02:56:17.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.011906866s
Jun 13 02:56:19.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.013785841s
Jun 13 02:56:21.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.012272693s
Jun 13 02:56:23.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.014583174s
Jun 13 02:56:25.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.011243318s
Jun 13 02:56:27.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.012679468s
Jun 13 02:56:29.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.012303294s
Jun 13 02:56:31.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.012752301s
Jun 13 02:56:33.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.012116069s
Jun 13 02:56:35.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.014448923s
Jun 13 02:56:37.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.013134227s
Jun 13 02:56:39.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.013159934s
Jun 13 02:56:41.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.028852477s
Jun 13 02:56:43.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.011163626s
Jun 13 02:56:45.099: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.015885836s
Jun 13 02:56:47.189: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.105683671s
Jun 13 02:56:49.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.013599822s
Jun 13 02:56:51.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.011766718s
Jun 13 02:56:53.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.014268751s
Jun 13 02:56:55.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.011039738s
Jun 13 02:56:57.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.012874432s
Jun 13 02:56:59.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.013578118s
Jun 13 02:57:01.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.014812849s
Jun 13 02:57:03.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.01215942s
Jun 13 02:57:05.102: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.018692256s
Jun 13 02:57:07.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.01245397s
Jun 13 02:57:09.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.013756186s
Jun 13 02:57:11.099: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.016341958s
Jun 13 02:57:13.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.012953809s
Jun 13 02:57:15.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.012681427s
Jun 13 02:57:17.099: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.015899252s
Jun 13 02:57:19.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.01152842s
Jun 13 02:57:21.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.012695467s
Jun 13 02:57:23.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.012925139s
Jun 13 02:57:25.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.013932285s
Jun 13 02:57:27.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.013824133s
Jun 13 02:57:29.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.011862887s
Jun 13 02:57:31.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.013689264s
Jun 13 02:57:33.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.012252368s
Jun 13 02:57:35.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.012801259s
Jun 13 02:57:37.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.012529979s
Jun 13 02:57:39.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.013006336s
Jun 13 02:57:41.101: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.018273691s
Jun 13 02:57:43.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.011737868s
Jun 13 02:57:45.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.013892389s
Jun 13 02:57:47.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.01284783s
Jun 13 02:57:49.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.015264398s
Jun 13 02:57:51.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.013289237s
Jun 13 02:57:53.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.015214135s
Jun 13 02:57:55.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.013049925s
Jun 13 02:57:57.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.013814616s
Jun 13 02:57:59.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.012549332s
Jun 13 02:58:01.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.013248176s
Jun 13 02:58:03.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.013459058s
Jun 13 02:58:05.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.011317677s
Jun 13 02:58:07.107: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.024256955s
Jun 13 02:58:09.100: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.016924499s
Jun 13 02:58:11.101: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.017782338s
Jun 13 02:58:13.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013461821s
Jun 13 02:58:15.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.015168538s
Jun 13 02:58:17.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012118753s
Jun 13 02:58:17.101: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017882355s
STEP: removing the label kubernetes.io/e2e-88d69fea-421c-4c8c-bff7-8a3e8a683440 off the node sks-test-v1-25-9-workergroup-469fm 06/13/23 02:58:17.101
STEP: verifying the node doesn't have the label kubernetes.io/e2e-88d69fea-421c-4c8c-bff7-8a3e8a683440 06/13/23 02:58:17.124
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jun 13 02:58:17.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4304" for this suite. 06/13/23 02:58:17.137
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":125,"skipped":2435,"failed":0}
------------------------------
• [SLOW TEST] [306.360 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:53:10.79
    Jun 13 02:53:10.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename sched-pred 06/13/23 02:53:10.792
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:53:10.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:53:10.833
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jun 13 02:53:10.839: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 13 02:53:10.856: INFO: Waiting for terminating namespaces to be deleted...
    Jun 13 02:53:10.866: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-2q6k2 before test
    Jun 13 02:53:10.886: INFO: calico-node-xb8kf from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
    Jun 13 02:53:10.886: INFO: 	Container calico-node ready: true, restart count 0
    Jun 13 02:53:10.886: INFO: calico-typha-79dfdd7d65-w95lk from calico-system started at 2023-06-13 02:04:34 +0000 UTC (1 container statuses recorded)
    Jun 13 02:53:10.886: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 13 02:53:10.886: INFO: csi-node-driver-vkf24 from calico-system started at 2023-06-13 02:05:19 +0000 UTC (2 container statuses recorded)
    Jun 13 02:53:10.886: INFO: 	Container calico-csi ready: true, restart count 0
    Jun 13 02:53:10.886: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun 13 02:53:10.886: INFO: test-webserver-67be0501-ec94-4e25-a28b-bacc17b5e0ad from container-probe-5554 started at 2023-06-13 02:52:10 +0000 UTC (1 container statuses recorded)
    Jun 13 02:53:10.886: INFO: 	Container test-webserver ready: false, restart count 0
    Jun 13 02:53:10.886: INFO: kube-proxy-g2qcx from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
    Jun 13 02:53:10.886: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 13 02:53:10.886: INFO: smtx-elf-csi-driver-node-plugin-gqbkk from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
    Jun 13 02:53:10.886: INFO: 	Container csi-driver ready: true, restart count 0
    Jun 13 02:53:10.886: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun 13 02:53:10.886: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 13 02:53:10.886: INFO: sonobuoy from sonobuoy started at 2023-06-13 02:14:11 +0000 UTC (1 container statuses recorded)
    Jun 13 02:53:10.886: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 13 02:53:10.886: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-f9dsl from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 02:53:10.886: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 02:53:10.886: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 13 02:53:10.886: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-469fm before test
    Jun 13 02:53:10.920: INFO: calico-node-lhmmz from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
    Jun 13 02:53:10.920: INFO: 	Container calico-node ready: true, restart count 0
    Jun 13 02:53:10.920: INFO: csi-node-driver-jrfzt from calico-system started at 2023-06-13 02:04:55 +0000 UTC (2 container statuses recorded)
    Jun 13 02:53:10.920: INFO: 	Container calico-csi ready: true, restart count 0
    Jun 13 02:53:10.920: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun 13 02:53:10.920: INFO: kube-proxy-fn2dq from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
    Jun 13 02:53:10.920: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 13 02:53:10.920: INFO: snapshot-controller-76c6888c5-9m5td from kube-system started at 2023-06-13 02:04:55 +0000 UTC (1 container statuses recorded)
    Jun 13 02:53:10.920: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun 13 02:53:10.920: INFO: smtx-elf-csi-driver-node-plugin-h8k7m from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
    Jun 13 02:53:10.920: INFO: 	Container csi-driver ready: true, restart count 0
    Jun 13 02:53:10.920: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun 13 02:53:10.920: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 13 02:53:10.920: INFO: tigera-operator-6f49bd984-r7pkz from sks-system started at 2023-06-13 02:04:29 +0000 UTC (1 container statuses recorded)
    Jun 13 02:53:10.920: INFO: 	Container tigera-operator ready: true, restart count 0
    Jun 13 02:53:10.920: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-hcvsr from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 02:53:10.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 02:53:10.920: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 13 02:53:10.920: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-l5gcd before test
    Jun 13 02:53:10.943: INFO: calico-node-fptww from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
    Jun 13 02:53:10.943: INFO: 	Container calico-node ready: true, restart count 0
    Jun 13 02:53:10.943: INFO: calico-typha-79dfdd7d65-nrkt5 from calico-system started at 2023-06-13 02:04:43 +0000 UTC (1 container statuses recorded)
    Jun 13 02:53:10.943: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 13 02:53:10.943: INFO: csi-node-driver-66bp4 from calico-system started at 2023-06-13 02:05:20 +0000 UTC (2 container statuses recorded)
    Jun 13 02:53:10.943: INFO: 	Container calico-csi ready: true, restart count 0
    Jun 13 02:53:10.943: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun 13 02:53:10.943: INFO: kube-proxy-669th from kube-system started at 2023-06-13 02:03:17 +0000 UTC (1 container statuses recorded)
    Jun 13 02:53:10.943: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 13 02:53:10.943: INFO: smtx-elf-csi-driver-node-plugin-dtt2k from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
    Jun 13 02:53:10.943: INFO: 	Container csi-driver ready: true, restart count 0
    Jun 13 02:53:10.943: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun 13 02:53:10.943: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 13 02:53:10.943: INFO: sonobuoy-e2e-job-246c2b6dcb314696 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 02:53:10.943: INFO: 	Container e2e ready: true, restart count 0
    Jun 13 02:53:10.943: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 02:53:10.943: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-bvbp2 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 02:53:10.943: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 02:53:10.943: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/13/23 02:53:10.943
    Jun 13 02:53:10.960: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4304" to be "running"
    Jun 13 02:53:10.972: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 11.652326ms
    Jun 13 02:53:12.980: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.019962932s
    Jun 13 02:53:12.980: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/13/23 02:53:12.986
    STEP: Trying to apply a random label on the found node. 06/13/23 02:53:13.023
    STEP: verifying the node has the label kubernetes.io/e2e-88d69fea-421c-4c8c-bff7-8a3e8a683440 95 06/13/23 02:53:13.038
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 06/13/23 02:53:13.044
    Jun 13 02:53:13.055: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4304" to be "not pending"
    Jun 13 02:53:13.061: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.680375ms
    Jun 13 02:53:15.068: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012821143s
    Jun 13 02:53:17.069: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.013394405s
    Jun 13 02:53:17.069: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.255.64.102 on the node which pod4 resides and expect not scheduled 06/13/23 02:53:17.069
    Jun 13 02:53:17.083: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4304" to be "not pending"
    Jun 13 02:53:17.088: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.660122ms
    Jun 13 02:53:19.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012879287s
    Jun 13 02:53:21.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010737762s
    Jun 13 02:53:23.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011971643s
    Jun 13 02:53:25.104: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020778118s
    Jun 13 02:53:27.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.012085647s
    Jun 13 02:53:29.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.011331342s
    Jun 13 02:53:31.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.011899518s
    Jun 13 02:53:33.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.011828924s
    Jun 13 02:53:35.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.012315292s
    Jun 13 02:53:37.103: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.019751729s
    Jun 13 02:53:39.102: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.019005848s
    Jun 13 02:53:41.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.011313321s
    Jun 13 02:53:43.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.011313712s
    Jun 13 02:53:45.101: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.01749275s
    Jun 13 02:53:47.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012683404s
    Jun 13 02:53:49.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.012688604s
    Jun 13 02:53:51.102: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.01869696s
    Jun 13 02:53:53.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.011242253s
    Jun 13 02:53:55.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.013558286s
    Jun 13 02:53:57.099: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.015848053s
    Jun 13 02:53:59.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.010732779s
    Jun 13 02:54:01.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.011813167s
    Jun 13 02:54:03.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.0125845s
    Jun 13 02:54:05.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.012270806s
    Jun 13 02:54:07.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.012666795s
    Jun 13 02:54:09.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.013743396s
    Jun 13 02:54:11.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.015348785s
    Jun 13 02:54:13.128: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.045313683s
    Jun 13 02:54:15.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.012823925s
    Jun 13 02:54:17.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011719653s
    Jun 13 02:54:19.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.012460353s
    Jun 13 02:54:21.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.012909381s
    Jun 13 02:54:23.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.015021626s
    Jun 13 02:54:25.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.013044185s
    Jun 13 02:54:27.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.012694672s
    Jun 13 02:54:29.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.01220182s
    Jun 13 02:54:31.099: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.01580732s
    Jun 13 02:54:33.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.012731354s
    Jun 13 02:54:35.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.015311396s
    Jun 13 02:54:37.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.012027112s
    Jun 13 02:54:39.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.013297787s
    Jun 13 02:54:41.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011319087s
    Jun 13 02:54:43.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.012210127s
    Jun 13 02:54:45.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.011447345s
    Jun 13 02:54:47.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.011672556s
    Jun 13 02:54:49.186: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.102480965s
    Jun 13 02:54:51.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.011294432s
    Jun 13 02:54:53.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.013557424s
    Jun 13 02:54:55.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.011242019s
    Jun 13 02:54:57.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.012841825s
    Jun 13 02:54:59.106: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.022594365s
    Jun 13 02:55:01.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010838692s
    Jun 13 02:55:03.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.012372686s
    Jun 13 02:55:05.099: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.015457158s
    Jun 13 02:55:07.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.012755623s
    Jun 13 02:55:09.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.011742262s
    Jun 13 02:55:11.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01186342s
    Jun 13 02:55:13.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010795863s
    Jun 13 02:55:15.100: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.017216292s
    Jun 13 02:55:17.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.014318162s
    Jun 13 02:55:19.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.012244415s
    Jun 13 02:55:21.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.011344784s
    Jun 13 02:55:23.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.012766613s
    Jun 13 02:55:25.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.013542546s
    Jun 13 02:55:27.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.013915871s
    Jun 13 02:55:29.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.011389525s
    Jun 13 02:55:31.125: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.042045439s
    Jun 13 02:55:33.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.011972678s
    Jun 13 02:55:35.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.012327663s
    Jun 13 02:55:37.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.012870783s
    Jun 13 02:55:39.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.011963926s
    Jun 13 02:55:41.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.01386232s
    Jun 13 02:55:43.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.012375395s
    Jun 13 02:55:45.106: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.022464956s
    Jun 13 02:55:47.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.011547182s
    Jun 13 02:55:49.106: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.022693494s
    Jun 13 02:55:51.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.014599017s
    Jun 13 02:55:53.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.011643993s
    Jun 13 02:55:55.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.011813103s
    Jun 13 02:55:57.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.012359679s
    Jun 13 02:55:59.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.012215423s
    Jun 13 02:56:01.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.011547722s
    Jun 13 02:56:03.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.011635419s
    Jun 13 02:56:05.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.012494157s
    Jun 13 02:56:07.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.014830615s
    Jun 13 02:56:09.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.013769185s
    Jun 13 02:56:11.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.010781297s
    Jun 13 02:56:13.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.011775257s
    Jun 13 02:56:15.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.012909347s
    Jun 13 02:56:17.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.011906866s
    Jun 13 02:56:19.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.013785841s
    Jun 13 02:56:21.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.012272693s
    Jun 13 02:56:23.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.014583174s
    Jun 13 02:56:25.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.011243318s
    Jun 13 02:56:27.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.012679468s
    Jun 13 02:56:29.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.012303294s
    Jun 13 02:56:31.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.012752301s
    Jun 13 02:56:33.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.012116069s
    Jun 13 02:56:35.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.014448923s
    Jun 13 02:56:37.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.013134227s
    Jun 13 02:56:39.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.013159934s
    Jun 13 02:56:41.112: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.028852477s
    Jun 13 02:56:43.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.011163626s
    Jun 13 02:56:45.099: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.015885836s
    Jun 13 02:56:47.189: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.105683671s
    Jun 13 02:56:49.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.013599822s
    Jun 13 02:56:51.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.011766718s
    Jun 13 02:56:53.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.014268751s
    Jun 13 02:56:55.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.011039738s
    Jun 13 02:56:57.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.012874432s
    Jun 13 02:56:59.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.013578118s
    Jun 13 02:57:01.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.014812849s
    Jun 13 02:57:03.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.01215942s
    Jun 13 02:57:05.102: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.018692256s
    Jun 13 02:57:07.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.01245397s
    Jun 13 02:57:09.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.013756186s
    Jun 13 02:57:11.099: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.016341958s
    Jun 13 02:57:13.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.012953809s
    Jun 13 02:57:15.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.012681427s
    Jun 13 02:57:17.099: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.015899252s
    Jun 13 02:57:19.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.01152842s
    Jun 13 02:57:21.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.012695467s
    Jun 13 02:57:23.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.012925139s
    Jun 13 02:57:25.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.013932285s
    Jun 13 02:57:27.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.013824133s
    Jun 13 02:57:29.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.011862887s
    Jun 13 02:57:31.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.013689264s
    Jun 13 02:57:33.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.012252368s
    Jun 13 02:57:35.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.012801259s
    Jun 13 02:57:37.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.012529979s
    Jun 13 02:57:39.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.013006336s
    Jun 13 02:57:41.101: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.018273691s
    Jun 13 02:57:43.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.011737868s
    Jun 13 02:57:45.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.013892389s
    Jun 13 02:57:47.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.01284783s
    Jun 13 02:57:49.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.015264398s
    Jun 13 02:57:51.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.013289237s
    Jun 13 02:57:53.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.015214135s
    Jun 13 02:57:55.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.013049925s
    Jun 13 02:57:57.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.013814616s
    Jun 13 02:57:59.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.012549332s
    Jun 13 02:58:01.096: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.013248176s
    Jun 13 02:58:03.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.013459058s
    Jun 13 02:58:05.094: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.011317677s
    Jun 13 02:58:07.107: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.024256955s
    Jun 13 02:58:09.100: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.016924499s
    Jun 13 02:58:11.101: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.017782338s
    Jun 13 02:58:13.097: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.013461821s
    Jun 13 02:58:15.098: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.015168538s
    Jun 13 02:58:17.095: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012118753s
    Jun 13 02:58:17.101: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.017882355s
    STEP: removing the label kubernetes.io/e2e-88d69fea-421c-4c8c-bff7-8a3e8a683440 off the node sks-test-v1-25-9-workergroup-469fm 06/13/23 02:58:17.101
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-88d69fea-421c-4c8c-bff7-8a3e8a683440 06/13/23 02:58:17.124
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 02:58:17.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4304" for this suite. 06/13/23 02:58:17.137
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:58:17.151
Jun 13 02:58:17.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename disruption 06/13/23 02:58:17.153
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:58:17.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:58:17.187
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:58:17.193
Jun 13 02:58:17.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename disruption-2 06/13/23 02:58:17.195
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:58:17.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:58:17.223
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 06/13/23 02:58:17.241
STEP: Waiting for the pdb to be processed 06/13/23 02:58:19.27
STEP: Waiting for the pdb to be processed 06/13/23 02:58:19.369
STEP: listing a collection of PDBs across all namespaces 06/13/23 02:58:19.438
STEP: listing a collection of PDBs in namespace disruption-568 06/13/23 02:58:19.446
STEP: deleting a collection of PDBs 06/13/23 02:58:19.454
STEP: Waiting for the PDB collection to be deleted 06/13/23 02:58:19.474
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jun 13 02:58:19.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-3123" for this suite. 06/13/23 02:58:19.486
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 13 02:58:19.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-568" for this suite. 06/13/23 02:58:19.509
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":126,"skipped":2443,"failed":0}
------------------------------
• [2.373 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:58:17.151
    Jun 13 02:58:17.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename disruption 06/13/23 02:58:17.153
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:58:17.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:58:17.187
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:58:17.193
    Jun 13 02:58:17.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename disruption-2 06/13/23 02:58:17.195
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:58:17.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:58:17.223
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 06/13/23 02:58:17.241
    STEP: Waiting for the pdb to be processed 06/13/23 02:58:19.27
    STEP: Waiting for the pdb to be processed 06/13/23 02:58:19.369
    STEP: listing a collection of PDBs across all namespaces 06/13/23 02:58:19.438
    STEP: listing a collection of PDBs in namespace disruption-568 06/13/23 02:58:19.446
    STEP: deleting a collection of PDBs 06/13/23 02:58:19.454
    STEP: Waiting for the PDB collection to be deleted 06/13/23 02:58:19.474
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jun 13 02:58:19.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-3123" for this suite. 06/13/23 02:58:19.486
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 13 02:58:19.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-568" for this suite. 06/13/23 02:58:19.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:58:19.524
Jun 13 02:58:19.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 02:58:19.526
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:58:19.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:58:19.553
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 02:58:19.581
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 02:58:19.905
STEP: Deploying the webhook pod 06/13/23 02:58:19.921
STEP: Wait for the deployment to be ready 06/13/23 02:58:19.956
Jun 13 02:58:19.972: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 06/13/23 02:58:21.998
STEP: Verifying the service has paired with the endpoint 06/13/23 02:58:22.055
Jun 13 02:58:23.056: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 06/13/23 02:58:23.063
STEP: create a namespace for the webhook 06/13/23 02:58:23.092
STEP: create a configmap should be unconditionally rejected by the webhook 06/13/23 02:58:23.104
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 02:58:23.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3025" for this suite. 06/13/23 02:58:23.169
STEP: Destroying namespace "webhook-3025-markers" for this suite. 06/13/23 02:58:23.201
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":127,"skipped":2456,"failed":0}
------------------------------
• [3.818 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:58:19.524
    Jun 13 02:58:19.525: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 02:58:19.526
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:58:19.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:58:19.553
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 02:58:19.581
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 02:58:19.905
    STEP: Deploying the webhook pod 06/13/23 02:58:19.921
    STEP: Wait for the deployment to be ready 06/13/23 02:58:19.956
    Jun 13 02:58:19.972: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 06/13/23 02:58:21.998
    STEP: Verifying the service has paired with the endpoint 06/13/23 02:58:22.055
    Jun 13 02:58:23.056: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 06/13/23 02:58:23.063
    STEP: create a namespace for the webhook 06/13/23 02:58:23.092
    STEP: create a configmap should be unconditionally rejected by the webhook 06/13/23 02:58:23.104
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 02:58:23.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3025" for this suite. 06/13/23 02:58:23.169
    STEP: Destroying namespace "webhook-3025-markers" for this suite. 06/13/23 02:58:23.201
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:58:23.343
Jun 13 02:58:23.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename gc 06/13/23 02:58:23.344
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:58:23.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:58:23.432
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 06/13/23 02:58:23.448
STEP: delete the rc 06/13/23 02:58:28.52
STEP: wait for the rc to be deleted 06/13/23 02:58:28.594
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 06/13/23 02:58:33.636
STEP: Gathering metrics 06/13/23 02:59:03.661
Jun 13 02:59:03.704: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" in namespace "kube-system" to be "running and ready"
Jun 13 02:59:03.713: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq": Phase="Running", Reason="", readiness=true. Elapsed: 9.417631ms
Jun 13 02:59:03.713: INFO: The phase of Pod kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq is Running (Ready = true)
Jun 13 02:59:03.713: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" satisfied condition "running and ready"
Jun 13 02:59:03.825: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 13 02:59:03.825: INFO: Deleting pod "simpletest.rc-22kt5" in namespace "gc-8933"
Jun 13 02:59:03.902: INFO: Deleting pod "simpletest.rc-2lhd4" in namespace "gc-8933"
Jun 13 02:59:03.937: INFO: Deleting pod "simpletest.rc-2ms8r" in namespace "gc-8933"
Jun 13 02:59:03.979: INFO: Deleting pod "simpletest.rc-2pwfb" in namespace "gc-8933"
Jun 13 02:59:04.030: INFO: Deleting pod "simpletest.rc-2vvj4" in namespace "gc-8933"
Jun 13 02:59:04.055: INFO: Deleting pod "simpletest.rc-4269q" in namespace "gc-8933"
Jun 13 02:59:04.105: INFO: Deleting pod "simpletest.rc-44jc4" in namespace "gc-8933"
Jun 13 02:59:04.129: INFO: Deleting pod "simpletest.rc-44vtd" in namespace "gc-8933"
Jun 13 02:59:04.151: INFO: Deleting pod "simpletest.rc-44wcx" in namespace "gc-8933"
Jun 13 02:59:04.186: INFO: Deleting pod "simpletest.rc-4qrg8" in namespace "gc-8933"
Jun 13 02:59:04.224: INFO: Deleting pod "simpletest.rc-4v52j" in namespace "gc-8933"
Jun 13 02:59:04.249: INFO: Deleting pod "simpletest.rc-56qns" in namespace "gc-8933"
Jun 13 02:59:04.280: INFO: Deleting pod "simpletest.rc-59sqr" in namespace "gc-8933"
Jun 13 02:59:04.308: INFO: Deleting pod "simpletest.rc-5lxlx" in namespace "gc-8933"
Jun 13 02:59:04.357: INFO: Deleting pod "simpletest.rc-5qjtd" in namespace "gc-8933"
Jun 13 02:59:04.394: INFO: Deleting pod "simpletest.rc-5r7sz" in namespace "gc-8933"
Jun 13 02:59:04.418: INFO: Deleting pod "simpletest.rc-658f5" in namespace "gc-8933"
Jun 13 02:59:04.438: INFO: Deleting pod "simpletest.rc-6ktl5" in namespace "gc-8933"
Jun 13 02:59:04.492: INFO: Deleting pod "simpletest.rc-6qv6d" in namespace "gc-8933"
Jun 13 02:59:04.528: INFO: Deleting pod "simpletest.rc-6r4t5" in namespace "gc-8933"
Jun 13 02:59:04.561: INFO: Deleting pod "simpletest.rc-7276k" in namespace "gc-8933"
Jun 13 02:59:04.660: INFO: Deleting pod "simpletest.rc-77fjx" in namespace "gc-8933"
Jun 13 02:59:04.719: INFO: Deleting pod "simpletest.rc-7mk9j" in namespace "gc-8933"
Jun 13 02:59:04.744: INFO: Deleting pod "simpletest.rc-8g7l8" in namespace "gc-8933"
Jun 13 02:59:04.773: INFO: Deleting pod "simpletest.rc-8zn2l" in namespace "gc-8933"
Jun 13 02:59:04.833: INFO: Deleting pod "simpletest.rc-97rx9" in namespace "gc-8933"
Jun 13 02:59:04.856: INFO: Deleting pod "simpletest.rc-9lgmr" in namespace "gc-8933"
Jun 13 02:59:04.873: INFO: Deleting pod "simpletest.rc-b5m6s" in namespace "gc-8933"
Jun 13 02:59:04.891: INFO: Deleting pod "simpletest.rc-bdm5m" in namespace "gc-8933"
Jun 13 02:59:04.931: INFO: Deleting pod "simpletest.rc-bhjcb" in namespace "gc-8933"
Jun 13 02:59:04.953: INFO: Deleting pod "simpletest.rc-ck4zw" in namespace "gc-8933"
Jun 13 02:59:04.982: INFO: Deleting pod "simpletest.rc-crsv7" in namespace "gc-8933"
Jun 13 02:59:05.044: INFO: Deleting pod "simpletest.rc-djrfp" in namespace "gc-8933"
Jun 13 02:59:05.065: INFO: Deleting pod "simpletest.rc-fdd7t" in namespace "gc-8933"
Jun 13 02:59:05.091: INFO: Deleting pod "simpletest.rc-fkjqv" in namespace "gc-8933"
Jun 13 02:59:05.114: INFO: Deleting pod "simpletest.rc-fwgdp" in namespace "gc-8933"
Jun 13 02:59:05.223: INFO: Deleting pod "simpletest.rc-fwtnj" in namespace "gc-8933"
Jun 13 02:59:05.262: INFO: Deleting pod "simpletest.rc-ghv2n" in namespace "gc-8933"
Jun 13 02:59:05.292: INFO: Deleting pod "simpletest.rc-gznrv" in namespace "gc-8933"
Jun 13 02:59:05.319: INFO: Deleting pod "simpletest.rc-hlnhp" in namespace "gc-8933"
Jun 13 02:59:05.373: INFO: Deleting pod "simpletest.rc-hqgf7" in namespace "gc-8933"
Jun 13 02:59:05.398: INFO: Deleting pod "simpletest.rc-ht972" in namespace "gc-8933"
Jun 13 02:59:05.456: INFO: Deleting pod "simpletest.rc-j7mht" in namespace "gc-8933"
Jun 13 02:59:05.477: INFO: Deleting pod "simpletest.rc-k2h6z" in namespace "gc-8933"
Jun 13 02:59:05.497: INFO: Deleting pod "simpletest.rc-k46wz" in namespace "gc-8933"
Jun 13 02:59:05.524: INFO: Deleting pod "simpletest.rc-k68jq" in namespace "gc-8933"
Jun 13 02:59:05.666: INFO: Deleting pod "simpletest.rc-kd6cg" in namespace "gc-8933"
Jun 13 02:59:05.706: INFO: Deleting pod "simpletest.rc-kjhcb" in namespace "gc-8933"
Jun 13 02:59:05.730: INFO: Deleting pod "simpletest.rc-kl7dv" in namespace "gc-8933"
Jun 13 02:59:05.756: INFO: Deleting pod "simpletest.rc-knd55" in namespace "gc-8933"
Jun 13 02:59:05.786: INFO: Deleting pod "simpletest.rc-knxfq" in namespace "gc-8933"
Jun 13 02:59:05.951: INFO: Deleting pod "simpletest.rc-lzcq4" in namespace "gc-8933"
Jun 13 02:59:05.970: INFO: Deleting pod "simpletest.rc-mclqf" in namespace "gc-8933"
Jun 13 02:59:06.118: INFO: Deleting pod "simpletest.rc-mjnkj" in namespace "gc-8933"
Jun 13 02:59:06.195: INFO: Deleting pod "simpletest.rc-mkhbs" in namespace "gc-8933"
Jun 13 02:59:06.229: INFO: Deleting pod "simpletest.rc-mn56c" in namespace "gc-8933"
Jun 13 02:59:06.249: INFO: Deleting pod "simpletest.rc-nbjwh" in namespace "gc-8933"
Jun 13 02:59:06.301: INFO: Deleting pod "simpletest.rc-ngd4h" in namespace "gc-8933"
Jun 13 02:59:06.334: INFO: Deleting pod "simpletest.rc-nm74n" in namespace "gc-8933"
Jun 13 02:59:06.358: INFO: Deleting pod "simpletest.rc-nwsqm" in namespace "gc-8933"
Jun 13 02:59:06.381: INFO: Deleting pod "simpletest.rc-p6cdt" in namespace "gc-8933"
Jun 13 02:59:06.511: INFO: Deleting pod "simpletest.rc-phspn" in namespace "gc-8933"
Jun 13 02:59:06.598: INFO: Deleting pod "simpletest.rc-psft4" in namespace "gc-8933"
Jun 13 02:59:06.856: INFO: Deleting pod "simpletest.rc-q5ffk" in namespace "gc-8933"
Jun 13 02:59:06.894: INFO: Deleting pod "simpletest.rc-qdjjm" in namespace "gc-8933"
Jun 13 02:59:07.209: INFO: Deleting pod "simpletest.rc-qlnd6" in namespace "gc-8933"
Jun 13 02:59:07.316: INFO: Deleting pod "simpletest.rc-qx8hv" in namespace "gc-8933"
Jun 13 02:59:07.344: INFO: Deleting pod "simpletest.rc-qxwwq" in namespace "gc-8933"
Jun 13 02:59:07.388: INFO: Deleting pod "simpletest.rc-qzsft" in namespace "gc-8933"
Jun 13 02:59:07.431: INFO: Deleting pod "simpletest.rc-r6sqb" in namespace "gc-8933"
Jun 13 02:59:07.453: INFO: Deleting pod "simpletest.rc-r9bhp" in namespace "gc-8933"
Jun 13 02:59:07.487: INFO: Deleting pod "simpletest.rc-rdqdn" in namespace "gc-8933"
Jun 13 02:59:07.553: INFO: Deleting pod "simpletest.rc-rp4bn" in namespace "gc-8933"
Jun 13 02:59:07.676: INFO: Deleting pod "simpletest.rc-rqs56" in namespace "gc-8933"
Jun 13 02:59:07.827: INFO: Deleting pod "simpletest.rc-rsvr6" in namespace "gc-8933"
Jun 13 02:59:07.909: INFO: Deleting pod "simpletest.rc-s9fm5" in namespace "gc-8933"
Jun 13 02:59:07.935: INFO: Deleting pod "simpletest.rc-sw5cx" in namespace "gc-8933"
Jun 13 02:59:08.049: INFO: Deleting pod "simpletest.rc-szl7s" in namespace "gc-8933"
Jun 13 02:59:08.236: INFO: Deleting pod "simpletest.rc-t2qs4" in namespace "gc-8933"
Jun 13 02:59:08.260: INFO: Deleting pod "simpletest.rc-tglv9" in namespace "gc-8933"
Jun 13 02:59:08.285: INFO: Deleting pod "simpletest.rc-tnv29" in namespace "gc-8933"
Jun 13 02:59:08.312: INFO: Deleting pod "simpletest.rc-twx7v" in namespace "gc-8933"
Jun 13 02:59:08.330: INFO: Deleting pod "simpletest.rc-vq6s4" in namespace "gc-8933"
Jun 13 02:59:08.378: INFO: Deleting pod "simpletest.rc-vw5nv" in namespace "gc-8933"
Jun 13 02:59:08.403: INFO: Deleting pod "simpletest.rc-w224b" in namespace "gc-8933"
Jun 13 02:59:08.476: INFO: Deleting pod "simpletest.rc-w7mmd" in namespace "gc-8933"
Jun 13 02:59:08.606: INFO: Deleting pod "simpletest.rc-w7vnc" in namespace "gc-8933"
Jun 13 02:59:08.704: INFO: Deleting pod "simpletest.rc-wcp8b" in namespace "gc-8933"
Jun 13 02:59:08.861: INFO: Deleting pod "simpletest.rc-wgwtc" in namespace "gc-8933"
Jun 13 02:59:08.894: INFO: Deleting pod "simpletest.rc-wm2xk" in namespace "gc-8933"
Jun 13 02:59:08.919: INFO: Deleting pod "simpletest.rc-wql8k" in namespace "gc-8933"
Jun 13 02:59:08.995: INFO: Deleting pod "simpletest.rc-wsfgr" in namespace "gc-8933"
Jun 13 02:59:09.028: INFO: Deleting pod "simpletest.rc-x8bbv" in namespace "gc-8933"
Jun 13 02:59:09.093: INFO: Deleting pod "simpletest.rc-xl4hl" in namespace "gc-8933"
Jun 13 02:59:09.127: INFO: Deleting pod "simpletest.rc-xlvsq" in namespace "gc-8933"
Jun 13 02:59:09.147: INFO: Deleting pod "simpletest.rc-xvhsj" in namespace "gc-8933"
Jun 13 02:59:09.174: INFO: Deleting pod "simpletest.rc-xxcb8" in namespace "gc-8933"
Jun 13 02:59:09.216: INFO: Deleting pod "simpletest.rc-znzhl" in namespace "gc-8933"
Jun 13 02:59:09.253: INFO: Deleting pod "simpletest.rc-zqnfv" in namespace "gc-8933"
Jun 13 02:59:09.286: INFO: Deleting pod "simpletest.rc-ztc89" in namespace "gc-8933"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 13 02:59:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8933" for this suite. 06/13/23 02:59:09.844
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":128,"skipped":2485,"failed":0}
------------------------------
• [SLOW TEST] [46.614 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:58:23.343
    Jun 13 02:58:23.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename gc 06/13/23 02:58:23.344
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:58:23.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:58:23.432
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 06/13/23 02:58:23.448
    STEP: delete the rc 06/13/23 02:58:28.52
    STEP: wait for the rc to be deleted 06/13/23 02:58:28.594
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 06/13/23 02:58:33.636
    STEP: Gathering metrics 06/13/23 02:59:03.661
    Jun 13 02:59:03.704: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" in namespace "kube-system" to be "running and ready"
    Jun 13 02:59:03.713: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq": Phase="Running", Reason="", readiness=true. Elapsed: 9.417631ms
    Jun 13 02:59:03.713: INFO: The phase of Pod kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq is Running (Ready = true)
    Jun 13 02:59:03.713: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" satisfied condition "running and ready"
    Jun 13 02:59:03.825: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jun 13 02:59:03.825: INFO: Deleting pod "simpletest.rc-22kt5" in namespace "gc-8933"
    Jun 13 02:59:03.902: INFO: Deleting pod "simpletest.rc-2lhd4" in namespace "gc-8933"
    Jun 13 02:59:03.937: INFO: Deleting pod "simpletest.rc-2ms8r" in namespace "gc-8933"
    Jun 13 02:59:03.979: INFO: Deleting pod "simpletest.rc-2pwfb" in namespace "gc-8933"
    Jun 13 02:59:04.030: INFO: Deleting pod "simpletest.rc-2vvj4" in namespace "gc-8933"
    Jun 13 02:59:04.055: INFO: Deleting pod "simpletest.rc-4269q" in namespace "gc-8933"
    Jun 13 02:59:04.105: INFO: Deleting pod "simpletest.rc-44jc4" in namespace "gc-8933"
    Jun 13 02:59:04.129: INFO: Deleting pod "simpletest.rc-44vtd" in namespace "gc-8933"
    Jun 13 02:59:04.151: INFO: Deleting pod "simpletest.rc-44wcx" in namespace "gc-8933"
    Jun 13 02:59:04.186: INFO: Deleting pod "simpletest.rc-4qrg8" in namespace "gc-8933"
    Jun 13 02:59:04.224: INFO: Deleting pod "simpletest.rc-4v52j" in namespace "gc-8933"
    Jun 13 02:59:04.249: INFO: Deleting pod "simpletest.rc-56qns" in namespace "gc-8933"
    Jun 13 02:59:04.280: INFO: Deleting pod "simpletest.rc-59sqr" in namespace "gc-8933"
    Jun 13 02:59:04.308: INFO: Deleting pod "simpletest.rc-5lxlx" in namespace "gc-8933"
    Jun 13 02:59:04.357: INFO: Deleting pod "simpletest.rc-5qjtd" in namespace "gc-8933"
    Jun 13 02:59:04.394: INFO: Deleting pod "simpletest.rc-5r7sz" in namespace "gc-8933"
    Jun 13 02:59:04.418: INFO: Deleting pod "simpletest.rc-658f5" in namespace "gc-8933"
    Jun 13 02:59:04.438: INFO: Deleting pod "simpletest.rc-6ktl5" in namespace "gc-8933"
    Jun 13 02:59:04.492: INFO: Deleting pod "simpletest.rc-6qv6d" in namespace "gc-8933"
    Jun 13 02:59:04.528: INFO: Deleting pod "simpletest.rc-6r4t5" in namespace "gc-8933"
    Jun 13 02:59:04.561: INFO: Deleting pod "simpletest.rc-7276k" in namespace "gc-8933"
    Jun 13 02:59:04.660: INFO: Deleting pod "simpletest.rc-77fjx" in namespace "gc-8933"
    Jun 13 02:59:04.719: INFO: Deleting pod "simpletest.rc-7mk9j" in namespace "gc-8933"
    Jun 13 02:59:04.744: INFO: Deleting pod "simpletest.rc-8g7l8" in namespace "gc-8933"
    Jun 13 02:59:04.773: INFO: Deleting pod "simpletest.rc-8zn2l" in namespace "gc-8933"
    Jun 13 02:59:04.833: INFO: Deleting pod "simpletest.rc-97rx9" in namespace "gc-8933"
    Jun 13 02:59:04.856: INFO: Deleting pod "simpletest.rc-9lgmr" in namespace "gc-8933"
    Jun 13 02:59:04.873: INFO: Deleting pod "simpletest.rc-b5m6s" in namespace "gc-8933"
    Jun 13 02:59:04.891: INFO: Deleting pod "simpletest.rc-bdm5m" in namespace "gc-8933"
    Jun 13 02:59:04.931: INFO: Deleting pod "simpletest.rc-bhjcb" in namespace "gc-8933"
    Jun 13 02:59:04.953: INFO: Deleting pod "simpletest.rc-ck4zw" in namespace "gc-8933"
    Jun 13 02:59:04.982: INFO: Deleting pod "simpletest.rc-crsv7" in namespace "gc-8933"
    Jun 13 02:59:05.044: INFO: Deleting pod "simpletest.rc-djrfp" in namespace "gc-8933"
    Jun 13 02:59:05.065: INFO: Deleting pod "simpletest.rc-fdd7t" in namespace "gc-8933"
    Jun 13 02:59:05.091: INFO: Deleting pod "simpletest.rc-fkjqv" in namespace "gc-8933"
    Jun 13 02:59:05.114: INFO: Deleting pod "simpletest.rc-fwgdp" in namespace "gc-8933"
    Jun 13 02:59:05.223: INFO: Deleting pod "simpletest.rc-fwtnj" in namespace "gc-8933"
    Jun 13 02:59:05.262: INFO: Deleting pod "simpletest.rc-ghv2n" in namespace "gc-8933"
    Jun 13 02:59:05.292: INFO: Deleting pod "simpletest.rc-gznrv" in namespace "gc-8933"
    Jun 13 02:59:05.319: INFO: Deleting pod "simpletest.rc-hlnhp" in namespace "gc-8933"
    Jun 13 02:59:05.373: INFO: Deleting pod "simpletest.rc-hqgf7" in namespace "gc-8933"
    Jun 13 02:59:05.398: INFO: Deleting pod "simpletest.rc-ht972" in namespace "gc-8933"
    Jun 13 02:59:05.456: INFO: Deleting pod "simpletest.rc-j7mht" in namespace "gc-8933"
    Jun 13 02:59:05.477: INFO: Deleting pod "simpletest.rc-k2h6z" in namespace "gc-8933"
    Jun 13 02:59:05.497: INFO: Deleting pod "simpletest.rc-k46wz" in namespace "gc-8933"
    Jun 13 02:59:05.524: INFO: Deleting pod "simpletest.rc-k68jq" in namespace "gc-8933"
    Jun 13 02:59:05.666: INFO: Deleting pod "simpletest.rc-kd6cg" in namespace "gc-8933"
    Jun 13 02:59:05.706: INFO: Deleting pod "simpletest.rc-kjhcb" in namespace "gc-8933"
    Jun 13 02:59:05.730: INFO: Deleting pod "simpletest.rc-kl7dv" in namespace "gc-8933"
    Jun 13 02:59:05.756: INFO: Deleting pod "simpletest.rc-knd55" in namespace "gc-8933"
    Jun 13 02:59:05.786: INFO: Deleting pod "simpletest.rc-knxfq" in namespace "gc-8933"
    Jun 13 02:59:05.951: INFO: Deleting pod "simpletest.rc-lzcq4" in namespace "gc-8933"
    Jun 13 02:59:05.970: INFO: Deleting pod "simpletest.rc-mclqf" in namespace "gc-8933"
    Jun 13 02:59:06.118: INFO: Deleting pod "simpletest.rc-mjnkj" in namespace "gc-8933"
    Jun 13 02:59:06.195: INFO: Deleting pod "simpletest.rc-mkhbs" in namespace "gc-8933"
    Jun 13 02:59:06.229: INFO: Deleting pod "simpletest.rc-mn56c" in namespace "gc-8933"
    Jun 13 02:59:06.249: INFO: Deleting pod "simpletest.rc-nbjwh" in namespace "gc-8933"
    Jun 13 02:59:06.301: INFO: Deleting pod "simpletest.rc-ngd4h" in namespace "gc-8933"
    Jun 13 02:59:06.334: INFO: Deleting pod "simpletest.rc-nm74n" in namespace "gc-8933"
    Jun 13 02:59:06.358: INFO: Deleting pod "simpletest.rc-nwsqm" in namespace "gc-8933"
    Jun 13 02:59:06.381: INFO: Deleting pod "simpletest.rc-p6cdt" in namespace "gc-8933"
    Jun 13 02:59:06.511: INFO: Deleting pod "simpletest.rc-phspn" in namespace "gc-8933"
    Jun 13 02:59:06.598: INFO: Deleting pod "simpletest.rc-psft4" in namespace "gc-8933"
    Jun 13 02:59:06.856: INFO: Deleting pod "simpletest.rc-q5ffk" in namespace "gc-8933"
    Jun 13 02:59:06.894: INFO: Deleting pod "simpletest.rc-qdjjm" in namespace "gc-8933"
    Jun 13 02:59:07.209: INFO: Deleting pod "simpletest.rc-qlnd6" in namespace "gc-8933"
    Jun 13 02:59:07.316: INFO: Deleting pod "simpletest.rc-qx8hv" in namespace "gc-8933"
    Jun 13 02:59:07.344: INFO: Deleting pod "simpletest.rc-qxwwq" in namespace "gc-8933"
    Jun 13 02:59:07.388: INFO: Deleting pod "simpletest.rc-qzsft" in namespace "gc-8933"
    Jun 13 02:59:07.431: INFO: Deleting pod "simpletest.rc-r6sqb" in namespace "gc-8933"
    Jun 13 02:59:07.453: INFO: Deleting pod "simpletest.rc-r9bhp" in namespace "gc-8933"
    Jun 13 02:59:07.487: INFO: Deleting pod "simpletest.rc-rdqdn" in namespace "gc-8933"
    Jun 13 02:59:07.553: INFO: Deleting pod "simpletest.rc-rp4bn" in namespace "gc-8933"
    Jun 13 02:59:07.676: INFO: Deleting pod "simpletest.rc-rqs56" in namespace "gc-8933"
    Jun 13 02:59:07.827: INFO: Deleting pod "simpletest.rc-rsvr6" in namespace "gc-8933"
    Jun 13 02:59:07.909: INFO: Deleting pod "simpletest.rc-s9fm5" in namespace "gc-8933"
    Jun 13 02:59:07.935: INFO: Deleting pod "simpletest.rc-sw5cx" in namespace "gc-8933"
    Jun 13 02:59:08.049: INFO: Deleting pod "simpletest.rc-szl7s" in namespace "gc-8933"
    Jun 13 02:59:08.236: INFO: Deleting pod "simpletest.rc-t2qs4" in namespace "gc-8933"
    Jun 13 02:59:08.260: INFO: Deleting pod "simpletest.rc-tglv9" in namespace "gc-8933"
    Jun 13 02:59:08.285: INFO: Deleting pod "simpletest.rc-tnv29" in namespace "gc-8933"
    Jun 13 02:59:08.312: INFO: Deleting pod "simpletest.rc-twx7v" in namespace "gc-8933"
    Jun 13 02:59:08.330: INFO: Deleting pod "simpletest.rc-vq6s4" in namespace "gc-8933"
    Jun 13 02:59:08.378: INFO: Deleting pod "simpletest.rc-vw5nv" in namespace "gc-8933"
    Jun 13 02:59:08.403: INFO: Deleting pod "simpletest.rc-w224b" in namespace "gc-8933"
    Jun 13 02:59:08.476: INFO: Deleting pod "simpletest.rc-w7mmd" in namespace "gc-8933"
    Jun 13 02:59:08.606: INFO: Deleting pod "simpletest.rc-w7vnc" in namespace "gc-8933"
    Jun 13 02:59:08.704: INFO: Deleting pod "simpletest.rc-wcp8b" in namespace "gc-8933"
    Jun 13 02:59:08.861: INFO: Deleting pod "simpletest.rc-wgwtc" in namespace "gc-8933"
    Jun 13 02:59:08.894: INFO: Deleting pod "simpletest.rc-wm2xk" in namespace "gc-8933"
    Jun 13 02:59:08.919: INFO: Deleting pod "simpletest.rc-wql8k" in namespace "gc-8933"
    Jun 13 02:59:08.995: INFO: Deleting pod "simpletest.rc-wsfgr" in namespace "gc-8933"
    Jun 13 02:59:09.028: INFO: Deleting pod "simpletest.rc-x8bbv" in namespace "gc-8933"
    Jun 13 02:59:09.093: INFO: Deleting pod "simpletest.rc-xl4hl" in namespace "gc-8933"
    Jun 13 02:59:09.127: INFO: Deleting pod "simpletest.rc-xlvsq" in namespace "gc-8933"
    Jun 13 02:59:09.147: INFO: Deleting pod "simpletest.rc-xvhsj" in namespace "gc-8933"
    Jun 13 02:59:09.174: INFO: Deleting pod "simpletest.rc-xxcb8" in namespace "gc-8933"
    Jun 13 02:59:09.216: INFO: Deleting pod "simpletest.rc-znzhl" in namespace "gc-8933"
    Jun 13 02:59:09.253: INFO: Deleting pod "simpletest.rc-zqnfv" in namespace "gc-8933"
    Jun 13 02:59:09.286: INFO: Deleting pod "simpletest.rc-ztc89" in namespace "gc-8933"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 13 02:59:09.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8933" for this suite. 06/13/23 02:59:09.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:59:09.962
Jun 13 02:59:09.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename resourcequota 06/13/23 02:59:09.967
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:59:10.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:59:10.098
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 06/13/23 02:59:10.115
STEP: Ensuring ResourceQuota status is calculated 06/13/23 02:59:10.169
STEP: Creating a ResourceQuota with not terminating scope 06/13/23 02:59:12.18
STEP: Ensuring ResourceQuota status is calculated 06/13/23 02:59:12.192
STEP: Creating a long running pod 06/13/23 02:59:14.202
STEP: Ensuring resource quota with not terminating scope captures the pod usage 06/13/23 02:59:14.234
STEP: Ensuring resource quota with terminating scope ignored the pod usage 06/13/23 02:59:16.242
STEP: Deleting the pod 06/13/23 02:59:18.247
STEP: Ensuring resource quota status released the pod usage 06/13/23 02:59:18.261
STEP: Creating a terminating pod 06/13/23 02:59:20.31
STEP: Ensuring resource quota with terminating scope captures the pod usage 06/13/23 02:59:20.334
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 06/13/23 02:59:22.339
STEP: Deleting the pod 06/13/23 02:59:24.345
STEP: Ensuring resource quota status released the pod usage 06/13/23 02:59:24.359
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 13 02:59:26.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9290" for this suite. 06/13/23 02:59:26.375
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":129,"skipped":2522,"failed":0}
------------------------------
• [SLOW TEST] [16.425 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:59:09.962
    Jun 13 02:59:09.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename resourcequota 06/13/23 02:59:09.967
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:59:10.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:59:10.098
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 06/13/23 02:59:10.115
    STEP: Ensuring ResourceQuota status is calculated 06/13/23 02:59:10.169
    STEP: Creating a ResourceQuota with not terminating scope 06/13/23 02:59:12.18
    STEP: Ensuring ResourceQuota status is calculated 06/13/23 02:59:12.192
    STEP: Creating a long running pod 06/13/23 02:59:14.202
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 06/13/23 02:59:14.234
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 06/13/23 02:59:16.242
    STEP: Deleting the pod 06/13/23 02:59:18.247
    STEP: Ensuring resource quota status released the pod usage 06/13/23 02:59:18.261
    STEP: Creating a terminating pod 06/13/23 02:59:20.31
    STEP: Ensuring resource quota with terminating scope captures the pod usage 06/13/23 02:59:20.334
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 06/13/23 02:59:22.339
    STEP: Deleting the pod 06/13/23 02:59:24.345
    STEP: Ensuring resource quota status released the pod usage 06/13/23 02:59:24.359
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 13 02:59:26.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9290" for this suite. 06/13/23 02:59:26.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 02:59:26.388
Jun 13 02:59:26.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-probe 06/13/23 02:59:26.39
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:59:26.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:59:26.424
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca in namespace container-probe-1640 06/13/23 02:59:26.43
Jun 13 02:59:26.448: INFO: Waiting up to 5m0s for pod "liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca" in namespace "container-probe-1640" to be "not pending"
Jun 13 02:59:26.458: INFO: Pod "liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca": Phase="Pending", Reason="", readiness=false. Elapsed: 9.568894ms
Jun 13 02:59:28.467: INFO: Pod "liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca": Phase="Running", Reason="", readiness=true. Elapsed: 2.018812687s
Jun 13 02:59:28.467: INFO: Pod "liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca" satisfied condition "not pending"
Jun 13 02:59:28.467: INFO: Started pod liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca in namespace container-probe-1640
STEP: checking the pod's current state and verifying that restartCount is present 06/13/23 02:59:28.467
Jun 13 02:59:28.474: INFO: Initial restart count of pod liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca is 0
STEP: deleting the pod 06/13/23 03:03:28.623
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 13 03:03:28.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1640" for this suite. 06/13/23 03:03:28.666
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":130,"skipped":2563,"failed":0}
------------------------------
• [SLOW TEST] [242.312 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 02:59:26.388
    Jun 13 02:59:26.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-probe 06/13/23 02:59:26.39
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 02:59:26.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 02:59:26.424
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca in namespace container-probe-1640 06/13/23 02:59:26.43
    Jun 13 02:59:26.448: INFO: Waiting up to 5m0s for pod "liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca" in namespace "container-probe-1640" to be "not pending"
    Jun 13 02:59:26.458: INFO: Pod "liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca": Phase="Pending", Reason="", readiness=false. Elapsed: 9.568894ms
    Jun 13 02:59:28.467: INFO: Pod "liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca": Phase="Running", Reason="", readiness=true. Elapsed: 2.018812687s
    Jun 13 02:59:28.467: INFO: Pod "liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca" satisfied condition "not pending"
    Jun 13 02:59:28.467: INFO: Started pod liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca in namespace container-probe-1640
    STEP: checking the pod's current state and verifying that restartCount is present 06/13/23 02:59:28.467
    Jun 13 02:59:28.474: INFO: Initial restart count of pod liveness-22153894-b0ee-4f22-a0b3-1a3a7360fbca is 0
    STEP: deleting the pod 06/13/23 03:03:28.623
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 13 03:03:28.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1640" for this suite. 06/13/23 03:03:28.666
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:03:28.703
Jun 13 03:03:28.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:03:28.705
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:28.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:28.771
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 06/13/23 03:03:28.778
Jun 13 03:03:28.802: INFO: Waiting up to 5m0s for pod "annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d" in namespace "projected-4089" to be "running and ready"
Jun 13 03:03:28.809: INFO: Pod "annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.865378ms
Jun 13 03:03:28.809: INFO: The phase of Pod annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:03:30.821: INFO: Pod "annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018978571s
Jun 13 03:03:30.821: INFO: The phase of Pod annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:03:32.816: INFO: Pod "annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d": Phase="Running", Reason="", readiness=true. Elapsed: 4.013753766s
Jun 13 03:03:32.816: INFO: The phase of Pod annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d is Running (Ready = true)
Jun 13 03:03:32.816: INFO: Pod "annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d" satisfied condition "running and ready"
Jun 13 03:03:33.365: INFO: Successfully updated pod "annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 13 03:03:35.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4089" for this suite. 06/13/23 03:03:35.415
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":131,"skipped":2622,"failed":0}
------------------------------
• [SLOW TEST] [6.737 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:03:28.703
    Jun 13 03:03:28.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:03:28.705
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:28.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:28.771
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 06/13/23 03:03:28.778
    Jun 13 03:03:28.802: INFO: Waiting up to 5m0s for pod "annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d" in namespace "projected-4089" to be "running and ready"
    Jun 13 03:03:28.809: INFO: Pod "annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.865378ms
    Jun 13 03:03:28.809: INFO: The phase of Pod annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:03:30.821: INFO: Pod "annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018978571s
    Jun 13 03:03:30.821: INFO: The phase of Pod annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:03:32.816: INFO: Pod "annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d": Phase="Running", Reason="", readiness=true. Elapsed: 4.013753766s
    Jun 13 03:03:32.816: INFO: The phase of Pod annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d is Running (Ready = true)
    Jun 13 03:03:32.816: INFO: Pod "annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d" satisfied condition "running and ready"
    Jun 13 03:03:33.365: INFO: Successfully updated pod "annotationupdate9b083730-565b-4aa1-bd98-84a771fd118d"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 13 03:03:35.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4089" for this suite. 06/13/23 03:03:35.415
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:03:35.441
Jun 13 03:03:35.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename resourcequota 06/13/23 03:03:35.442
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:35.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:35.47
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 06/13/23 03:03:35.478
STEP: Getting a ResourceQuota 06/13/23 03:03:35.489
STEP: Updating a ResourceQuota 06/13/23 03:03:35.5
STEP: Verifying a ResourceQuota was modified 06/13/23 03:03:35.509
STEP: Deleting a ResourceQuota 06/13/23 03:03:35.521
STEP: Verifying the deleted ResourceQuota 06/13/23 03:03:35.536
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 13 03:03:35.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2086" for this suite. 06/13/23 03:03:35.564
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":132,"skipped":2637,"failed":0}
------------------------------
• [0.140 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:03:35.441
    Jun 13 03:03:35.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename resourcequota 06/13/23 03:03:35.442
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:35.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:35.47
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 06/13/23 03:03:35.478
    STEP: Getting a ResourceQuota 06/13/23 03:03:35.489
    STEP: Updating a ResourceQuota 06/13/23 03:03:35.5
    STEP: Verifying a ResourceQuota was modified 06/13/23 03:03:35.509
    STEP: Deleting a ResourceQuota 06/13/23 03:03:35.521
    STEP: Verifying the deleted ResourceQuota 06/13/23 03:03:35.536
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 13 03:03:35.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-2086" for this suite. 06/13/23 03:03:35.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:03:35.583
Jun 13 03:03:35.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 03:03:35.584
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:35.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:35.627
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 03:03:35.695
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:03:36.158
STEP: Deploying the webhook pod 06/13/23 03:03:36.173
STEP: Wait for the deployment to be ready 06/13/23 03:03:36.195
Jun 13 03:03:36.221: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/13/23 03:03:38.24
STEP: Verifying the service has paired with the endpoint 06/13/23 03:03:38.307
Jun 13 03:03:39.308: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 06/13/23 03:03:39.315
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 06/13/23 03:03:39.318
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 06/13/23 03:03:39.318
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 06/13/23 03:03:39.318
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 06/13/23 03:03:39.321
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 06/13/23 03:03:39.321
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 06/13/23 03:03:39.323
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:03:39.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2342" for this suite. 06/13/23 03:03:39.332
STEP: Destroying namespace "webhook-2342-markers" for this suite. 06/13/23 03:03:39.346
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":133,"skipped":2691,"failed":0}
------------------------------
• [3.868 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:03:35.583
    Jun 13 03:03:35.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 03:03:35.584
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:35.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:35.627
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 03:03:35.695
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:03:36.158
    STEP: Deploying the webhook pod 06/13/23 03:03:36.173
    STEP: Wait for the deployment to be ready 06/13/23 03:03:36.195
    Jun 13 03:03:36.221: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/13/23 03:03:38.24
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:03:38.307
    Jun 13 03:03:39.308: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 06/13/23 03:03:39.315
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 06/13/23 03:03:39.318
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 06/13/23 03:03:39.318
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 06/13/23 03:03:39.318
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 06/13/23 03:03:39.321
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 06/13/23 03:03:39.321
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 06/13/23 03:03:39.323
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:03:39.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2342" for this suite. 06/13/23 03:03:39.332
    STEP: Destroying namespace "webhook-2342-markers" for this suite. 06/13/23 03:03:39.346
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:03:39.453
Jun 13 03:03:39.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename deployment 06/13/23 03:03:39.454
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:39.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:39.499
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jun 13 03:03:39.506: INFO: Creating deployment "test-recreate-deployment"
Jun 13 03:03:39.515: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 13 03:03:39.529: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 13 03:03:41.544: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 13 03:03:41.552: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 13 03:03:41.570: INFO: Updating deployment test-recreate-deployment
Jun 13 03:03:41.570: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 13 03:03:41.685: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5787  3ce96c9d-6a79-450e-8cc0-e0e4a86395f3 27439 2 2023-06-13 03:03:39 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001632958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-13 03:03:41 +0000 UTC,LastTransitionTime:2023-06-13 03:03:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-06-13 03:03:41 +0000 UTC,LastTransitionTime:2023-06-13 03:03:39 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 13 03:03:41.692: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-5787  426c6452-d2b6-471c-9f01-3e285f7706bd 27437 1 2023-06-13 03:03:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3ce96c9d-6a79-450e-8cc0-e0e4a86395f3 0xc001632e40 0xc001632e41}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ce96c9d-6a79-450e-8cc0-e0e4a86395f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001632ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 13 03:03:41.692: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 13 03:03:41.692: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-5787  edb21fb6-31c4-4bc2-929d-29dfdc85110c 27428 2 2023-06-13 03:03:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3ce96c9d-6a79-450e-8cc0-e0e4a86395f3 0xc001632d27 0xc001632d28}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ce96c9d-6a79-450e-8cc0-e0e4a86395f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001632dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 13 03:03:41.701: INFO: Pod "test-recreate-deployment-9d58999df-xpq98" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-xpq98 test-recreate-deployment-9d58999df- deployment-5787  6b9ef6e3-2481-45d7-854b-36311a4cef59 27440 0 2023-06-13 03:03:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 426c6452-d2b6-471c-9f01-3e285f7706bd 0xc002cb0b60 0xc002cb0b61}] [] [{kube-controller-manager Update v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"426c6452-d2b6-471c-9f01-3e285f7706bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j8mnv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j8mnv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:03:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:03:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:03:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:03:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:03:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 13 03:03:41.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5787" for this suite. 06/13/23 03:03:41.711
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":134,"skipped":2714,"failed":0}
------------------------------
• [2.277 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:03:39.453
    Jun 13 03:03:39.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename deployment 06/13/23 03:03:39.454
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:39.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:39.499
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jun 13 03:03:39.506: INFO: Creating deployment "test-recreate-deployment"
    Jun 13 03:03:39.515: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jun 13 03:03:39.529: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jun 13 03:03:41.544: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jun 13 03:03:41.552: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jun 13 03:03:41.570: INFO: Updating deployment test-recreate-deployment
    Jun 13 03:03:41.570: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 13 03:03:41.685: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-5787  3ce96c9d-6a79-450e-8cc0-e0e4a86395f3 27439 2 2023-06-13 03:03:39 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001632958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-13 03:03:41 +0000 UTC,LastTransitionTime:2023-06-13 03:03:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-06-13 03:03:41 +0000 UTC,LastTransitionTime:2023-06-13 03:03:39 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jun 13 03:03:41.692: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-5787  426c6452-d2b6-471c-9f01-3e285f7706bd 27437 1 2023-06-13 03:03:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 3ce96c9d-6a79-450e-8cc0-e0e4a86395f3 0xc001632e40 0xc001632e41}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ce96c9d-6a79-450e-8cc0-e0e4a86395f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001632ed8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 03:03:41.692: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jun 13 03:03:41.692: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-5787  edb21fb6-31c4-4bc2-929d-29dfdc85110c 27428 2 2023-06-13 03:03:39 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 3ce96c9d-6a79-450e-8cc0-e0e4a86395f3 0xc001632d27 0xc001632d28}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3ce96c9d-6a79-450e-8cc0-e0e4a86395f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001632dd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 03:03:41.701: INFO: Pod "test-recreate-deployment-9d58999df-xpq98" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-xpq98 test-recreate-deployment-9d58999df- deployment-5787  6b9ef6e3-2481-45d7-854b-36311a4cef59 27440 0 2023-06-13 03:03:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 426c6452-d2b6-471c-9f01-3e285f7706bd 0xc002cb0b60 0xc002cb0b61}] [] [{kube-controller-manager Update v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"426c6452-d2b6-471c-9f01-3e285f7706bd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:03:41 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j8mnv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j8mnv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:03:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:03:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:03:41 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:03:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:03:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 13 03:03:41.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5787" for this suite. 06/13/23 03:03:41.711
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:03:41.731
Jun 13 03:03:41.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:03:41.733
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:41.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:41.758
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:03:41.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8604" for this suite. 06/13/23 03:03:41.79
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":135,"skipped":2716,"failed":0}
------------------------------
• [0.072 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:03:41.731
    Jun 13 03:03:41.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:03:41.733
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:41.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:41.758
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:03:41.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8604" for this suite. 06/13/23 03:03:41.79
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:03:41.804
Jun 13 03:03:41.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename events 06/13/23 03:03:41.805
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:41.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:41.839
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 06/13/23 03:03:41.845
Jun 13 03:03:41.855: INFO: created test-event-1
Jun 13 03:03:41.864: INFO: created test-event-2
Jun 13 03:03:41.874: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 06/13/23 03:03:41.874
STEP: delete collection of events 06/13/23 03:03:41.88
Jun 13 03:03:41.880: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 06/13/23 03:03:41.92
Jun 13 03:03:41.920: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jun 13 03:03:41.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6502" for this suite. 06/13/23 03:03:41.936
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":136,"skipped":2728,"failed":0}
------------------------------
• [0.143 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:03:41.804
    Jun 13 03:03:41.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename events 06/13/23 03:03:41.805
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:41.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:41.839
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 06/13/23 03:03:41.845
    Jun 13 03:03:41.855: INFO: created test-event-1
    Jun 13 03:03:41.864: INFO: created test-event-2
    Jun 13 03:03:41.874: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 06/13/23 03:03:41.874
    STEP: delete collection of events 06/13/23 03:03:41.88
    Jun 13 03:03:41.880: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 06/13/23 03:03:41.92
    Jun 13 03:03:41.920: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jun 13 03:03:41.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6502" for this suite. 06/13/23 03:03:41.936
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:03:41.948
Jun 13 03:03:41.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:03:41.949
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:41.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:41.995
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179
STEP: creating service in namespace services-5011 06/13/23 03:03:42.001
STEP: creating service affinity-clusterip-transition in namespace services-5011 06/13/23 03:03:42.001
STEP: creating replication controller affinity-clusterip-transition in namespace services-5011 06/13/23 03:03:42.032
I0613 03:03:42.042554      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5011, replica count: 3
I0613 03:03:45.095361      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 13 03:03:45.113: INFO: Creating new exec pod
Jun 13 03:03:45.174: INFO: Waiting up to 5m0s for pod "execpod-affinitydvhf6" in namespace "services-5011" to be "running"
Jun 13 03:03:45.185: INFO: Pod "execpod-affinitydvhf6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.855732ms
Jun 13 03:03:47.190: INFO: Pod "execpod-affinitydvhf6": Phase="Running", Reason="", readiness=true. Elapsed: 2.015888828s
Jun 13 03:03:47.190: INFO: Pod "execpod-affinitydvhf6" satisfied condition "running"
Jun 13 03:03:48.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-5011 exec execpod-affinitydvhf6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jun 13 03:03:48.435: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun 13 03:03:48.435: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 03:03:48.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-5011 exec execpod-affinitydvhf6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.158.221 80'
Jun 13 03:03:48.650: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.158.221 80\nConnection to 10.107.158.221 80 port [tcp/http] succeeded!\n"
Jun 13 03:03:48.650: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 03:03:48.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-5011 exec execpod-affinitydvhf6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.158.221:80/ ; done'
Jun 13 03:03:49.048: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n"
Jun 13 03:03:49.048: INFO: stdout: "\naffinity-clusterip-transition-zwfrc\naffinity-clusterip-transition-zwfrc\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-zwfrc\naffinity-clusterip-transition-6zc9j\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-6zc9j\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-6zc9j\naffinity-clusterip-transition-zwfrc\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-zwfrc\naffinity-clusterip-transition-6zc9j\naffinity-clusterip-transition-6zc9j\naffinity-clusterip-transition-mrhq4"
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-zwfrc
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-zwfrc
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-zwfrc
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-6zc9j
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-6zc9j
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-6zc9j
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-zwfrc
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-zwfrc
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-6zc9j
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-6zc9j
Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-5011 exec execpod-affinitydvhf6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.158.221:80/ ; done'
Jun 13 03:03:49.453: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n"
Jun 13 03:03:49.453: INFO: stdout: "\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4"
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
Jun 13 03:03:49.453: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5011, will wait for the garbage collector to delete the pods 06/13/23 03:03:49.489
Jun 13 03:03:49.570: INFO: Deleting ReplicationController affinity-clusterip-transition took: 21.9413ms
Jun 13 03:03:49.671: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.748226ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:03:52.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5011" for this suite. 06/13/23 03:03:52.356
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":137,"skipped":2730,"failed":0}
------------------------------
• [SLOW TEST] [10.422 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:03:41.948
    Jun 13 03:03:41.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:03:41.949
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:41.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:41.995
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2179
    STEP: creating service in namespace services-5011 06/13/23 03:03:42.001
    STEP: creating service affinity-clusterip-transition in namespace services-5011 06/13/23 03:03:42.001
    STEP: creating replication controller affinity-clusterip-transition in namespace services-5011 06/13/23 03:03:42.032
    I0613 03:03:42.042554      18 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-5011, replica count: 3
    I0613 03:03:45.095361      18 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 13 03:03:45.113: INFO: Creating new exec pod
    Jun 13 03:03:45.174: INFO: Waiting up to 5m0s for pod "execpod-affinitydvhf6" in namespace "services-5011" to be "running"
    Jun 13 03:03:45.185: INFO: Pod "execpod-affinitydvhf6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.855732ms
    Jun 13 03:03:47.190: INFO: Pod "execpod-affinitydvhf6": Phase="Running", Reason="", readiness=true. Elapsed: 2.015888828s
    Jun 13 03:03:47.190: INFO: Pod "execpod-affinitydvhf6" satisfied condition "running"
    Jun 13 03:03:48.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-5011 exec execpod-affinitydvhf6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jun 13 03:03:48.435: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jun 13 03:03:48.435: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 03:03:48.435: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-5011 exec execpod-affinitydvhf6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.158.221 80'
    Jun 13 03:03:48.650: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.158.221 80\nConnection to 10.107.158.221 80 port [tcp/http] succeeded!\n"
    Jun 13 03:03:48.650: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 03:03:48.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-5011 exec execpod-affinitydvhf6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.158.221:80/ ; done'
    Jun 13 03:03:49.048: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n"
    Jun 13 03:03:49.048: INFO: stdout: "\naffinity-clusterip-transition-zwfrc\naffinity-clusterip-transition-zwfrc\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-zwfrc\naffinity-clusterip-transition-6zc9j\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-6zc9j\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-6zc9j\naffinity-clusterip-transition-zwfrc\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-zwfrc\naffinity-clusterip-transition-6zc9j\naffinity-clusterip-transition-6zc9j\naffinity-clusterip-transition-mrhq4"
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-zwfrc
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-zwfrc
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-zwfrc
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-6zc9j
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-6zc9j
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-6zc9j
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-zwfrc
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-zwfrc
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-6zc9j
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-6zc9j
    Jun 13 03:03:49.048: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-5011 exec execpod-affinitydvhf6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.158.221:80/ ; done'
    Jun 13 03:03:49.453: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.158.221:80/\n"
    Jun 13 03:03:49.453: INFO: stdout: "\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4\naffinity-clusterip-transition-mrhq4"
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Received response from host: affinity-clusterip-transition-mrhq4
    Jun 13 03:03:49.453: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-5011, will wait for the garbage collector to delete the pods 06/13/23 03:03:49.489
    Jun 13 03:03:49.570: INFO: Deleting ReplicationController affinity-clusterip-transition took: 21.9413ms
    Jun 13 03:03:49.671: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.748226ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:03:52.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5011" for this suite. 06/13/23 03:03:52.356
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:03:52.37
Jun 13 03:03:52.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-lifecycle-hook 06/13/23 03:03:52.372
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:52.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:52.404
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 06/13/23 03:03:52.427
Jun 13 03:03:52.447: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8141" to be "running and ready"
Jun 13 03:03:52.452: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.958342ms
Jun 13 03:03:52.452: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:03:54.460: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013216026s
Jun 13 03:03:54.460: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 13 03:03:54.460: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 06/13/23 03:03:54.466
Jun 13 03:03:54.478: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8141" to be "running and ready"
Jun 13 03:03:54.489: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.782391ms
Jun 13 03:03:54.489: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:03:56.502: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024137526s
Jun 13 03:03:56.502: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jun 13 03:03:56.502: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 06/13/23 03:03:56.509
Jun 13 03:03:56.521: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 13 03:03:56.534: INFO: Pod pod-with-prestop-http-hook still exists
Jun 13 03:03:58.535: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 13 03:03:58.553: INFO: Pod pod-with-prestop-http-hook still exists
Jun 13 03:04:00.536: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 13 03:04:00.545: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 06/13/23 03:04:00.545
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jun 13 03:04:00.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8141" for this suite. 06/13/23 03:04:00.62
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":138,"skipped":2733,"failed":0}
------------------------------
• [SLOW TEST] [8.269 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:03:52.37
    Jun 13 03:03:52.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/13/23 03:03:52.372
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:03:52.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:03:52.404
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 06/13/23 03:03:52.427
    Jun 13 03:03:52.447: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8141" to be "running and ready"
    Jun 13 03:03:52.452: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.958342ms
    Jun 13 03:03:52.452: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:03:54.460: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013216026s
    Jun 13 03:03:54.460: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 13 03:03:54.460: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 06/13/23 03:03:54.466
    Jun 13 03:03:54.478: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-8141" to be "running and ready"
    Jun 13 03:03:54.489: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 10.782391ms
    Jun 13 03:03:54.489: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:03:56.502: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.024137526s
    Jun 13 03:03:56.502: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jun 13 03:03:56.502: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 06/13/23 03:03:56.509
    Jun 13 03:03:56.521: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun 13 03:03:56.534: INFO: Pod pod-with-prestop-http-hook still exists
    Jun 13 03:03:58.535: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun 13 03:03:58.553: INFO: Pod pod-with-prestop-http-hook still exists
    Jun 13 03:04:00.536: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun 13 03:04:00.545: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 06/13/23 03:04:00.545
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jun 13 03:04:00.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8141" for this suite. 06/13/23 03:04:00.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:04:00.641
Jun 13 03:04:00.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename namespaces 06/13/23 03:04:00.644
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:00.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:00.69
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 06/13/23 03:04:00.698
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:00.725
STEP: Creating a pod in the namespace 06/13/23 03:04:00.735
STEP: Waiting for the pod to have running status 06/13/23 03:04:00.761
Jun 13 03:04:00.761: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8646" to be "running"
Jun 13 03:04:00.771: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.853544ms
Jun 13 03:04:02.778: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016761266s
Jun 13 03:04:02.778: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 06/13/23 03:04:02.778
STEP: Waiting for the namespace to be removed. 06/13/23 03:04:02.795
STEP: Recreating the namespace 06/13/23 03:04:13.803
STEP: Verifying there are no pods in the namespace 06/13/23 03:04:13.833
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:04:13.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3105" for this suite. 06/13/23 03:04:13.847
STEP: Destroying namespace "nsdeletetest-8646" for this suite. 06/13/23 03:04:13.862
Jun 13 03:04:13.869: INFO: Namespace nsdeletetest-8646 was already deleted
STEP: Destroying namespace "nsdeletetest-8259" for this suite. 06/13/23 03:04:13.869
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":139,"skipped":2749,"failed":0}
------------------------------
• [SLOW TEST] [13.246 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:04:00.641
    Jun 13 03:04:00.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename namespaces 06/13/23 03:04:00.644
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:00.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:00.69
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 06/13/23 03:04:00.698
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:00.725
    STEP: Creating a pod in the namespace 06/13/23 03:04:00.735
    STEP: Waiting for the pod to have running status 06/13/23 03:04:00.761
    Jun 13 03:04:00.761: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-8646" to be "running"
    Jun 13 03:04:00.771: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.853544ms
    Jun 13 03:04:02.778: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016761266s
    Jun 13 03:04:02.778: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 06/13/23 03:04:02.778
    STEP: Waiting for the namespace to be removed. 06/13/23 03:04:02.795
    STEP: Recreating the namespace 06/13/23 03:04:13.803
    STEP: Verifying there are no pods in the namespace 06/13/23 03:04:13.833
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:04:13.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3105" for this suite. 06/13/23 03:04:13.847
    STEP: Destroying namespace "nsdeletetest-8646" for this suite. 06/13/23 03:04:13.862
    Jun 13 03:04:13.869: INFO: Namespace nsdeletetest-8646 was already deleted
    STEP: Destroying namespace "nsdeletetest-8259" for this suite. 06/13/23 03:04:13.869
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:04:13.887
Jun 13 03:04:13.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename endpointslice 06/13/23 03:04:13.889
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:13.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:13.919
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 06/13/23 03:04:13.926
STEP: getting /apis/discovery.k8s.io 06/13/23 03:04:13.931
STEP: getting /apis/discovery.k8s.iov1 06/13/23 03:04:13.934
STEP: creating 06/13/23 03:04:13.936
STEP: getting 06/13/23 03:04:13.996
STEP: listing 06/13/23 03:04:14.002
STEP: watching 06/13/23 03:04:14.008
Jun 13 03:04:14.008: INFO: starting watch
STEP: cluster-wide listing 06/13/23 03:04:14.01
STEP: cluster-wide watching 06/13/23 03:04:14.017
Jun 13 03:04:14.017: INFO: starting watch
STEP: patching 06/13/23 03:04:14.019
STEP: updating 06/13/23 03:04:14.029
Jun 13 03:04:14.043: INFO: waiting for watch events with expected annotations
Jun 13 03:04:14.043: INFO: saw patched and updated annotations
STEP: deleting 06/13/23 03:04:14.043
STEP: deleting a collection 06/13/23 03:04:14.067
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jun 13 03:04:14.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6439" for this suite. 06/13/23 03:04:14.115
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":140,"skipped":2751,"failed":0}
------------------------------
• [0.247 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:04:13.887
    Jun 13 03:04:13.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename endpointslice 06/13/23 03:04:13.889
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:13.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:13.919
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 06/13/23 03:04:13.926
    STEP: getting /apis/discovery.k8s.io 06/13/23 03:04:13.931
    STEP: getting /apis/discovery.k8s.iov1 06/13/23 03:04:13.934
    STEP: creating 06/13/23 03:04:13.936
    STEP: getting 06/13/23 03:04:13.996
    STEP: listing 06/13/23 03:04:14.002
    STEP: watching 06/13/23 03:04:14.008
    Jun 13 03:04:14.008: INFO: starting watch
    STEP: cluster-wide listing 06/13/23 03:04:14.01
    STEP: cluster-wide watching 06/13/23 03:04:14.017
    Jun 13 03:04:14.017: INFO: starting watch
    STEP: patching 06/13/23 03:04:14.019
    STEP: updating 06/13/23 03:04:14.029
    Jun 13 03:04:14.043: INFO: waiting for watch events with expected annotations
    Jun 13 03:04:14.043: INFO: saw patched and updated annotations
    STEP: deleting 06/13/23 03:04:14.043
    STEP: deleting a collection 06/13/23 03:04:14.067
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jun 13 03:04:14.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6439" for this suite. 06/13/23 03:04:14.115
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:04:14.134
Jun 13 03:04:14.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 06/13/23 03:04:14.136
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:14.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:14.167
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 06/13/23 03:04:14.174
STEP: Creating hostNetwork=false pod 06/13/23 03:04:14.174
Jun 13 03:04:14.190: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7902" to be "running and ready"
Jun 13 03:04:14.200: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.461806ms
Jun 13 03:04:14.200: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:04:16.220: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029412889s
Jun 13 03:04:16.220: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:04:18.207: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016322498s
Jun 13 03:04:18.207: INFO: The phase of Pod test-pod is Running (Ready = true)
Jun 13 03:04:18.207: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 06/13/23 03:04:18.216
Jun 13 03:04:18.227: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7902" to be "running and ready"
Jun 13 03:04:18.235: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050765ms
Jun 13 03:04:18.235: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:04:20.243: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016506673s
Jun 13 03:04:20.243: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jun 13 03:04:20.243: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 06/13/23 03:04:20.251
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 06/13/23 03:04:20.251
Jun 13 03:04:20.252: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:04:20.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:04:20.253: INFO: ExecWithOptions: Clientset creation
Jun 13 03:04:20.253: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 13 03:04:20.368: INFO: Exec stderr: ""
Jun 13 03:04:20.368: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:04:20.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:04:20.369: INFO: ExecWithOptions: Clientset creation
Jun 13 03:04:20.370: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 13 03:04:20.460: INFO: Exec stderr: ""
Jun 13 03:04:20.460: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:04:20.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:04:20.461: INFO: ExecWithOptions: Clientset creation
Jun 13 03:04:20.461: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 13 03:04:20.572: INFO: Exec stderr: ""
Jun 13 03:04:20.572: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:04:20.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:04:20.573: INFO: ExecWithOptions: Clientset creation
Jun 13 03:04:20.573: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 13 03:04:20.673: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 06/13/23 03:04:20.673
Jun 13 03:04:20.673: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:04:20.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:04:20.674: INFO: ExecWithOptions: Clientset creation
Jun 13 03:04:20.674: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun 13 03:04:20.770: INFO: Exec stderr: ""
Jun 13 03:04:20.770: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:04:20.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:04:20.771: INFO: ExecWithOptions: Clientset creation
Jun 13 03:04:20.771: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun 13 03:04:20.892: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 06/13/23 03:04:20.892
Jun 13 03:04:20.892: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:04:20.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:04:20.893: INFO: ExecWithOptions: Clientset creation
Jun 13 03:04:20.893: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 13 03:04:21.004: INFO: Exec stderr: ""
Jun 13 03:04:21.004: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:04:21.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:04:21.005: INFO: ExecWithOptions: Clientset creation
Jun 13 03:04:21.005: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 13 03:04:21.101: INFO: Exec stderr: ""
Jun 13 03:04:21.101: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:04:21.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:04:21.102: INFO: ExecWithOptions: Clientset creation
Jun 13 03:04:21.102: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 13 03:04:21.197: INFO: Exec stderr: ""
Jun 13 03:04:21.197: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:04:21.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:04:21.198: INFO: ExecWithOptions: Clientset creation
Jun 13 03:04:21.198: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 13 03:04:21.284: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jun 13 03:04:21.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7902" for this suite. 06/13/23 03:04:21.301
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":141,"skipped":2752,"failed":0}
------------------------------
• [SLOW TEST] [7.175 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:04:14.134
    Jun 13 03:04:14.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 06/13/23 03:04:14.136
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:14.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:14.167
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 06/13/23 03:04:14.174
    STEP: Creating hostNetwork=false pod 06/13/23 03:04:14.174
    Jun 13 03:04:14.190: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-7902" to be "running and ready"
    Jun 13 03:04:14.200: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.461806ms
    Jun 13 03:04:14.200: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:04:16.220: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029412889s
    Jun 13 03:04:16.220: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:04:18.207: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.016322498s
    Jun 13 03:04:18.207: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jun 13 03:04:18.207: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 06/13/23 03:04:18.216
    Jun 13 03:04:18.227: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-7902" to be "running and ready"
    Jun 13 03:04:18.235: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.050765ms
    Jun 13 03:04:18.235: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:04:20.243: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.016506673s
    Jun 13 03:04:20.243: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jun 13 03:04:20.243: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 06/13/23 03:04:20.251
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 06/13/23 03:04:20.251
    Jun 13 03:04:20.252: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:04:20.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:04:20.253: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:04:20.253: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 13 03:04:20.368: INFO: Exec stderr: ""
    Jun 13 03:04:20.368: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:04:20.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:04:20.369: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:04:20.370: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 13 03:04:20.460: INFO: Exec stderr: ""
    Jun 13 03:04:20.460: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:04:20.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:04:20.461: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:04:20.461: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 13 03:04:20.572: INFO: Exec stderr: ""
    Jun 13 03:04:20.572: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:04:20.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:04:20.573: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:04:20.573: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 13 03:04:20.673: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 06/13/23 03:04:20.673
    Jun 13 03:04:20.673: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:04:20.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:04:20.674: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:04:20.674: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jun 13 03:04:20.770: INFO: Exec stderr: ""
    Jun 13 03:04:20.770: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:04:20.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:04:20.771: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:04:20.771: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jun 13 03:04:20.892: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 06/13/23 03:04:20.892
    Jun 13 03:04:20.892: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:04:20.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:04:20.893: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:04:20.893: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 13 03:04:21.004: INFO: Exec stderr: ""
    Jun 13 03:04:21.004: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:04:21.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:04:21.005: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:04:21.005: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 13 03:04:21.101: INFO: Exec stderr: ""
    Jun 13 03:04:21.101: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:04:21.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:04:21.102: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:04:21.102: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 13 03:04:21.197: INFO: Exec stderr: ""
    Jun 13 03:04:21.197: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7902 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:04:21.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:04:21.198: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:04:21.198: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-7902/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 13 03:04:21.284: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jun 13 03:04:21.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-7902" for this suite. 06/13/23 03:04:21.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:04:21.31
Jun 13 03:04:21.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:04:21.312
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:21.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:21.339
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-2b010a30-38c0-4a07-93e3-3d5adffcccca 06/13/23 03:04:21.346
STEP: Creating a pod to test consume secrets 06/13/23 03:04:21.355
Jun 13 03:04:21.371: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402" in namespace "projected-4902" to be "Succeeded or Failed"
Jun 13 03:04:21.382: INFO: Pod "pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402": Phase="Pending", Reason="", readiness=false. Elapsed: 11.21566ms
Jun 13 03:04:23.389: INFO: Pod "pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018366072s
Jun 13 03:04:25.390: INFO: Pod "pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019037286s
STEP: Saw pod success 06/13/23 03:04:25.39
Jun 13 03:04:25.390: INFO: Pod "pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402" satisfied condition "Succeeded or Failed"
Jun 13 03:04:25.401: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-l5gcd pod pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402 container secret-volume-test: <nil>
STEP: delete the pod 06/13/23 03:04:25.416
Jun 13 03:04:25.442: INFO: Waiting for pod pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402 to disappear
Jun 13 03:04:25.449: INFO: Pod pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 13 03:04:25.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4902" for this suite. 06/13/23 03:04:25.459
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":142,"skipped":2761,"failed":0}
------------------------------
• [4.168 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:04:21.31
    Jun 13 03:04:21.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:04:21.312
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:21.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:21.339
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-2b010a30-38c0-4a07-93e3-3d5adffcccca 06/13/23 03:04:21.346
    STEP: Creating a pod to test consume secrets 06/13/23 03:04:21.355
    Jun 13 03:04:21.371: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402" in namespace "projected-4902" to be "Succeeded or Failed"
    Jun 13 03:04:21.382: INFO: Pod "pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402": Phase="Pending", Reason="", readiness=false. Elapsed: 11.21566ms
    Jun 13 03:04:23.389: INFO: Pod "pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018366072s
    Jun 13 03:04:25.390: INFO: Pod "pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019037286s
    STEP: Saw pod success 06/13/23 03:04:25.39
    Jun 13 03:04:25.390: INFO: Pod "pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402" satisfied condition "Succeeded or Failed"
    Jun 13 03:04:25.401: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-l5gcd pod pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402 container secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:04:25.416
    Jun 13 03:04:25.442: INFO: Waiting for pod pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402 to disappear
    Jun 13 03:04:25.449: INFO: Pod pod-projected-secrets-bd3eb99c-7549-4bfc-97ad-ed8dd73f5402 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 13 03:04:25.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4902" for this suite. 06/13/23 03:04:25.459
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:04:25.478
Jun 13 03:04:25.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-runtime 06/13/23 03:04:25.479
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:25.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:25.535
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 06/13/23 03:04:25.548
STEP: wait for the container to reach Succeeded 06/13/23 03:04:25.59
STEP: get the container status 06/13/23 03:04:29.661
STEP: the container should be terminated 06/13/23 03:04:29.667
STEP: the termination message should be set 06/13/23 03:04:29.667
Jun 13 03:04:29.667: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 06/13/23 03:04:29.667
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 13 03:04:29.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6722" for this suite. 06/13/23 03:04:29.715
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":143,"skipped":2764,"failed":0}
------------------------------
• [4.274 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:04:25.478
    Jun 13 03:04:25.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-runtime 06/13/23 03:04:25.479
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:25.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:25.535
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 06/13/23 03:04:25.548
    STEP: wait for the container to reach Succeeded 06/13/23 03:04:25.59
    STEP: get the container status 06/13/23 03:04:29.661
    STEP: the container should be terminated 06/13/23 03:04:29.667
    STEP: the termination message should be set 06/13/23 03:04:29.667
    Jun 13 03:04:29.667: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 06/13/23 03:04:29.667
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 13 03:04:29.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-6722" for this suite. 06/13/23 03:04:29.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:04:29.754
Jun 13 03:04:29.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename discovery 06/13/23 03:04:29.755
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:29.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:29.789
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 06/13/23 03:04:29.805
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jun 13 03:04:30.454: INFO: Checking APIGroup: apiregistration.k8s.io
Jun 13 03:04:30.457: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun 13 03:04:30.457: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jun 13 03:04:30.457: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun 13 03:04:30.457: INFO: Checking APIGroup: apps
Jun 13 03:04:30.461: INFO: PreferredVersion.GroupVersion: apps/v1
Jun 13 03:04:30.461: INFO: Versions found [{apps/v1 v1}]
Jun 13 03:04:30.461: INFO: apps/v1 matches apps/v1
Jun 13 03:04:30.461: INFO: Checking APIGroup: events.k8s.io
Jun 13 03:04:30.464: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun 13 03:04:30.464: INFO: Versions found [{events.k8s.io/v1 v1}]
Jun 13 03:04:30.464: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun 13 03:04:30.464: INFO: Checking APIGroup: authentication.k8s.io
Jun 13 03:04:30.467: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun 13 03:04:30.467: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jun 13 03:04:30.467: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun 13 03:04:30.467: INFO: Checking APIGroup: authorization.k8s.io
Jun 13 03:04:30.469: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun 13 03:04:30.469: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jun 13 03:04:30.469: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun 13 03:04:30.469: INFO: Checking APIGroup: autoscaling
Jun 13 03:04:30.472: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jun 13 03:04:30.472: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jun 13 03:04:30.472: INFO: autoscaling/v2 matches autoscaling/v2
Jun 13 03:04:30.472: INFO: Checking APIGroup: batch
Jun 13 03:04:30.475: INFO: PreferredVersion.GroupVersion: batch/v1
Jun 13 03:04:30.475: INFO: Versions found [{batch/v1 v1}]
Jun 13 03:04:30.475: INFO: batch/v1 matches batch/v1
Jun 13 03:04:30.475: INFO: Checking APIGroup: certificates.k8s.io
Jun 13 03:04:30.478: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun 13 03:04:30.478: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jun 13 03:04:30.478: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun 13 03:04:30.478: INFO: Checking APIGroup: networking.k8s.io
Jun 13 03:04:30.480: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun 13 03:04:30.480: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jun 13 03:04:30.480: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun 13 03:04:30.480: INFO: Checking APIGroup: policy
Jun 13 03:04:30.482: INFO: PreferredVersion.GroupVersion: policy/v1
Jun 13 03:04:30.482: INFO: Versions found [{policy/v1 v1}]
Jun 13 03:04:30.482: INFO: policy/v1 matches policy/v1
Jun 13 03:04:30.482: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun 13 03:04:30.486: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun 13 03:04:30.486: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jun 13 03:04:30.486: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun 13 03:04:30.486: INFO: Checking APIGroup: storage.k8s.io
Jun 13 03:04:30.488: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun 13 03:04:30.488: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun 13 03:04:30.488: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun 13 03:04:30.488: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun 13 03:04:30.491: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun 13 03:04:30.491: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jun 13 03:04:30.491: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun 13 03:04:30.491: INFO: Checking APIGroup: apiextensions.k8s.io
Jun 13 03:04:30.494: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun 13 03:04:30.494: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jun 13 03:04:30.494: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun 13 03:04:30.494: INFO: Checking APIGroup: scheduling.k8s.io
Jun 13 03:04:30.496: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun 13 03:04:30.496: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jun 13 03:04:30.496: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun 13 03:04:30.496: INFO: Checking APIGroup: coordination.k8s.io
Jun 13 03:04:30.499: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun 13 03:04:30.499: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jun 13 03:04:30.499: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun 13 03:04:30.499: INFO: Checking APIGroup: node.k8s.io
Jun 13 03:04:30.501: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun 13 03:04:30.501: INFO: Versions found [{node.k8s.io/v1 v1}]
Jun 13 03:04:30.501: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun 13 03:04:30.501: INFO: Checking APIGroup: discovery.k8s.io
Jun 13 03:04:30.505: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jun 13 03:04:30.505: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jun 13 03:04:30.505: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jun 13 03:04:30.505: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun 13 03:04:30.509: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jun 13 03:04:30.509: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jun 13 03:04:30.509: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jun 13 03:04:30.509: INFO: Checking APIGroup: projectcalico.org
Jun 13 03:04:30.511: INFO: PreferredVersion.GroupVersion: projectcalico.org/v3
Jun 13 03:04:30.511: INFO: Versions found [{projectcalico.org/v3 v3}]
Jun 13 03:04:30.511: INFO: projectcalico.org/v3 matches projectcalico.org/v3
Jun 13 03:04:30.511: INFO: Checking APIGroup: crd.projectcalico.org
Jun 13 03:04:30.517: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jun 13 03:04:30.517: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jun 13 03:04:30.517: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jun 13 03:04:30.517: INFO: Checking APIGroup: operator.tigera.io
Jun 13 03:04:30.520: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Jun 13 03:04:30.520: INFO: Versions found [{operator.tigera.io/v1 v1}]
Jun 13 03:04:30.520: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Jun 13 03:04:30.520: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jun 13 03:04:30.524: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jun 13 03:04:30.524: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jun 13 03:04:30.524: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jun 13 03:04:30.524: INFO: Checking APIGroup: internal.packaging.carvel.dev
Jun 13 03:04:30.526: INFO: PreferredVersion.GroupVersion: internal.packaging.carvel.dev/v1alpha1
Jun 13 03:04:30.526: INFO: Versions found [{internal.packaging.carvel.dev/v1alpha1 v1alpha1}]
Jun 13 03:04:30.526: INFO: internal.packaging.carvel.dev/v1alpha1 matches internal.packaging.carvel.dev/v1alpha1
Jun 13 03:04:30.526: INFO: Checking APIGroup: kappctrl.k14s.io
Jun 13 03:04:30.530: INFO: PreferredVersion.GroupVersion: kappctrl.k14s.io/v1alpha1
Jun 13 03:04:30.530: INFO: Versions found [{kappctrl.k14s.io/v1alpha1 v1alpha1}]
Jun 13 03:04:30.530: INFO: kappctrl.k14s.io/v1alpha1 matches kappctrl.k14s.io/v1alpha1
Jun 13 03:04:30.530: INFO: Checking APIGroup: packaging.carvel.dev
Jun 13 03:04:30.536: INFO: PreferredVersion.GroupVersion: packaging.carvel.dev/v1alpha1
Jun 13 03:04:30.536: INFO: Versions found [{packaging.carvel.dev/v1alpha1 v1alpha1}]
Jun 13 03:04:30.536: INFO: packaging.carvel.dev/v1alpha1 matches packaging.carvel.dev/v1alpha1
Jun 13 03:04:30.536: INFO: Checking APIGroup: data.packaging.carvel.dev
Jun 13 03:04:30.541: INFO: PreferredVersion.GroupVersion: data.packaging.carvel.dev/v1alpha1
Jun 13 03:04:30.541: INFO: Versions found [{data.packaging.carvel.dev/v1alpha1 v1alpha1}]
Jun 13 03:04:30.541: INFO: data.packaging.carvel.dev/v1alpha1 matches data.packaging.carvel.dev/v1alpha1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jun 13 03:04:30.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-7910" for this suite. 06/13/23 03:04:30.558
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":144,"skipped":2783,"failed":0}
------------------------------
• [0.816 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:04:29.754
    Jun 13 03:04:29.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename discovery 06/13/23 03:04:29.755
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:29.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:29.789
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 06/13/23 03:04:29.805
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jun 13 03:04:30.454: INFO: Checking APIGroup: apiregistration.k8s.io
    Jun 13 03:04:30.457: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jun 13 03:04:30.457: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jun 13 03:04:30.457: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jun 13 03:04:30.457: INFO: Checking APIGroup: apps
    Jun 13 03:04:30.461: INFO: PreferredVersion.GroupVersion: apps/v1
    Jun 13 03:04:30.461: INFO: Versions found [{apps/v1 v1}]
    Jun 13 03:04:30.461: INFO: apps/v1 matches apps/v1
    Jun 13 03:04:30.461: INFO: Checking APIGroup: events.k8s.io
    Jun 13 03:04:30.464: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jun 13 03:04:30.464: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jun 13 03:04:30.464: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jun 13 03:04:30.464: INFO: Checking APIGroup: authentication.k8s.io
    Jun 13 03:04:30.467: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jun 13 03:04:30.467: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jun 13 03:04:30.467: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jun 13 03:04:30.467: INFO: Checking APIGroup: authorization.k8s.io
    Jun 13 03:04:30.469: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jun 13 03:04:30.469: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jun 13 03:04:30.469: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jun 13 03:04:30.469: INFO: Checking APIGroup: autoscaling
    Jun 13 03:04:30.472: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jun 13 03:04:30.472: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jun 13 03:04:30.472: INFO: autoscaling/v2 matches autoscaling/v2
    Jun 13 03:04:30.472: INFO: Checking APIGroup: batch
    Jun 13 03:04:30.475: INFO: PreferredVersion.GroupVersion: batch/v1
    Jun 13 03:04:30.475: INFO: Versions found [{batch/v1 v1}]
    Jun 13 03:04:30.475: INFO: batch/v1 matches batch/v1
    Jun 13 03:04:30.475: INFO: Checking APIGroup: certificates.k8s.io
    Jun 13 03:04:30.478: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jun 13 03:04:30.478: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jun 13 03:04:30.478: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jun 13 03:04:30.478: INFO: Checking APIGroup: networking.k8s.io
    Jun 13 03:04:30.480: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jun 13 03:04:30.480: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jun 13 03:04:30.480: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jun 13 03:04:30.480: INFO: Checking APIGroup: policy
    Jun 13 03:04:30.482: INFO: PreferredVersion.GroupVersion: policy/v1
    Jun 13 03:04:30.482: INFO: Versions found [{policy/v1 v1}]
    Jun 13 03:04:30.482: INFO: policy/v1 matches policy/v1
    Jun 13 03:04:30.482: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jun 13 03:04:30.486: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jun 13 03:04:30.486: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jun 13 03:04:30.486: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jun 13 03:04:30.486: INFO: Checking APIGroup: storage.k8s.io
    Jun 13 03:04:30.488: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jun 13 03:04:30.488: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jun 13 03:04:30.488: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jun 13 03:04:30.488: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jun 13 03:04:30.491: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jun 13 03:04:30.491: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jun 13 03:04:30.491: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jun 13 03:04:30.491: INFO: Checking APIGroup: apiextensions.k8s.io
    Jun 13 03:04:30.494: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jun 13 03:04:30.494: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jun 13 03:04:30.494: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jun 13 03:04:30.494: INFO: Checking APIGroup: scheduling.k8s.io
    Jun 13 03:04:30.496: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jun 13 03:04:30.496: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jun 13 03:04:30.496: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jun 13 03:04:30.496: INFO: Checking APIGroup: coordination.k8s.io
    Jun 13 03:04:30.499: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jun 13 03:04:30.499: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jun 13 03:04:30.499: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jun 13 03:04:30.499: INFO: Checking APIGroup: node.k8s.io
    Jun 13 03:04:30.501: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jun 13 03:04:30.501: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jun 13 03:04:30.501: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jun 13 03:04:30.501: INFO: Checking APIGroup: discovery.k8s.io
    Jun 13 03:04:30.505: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jun 13 03:04:30.505: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jun 13 03:04:30.505: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jun 13 03:04:30.505: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jun 13 03:04:30.509: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jun 13 03:04:30.509: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jun 13 03:04:30.509: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jun 13 03:04:30.509: INFO: Checking APIGroup: projectcalico.org
    Jun 13 03:04:30.511: INFO: PreferredVersion.GroupVersion: projectcalico.org/v3
    Jun 13 03:04:30.511: INFO: Versions found [{projectcalico.org/v3 v3}]
    Jun 13 03:04:30.511: INFO: projectcalico.org/v3 matches projectcalico.org/v3
    Jun 13 03:04:30.511: INFO: Checking APIGroup: crd.projectcalico.org
    Jun 13 03:04:30.517: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jun 13 03:04:30.517: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jun 13 03:04:30.517: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jun 13 03:04:30.517: INFO: Checking APIGroup: operator.tigera.io
    Jun 13 03:04:30.520: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
    Jun 13 03:04:30.520: INFO: Versions found [{operator.tigera.io/v1 v1}]
    Jun 13 03:04:30.520: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
    Jun 13 03:04:30.520: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jun 13 03:04:30.524: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jun 13 03:04:30.524: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Jun 13 03:04:30.524: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jun 13 03:04:30.524: INFO: Checking APIGroup: internal.packaging.carvel.dev
    Jun 13 03:04:30.526: INFO: PreferredVersion.GroupVersion: internal.packaging.carvel.dev/v1alpha1
    Jun 13 03:04:30.526: INFO: Versions found [{internal.packaging.carvel.dev/v1alpha1 v1alpha1}]
    Jun 13 03:04:30.526: INFO: internal.packaging.carvel.dev/v1alpha1 matches internal.packaging.carvel.dev/v1alpha1
    Jun 13 03:04:30.526: INFO: Checking APIGroup: kappctrl.k14s.io
    Jun 13 03:04:30.530: INFO: PreferredVersion.GroupVersion: kappctrl.k14s.io/v1alpha1
    Jun 13 03:04:30.530: INFO: Versions found [{kappctrl.k14s.io/v1alpha1 v1alpha1}]
    Jun 13 03:04:30.530: INFO: kappctrl.k14s.io/v1alpha1 matches kappctrl.k14s.io/v1alpha1
    Jun 13 03:04:30.530: INFO: Checking APIGroup: packaging.carvel.dev
    Jun 13 03:04:30.536: INFO: PreferredVersion.GroupVersion: packaging.carvel.dev/v1alpha1
    Jun 13 03:04:30.536: INFO: Versions found [{packaging.carvel.dev/v1alpha1 v1alpha1}]
    Jun 13 03:04:30.536: INFO: packaging.carvel.dev/v1alpha1 matches packaging.carvel.dev/v1alpha1
    Jun 13 03:04:30.536: INFO: Checking APIGroup: data.packaging.carvel.dev
    Jun 13 03:04:30.541: INFO: PreferredVersion.GroupVersion: data.packaging.carvel.dev/v1alpha1
    Jun 13 03:04:30.541: INFO: Versions found [{data.packaging.carvel.dev/v1alpha1 v1alpha1}]
    Jun 13 03:04:30.541: INFO: data.packaging.carvel.dev/v1alpha1 matches data.packaging.carvel.dev/v1alpha1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jun 13 03:04:30.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-7910" for this suite. 06/13/23 03:04:30.558
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:04:30.571
Jun 13 03:04:30.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:04:30.572
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:30.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:30.606
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 06/13/23 03:04:30.612
Jun 13 03:04:30.637: INFO: Waiting up to 5m0s for pod "downward-api-832343c6-d980-499a-a846-d79472e30fcc" in namespace "downward-api-8036" to be "Succeeded or Failed"
Jun 13 03:04:30.652: INFO: Pod "downward-api-832343c6-d980-499a-a846-d79472e30fcc": Phase="Pending", Reason="", readiness=false. Elapsed: 15.281655ms
Jun 13 03:04:32.666: INFO: Pod "downward-api-832343c6-d980-499a-a846-d79472e30fcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029269801s
Jun 13 03:04:34.660: INFO: Pod "downward-api-832343c6-d980-499a-a846-d79472e30fcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023457347s
STEP: Saw pod success 06/13/23 03:04:34.66
Jun 13 03:04:34.661: INFO: Pod "downward-api-832343c6-d980-499a-a846-d79472e30fcc" satisfied condition "Succeeded or Failed"
Jun 13 03:04:34.668: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downward-api-832343c6-d980-499a-a846-d79472e30fcc container dapi-container: <nil>
STEP: delete the pod 06/13/23 03:04:34.688
Jun 13 03:04:34.710: INFO: Waiting for pod downward-api-832343c6-d980-499a-a846-d79472e30fcc to disappear
Jun 13 03:04:34.718: INFO: Pod downward-api-832343c6-d980-499a-a846-d79472e30fcc no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 13 03:04:34.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8036" for this suite. 06/13/23 03:04:34.728
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":145,"skipped":2792,"failed":0}
------------------------------
• [4.171 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:04:30.571
    Jun 13 03:04:30.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:04:30.572
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:30.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:30.606
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 06/13/23 03:04:30.612
    Jun 13 03:04:30.637: INFO: Waiting up to 5m0s for pod "downward-api-832343c6-d980-499a-a846-d79472e30fcc" in namespace "downward-api-8036" to be "Succeeded or Failed"
    Jun 13 03:04:30.652: INFO: Pod "downward-api-832343c6-d980-499a-a846-d79472e30fcc": Phase="Pending", Reason="", readiness=false. Elapsed: 15.281655ms
    Jun 13 03:04:32.666: INFO: Pod "downward-api-832343c6-d980-499a-a846-d79472e30fcc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029269801s
    Jun 13 03:04:34.660: INFO: Pod "downward-api-832343c6-d980-499a-a846-d79472e30fcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023457347s
    STEP: Saw pod success 06/13/23 03:04:34.66
    Jun 13 03:04:34.661: INFO: Pod "downward-api-832343c6-d980-499a-a846-d79472e30fcc" satisfied condition "Succeeded or Failed"
    Jun 13 03:04:34.668: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downward-api-832343c6-d980-499a-a846-d79472e30fcc container dapi-container: <nil>
    STEP: delete the pod 06/13/23 03:04:34.688
    Jun 13 03:04:34.710: INFO: Waiting for pod downward-api-832343c6-d980-499a-a846-d79472e30fcc to disappear
    Jun 13 03:04:34.718: INFO: Pod downward-api-832343c6-d980-499a-a846-d79472e30fcc no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 13 03:04:34.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8036" for this suite. 06/13/23 03:04:34.728
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:04:34.747
Jun 13 03:04:34.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename statefulset 06/13/23 03:04:34.749
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:34.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:34.83
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3581 06/13/23 03:04:34.873
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 06/13/23 03:04:34.894
STEP: Creating pod with conflicting port in namespace statefulset-3581 06/13/23 03:04:34.911
STEP: Waiting until pod test-pod will start running in namespace statefulset-3581 06/13/23 03:04:34.938
Jun 13 03:04:34.938: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3581" to be "running"
Jun 13 03:04:34.947: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.8087ms
Jun 13 03:04:37.003: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064743917s
Jun 13 03:04:38.957: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.019115421s
Jun 13 03:04:38.957: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3581 06/13/23 03:04:38.957
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3581 06/13/23 03:04:38.972
Jun 13 03:04:38.991: INFO: Observed stateful pod in namespace: statefulset-3581, name: ss-0, uid: 4301d3d0-cdda-4a7b-8dfb-29aebccdbf07, status phase: Pending. Waiting for statefulset controller to delete.
Jun 13 03:04:39.012: INFO: Observed stateful pod in namespace: statefulset-3581, name: ss-0, uid: 4301d3d0-cdda-4a7b-8dfb-29aebccdbf07, status phase: Failed. Waiting for statefulset controller to delete.
Jun 13 03:04:39.032: INFO: Observed stateful pod in namespace: statefulset-3581, name: ss-0, uid: 4301d3d0-cdda-4a7b-8dfb-29aebccdbf07, status phase: Failed. Waiting for statefulset controller to delete.
Jun 13 03:04:39.048: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3581
STEP: Removing pod with conflicting port in namespace statefulset-3581 06/13/23 03:04:39.048
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3581 and will be in running state 06/13/23 03:04:39.084
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 13 03:04:41.108: INFO: Deleting all statefulset in ns statefulset-3581
Jun 13 03:04:41.115: INFO: Scaling statefulset ss to 0
Jun 13 03:04:51.186: INFO: Waiting for statefulset status.replicas updated to 0
Jun 13 03:04:51.192: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 13 03:04:51.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3581" for this suite. 06/13/23 03:04:51.23
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":146,"skipped":2867,"failed":0}
------------------------------
• [SLOW TEST] [16.499 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:04:34.747
    Jun 13 03:04:34.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename statefulset 06/13/23 03:04:34.749
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:34.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:34.83
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3581 06/13/23 03:04:34.873
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 06/13/23 03:04:34.894
    STEP: Creating pod with conflicting port in namespace statefulset-3581 06/13/23 03:04:34.911
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3581 06/13/23 03:04:34.938
    Jun 13 03:04:34.938: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3581" to be "running"
    Jun 13 03:04:34.947: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 8.8087ms
    Jun 13 03:04:37.003: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064743917s
    Jun 13 03:04:38.957: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.019115421s
    Jun 13 03:04:38.957: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3581 06/13/23 03:04:38.957
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3581 06/13/23 03:04:38.972
    Jun 13 03:04:38.991: INFO: Observed stateful pod in namespace: statefulset-3581, name: ss-0, uid: 4301d3d0-cdda-4a7b-8dfb-29aebccdbf07, status phase: Pending. Waiting for statefulset controller to delete.
    Jun 13 03:04:39.012: INFO: Observed stateful pod in namespace: statefulset-3581, name: ss-0, uid: 4301d3d0-cdda-4a7b-8dfb-29aebccdbf07, status phase: Failed. Waiting for statefulset controller to delete.
    Jun 13 03:04:39.032: INFO: Observed stateful pod in namespace: statefulset-3581, name: ss-0, uid: 4301d3d0-cdda-4a7b-8dfb-29aebccdbf07, status phase: Failed. Waiting for statefulset controller to delete.
    Jun 13 03:04:39.048: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3581
    STEP: Removing pod with conflicting port in namespace statefulset-3581 06/13/23 03:04:39.048
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3581 and will be in running state 06/13/23 03:04:39.084
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 13 03:04:41.108: INFO: Deleting all statefulset in ns statefulset-3581
    Jun 13 03:04:41.115: INFO: Scaling statefulset ss to 0
    Jun 13 03:04:51.186: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 13 03:04:51.192: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 13 03:04:51.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3581" for this suite. 06/13/23 03:04:51.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:04:51.247
Jun 13 03:04:51.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename runtimeclass 06/13/23 03:04:51.248
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:51.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:51.307
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 13 03:04:51.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3804" for this suite. 06/13/23 03:04:51.339
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":147,"skipped":2878,"failed":0}
------------------------------
• [0.120 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:04:51.247
    Jun 13 03:04:51.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename runtimeclass 06/13/23 03:04:51.248
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:51.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:51.307
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 13 03:04:51.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3804" for this suite. 06/13/23 03:04:51.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:04:51.37
Jun 13 03:04:51.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename deployment 06/13/23 03:04:51.372
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:51.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:51.411
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jun 13 03:04:51.420: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 13 03:04:51.440: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 13 03:04:56.449: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/13/23 03:04:56.449
Jun 13 03:04:56.449: INFO: Creating deployment "test-rolling-update-deployment"
Jun 13 03:04:56.590: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 13 03:04:56.611: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 13 03:04:58.627: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 13 03:04:58.639: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 13 03:04:58.664: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5263  cc3a9eed-c50c-4ec3-9a86-30a3329316a7 28393 1 2023-06-13 03:04:56 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-13 03:04:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:04:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e2eab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-13 03:04:56 +0000 UTC,LastTransitionTime:2023-06-13 03:04:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-06-13 03:04:58 +0000 UTC,LastTransitionTime:2023-06-13 03:04:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 13 03:04:58.672: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-5263  c34778fa-6af9-4e24-9407-1dc842a99690 28383 1 2023-06-13 03:04:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment cc3a9eed-c50c-4ec3-9a86-30a3329316a7 0xc005e2f6f7 0xc005e2f6f8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:04:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc3a9eed-c50c-4ec3-9a86-30a3329316a7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:04:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e2f7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 13 03:04:58.672: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 13 03:04:58.672: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5263  85d5072f-b2be-4699-b82d-e59f6e4313bd 28392 2 2023-06-13 03:04:51 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment cc3a9eed-c50c-4ec3-9a86-30a3329316a7 0xc005e2f5c7 0xc005e2f5c8}] [] [{e2e.test Update apps/v1 2023-06-13 03:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:04:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc3a9eed-c50c-4ec3-9a86-30a3329316a7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:04:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005e2f688 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 13 03:04:58.679: INFO: Pod "test-rolling-update-deployment-78f575d8ff-j4tk5" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-j4tk5 test-rolling-update-deployment-78f575d8ff- deployment-5263  deb3a439-5bec-4da0-a60c-31de4d7b21e4 28382 0 2023-06-13 03:04:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:fd6ce367ab42514bd6e9228c03d4069ff65ac4d275b462a4e79d2d5c18a11183 cni.projectcalico.org/podIP:172.30.77.134/32 cni.projectcalico.org/podIPs:172.30.77.134/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff c34778fa-6af9-4e24-9407-1dc842a99690 0xc003b5eda7 0xc003b5eda8}] [] [{kube-controller-manager Update v1 2023-06-13 03:04:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c34778fa-6af9-4e24-9407-1dc842a99690\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:04:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:04:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4h6z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4h6z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:04:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:04:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:04:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:04:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.77.134,StartTime:2023-06-13 03:04:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:04:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://b4081f7a2b259181f42fc62d1b431003a0f060b2650e81fcfec5342cc71c2d8c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.77.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 13 03:04:58.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5263" for this suite. 06/13/23 03:04:58.695
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":148,"skipped":2937,"failed":0}
------------------------------
• [SLOW TEST] [7.384 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:04:51.37
    Jun 13 03:04:51.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename deployment 06/13/23 03:04:51.372
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:51.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:51.411
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jun 13 03:04:51.420: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jun 13 03:04:51.440: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 13 03:04:56.449: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/13/23 03:04:56.449
    Jun 13 03:04:56.449: INFO: Creating deployment "test-rolling-update-deployment"
    Jun 13 03:04:56.590: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jun 13 03:04:56.611: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jun 13 03:04:58.627: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jun 13 03:04:58.639: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 13 03:04:58.664: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5263  cc3a9eed-c50c-4ec3-9a86-30a3329316a7 28393 1 2023-06-13 03:04:56 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-13 03:04:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:04:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e2eab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-13 03:04:56 +0000 UTC,LastTransitionTime:2023-06-13 03:04:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-06-13 03:04:58 +0000 UTC,LastTransitionTime:2023-06-13 03:04:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 13 03:04:58.672: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-5263  c34778fa-6af9-4e24-9407-1dc842a99690 28383 1 2023-06-13 03:04:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment cc3a9eed-c50c-4ec3-9a86-30a3329316a7 0xc005e2f6f7 0xc005e2f6f8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:04:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc3a9eed-c50c-4ec3-9a86-30a3329316a7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:04:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e2f7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 03:04:58.672: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jun 13 03:04:58.672: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5263  85d5072f-b2be-4699-b82d-e59f6e4313bd 28392 2 2023-06-13 03:04:51 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment cc3a9eed-c50c-4ec3-9a86-30a3329316a7 0xc005e2f5c7 0xc005e2f5c8}] [] [{e2e.test Update apps/v1 2023-06-13 03:04:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:04:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cc3a9eed-c50c-4ec3-9a86-30a3329316a7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:04:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005e2f688 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 03:04:58.679: INFO: Pod "test-rolling-update-deployment-78f575d8ff-j4tk5" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-j4tk5 test-rolling-update-deployment-78f575d8ff- deployment-5263  deb3a439-5bec-4da0-a60c-31de4d7b21e4 28382 0 2023-06-13 03:04:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:fd6ce367ab42514bd6e9228c03d4069ff65ac4d275b462a4e79d2d5c18a11183 cni.projectcalico.org/podIP:172.30.77.134/32 cni.projectcalico.org/podIPs:172.30.77.134/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff c34778fa-6af9-4e24-9407-1dc842a99690 0xc003b5eda7 0xc003b5eda8}] [] [{kube-controller-manager Update v1 2023-06-13 03:04:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c34778fa-6af9-4e24-9407-1dc842a99690\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:04:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:04:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g4h6z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g4h6z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:04:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:04:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:04:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:04:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.77.134,StartTime:2023-06-13 03:04:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:04:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://b4081f7a2b259181f42fc62d1b431003a0f060b2650e81fcfec5342cc71c2d8c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.77.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 13 03:04:58.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5263" for this suite. 06/13/23 03:04:58.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:04:58.755
Jun 13 03:04:58.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename replicaset 06/13/23 03:04:58.756
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:58.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:58.787
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 06/13/23 03:04:58.792
Jun 13 03:04:58.805: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 13 03:05:03.813: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/13/23 03:05:03.813
STEP: getting scale subresource 06/13/23 03:05:03.813
STEP: updating a scale subresource 06/13/23 03:05:03.823
STEP: verifying the replicaset Spec.Replicas was modified 06/13/23 03:05:03.838
STEP: Patch a scale subresource 06/13/23 03:05:03.849
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 13 03:05:03.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6569" for this suite. 06/13/23 03:05:03.896
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":149,"skipped":2942,"failed":0}
------------------------------
• [SLOW TEST] [5.159 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:04:58.755
    Jun 13 03:04:58.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename replicaset 06/13/23 03:04:58.756
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:04:58.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:04:58.787
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 06/13/23 03:04:58.792
    Jun 13 03:04:58.805: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 13 03:05:03.813: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/13/23 03:05:03.813
    STEP: getting scale subresource 06/13/23 03:05:03.813
    STEP: updating a scale subresource 06/13/23 03:05:03.823
    STEP: verifying the replicaset Spec.Replicas was modified 06/13/23 03:05:03.838
    STEP: Patch a scale subresource 06/13/23 03:05:03.849
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 13 03:05:03.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6569" for this suite. 06/13/23 03:05:03.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:05:03.915
Jun 13 03:05:03.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename statefulset 06/13/23 03:05:03.916
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:05:03.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:05:03.96
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4841 06/13/23 03:05:03.97
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-4841 06/13/23 03:05:04.008
Jun 13 03:05:04.033: INFO: Found 0 stateful pods, waiting for 1
Jun 13 03:05:14.042: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 06/13/23 03:05:14.055
STEP: Getting /status 06/13/23 03:05:14.071
Jun 13 03:05:14.081: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 06/13/23 03:05:14.081
Jun 13 03:05:14.101: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 06/13/23 03:05:14.101
Jun 13 03:05:14.105: INFO: Observed &StatefulSet event: ADDED
Jun 13 03:05:14.105: INFO: Found Statefulset ss in namespace statefulset-4841 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 13 03:05:14.105: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 06/13/23 03:05:14.105
Jun 13 03:05:14.105: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 13 03:05:14.119: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 06/13/23 03:05:14.119
Jun 13 03:05:14.123: INFO: Observed &StatefulSet event: ADDED
Jun 13 03:05:14.123: INFO: Observed Statefulset ss in namespace statefulset-4841 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 13 03:05:14.123: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 13 03:05:14.123: INFO: Deleting all statefulset in ns statefulset-4841
Jun 13 03:05:14.128: INFO: Scaling statefulset ss to 0
Jun 13 03:05:24.166: INFO: Waiting for statefulset status.replicas updated to 0
Jun 13 03:05:24.172: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 13 03:05:24.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4841" for this suite. 06/13/23 03:05:24.211
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":150,"skipped":2965,"failed":0}
------------------------------
• [SLOW TEST] [20.306 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:05:03.915
    Jun 13 03:05:03.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename statefulset 06/13/23 03:05:03.916
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:05:03.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:05:03.96
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4841 06/13/23 03:05:03.97
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-4841 06/13/23 03:05:04.008
    Jun 13 03:05:04.033: INFO: Found 0 stateful pods, waiting for 1
    Jun 13 03:05:14.042: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 06/13/23 03:05:14.055
    STEP: Getting /status 06/13/23 03:05:14.071
    Jun 13 03:05:14.081: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 06/13/23 03:05:14.081
    Jun 13 03:05:14.101: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 06/13/23 03:05:14.101
    Jun 13 03:05:14.105: INFO: Observed &StatefulSet event: ADDED
    Jun 13 03:05:14.105: INFO: Found Statefulset ss in namespace statefulset-4841 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 13 03:05:14.105: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 06/13/23 03:05:14.105
    Jun 13 03:05:14.105: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun 13 03:05:14.119: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 06/13/23 03:05:14.119
    Jun 13 03:05:14.123: INFO: Observed &StatefulSet event: ADDED
    Jun 13 03:05:14.123: INFO: Observed Statefulset ss in namespace statefulset-4841 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 13 03:05:14.123: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 13 03:05:14.123: INFO: Deleting all statefulset in ns statefulset-4841
    Jun 13 03:05:14.128: INFO: Scaling statefulset ss to 0
    Jun 13 03:05:24.166: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 13 03:05:24.172: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 13 03:05:24.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4841" for this suite. 06/13/23 03:05:24.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:05:24.222
Jun 13 03:05:24.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename subpath 06/13/23 03:05:24.224
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:05:24.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:05:24.257
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/13/23 03:05:24.263
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-rb95 06/13/23 03:05:24.28
STEP: Creating a pod to test atomic-volume-subpath 06/13/23 03:05:24.28
Jun 13 03:05:24.337: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rb95" in namespace "subpath-1386" to be "Succeeded or Failed"
Jun 13 03:05:24.343: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Pending", Reason="", readiness=false. Elapsed: 6.235649ms
Jun 13 03:05:26.350: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 2.013327443s
Jun 13 03:05:28.352: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 4.015113704s
Jun 13 03:05:30.354: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 6.016988754s
Jun 13 03:05:32.356: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 8.018766564s
Jun 13 03:05:34.353: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 10.015819276s
Jun 13 03:05:36.350: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 12.01322276s
Jun 13 03:05:38.352: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 14.01475393s
Jun 13 03:05:40.351: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 16.014047528s
Jun 13 03:05:42.352: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 18.015224428s
Jun 13 03:05:44.353: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 20.01579609s
Jun 13 03:05:46.351: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=false. Elapsed: 22.014420114s
Jun 13 03:05:48.353: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016318897s
STEP: Saw pod success 06/13/23 03:05:48.353
Jun 13 03:05:48.354: INFO: Pod "pod-subpath-test-configmap-rb95" satisfied condition "Succeeded or Failed"
Jun 13 03:05:48.367: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-subpath-test-configmap-rb95 container test-container-subpath-configmap-rb95: <nil>
STEP: delete the pod 06/13/23 03:05:48.384
Jun 13 03:05:48.478: INFO: Waiting for pod pod-subpath-test-configmap-rb95 to disappear
Jun 13 03:05:48.491: INFO: Pod pod-subpath-test-configmap-rb95 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-rb95 06/13/23 03:05:48.492
Jun 13 03:05:48.492: INFO: Deleting pod "pod-subpath-test-configmap-rb95" in namespace "subpath-1386"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 13 03:05:48.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1386" for this suite. 06/13/23 03:05:48.511
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":151,"skipped":2974,"failed":0}
------------------------------
• [SLOW TEST] [24.334 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:05:24.222
    Jun 13 03:05:24.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename subpath 06/13/23 03:05:24.224
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:05:24.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:05:24.257
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/13/23 03:05:24.263
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-rb95 06/13/23 03:05:24.28
    STEP: Creating a pod to test atomic-volume-subpath 06/13/23 03:05:24.28
    Jun 13 03:05:24.337: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-rb95" in namespace "subpath-1386" to be "Succeeded or Failed"
    Jun 13 03:05:24.343: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Pending", Reason="", readiness=false. Elapsed: 6.235649ms
    Jun 13 03:05:26.350: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 2.013327443s
    Jun 13 03:05:28.352: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 4.015113704s
    Jun 13 03:05:30.354: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 6.016988754s
    Jun 13 03:05:32.356: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 8.018766564s
    Jun 13 03:05:34.353: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 10.015819276s
    Jun 13 03:05:36.350: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 12.01322276s
    Jun 13 03:05:38.352: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 14.01475393s
    Jun 13 03:05:40.351: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 16.014047528s
    Jun 13 03:05:42.352: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 18.015224428s
    Jun 13 03:05:44.353: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=true. Elapsed: 20.01579609s
    Jun 13 03:05:46.351: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Running", Reason="", readiness=false. Elapsed: 22.014420114s
    Jun 13 03:05:48.353: INFO: Pod "pod-subpath-test-configmap-rb95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.016318897s
    STEP: Saw pod success 06/13/23 03:05:48.353
    Jun 13 03:05:48.354: INFO: Pod "pod-subpath-test-configmap-rb95" satisfied condition "Succeeded or Failed"
    Jun 13 03:05:48.367: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-subpath-test-configmap-rb95 container test-container-subpath-configmap-rb95: <nil>
    STEP: delete the pod 06/13/23 03:05:48.384
    Jun 13 03:05:48.478: INFO: Waiting for pod pod-subpath-test-configmap-rb95 to disappear
    Jun 13 03:05:48.491: INFO: Pod pod-subpath-test-configmap-rb95 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-rb95 06/13/23 03:05:48.492
    Jun 13 03:05:48.492: INFO: Deleting pod "pod-subpath-test-configmap-rb95" in namespace "subpath-1386"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 13 03:05:48.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1386" for this suite. 06/13/23 03:05:48.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:05:48.557
Jun 13 03:05:48.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 03:05:48.559
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:05:48.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:05:48.629
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 03:05:48.663
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:05:49.054
STEP: Deploying the webhook pod 06/13/23 03:05:49.069
STEP: Wait for the deployment to be ready 06/13/23 03:05:49.092
Jun 13 03:05:49.103: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/13/23 03:05:51.179
STEP: Verifying the service has paired with the endpoint 06/13/23 03:05:51.301
Jun 13 03:05:52.302: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 06/13/23 03:05:52.31
STEP: create a pod that should be denied by the webhook 06/13/23 03:05:52.344
STEP: create a pod that causes the webhook to hang 06/13/23 03:05:52.37
STEP: create a configmap that should be denied by the webhook 06/13/23 03:06:02.385
STEP: create a configmap that should be admitted by the webhook 06/13/23 03:06:02.411
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 06/13/23 03:06:02.431
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 06/13/23 03:06:02.453
STEP: create a namespace that bypass the webhook 06/13/23 03:06:02.47
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 06/13/23 03:06:02.484
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:06:02.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2286" for this suite. 06/13/23 03:06:02.553
STEP: Destroying namespace "webhook-2286-markers" for this suite. 06/13/23 03:06:02.567
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":152,"skipped":2988,"failed":0}
------------------------------
• [SLOW TEST] [14.143 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:05:48.557
    Jun 13 03:05:48.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 03:05:48.559
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:05:48.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:05:48.629
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 03:05:48.663
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:05:49.054
    STEP: Deploying the webhook pod 06/13/23 03:05:49.069
    STEP: Wait for the deployment to be ready 06/13/23 03:05:49.092
    Jun 13 03:05:49.103: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/13/23 03:05:51.179
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:05:51.301
    Jun 13 03:05:52.302: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 06/13/23 03:05:52.31
    STEP: create a pod that should be denied by the webhook 06/13/23 03:05:52.344
    STEP: create a pod that causes the webhook to hang 06/13/23 03:05:52.37
    STEP: create a configmap that should be denied by the webhook 06/13/23 03:06:02.385
    STEP: create a configmap that should be admitted by the webhook 06/13/23 03:06:02.411
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 06/13/23 03:06:02.431
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 06/13/23 03:06:02.453
    STEP: create a namespace that bypass the webhook 06/13/23 03:06:02.47
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 06/13/23 03:06:02.484
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:06:02.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2286" for this suite. 06/13/23 03:06:02.553
    STEP: Destroying namespace "webhook-2286-markers" for this suite. 06/13/23 03:06:02.567
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:06:02.702
Jun 13 03:06:02.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:06:02.704
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:02.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:02.754
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 06/13/23 03:06:02.789
Jun 13 03:06:02.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1" in namespace "projected-5005" to be "Succeeded or Failed"
Jun 13 03:06:02.835: INFO: Pod "downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.572591ms
Jun 13 03:06:04.843: INFO: Pod "downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027387567s
Jun 13 03:06:06.844: INFO: Pod "downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027685584s
STEP: Saw pod success 06/13/23 03:06:06.844
Jun 13 03:06:06.844: INFO: Pod "downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1" satisfied condition "Succeeded or Failed"
Jun 13 03:06:06.855: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1 container client-container: <nil>
STEP: delete the pod 06/13/23 03:06:06.873
Jun 13 03:06:06.907: INFO: Waiting for pod downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1 to disappear
Jun 13 03:06:06.917: INFO: Pod downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 13 03:06:06.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5005" for this suite. 06/13/23 03:06:06.926
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":153,"skipped":3004,"failed":0}
------------------------------
• [4.248 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:06:02.702
    Jun 13 03:06:02.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:06:02.704
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:02.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:02.754
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 06/13/23 03:06:02.789
    Jun 13 03:06:02.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1" in namespace "projected-5005" to be "Succeeded or Failed"
    Jun 13 03:06:02.835: INFO: Pod "downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.572591ms
    Jun 13 03:06:04.843: INFO: Pod "downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027387567s
    Jun 13 03:06:06.844: INFO: Pod "downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027685584s
    STEP: Saw pod success 06/13/23 03:06:06.844
    Jun 13 03:06:06.844: INFO: Pod "downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1" satisfied condition "Succeeded or Failed"
    Jun 13 03:06:06.855: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1 container client-container: <nil>
    STEP: delete the pod 06/13/23 03:06:06.873
    Jun 13 03:06:06.907: INFO: Waiting for pod downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1 to disappear
    Jun 13 03:06:06.917: INFO: Pod downwardapi-volume-b97538c7-3859-48b0-92f6-907e8b5536e1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 13 03:06:06.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5005" for this suite. 06/13/23 03:06:06.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:06:06.952
Jun 13 03:06:06.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:06:06.954
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:06.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:06.995
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-65b2ecf4-3eb9-4d75-bdac-8da060af368e 06/13/23 03:06:07.006
STEP: Creating a pod to test consume secrets 06/13/23 03:06:07.027
Jun 13 03:06:07.050: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3" in namespace "projected-8347" to be "Succeeded or Failed"
Jun 13 03:06:07.057: INFO: Pod "pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.998634ms
Jun 13 03:06:09.064: INFO: Pod "pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014081189s
Jun 13 03:06:11.101: INFO: Pod "pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051278399s
Jun 13 03:06:13.066: INFO: Pod "pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015736828s
STEP: Saw pod success 06/13/23 03:06:13.066
Jun 13 03:06:13.066: INFO: Pod "pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3" satisfied condition "Succeeded or Failed"
Jun 13 03:06:13.075: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/13/23 03:06:13.087
Jun 13 03:06:13.111: INFO: Waiting for pod pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3 to disappear
Jun 13 03:06:13.117: INFO: Pod pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 13 03:06:13.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8347" for this suite. 06/13/23 03:06:13.132
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":154,"skipped":3036,"failed":0}
------------------------------
• [SLOW TEST] [6.197 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:06:06.952
    Jun 13 03:06:06.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:06:06.954
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:06.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:06.995
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-65b2ecf4-3eb9-4d75-bdac-8da060af368e 06/13/23 03:06:07.006
    STEP: Creating a pod to test consume secrets 06/13/23 03:06:07.027
    Jun 13 03:06:07.050: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3" in namespace "projected-8347" to be "Succeeded or Failed"
    Jun 13 03:06:07.057: INFO: Pod "pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.998634ms
    Jun 13 03:06:09.064: INFO: Pod "pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014081189s
    Jun 13 03:06:11.101: INFO: Pod "pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051278399s
    Jun 13 03:06:13.066: INFO: Pod "pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015736828s
    STEP: Saw pod success 06/13/23 03:06:13.066
    Jun 13 03:06:13.066: INFO: Pod "pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3" satisfied condition "Succeeded or Failed"
    Jun 13 03:06:13.075: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:06:13.087
    Jun 13 03:06:13.111: INFO: Waiting for pod pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3 to disappear
    Jun 13 03:06:13.117: INFO: Pod pod-projected-secrets-d43a40f7-5ead-469c-ad5a-670274005fb3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 13 03:06:13.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8347" for this suite. 06/13/23 03:06:13.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:06:13.152
Jun 13 03:06:13.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 03:06:13.154
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:13.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:13.198
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 03:06:13.236
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:06:13.886
STEP: Deploying the webhook pod 06/13/23 03:06:13.895
STEP: Wait for the deployment to be ready 06/13/23 03:06:13.941
Jun 13 03:06:13.961: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/13/23 03:06:15.99
STEP: Verifying the service has paired with the endpoint 06/13/23 03:06:16.046
Jun 13 03:06:17.046: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 06/13/23 03:06:17.055
STEP: create a pod 06/13/23 03:06:17.086
Jun 13 03:06:17.099: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8653" to be "running"
Jun 13 03:06:17.114: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.898867ms
Jun 13 03:06:19.127: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028429809s
Jun 13 03:06:19.127: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 06/13/23 03:06:19.127
Jun 13 03:06:19.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=webhook-8653 attach --namespace=webhook-8653 to-be-attached-pod -i -c=container1'
Jun 13 03:06:19.269: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:06:19.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8653" for this suite. 06/13/23 03:06:19.293
STEP: Destroying namespace "webhook-8653-markers" for this suite. 06/13/23 03:06:19.306
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":155,"skipped":3112,"failed":0}
------------------------------
• [SLOW TEST] [6.265 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:06:13.152
    Jun 13 03:06:13.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 03:06:13.154
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:13.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:13.198
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 03:06:13.236
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:06:13.886
    STEP: Deploying the webhook pod 06/13/23 03:06:13.895
    STEP: Wait for the deployment to be ready 06/13/23 03:06:13.941
    Jun 13 03:06:13.961: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/13/23 03:06:15.99
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:06:16.046
    Jun 13 03:06:17.046: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 06/13/23 03:06:17.055
    STEP: create a pod 06/13/23 03:06:17.086
    Jun 13 03:06:17.099: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8653" to be "running"
    Jun 13 03:06:17.114: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 14.898867ms
    Jun 13 03:06:19.127: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.028429809s
    Jun 13 03:06:19.127: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 06/13/23 03:06:19.127
    Jun 13 03:06:19.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=webhook-8653 attach --namespace=webhook-8653 to-be-attached-pod -i -c=container1'
    Jun 13 03:06:19.269: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:06:19.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8653" for this suite. 06/13/23 03:06:19.293
    STEP: Destroying namespace "webhook-8653-markers" for this suite. 06/13/23 03:06:19.306
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:06:19.424
Jun 13 03:06:19.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pods 06/13/23 03:06:19.426
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:19.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:19.468
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jun 13 03:06:19.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: creating the pod 06/13/23 03:06:19.476
STEP: submitting the pod to kubernetes 06/13/23 03:06:19.476
Jun 13 03:06:19.496: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-c9dfad6e-5b92-4d89-b995-b8c97337e2a8" in namespace "pods-7293" to be "running and ready"
Jun 13 03:06:19.503: INFO: Pod "pod-exec-websocket-c9dfad6e-5b92-4d89-b995-b8c97337e2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17286ms
Jun 13 03:06:19.503: INFO: The phase of Pod pod-exec-websocket-c9dfad6e-5b92-4d89-b995-b8c97337e2a8 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:06:21.512: INFO: Pod "pod-exec-websocket-c9dfad6e-5b92-4d89-b995-b8c97337e2a8": Phase="Running", Reason="", readiness=true. Elapsed: 2.016576121s
Jun 13 03:06:21.512: INFO: The phase of Pod pod-exec-websocket-c9dfad6e-5b92-4d89-b995-b8c97337e2a8 is Running (Ready = true)
Jun 13 03:06:21.512: INFO: Pod "pod-exec-websocket-c9dfad6e-5b92-4d89-b995-b8c97337e2a8" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 13 03:06:21.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7293" for this suite. 06/13/23 03:06:21.669
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":156,"skipped":3136,"failed":0}
------------------------------
• [2.274 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:06:19.424
    Jun 13 03:06:19.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pods 06/13/23 03:06:19.426
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:19.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:19.468
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jun 13 03:06:19.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: creating the pod 06/13/23 03:06:19.476
    STEP: submitting the pod to kubernetes 06/13/23 03:06:19.476
    Jun 13 03:06:19.496: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-c9dfad6e-5b92-4d89-b995-b8c97337e2a8" in namespace "pods-7293" to be "running and ready"
    Jun 13 03:06:19.503: INFO: Pod "pod-exec-websocket-c9dfad6e-5b92-4d89-b995-b8c97337e2a8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.17286ms
    Jun 13 03:06:19.503: INFO: The phase of Pod pod-exec-websocket-c9dfad6e-5b92-4d89-b995-b8c97337e2a8 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:06:21.512: INFO: Pod "pod-exec-websocket-c9dfad6e-5b92-4d89-b995-b8c97337e2a8": Phase="Running", Reason="", readiness=true. Elapsed: 2.016576121s
    Jun 13 03:06:21.512: INFO: The phase of Pod pod-exec-websocket-c9dfad6e-5b92-4d89-b995-b8c97337e2a8 is Running (Ready = true)
    Jun 13 03:06:21.512: INFO: Pod "pod-exec-websocket-c9dfad6e-5b92-4d89-b995-b8c97337e2a8" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 13 03:06:21.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7293" for this suite. 06/13/23 03:06:21.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:06:21.699
Jun 13 03:06:21.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:06:21.7
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:21.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:21.741
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 06/13/23 03:06:21.753
Jun 13 03:06:21.776: INFO: Waiting up to 5m0s for pod "downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7" in namespace "downward-api-791" to be "Succeeded or Failed"
Jun 13 03:06:21.784: INFO: Pod "downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.46877ms
Jun 13 03:06:23.793: INFO: Pod "downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016463187s
Jun 13 03:06:25.793: INFO: Pod "downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016177438s
STEP: Saw pod success 06/13/23 03:06:25.793
Jun 13 03:06:25.793: INFO: Pod "downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7" satisfied condition "Succeeded or Failed"
Jun 13 03:06:25.799: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7 container client-container: <nil>
STEP: delete the pod 06/13/23 03:06:25.818
Jun 13 03:06:25.844: INFO: Waiting for pod downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7 to disappear
Jun 13 03:06:25.851: INFO: Pod downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 13 03:06:25.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-791" for this suite. 06/13/23 03:06:25.862
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":157,"skipped":3157,"failed":0}
------------------------------
• [4.179 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:06:21.699
    Jun 13 03:06:21.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:06:21.7
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:21.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:21.741
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 06/13/23 03:06:21.753
    Jun 13 03:06:21.776: INFO: Waiting up to 5m0s for pod "downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7" in namespace "downward-api-791" to be "Succeeded or Failed"
    Jun 13 03:06:21.784: INFO: Pod "downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.46877ms
    Jun 13 03:06:23.793: INFO: Pod "downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016463187s
    Jun 13 03:06:25.793: INFO: Pod "downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016177438s
    STEP: Saw pod success 06/13/23 03:06:25.793
    Jun 13 03:06:25.793: INFO: Pod "downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7" satisfied condition "Succeeded or Failed"
    Jun 13 03:06:25.799: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7 container client-container: <nil>
    STEP: delete the pod 06/13/23 03:06:25.818
    Jun 13 03:06:25.844: INFO: Waiting for pod downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7 to disappear
    Jun 13 03:06:25.851: INFO: Pod downwardapi-volume-041f2743-7c68-49cc-a220-28c842d173f7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 13 03:06:25.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-791" for this suite. 06/13/23 03:06:25.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:06:25.879
Jun 13 03:06:25.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename controllerrevisions 06/13/23 03:06:25.881
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:25.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:25.919
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-4dqnt-daemon-set" 06/13/23 03:06:25.964
STEP: Check that daemon pods launch on every node of the cluster. 06/13/23 03:06:25.975
Jun 13 03:06:25.984: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:06:25.984: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:06:25.984: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:06:25.990: INFO: Number of nodes with available pods controlled by daemonset e2e-4dqnt-daemon-set: 0
Jun 13 03:06:25.990: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 03:06:27.016: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:06:27.016: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:06:27.016: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:06:27.037: INFO: Number of nodes with available pods controlled by daemonset e2e-4dqnt-daemon-set: 0
Jun 13 03:06:27.037: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 03:06:28.003: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:06:28.003: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:06:28.003: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:06:28.019: INFO: Number of nodes with available pods controlled by daemonset e2e-4dqnt-daemon-set: 3
Jun 13 03:06:28.019: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-4dqnt-daemon-set
STEP: Confirm DaemonSet "e2e-4dqnt-daemon-set" successfully created with "daemonset-name=e2e-4dqnt-daemon-set" label 06/13/23 03:06:28.029
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-4dqnt-daemon-set" 06/13/23 03:06:28.046
Jun 13 03:06:28.053: INFO: Located ControllerRevision: "e2e-4dqnt-daemon-set-5696787c9d"
STEP: Patching ControllerRevision "e2e-4dqnt-daemon-set-5696787c9d" 06/13/23 03:06:28.059
Jun 13 03:06:28.203: INFO: e2e-4dqnt-daemon-set-5696787c9d has been patched
STEP: Create a new ControllerRevision 06/13/23 03:06:28.203
Jun 13 03:06:28.216: INFO: Created ControllerRevision: e2e-4dqnt-daemon-set-cd6dcd54d
STEP: Confirm that there are two ControllerRevisions 06/13/23 03:06:28.216
Jun 13 03:06:28.216: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 13 03:06:28.223: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-4dqnt-daemon-set-5696787c9d" 06/13/23 03:06:28.223
STEP: Confirm that there is only one ControllerRevision 06/13/23 03:06:28.274
Jun 13 03:06:28.274: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 13 03:06:28.281: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-4dqnt-daemon-set-cd6dcd54d" 06/13/23 03:06:28.287
Jun 13 03:06:28.353: INFO: e2e-4dqnt-daemon-set-cd6dcd54d has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 06/13/23 03:06:28.353
W0613 03:06:28.423647      18 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 06/13/23 03:06:28.423
Jun 13 03:06:28.423: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 13 03:06:29.456: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 13 03:06:29.464: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-4dqnt-daemon-set-cd6dcd54d=updated" 06/13/23 03:06:29.464
STEP: Confirm that there is only one ControllerRevision 06/13/23 03:06:29.488
Jun 13 03:06:29.488: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 13 03:06:29.494: INFO: Found 1 ControllerRevisions
Jun 13 03:06:29.501: INFO: ControllerRevision "e2e-4dqnt-daemon-set-7fdd488db5" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-4dqnt-daemon-set" 06/13/23 03:06:29.507
STEP: deleting DaemonSet.extensions e2e-4dqnt-daemon-set in namespace controllerrevisions-9491, will wait for the garbage collector to delete the pods 06/13/23 03:06:29.508
Jun 13 03:06:29.582: INFO: Deleting DaemonSet.extensions e2e-4dqnt-daemon-set took: 17.352236ms
Jun 13 03:06:29.682: INFO: Terminating DaemonSet.extensions e2e-4dqnt-daemon-set pods took: 100.203103ms
Jun 13 03:06:30.894: INFO: Number of nodes with available pods controlled by daemonset e2e-4dqnt-daemon-set: 0
Jun 13 03:06:30.894: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-4dqnt-daemon-set
Jun 13 03:06:30.901: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29324"},"items":null}

Jun 13 03:06:30.914: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29324"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:06:30.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-9491" for this suite. 06/13/23 03:06:30.967
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":158,"skipped":3192,"failed":0}
------------------------------
• [SLOW TEST] [5.103 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:06:25.879
    Jun 13 03:06:25.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename controllerrevisions 06/13/23 03:06:25.881
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:25.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:25.919
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-4dqnt-daemon-set" 06/13/23 03:06:25.964
    STEP: Check that daemon pods launch on every node of the cluster. 06/13/23 03:06:25.975
    Jun 13 03:06:25.984: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:06:25.984: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:06:25.984: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:06:25.990: INFO: Number of nodes with available pods controlled by daemonset e2e-4dqnt-daemon-set: 0
    Jun 13 03:06:25.990: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 03:06:27.016: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:06:27.016: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:06:27.016: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:06:27.037: INFO: Number of nodes with available pods controlled by daemonset e2e-4dqnt-daemon-set: 0
    Jun 13 03:06:27.037: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 03:06:28.003: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:06:28.003: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:06:28.003: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:06:28.019: INFO: Number of nodes with available pods controlled by daemonset e2e-4dqnt-daemon-set: 3
    Jun 13 03:06:28.019: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-4dqnt-daemon-set
    STEP: Confirm DaemonSet "e2e-4dqnt-daemon-set" successfully created with "daemonset-name=e2e-4dqnt-daemon-set" label 06/13/23 03:06:28.029
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-4dqnt-daemon-set" 06/13/23 03:06:28.046
    Jun 13 03:06:28.053: INFO: Located ControllerRevision: "e2e-4dqnt-daemon-set-5696787c9d"
    STEP: Patching ControllerRevision "e2e-4dqnt-daemon-set-5696787c9d" 06/13/23 03:06:28.059
    Jun 13 03:06:28.203: INFO: e2e-4dqnt-daemon-set-5696787c9d has been patched
    STEP: Create a new ControllerRevision 06/13/23 03:06:28.203
    Jun 13 03:06:28.216: INFO: Created ControllerRevision: e2e-4dqnt-daemon-set-cd6dcd54d
    STEP: Confirm that there are two ControllerRevisions 06/13/23 03:06:28.216
    Jun 13 03:06:28.216: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 13 03:06:28.223: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-4dqnt-daemon-set-5696787c9d" 06/13/23 03:06:28.223
    STEP: Confirm that there is only one ControllerRevision 06/13/23 03:06:28.274
    Jun 13 03:06:28.274: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 13 03:06:28.281: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-4dqnt-daemon-set-cd6dcd54d" 06/13/23 03:06:28.287
    Jun 13 03:06:28.353: INFO: e2e-4dqnt-daemon-set-cd6dcd54d has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 06/13/23 03:06:28.353
    W0613 03:06:28.423647      18 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 06/13/23 03:06:28.423
    Jun 13 03:06:28.423: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 13 03:06:29.456: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 13 03:06:29.464: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-4dqnt-daemon-set-cd6dcd54d=updated" 06/13/23 03:06:29.464
    STEP: Confirm that there is only one ControllerRevision 06/13/23 03:06:29.488
    Jun 13 03:06:29.488: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 13 03:06:29.494: INFO: Found 1 ControllerRevisions
    Jun 13 03:06:29.501: INFO: ControllerRevision "e2e-4dqnt-daemon-set-7fdd488db5" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-4dqnt-daemon-set" 06/13/23 03:06:29.507
    STEP: deleting DaemonSet.extensions e2e-4dqnt-daemon-set in namespace controllerrevisions-9491, will wait for the garbage collector to delete the pods 06/13/23 03:06:29.508
    Jun 13 03:06:29.582: INFO: Deleting DaemonSet.extensions e2e-4dqnt-daemon-set took: 17.352236ms
    Jun 13 03:06:29.682: INFO: Terminating DaemonSet.extensions e2e-4dqnt-daemon-set pods took: 100.203103ms
    Jun 13 03:06:30.894: INFO: Number of nodes with available pods controlled by daemonset e2e-4dqnt-daemon-set: 0
    Jun 13 03:06:30.894: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-4dqnt-daemon-set
    Jun 13 03:06:30.901: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29324"},"items":null}

    Jun 13 03:06:30.914: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29324"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:06:30.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-9491" for this suite. 06/13/23 03:06:30.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:06:30.984
Jun 13 03:06:30.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 03:06:30.986
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:31.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:31.024
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 03:06:31.056
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:06:31.742
STEP: Deploying the webhook pod 06/13/23 03:06:31.762
STEP: Wait for the deployment to be ready 06/13/23 03:06:31.805
Jun 13 03:06:31.840: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 13 03:06:33.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 6, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 6, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 6, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 6, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/13/23 03:06:35.885
STEP: Verifying the service has paired with the endpoint 06/13/23 03:06:35.923
Jun 13 03:06:36.924: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jun 13 03:06:36.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8429-crds.webhook.example.com via the AdmissionRegistration API 06/13/23 03:06:37.458
STEP: Creating a custom resource while v1 is storage version 06/13/23 03:06:37.485
STEP: Patching Custom Resource Definition to set v2 as storage 06/13/23 03:06:39.569
STEP: Patching the custom resource while v2 is storage version 06/13/23 03:06:39.594
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:06:40.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9683" for this suite. 06/13/23 03:06:40.153
STEP: Destroying namespace "webhook-9683-markers" for this suite. 06/13/23 03:06:40.169
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":159,"skipped":3221,"failed":0}
------------------------------
• [SLOW TEST] [9.399 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:06:30.984
    Jun 13 03:06:30.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 03:06:30.986
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:31.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:31.024
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 03:06:31.056
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:06:31.742
    STEP: Deploying the webhook pod 06/13/23 03:06:31.762
    STEP: Wait for the deployment to be ready 06/13/23 03:06:31.805
    Jun 13 03:06:31.840: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 13 03:06:33.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 6, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 6, 31, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 6, 31, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 6, 31, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/13/23 03:06:35.885
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:06:35.923
    Jun 13 03:06:36.924: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jun 13 03:06:36.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8429-crds.webhook.example.com via the AdmissionRegistration API 06/13/23 03:06:37.458
    STEP: Creating a custom resource while v1 is storage version 06/13/23 03:06:37.485
    STEP: Patching Custom Resource Definition to set v2 as storage 06/13/23 03:06:39.569
    STEP: Patching the custom resource while v2 is storage version 06/13/23 03:06:39.594
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:06:40.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9683" for this suite. 06/13/23 03:06:40.153
    STEP: Destroying namespace "webhook-9683-markers" for this suite. 06/13/23 03:06:40.169
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:06:40.384
Jun 13 03:06:40.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename dns 06/13/23 03:06:40.386
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:40.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:40.514
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8609.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8609.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 06/13/23 03:06:40.521
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8609.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8609.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 06/13/23 03:06:40.522
STEP: creating a pod to probe /etc/hosts 06/13/23 03:06:40.522
STEP: submitting the pod to kubernetes 06/13/23 03:06:40.522
Jun 13 03:06:40.572: INFO: Waiting up to 15m0s for pod "dns-test-3f55ced8-aa2f-4c7b-9ee5-cf49f7d64a36" in namespace "dns-8609" to be "running"
Jun 13 03:06:40.584: INFO: Pod "dns-test-3f55ced8-aa2f-4c7b-9ee5-cf49f7d64a36": Phase="Pending", Reason="", readiness=false. Elapsed: 11.644666ms
Jun 13 03:06:42.608: INFO: Pod "dns-test-3f55ced8-aa2f-4c7b-9ee5-cf49f7d64a36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03591543s
Jun 13 03:06:44.594: INFO: Pod "dns-test-3f55ced8-aa2f-4c7b-9ee5-cf49f7d64a36": Phase="Running", Reason="", readiness=true. Elapsed: 4.022331978s
Jun 13 03:06:44.594: INFO: Pod "dns-test-3f55ced8-aa2f-4c7b-9ee5-cf49f7d64a36" satisfied condition "running"
STEP: retrieving the pod 06/13/23 03:06:44.594
STEP: looking for the results for each expected name from probers 06/13/23 03:06:44.602
Jun 13 03:06:44.649: INFO: DNS probes using dns-8609/dns-test-3f55ced8-aa2f-4c7b-9ee5-cf49f7d64a36 succeeded

STEP: deleting the pod 06/13/23 03:06:44.649
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 13 03:06:44.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8609" for this suite. 06/13/23 03:06:44.687
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":160,"skipped":3261,"failed":0}
------------------------------
• [4.347 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:06:40.384
    Jun 13 03:06:40.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename dns 06/13/23 03:06:40.386
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:40.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:40.514
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8609.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8609.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     06/13/23 03:06:40.521
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8609.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8609.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     06/13/23 03:06:40.522
    STEP: creating a pod to probe /etc/hosts 06/13/23 03:06:40.522
    STEP: submitting the pod to kubernetes 06/13/23 03:06:40.522
    Jun 13 03:06:40.572: INFO: Waiting up to 15m0s for pod "dns-test-3f55ced8-aa2f-4c7b-9ee5-cf49f7d64a36" in namespace "dns-8609" to be "running"
    Jun 13 03:06:40.584: INFO: Pod "dns-test-3f55ced8-aa2f-4c7b-9ee5-cf49f7d64a36": Phase="Pending", Reason="", readiness=false. Elapsed: 11.644666ms
    Jun 13 03:06:42.608: INFO: Pod "dns-test-3f55ced8-aa2f-4c7b-9ee5-cf49f7d64a36": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03591543s
    Jun 13 03:06:44.594: INFO: Pod "dns-test-3f55ced8-aa2f-4c7b-9ee5-cf49f7d64a36": Phase="Running", Reason="", readiness=true. Elapsed: 4.022331978s
    Jun 13 03:06:44.594: INFO: Pod "dns-test-3f55ced8-aa2f-4c7b-9ee5-cf49f7d64a36" satisfied condition "running"
    STEP: retrieving the pod 06/13/23 03:06:44.594
    STEP: looking for the results for each expected name from probers 06/13/23 03:06:44.602
    Jun 13 03:06:44.649: INFO: DNS probes using dns-8609/dns-test-3f55ced8-aa2f-4c7b-9ee5-cf49f7d64a36 succeeded

    STEP: deleting the pod 06/13/23 03:06:44.649
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 13 03:06:44.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8609" for this suite. 06/13/23 03:06:44.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:06:44.732
Jun 13 03:06:44.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename dns 06/13/23 03:06:44.733
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:44.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:44.771
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 06/13/23 03:06:44.782
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5886.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5886.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 06/13/23 03:06:44.8
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5886.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5886.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 06/13/23 03:06:44.8
STEP: creating a pod to probe DNS 06/13/23 03:06:44.8
STEP: submitting the pod to kubernetes 06/13/23 03:06:44.8
Jun 13 03:06:44.819: INFO: Waiting up to 15m0s for pod "dns-test-ed6cbe6e-15d9-452d-a076-643870e52574" in namespace "dns-5886" to be "running"
Jun 13 03:06:44.828: INFO: Pod "dns-test-ed6cbe6e-15d9-452d-a076-643870e52574": Phase="Pending", Reason="", readiness=false. Elapsed: 8.995676ms
Jun 13 03:06:46.835: INFO: Pod "dns-test-ed6cbe6e-15d9-452d-a076-643870e52574": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015942328s
Jun 13 03:06:48.877: INFO: Pod "dns-test-ed6cbe6e-15d9-452d-a076-643870e52574": Phase="Running", Reason="", readiness=true. Elapsed: 4.058233257s
Jun 13 03:06:48.877: INFO: Pod "dns-test-ed6cbe6e-15d9-452d-a076-643870e52574" satisfied condition "running"
STEP: retrieving the pod 06/13/23 03:06:48.877
STEP: looking for the results for each expected name from probers 06/13/23 03:06:48.952
Jun 13 03:06:49.374: INFO: DNS probes using dns-5886/dns-test-ed6cbe6e-15d9-452d-a076-643870e52574 succeeded

STEP: deleting the pod 06/13/23 03:06:49.374
STEP: deleting the test headless service 06/13/23 03:06:49.622
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 13 03:06:49.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5886" for this suite. 06/13/23 03:06:50.008
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":161,"skipped":3273,"failed":0}
------------------------------
• [SLOW TEST] [5.314 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:06:44.732
    Jun 13 03:06:44.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename dns 06/13/23 03:06:44.733
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:44.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:44.771
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 06/13/23 03:06:44.782
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5886.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-5886.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     06/13/23 03:06:44.8
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-5886.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-5886.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     06/13/23 03:06:44.8
    STEP: creating a pod to probe DNS 06/13/23 03:06:44.8
    STEP: submitting the pod to kubernetes 06/13/23 03:06:44.8
    Jun 13 03:06:44.819: INFO: Waiting up to 15m0s for pod "dns-test-ed6cbe6e-15d9-452d-a076-643870e52574" in namespace "dns-5886" to be "running"
    Jun 13 03:06:44.828: INFO: Pod "dns-test-ed6cbe6e-15d9-452d-a076-643870e52574": Phase="Pending", Reason="", readiness=false. Elapsed: 8.995676ms
    Jun 13 03:06:46.835: INFO: Pod "dns-test-ed6cbe6e-15d9-452d-a076-643870e52574": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015942328s
    Jun 13 03:06:48.877: INFO: Pod "dns-test-ed6cbe6e-15d9-452d-a076-643870e52574": Phase="Running", Reason="", readiness=true. Elapsed: 4.058233257s
    Jun 13 03:06:48.877: INFO: Pod "dns-test-ed6cbe6e-15d9-452d-a076-643870e52574" satisfied condition "running"
    STEP: retrieving the pod 06/13/23 03:06:48.877
    STEP: looking for the results for each expected name from probers 06/13/23 03:06:48.952
    Jun 13 03:06:49.374: INFO: DNS probes using dns-5886/dns-test-ed6cbe6e-15d9-452d-a076-643870e52574 succeeded

    STEP: deleting the pod 06/13/23 03:06:49.374
    STEP: deleting the test headless service 06/13/23 03:06:49.622
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 13 03:06:49.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5886" for this suite. 06/13/23 03:06:50.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:06:50.047
Jun 13 03:06:50.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename resourcequota 06/13/23 03:06:50.048
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:50.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:50.105
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 06/13/23 03:06:50.123
STEP: Creating a ResourceQuota 06/13/23 03:06:55.131
STEP: Ensuring resource quota status is calculated 06/13/23 03:06:55.141
STEP: Creating a ReplicationController 06/13/23 03:06:57.152
STEP: Ensuring resource quota status captures replication controller creation 06/13/23 03:06:57.179
STEP: Deleting a ReplicationController 06/13/23 03:06:59.21
STEP: Ensuring resource quota status released usage 06/13/23 03:06:59.282
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 13 03:07:01.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6911" for this suite. 06/13/23 03:07:01.302
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":162,"skipped":3280,"failed":0}
------------------------------
• [SLOW TEST] [11.269 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:06:50.047
    Jun 13 03:06:50.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename resourcequota 06/13/23 03:06:50.048
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:06:50.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:06:50.105
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 06/13/23 03:06:50.123
    STEP: Creating a ResourceQuota 06/13/23 03:06:55.131
    STEP: Ensuring resource quota status is calculated 06/13/23 03:06:55.141
    STEP: Creating a ReplicationController 06/13/23 03:06:57.152
    STEP: Ensuring resource quota status captures replication controller creation 06/13/23 03:06:57.179
    STEP: Deleting a ReplicationController 06/13/23 03:06:59.21
    STEP: Ensuring resource quota status released usage 06/13/23 03:06:59.282
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 13 03:07:01.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6911" for this suite. 06/13/23 03:07:01.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:07:01.317
Jun 13 03:07:01.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename replicaset 06/13/23 03:07:01.318
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:01.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:01.363
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jun 13 03:07:01.384: INFO: Creating ReplicaSet my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2
Jun 13 03:07:01.409: INFO: Pod name my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2: Found 0 pods out of 1
Jun 13 03:07:06.430: INFO: Pod name my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2: Found 1 pods out of 1
Jun 13 03:07:06.430: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2" is running
Jun 13 03:07:06.430: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2-55rrn" in namespace "replicaset-8530" to be "running"
Jun 13 03:07:06.437: INFO: Pod "my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2-55rrn": Phase="Running", Reason="", readiness=true. Elapsed: 6.753502ms
Jun 13 03:07:06.437: INFO: Pod "my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2-55rrn" satisfied condition "running"
Jun 13 03:07:06.437: INFO: Pod "my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2-55rrn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:07:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:07:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:07:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:07:01 +0000 UTC Reason: Message:}])
Jun 13 03:07:06.437: INFO: Trying to dial the pod
Jun 13 03:07:11.491: INFO: Controller my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2: Got expected result from replica 1 [my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2-55rrn]: "my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2-55rrn", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 13 03:07:11.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8530" for this suite. 06/13/23 03:07:11.521
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":163,"skipped":3304,"failed":0}
------------------------------
• [SLOW TEST] [10.221 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:07:01.317
    Jun 13 03:07:01.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename replicaset 06/13/23 03:07:01.318
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:01.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:01.363
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jun 13 03:07:01.384: INFO: Creating ReplicaSet my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2
    Jun 13 03:07:01.409: INFO: Pod name my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2: Found 0 pods out of 1
    Jun 13 03:07:06.430: INFO: Pod name my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2: Found 1 pods out of 1
    Jun 13 03:07:06.430: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2" is running
    Jun 13 03:07:06.430: INFO: Waiting up to 5m0s for pod "my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2-55rrn" in namespace "replicaset-8530" to be "running"
    Jun 13 03:07:06.437: INFO: Pod "my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2-55rrn": Phase="Running", Reason="", readiness=true. Elapsed: 6.753502ms
    Jun 13 03:07:06.437: INFO: Pod "my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2-55rrn" satisfied condition "running"
    Jun 13 03:07:06.437: INFO: Pod "my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2-55rrn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:07:01 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:07:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:07:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:07:01 +0000 UTC Reason: Message:}])
    Jun 13 03:07:06.437: INFO: Trying to dial the pod
    Jun 13 03:07:11.491: INFO: Controller my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2: Got expected result from replica 1 [my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2-55rrn]: "my-hostname-basic-f8b3ce6c-ac0a-463c-a494-ef9dabd5aec2-55rrn", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 13 03:07:11.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8530" for this suite. 06/13/23 03:07:11.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:07:11.539
Jun 13 03:07:11.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename security-context-test 06/13/23 03:07:11.541
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:11.569
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:11.584
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jun 13 03:07:11.617: INFO: Waiting up to 5m0s for pod "busybox-user-65534-eca6e2c6-912e-4d6c-8f70-7328092eb31c" in namespace "security-context-test-6580" to be "Succeeded or Failed"
Jun 13 03:07:11.628: INFO: Pod "busybox-user-65534-eca6e2c6-912e-4d6c-8f70-7328092eb31c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.671973ms
Jun 13 03:07:13.642: INFO: Pod "busybox-user-65534-eca6e2c6-912e-4d6c-8f70-7328092eb31c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024401871s
Jun 13 03:07:15.636: INFO: Pod "busybox-user-65534-eca6e2c6-912e-4d6c-8f70-7328092eb31c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019266626s
Jun 13 03:07:17.636: INFO: Pod "busybox-user-65534-eca6e2c6-912e-4d6c-8f70-7328092eb31c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019030613s
Jun 13 03:07:17.636: INFO: Pod "busybox-user-65534-eca6e2c6-912e-4d6c-8f70-7328092eb31c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 13 03:07:17.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-6580" for this suite. 06/13/23 03:07:17.648
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":164,"skipped":3316,"failed":0}
------------------------------
• [SLOW TEST] [6.122 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:07:11.539
    Jun 13 03:07:11.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename security-context-test 06/13/23 03:07:11.541
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:11.569
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:11.584
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jun 13 03:07:11.617: INFO: Waiting up to 5m0s for pod "busybox-user-65534-eca6e2c6-912e-4d6c-8f70-7328092eb31c" in namespace "security-context-test-6580" to be "Succeeded or Failed"
    Jun 13 03:07:11.628: INFO: Pod "busybox-user-65534-eca6e2c6-912e-4d6c-8f70-7328092eb31c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.671973ms
    Jun 13 03:07:13.642: INFO: Pod "busybox-user-65534-eca6e2c6-912e-4d6c-8f70-7328092eb31c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024401871s
    Jun 13 03:07:15.636: INFO: Pod "busybox-user-65534-eca6e2c6-912e-4d6c-8f70-7328092eb31c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019266626s
    Jun 13 03:07:17.636: INFO: Pod "busybox-user-65534-eca6e2c6-912e-4d6c-8f70-7328092eb31c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019030613s
    Jun 13 03:07:17.636: INFO: Pod "busybox-user-65534-eca6e2c6-912e-4d6c-8f70-7328092eb31c" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 13 03:07:17.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-6580" for this suite. 06/13/23 03:07:17.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:07:17.665
Jun 13 03:07:17.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename security-context-test 06/13/23 03:07:17.666
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:17.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:17.711
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jun 13 03:07:17.753: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-a7823bb9-1c75-4705-9917-7af009d934fb" in namespace "security-context-test-2777" to be "Succeeded or Failed"
Jun 13 03:07:17.766: INFO: Pod "busybox-readonly-false-a7823bb9-1c75-4705-9917-7af009d934fb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.488165ms
Jun 13 03:07:19.809: INFO: Pod "busybox-readonly-false-a7823bb9-1c75-4705-9917-7af009d934fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056065492s
Jun 13 03:07:21.777: INFO: Pod "busybox-readonly-false-a7823bb9-1c75-4705-9917-7af009d934fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024112184s
Jun 13 03:07:21.777: INFO: Pod "busybox-readonly-false-a7823bb9-1c75-4705-9917-7af009d934fb" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 13 03:07:21.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2777" for this suite. 06/13/23 03:07:21.788
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":165,"skipped":3382,"failed":0}
------------------------------
• [4.152 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:07:17.665
    Jun 13 03:07:17.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename security-context-test 06/13/23 03:07:17.666
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:17.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:17.711
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jun 13 03:07:17.753: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-a7823bb9-1c75-4705-9917-7af009d934fb" in namespace "security-context-test-2777" to be "Succeeded or Failed"
    Jun 13 03:07:17.766: INFO: Pod "busybox-readonly-false-a7823bb9-1c75-4705-9917-7af009d934fb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.488165ms
    Jun 13 03:07:19.809: INFO: Pod "busybox-readonly-false-a7823bb9-1c75-4705-9917-7af009d934fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056065492s
    Jun 13 03:07:21.777: INFO: Pod "busybox-readonly-false-a7823bb9-1c75-4705-9917-7af009d934fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024112184s
    Jun 13 03:07:21.777: INFO: Pod "busybox-readonly-false-a7823bb9-1c75-4705-9917-7af009d934fb" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 13 03:07:21.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2777" for this suite. 06/13/23 03:07:21.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:07:21.818
Jun 13 03:07:21.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:07:21.82
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:21.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:21.854
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-474df100-4727-4226-88d0-0c9a60de6495 06/13/23 03:07:21.864
STEP: Creating secret with name secret-projected-all-test-volume-d3d4c075-fc51-4f5f-a6b6-031afe430f4f 06/13/23 03:07:21.872
STEP: Creating a pod to test Check all projections for projected volume plugin 06/13/23 03:07:21.883
Jun 13 03:07:21.900: INFO: Waiting up to 5m0s for pod "projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b" in namespace "projected-3297" to be "Succeeded or Failed"
Jun 13 03:07:21.907: INFO: Pod "projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.39833ms
Jun 13 03:07:23.916: INFO: Pod "projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016266054s
Jun 13 03:07:25.915: INFO: Pod "projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015436289s
Jun 13 03:07:27.915: INFO: Pod "projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01567521s
STEP: Saw pod success 06/13/23 03:07:27.915
Jun 13 03:07:27.916: INFO: Pod "projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b" satisfied condition "Succeeded or Failed"
Jun 13 03:07:27.923: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b container projected-all-volume-test: <nil>
STEP: delete the pod 06/13/23 03:07:27.937
Jun 13 03:07:27.960: INFO: Waiting for pod projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b to disappear
Jun 13 03:07:27.967: INFO: Pod projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jun 13 03:07:27.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3297" for this suite. 06/13/23 03:07:27.984
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":166,"skipped":3402,"failed":0}
------------------------------
• [SLOW TEST] [6.177 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:07:21.818
    Jun 13 03:07:21.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:07:21.82
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:21.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:21.854
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-474df100-4727-4226-88d0-0c9a60de6495 06/13/23 03:07:21.864
    STEP: Creating secret with name secret-projected-all-test-volume-d3d4c075-fc51-4f5f-a6b6-031afe430f4f 06/13/23 03:07:21.872
    STEP: Creating a pod to test Check all projections for projected volume plugin 06/13/23 03:07:21.883
    Jun 13 03:07:21.900: INFO: Waiting up to 5m0s for pod "projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b" in namespace "projected-3297" to be "Succeeded or Failed"
    Jun 13 03:07:21.907: INFO: Pod "projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.39833ms
    Jun 13 03:07:23.916: INFO: Pod "projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016266054s
    Jun 13 03:07:25.915: INFO: Pod "projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015436289s
    Jun 13 03:07:27.915: INFO: Pod "projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01567521s
    STEP: Saw pod success 06/13/23 03:07:27.915
    Jun 13 03:07:27.916: INFO: Pod "projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b" satisfied condition "Succeeded or Failed"
    Jun 13 03:07:27.923: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b container projected-all-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:07:27.937
    Jun 13 03:07:27.960: INFO: Waiting for pod projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b to disappear
    Jun 13 03:07:27.967: INFO: Pod projected-volume-5a7b5089-c311-4bc8-acf7-8eaab30a0f8b no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jun 13 03:07:27.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3297" for this suite. 06/13/23 03:07:27.984
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:07:27.996
Jun 13 03:07:27.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-lifecycle-hook 06/13/23 03:07:27.997
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:28.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:28.036
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 06/13/23 03:07:28.056
Jun 13 03:07:28.114: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8545" to be "running and ready"
Jun 13 03:07:28.132: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.224709ms
Jun 13 03:07:28.132: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:07:30.145: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.030925293s
Jun 13 03:07:30.145: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 13 03:07:30.145: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 06/13/23 03:07:30.153
Jun 13 03:07:30.298: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8545" to be "running and ready"
Jun 13 03:07:30.304: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.820735ms
Jun 13 03:07:30.304: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:07:32.312: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01400784s
Jun 13 03:07:32.312: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jun 13 03:07:32.312: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 06/13/23 03:07:32.319
Jun 13 03:07:32.336: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 13 03:07:32.346: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 13 03:07:34.347: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 13 03:07:34.353: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 13 03:07:36.347: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 13 03:07:36.356: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 06/13/23 03:07:36.356
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jun 13 03:07:36.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8545" for this suite. 06/13/23 03:07:36.395
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":167,"skipped":3403,"failed":0}
------------------------------
• [SLOW TEST] [8.414 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:07:27.996
    Jun 13 03:07:27.996: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/13/23 03:07:27.997
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:28.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:28.036
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 06/13/23 03:07:28.056
    Jun 13 03:07:28.114: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8545" to be "running and ready"
    Jun 13 03:07:28.132: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 18.224709ms
    Jun 13 03:07:28.132: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:07:30.145: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.030925293s
    Jun 13 03:07:30.145: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 13 03:07:30.145: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 06/13/23 03:07:30.153
    Jun 13 03:07:30.298: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8545" to be "running and ready"
    Jun 13 03:07:30.304: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.820735ms
    Jun 13 03:07:30.304: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:07:32.312: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.01400784s
    Jun 13 03:07:32.312: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jun 13 03:07:32.312: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 06/13/23 03:07:32.319
    Jun 13 03:07:32.336: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun 13 03:07:32.346: INFO: Pod pod-with-prestop-exec-hook still exists
    Jun 13 03:07:34.347: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun 13 03:07:34.353: INFO: Pod pod-with-prestop-exec-hook still exists
    Jun 13 03:07:36.347: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun 13 03:07:36.356: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 06/13/23 03:07:36.356
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jun 13 03:07:36.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8545" for this suite. 06/13/23 03:07:36.395
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:07:36.411
Jun 13 03:07:36.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:07:36.412
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:36.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:36.447
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4678 06/13/23 03:07:36.456
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/13/23 03:07:36.501
STEP: creating service externalsvc in namespace services-4678 06/13/23 03:07:36.501
STEP: creating replication controller externalsvc in namespace services-4678 06/13/23 03:07:36.547
I0613 03:07:36.565105      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4678, replica count: 2
I0613 03:07:39.616763      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 06/13/23 03:07:39.682
Jun 13 03:07:39.759: INFO: Creating new exec pod
Jun 13 03:07:39.770: INFO: Waiting up to 5m0s for pod "execpoddlhxw" in namespace "services-4678" to be "running"
Jun 13 03:07:39.780: INFO: Pod "execpoddlhxw": Phase="Pending", Reason="", readiness=false. Elapsed: 9.256819ms
Jun 13 03:07:41.789: INFO: Pod "execpoddlhxw": Phase="Running", Reason="", readiness=true. Elapsed: 2.018686286s
Jun 13 03:07:41.789: INFO: Pod "execpoddlhxw" satisfied condition "running"
Jun 13 03:07:41.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4678 exec execpoddlhxw -- /bin/sh -x -c nslookup clusterip-service.services-4678.svc.cluster.local'
Jun 13 03:07:42.076: INFO: stderr: "+ nslookup clusterip-service.services-4678.svc.cluster.local\n"
Jun 13 03:07:42.076: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-4678.svc.cluster.local\tcanonical name = externalsvc.services-4678.svc.cluster.local.\nName:\texternalsvc.services-4678.svc.cluster.local\nAddress: 10.96.186.85\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4678, will wait for the garbage collector to delete the pods 06/13/23 03:07:42.076
Jun 13 03:07:42.161: INFO: Deleting ReplicationController externalsvc took: 13.058337ms
Jun 13 03:07:42.262: INFO: Terminating ReplicationController externalsvc pods took: 101.185432ms
Jun 13 03:07:44.804: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:07:44.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4678" for this suite. 06/13/23 03:07:44.839
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":168,"skipped":3406,"failed":0}
------------------------------
• [SLOW TEST] [8.448 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:07:36.411
    Jun 13 03:07:36.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:07:36.412
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:36.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:36.447
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-4678 06/13/23 03:07:36.456
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/13/23 03:07:36.501
    STEP: creating service externalsvc in namespace services-4678 06/13/23 03:07:36.501
    STEP: creating replication controller externalsvc in namespace services-4678 06/13/23 03:07:36.547
    I0613 03:07:36.565105      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4678, replica count: 2
    I0613 03:07:39.616763      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 06/13/23 03:07:39.682
    Jun 13 03:07:39.759: INFO: Creating new exec pod
    Jun 13 03:07:39.770: INFO: Waiting up to 5m0s for pod "execpoddlhxw" in namespace "services-4678" to be "running"
    Jun 13 03:07:39.780: INFO: Pod "execpoddlhxw": Phase="Pending", Reason="", readiness=false. Elapsed: 9.256819ms
    Jun 13 03:07:41.789: INFO: Pod "execpoddlhxw": Phase="Running", Reason="", readiness=true. Elapsed: 2.018686286s
    Jun 13 03:07:41.789: INFO: Pod "execpoddlhxw" satisfied condition "running"
    Jun 13 03:07:41.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4678 exec execpoddlhxw -- /bin/sh -x -c nslookup clusterip-service.services-4678.svc.cluster.local'
    Jun 13 03:07:42.076: INFO: stderr: "+ nslookup clusterip-service.services-4678.svc.cluster.local\n"
    Jun 13 03:07:42.076: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-4678.svc.cluster.local\tcanonical name = externalsvc.services-4678.svc.cluster.local.\nName:\texternalsvc.services-4678.svc.cluster.local\nAddress: 10.96.186.85\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-4678, will wait for the garbage collector to delete the pods 06/13/23 03:07:42.076
    Jun 13 03:07:42.161: INFO: Deleting ReplicationController externalsvc took: 13.058337ms
    Jun 13 03:07:42.262: INFO: Terminating ReplicationController externalsvc pods took: 101.185432ms
    Jun 13 03:07:44.804: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:07:44.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4678" for this suite. 06/13/23 03:07:44.839
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:07:44.86
Jun 13 03:07:44.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename daemonsets 06/13/23 03:07:44.861
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:44.893
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:44.9
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 06/13/23 03:07:44.979
STEP: Check that daemon pods launch on every node of the cluster. 06/13/23 03:07:44.993
Jun 13 03:07:45.011: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:07:45.011: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:07:45.011: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:07:45.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:07:45.022: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 03:07:46.050: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:07:46.050: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:07:46.050: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:07:46.060: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:07:46.060: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 03:07:47.034: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:07:47.035: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:07:47.035: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:07:47.045: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 13 03:07:47.045: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
Jun 13 03:07:48.037: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:07:48.037: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:07:48.037: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:07:48.046: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 13 03:07:48.046: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 06/13/23 03:07:48.054
Jun 13 03:07:48.076: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 06/13/23 03:07:48.076
Jun 13 03:07:48.105: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 06/13/23 03:07:48.105
Jun 13 03:07:48.109: INFO: Observed &DaemonSet event: ADDED
Jun 13 03:07:48.109: INFO: Observed &DaemonSet event: MODIFIED
Jun 13 03:07:48.109: INFO: Observed &DaemonSet event: MODIFIED
Jun 13 03:07:48.109: INFO: Observed &DaemonSet event: MODIFIED
Jun 13 03:07:48.110: INFO: Observed &DaemonSet event: MODIFIED
Jun 13 03:07:48.110: INFO: Found daemon set daemon-set in namespace daemonsets-8998 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 13 03:07:48.110: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 06/13/23 03:07:48.11
STEP: watching for the daemon set status to be patched 06/13/23 03:07:48.124
Jun 13 03:07:48.127: INFO: Observed &DaemonSet event: ADDED
Jun 13 03:07:48.128: INFO: Observed &DaemonSet event: MODIFIED
Jun 13 03:07:48.128: INFO: Observed &DaemonSet event: MODIFIED
Jun 13 03:07:48.128: INFO: Observed &DaemonSet event: MODIFIED
Jun 13 03:07:48.128: INFO: Observed &DaemonSet event: MODIFIED
Jun 13 03:07:48.128: INFO: Observed daemon set daemon-set in namespace daemonsets-8998 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 13 03:07:48.128: INFO: Observed &DaemonSet event: MODIFIED
Jun 13 03:07:48.128: INFO: Found daemon set daemon-set in namespace daemonsets-8998 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jun 13 03:07:48.129: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/13/23 03:07:48.142
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8998, will wait for the garbage collector to delete the pods 06/13/23 03:07:48.142
Jun 13 03:07:48.217: INFO: Deleting DaemonSet.extensions daemon-set took: 13.837276ms
Jun 13 03:07:48.317: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.545183ms
Jun 13 03:07:51.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:07:51.226: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 13 03:07:51.232: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30261"},"items":null}

Jun 13 03:07:51.238: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30261"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:07:51.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8998" for this suite. 06/13/23 03:07:51.282
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":169,"skipped":3424,"failed":0}
------------------------------
• [SLOW TEST] [6.436 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:07:44.86
    Jun 13 03:07:44.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename daemonsets 06/13/23 03:07:44.861
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:44.893
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:44.9
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 06/13/23 03:07:44.979
    STEP: Check that daemon pods launch on every node of the cluster. 06/13/23 03:07:44.993
    Jun 13 03:07:45.011: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:07:45.011: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:07:45.011: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:07:45.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:07:45.022: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 03:07:46.050: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:07:46.050: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:07:46.050: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:07:46.060: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:07:46.060: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 03:07:47.034: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:07:47.035: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:07:47.035: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:07:47.045: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 13 03:07:47.045: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
    Jun 13 03:07:48.037: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:07:48.037: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:07:48.037: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:07:48.046: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 13 03:07:48.046: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 06/13/23 03:07:48.054
    Jun 13 03:07:48.076: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 06/13/23 03:07:48.076
    Jun 13 03:07:48.105: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 06/13/23 03:07:48.105
    Jun 13 03:07:48.109: INFO: Observed &DaemonSet event: ADDED
    Jun 13 03:07:48.109: INFO: Observed &DaemonSet event: MODIFIED
    Jun 13 03:07:48.109: INFO: Observed &DaemonSet event: MODIFIED
    Jun 13 03:07:48.109: INFO: Observed &DaemonSet event: MODIFIED
    Jun 13 03:07:48.110: INFO: Observed &DaemonSet event: MODIFIED
    Jun 13 03:07:48.110: INFO: Found daemon set daemon-set in namespace daemonsets-8998 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 13 03:07:48.110: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 06/13/23 03:07:48.11
    STEP: watching for the daemon set status to be patched 06/13/23 03:07:48.124
    Jun 13 03:07:48.127: INFO: Observed &DaemonSet event: ADDED
    Jun 13 03:07:48.128: INFO: Observed &DaemonSet event: MODIFIED
    Jun 13 03:07:48.128: INFO: Observed &DaemonSet event: MODIFIED
    Jun 13 03:07:48.128: INFO: Observed &DaemonSet event: MODIFIED
    Jun 13 03:07:48.128: INFO: Observed &DaemonSet event: MODIFIED
    Jun 13 03:07:48.128: INFO: Observed daemon set daemon-set in namespace daemonsets-8998 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 13 03:07:48.128: INFO: Observed &DaemonSet event: MODIFIED
    Jun 13 03:07:48.128: INFO: Found daemon set daemon-set in namespace daemonsets-8998 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jun 13 03:07:48.129: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/13/23 03:07:48.142
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8998, will wait for the garbage collector to delete the pods 06/13/23 03:07:48.142
    Jun 13 03:07:48.217: INFO: Deleting DaemonSet.extensions daemon-set took: 13.837276ms
    Jun 13 03:07:48.317: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.545183ms
    Jun 13 03:07:51.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:07:51.226: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 13 03:07:51.232: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30261"},"items":null}

    Jun 13 03:07:51.238: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30261"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:07:51.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-8998" for this suite. 06/13/23 03:07:51.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:07:51.298
Jun 13 03:07:51.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-runtime 06/13/23 03:07:51.3
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:51.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:51.336
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 06/13/23 03:07:51.344
STEP: wait for the container to reach Succeeded 06/13/23 03:07:51.366
STEP: get the container status 06/13/23 03:07:55.419
STEP: the container should be terminated 06/13/23 03:07:55.426
STEP: the termination message should be set 06/13/23 03:07:55.426
Jun 13 03:07:55.426: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 06/13/23 03:07:55.426
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 13 03:07:55.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7304" for this suite. 06/13/23 03:07:55.478
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":170,"skipped":3439,"failed":0}
------------------------------
• [4.200 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:07:51.298
    Jun 13 03:07:51.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-runtime 06/13/23 03:07:51.3
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:51.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:51.336
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 06/13/23 03:07:51.344
    STEP: wait for the container to reach Succeeded 06/13/23 03:07:51.366
    STEP: get the container status 06/13/23 03:07:55.419
    STEP: the container should be terminated 06/13/23 03:07:55.426
    STEP: the termination message should be set 06/13/23 03:07:55.426
    Jun 13 03:07:55.426: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 06/13/23 03:07:55.426
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 13 03:07:55.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7304" for this suite. 06/13/23 03:07:55.478
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:07:55.498
Jun 13 03:07:55.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:07:55.501
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:55.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:55.544
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 06/13/23 03:07:55.553
Jun 13 03:07:55.569: INFO: Waiting up to 5m0s for pod "downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145" in namespace "downward-api-2784" to be "Succeeded or Failed"
Jun 13 03:07:55.579: INFO: Pod "downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145": Phase="Pending", Reason="", readiness=false. Elapsed: 9.458857ms
Jun 13 03:07:57.586: INFO: Pod "downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01666261s
Jun 13 03:07:59.595: INFO: Pod "downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025743375s
STEP: Saw pod success 06/13/23 03:07:59.595
Jun 13 03:07:59.595: INFO: Pod "downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145" satisfied condition "Succeeded or Failed"
Jun 13 03:07:59.605: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145 container client-container: <nil>
STEP: delete the pod 06/13/23 03:07:59.638
Jun 13 03:07:59.886: INFO: Waiting for pod downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145 to disappear
Jun 13 03:07:59.895: INFO: Pod downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 13 03:07:59.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2784" for this suite. 06/13/23 03:07:59.904
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":171,"skipped":3447,"failed":0}
------------------------------
• [4.454 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:07:55.498
    Jun 13 03:07:55.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:07:55.501
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:07:55.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:07:55.544
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 06/13/23 03:07:55.553
    Jun 13 03:07:55.569: INFO: Waiting up to 5m0s for pod "downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145" in namespace "downward-api-2784" to be "Succeeded or Failed"
    Jun 13 03:07:55.579: INFO: Pod "downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145": Phase="Pending", Reason="", readiness=false. Elapsed: 9.458857ms
    Jun 13 03:07:57.586: INFO: Pod "downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01666261s
    Jun 13 03:07:59.595: INFO: Pod "downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025743375s
    STEP: Saw pod success 06/13/23 03:07:59.595
    Jun 13 03:07:59.595: INFO: Pod "downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145" satisfied condition "Succeeded or Failed"
    Jun 13 03:07:59.605: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145 container client-container: <nil>
    STEP: delete the pod 06/13/23 03:07:59.638
    Jun 13 03:07:59.886: INFO: Waiting for pod downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145 to disappear
    Jun 13 03:07:59.895: INFO: Pod downwardapi-volume-425046d4-7e2b-4b4c-b591-cc04540c2145 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 13 03:07:59.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2784" for this suite. 06/13/23 03:07:59.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:07:59.955
Jun 13 03:07:59.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-runtime 06/13/23 03:07:59.956
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:08:00.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:08:00.116
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 06/13/23 03:08:00.135
STEP: wait for the container to reach Failed 06/13/23 03:08:00.164
STEP: get the container status 06/13/23 03:08:04.278
STEP: the container should be terminated 06/13/23 03:08:04.29
STEP: the termination message should be set 06/13/23 03:08:04.29
Jun 13 03:08:04.290: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 06/13/23 03:08:04.29
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 13 03:08:04.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8050" for this suite. 06/13/23 03:08:04.349
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":172,"skipped":3489,"failed":0}
------------------------------
• [4.407 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:07:59.955
    Jun 13 03:07:59.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-runtime 06/13/23 03:07:59.956
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:08:00.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:08:00.116
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 06/13/23 03:08:00.135
    STEP: wait for the container to reach Failed 06/13/23 03:08:00.164
    STEP: get the container status 06/13/23 03:08:04.278
    STEP: the container should be terminated 06/13/23 03:08:04.29
    STEP: the termination message should be set 06/13/23 03:08:04.29
    Jun 13 03:08:04.290: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 06/13/23 03:08:04.29
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 13 03:08:04.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8050" for this suite. 06/13/23 03:08:04.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:08:04.362
Jun 13 03:08:04.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename conformance-tests 06/13/23 03:08:04.369
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:08:04.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:08:04.4
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 06/13/23 03:08:04.407
Jun 13 03:08:04.408: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jun 13 03:08:04.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-1487" for this suite. 06/13/23 03:08:04.431
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":173,"skipped":3502,"failed":0}
------------------------------
• [0.085 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:08:04.362
    Jun 13 03:08:04.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename conformance-tests 06/13/23 03:08:04.369
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:08:04.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:08:04.4
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 06/13/23 03:08:04.407
    Jun 13 03:08:04.408: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jun 13 03:08:04.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-1487" for this suite. 06/13/23 03:08:04.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:08:04.449
Jun 13 03:08:04.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename replicaset 06/13/23 03:08:04.451
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:08:04.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:08:04.489
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 06/13/23 03:08:04.496
Jun 13 03:08:04.509: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-8031" to be "running and ready"
Jun 13 03:08:04.516: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.219259ms
Jun 13 03:08:04.516: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:08:06.524: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.015018831s
Jun 13 03:08:06.524: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jun 13 03:08:06.524: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 06/13/23 03:08:06.53
STEP: Then the orphan pod is adopted 06/13/23 03:08:06.542
STEP: When the matched label of one of its pods change 06/13/23 03:08:07.556
Jun 13 03:08:07.566: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 06/13/23 03:08:07.586
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 13 03:08:08.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8031" for this suite. 06/13/23 03:08:08.619
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":174,"skipped":3535,"failed":0}
------------------------------
• [4.185 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:08:04.449
    Jun 13 03:08:04.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename replicaset 06/13/23 03:08:04.451
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:08:04.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:08:04.489
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 06/13/23 03:08:04.496
    Jun 13 03:08:04.509: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-8031" to be "running and ready"
    Jun 13 03:08:04.516: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 7.219259ms
    Jun 13 03:08:04.516: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:08:06.524: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.015018831s
    Jun 13 03:08:06.524: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jun 13 03:08:06.524: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 06/13/23 03:08:06.53
    STEP: Then the orphan pod is adopted 06/13/23 03:08:06.542
    STEP: When the matched label of one of its pods change 06/13/23 03:08:07.556
    Jun 13 03:08:07.566: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 06/13/23 03:08:07.586
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 13 03:08:08.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8031" for this suite. 06/13/23 03:08:08.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:08:08.635
Jun 13 03:08:08.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-probe 06/13/23 03:08:08.638
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:08:08.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:08:08.686
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jun 13 03:08:08.719: INFO: Waiting up to 5m0s for pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a" in namespace "container-probe-1947" to be "running and ready"
Jun 13 03:08:08.748: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.76895ms
Jun 13 03:08:08.748: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:08:10.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 2.035644281s
Jun 13 03:08:10.755: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
Jun 13 03:08:12.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.035575008s
Jun 13 03:08:12.755: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
Jun 13 03:08:14.787: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 6.067983438s
Jun 13 03:08:14.787: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
Jun 13 03:08:16.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 8.035297808s
Jun 13 03:08:16.755: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
Jun 13 03:08:18.757: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 10.037680996s
Jun 13 03:08:18.757: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
Jun 13 03:08:20.769: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 12.049958248s
Jun 13 03:08:20.769: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
Jun 13 03:08:22.817: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 14.097293226s
Jun 13 03:08:22.817: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
Jun 13 03:08:24.756: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 16.036904473s
Jun 13 03:08:24.756: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
Jun 13 03:08:26.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 18.035623174s
Jun 13 03:08:26.755: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
Jun 13 03:08:28.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 20.035519907s
Jun 13 03:08:28.755: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
Jun 13 03:08:30.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=true. Elapsed: 22.035767374s
Jun 13 03:08:30.755: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = true)
Jun 13 03:08:30.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a" satisfied condition "running and ready"
Jun 13 03:08:30.761: INFO: Container started at 2023-06-13 03:08:09 +0000 UTC, pod became ready at 2023-06-13 03:08:29 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 13 03:08:30.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1947" for this suite. 06/13/23 03:08:30.768
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":175,"skipped":3550,"failed":0}
------------------------------
• [SLOW TEST] [22.155 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:08:08.635
    Jun 13 03:08:08.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-probe 06/13/23 03:08:08.638
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:08:08.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:08:08.686
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jun 13 03:08:08.719: INFO: Waiting up to 5m0s for pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a" in namespace "container-probe-1947" to be "running and ready"
    Jun 13 03:08:08.748: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 28.76895ms
    Jun 13 03:08:08.748: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:08:10.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 2.035644281s
    Jun 13 03:08:10.755: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
    Jun 13 03:08:12.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.035575008s
    Jun 13 03:08:12.755: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
    Jun 13 03:08:14.787: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 6.067983438s
    Jun 13 03:08:14.787: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
    Jun 13 03:08:16.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 8.035297808s
    Jun 13 03:08:16.755: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
    Jun 13 03:08:18.757: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 10.037680996s
    Jun 13 03:08:18.757: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
    Jun 13 03:08:20.769: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 12.049958248s
    Jun 13 03:08:20.769: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
    Jun 13 03:08:22.817: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 14.097293226s
    Jun 13 03:08:22.817: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
    Jun 13 03:08:24.756: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 16.036904473s
    Jun 13 03:08:24.756: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
    Jun 13 03:08:26.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 18.035623174s
    Jun 13 03:08:26.755: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
    Jun 13 03:08:28.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=false. Elapsed: 20.035519907s
    Jun 13 03:08:28.755: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = false)
    Jun 13 03:08:30.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a": Phase="Running", Reason="", readiness=true. Elapsed: 22.035767374s
    Jun 13 03:08:30.755: INFO: The phase of Pod test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a is Running (Ready = true)
    Jun 13 03:08:30.755: INFO: Pod "test-webserver-3aa2b42d-99ef-40bc-a134-a24758a30b8a" satisfied condition "running and ready"
    Jun 13 03:08:30.761: INFO: Container started at 2023-06-13 03:08:09 +0000 UTC, pod became ready at 2023-06-13 03:08:29 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 13 03:08:30.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1947" for this suite. 06/13/23 03:08:30.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:08:30.791
Jun 13 03:08:30.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-probe 06/13/23 03:08:30.793
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:08:30.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:08:30.894
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b in namespace container-probe-3969 06/13/23 03:08:30.901
Jun 13 03:08:30.914: INFO: Waiting up to 5m0s for pod "busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b" in namespace "container-probe-3969" to be "not pending"
Jun 13 03:08:30.936: INFO: Pod "busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.88547ms
Jun 13 03:08:32.943: INFO: Pod "busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.029122301s
Jun 13 03:08:32.943: INFO: Pod "busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b" satisfied condition "not pending"
Jun 13 03:08:32.943: INFO: Started pod busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b in namespace container-probe-3969
STEP: checking the pod's current state and verifying that restartCount is present 06/13/23 03:08:32.943
Jun 13 03:08:32.949: INFO: Initial restart count of pod busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b is 0
STEP: deleting the pod 06/13/23 03:12:33.998
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 13 03:12:34.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3969" for this suite. 06/13/23 03:12:34.069
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":176,"skipped":3556,"failed":0}
------------------------------
• [SLOW TEST] [243.315 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:08:30.791
    Jun 13 03:08:30.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-probe 06/13/23 03:08:30.793
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:08:30.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:08:30.894
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b in namespace container-probe-3969 06/13/23 03:08:30.901
    Jun 13 03:08:30.914: INFO: Waiting up to 5m0s for pod "busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b" in namespace "container-probe-3969" to be "not pending"
    Jun 13 03:08:30.936: INFO: Pod "busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.88547ms
    Jun 13 03:08:32.943: INFO: Pod "busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b": Phase="Running", Reason="", readiness=true. Elapsed: 2.029122301s
    Jun 13 03:08:32.943: INFO: Pod "busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b" satisfied condition "not pending"
    Jun 13 03:08:32.943: INFO: Started pod busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b in namespace container-probe-3969
    STEP: checking the pod's current state and verifying that restartCount is present 06/13/23 03:08:32.943
    Jun 13 03:08:32.949: INFO: Initial restart count of pod busybox-d04a2278-e84d-4dbd-8f76-de4d158d8d5b is 0
    STEP: deleting the pod 06/13/23 03:12:33.998
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 13 03:12:34.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3969" for this suite. 06/13/23 03:12:34.069
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:12:34.107
Jun 13 03:12:34.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename job 06/13/23 03:12:34.11
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:12:34.156
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:12:34.161
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 06/13/23 03:12:34.167
STEP: Ensuring job reaches completions 06/13/23 03:12:34.177
STEP: Ensuring pods with index for job exist 06/13/23 03:12:44.183
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 13 03:12:44.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-844" for this suite. 06/13/23 03:12:44.2
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":177,"skipped":3580,"failed":0}
------------------------------
• [SLOW TEST] [10.105 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:12:34.107
    Jun 13 03:12:34.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename job 06/13/23 03:12:34.11
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:12:34.156
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:12:34.161
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 06/13/23 03:12:34.167
    STEP: Ensuring job reaches completions 06/13/23 03:12:34.177
    STEP: Ensuring pods with index for job exist 06/13/23 03:12:44.183
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 13 03:12:44.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-844" for this suite. 06/13/23 03:12:44.2
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:12:44.215
Jun 13 03:12:44.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename deployment 06/13/23 03:12:44.216
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:12:44.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:12:44.242
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 06/13/23 03:12:44.269
Jun 13 03:12:44.269: INFO: Creating simple deployment test-deployment-7vjdj
Jun 13 03:12:44.381: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 12, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 12, 44, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-deployment-7vjdj-777898ffcc\""}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 06/13/23 03:12:46.397
Jun 13 03:12:46.405: INFO: Deployment test-deployment-7vjdj has Conditions: [{Available True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7vjdj-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 06/13/23 03:12:46.405
Jun 13 03:12:46.424: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 12, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 12, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 12, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 12, 44, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-7vjdj-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 06/13/23 03:12:46.424
Jun 13 03:12:46.428: INFO: Observed &Deployment event: ADDED
Jun 13 03:12:46.428: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7vjdj-777898ffcc"}
Jun 13 03:12:46.428: INFO: Observed &Deployment event: MODIFIED
Jun 13 03:12:46.428: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7vjdj-777898ffcc"}
Jun 13 03:12:46.428: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 13 03:12:46.429: INFO: Observed &Deployment event: MODIFIED
Jun 13 03:12:46.429: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 13 03:12:46.429: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7vjdj-777898ffcc" is progressing.}
Jun 13 03:12:46.429: INFO: Observed &Deployment event: MODIFIED
Jun 13 03:12:46.429: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 13 03:12:46.429: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7vjdj-777898ffcc" has successfully progressed.}
Jun 13 03:12:46.430: INFO: Observed &Deployment event: MODIFIED
Jun 13 03:12:46.430: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 13 03:12:46.430: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7vjdj-777898ffcc" has successfully progressed.}
Jun 13 03:12:46.430: INFO: Found Deployment test-deployment-7vjdj in namespace deployment-8079 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 13 03:12:46.430: INFO: Deployment test-deployment-7vjdj has an updated status
STEP: patching the Statefulset Status 06/13/23 03:12:46.43
Jun 13 03:12:46.430: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 13 03:12:46.439: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 06/13/23 03:12:46.439
Jun 13 03:12:46.442: INFO: Observed &Deployment event: ADDED
Jun 13 03:12:46.442: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7vjdj-777898ffcc"}
Jun 13 03:12:46.442: INFO: Observed &Deployment event: MODIFIED
Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7vjdj-777898ffcc"}
Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 13 03:12:46.443: INFO: Observed &Deployment event: MODIFIED
Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7vjdj-777898ffcc" is progressing.}
Jun 13 03:12:46.443: INFO: Observed &Deployment event: MODIFIED
Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7vjdj-777898ffcc" has successfully progressed.}
Jun 13 03:12:46.443: INFO: Observed &Deployment event: MODIFIED
Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7vjdj-777898ffcc" has successfully progressed.}
Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 13 03:12:46.444: INFO: Observed &Deployment event: MODIFIED
Jun 13 03:12:46.444: INFO: Found deployment test-deployment-7vjdj in namespace deployment-8079 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jun 13 03:12:46.444: INFO: Deployment test-deployment-7vjdj has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 13 03:12:46.449: INFO: Deployment "test-deployment-7vjdj":
&Deployment{ObjectMeta:{test-deployment-7vjdj  deployment-8079  68a8273f-105e-4a84-b875-f2be0428760a 31768 1 2023-06-13 03:12:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-13 03:12:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-13 03:12:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-13 03:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037179b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-7vjdj-777898ffcc",LastUpdateTime:2023-06-13 03:12:46 +0000 UTC,LastTransitionTime:2023-06-13 03:12:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 13 03:12:46.454: INFO: New ReplicaSet "test-deployment-7vjdj-777898ffcc" of Deployment "test-deployment-7vjdj":
&ReplicaSet{ObjectMeta:{test-deployment-7vjdj-777898ffcc  deployment-8079  d252fe9a-e23b-4a46-af9e-f37c2277d4fb 31762 1 2023-06-13 03:12:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-7vjdj 68a8273f-105e-4a84-b875-f2be0428760a 0xc003717da0 0xc003717da1}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:12:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68a8273f-105e-4a84-b875-f2be0428760a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:12:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003717e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 13 03:12:46.460: INFO: Pod "test-deployment-7vjdj-777898ffcc-pb9rd" is available:
&Pod{ObjectMeta:{test-deployment-7vjdj-777898ffcc-pb9rd test-deployment-7vjdj-777898ffcc- deployment-8079  7aa90e53-97d7-4941-b942-2ae74ee990f3 31761 0 2023-06-13 03:12:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:203240cfbf78f24f07b65e99fdf3a51de31b897e5e6eb10dbfd01e2fe8e227ae cni.projectcalico.org/podIP:172.16.172.20/32 cni.projectcalico.org/podIPs:172.16.172.20/32] [{apps/v1 ReplicaSet test-deployment-7vjdj-777898ffcc d252fe9a-e23b-4a46-af9e-f37c2277d4fb 0xc0019e3d00 0xc0019e3d01}] [] [{kube-controller-manager Update v1 2023-06-13 03:12:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d252fe9a-e23b-4a46-af9e-f37c2277d4fb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:12:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:12:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgttv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgttv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:12:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:12:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.20,StartTime:2023-06-13 03:12:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:12:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://87e0dad0049a15948054cec3e89f6bd5fead9a989c2b62e554de6fd8ef9a50c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 13 03:12:46.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8079" for this suite. 06/13/23 03:12:46.472
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":178,"skipped":3622,"failed":0}
------------------------------
• [2.267 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:12:44.215
    Jun 13 03:12:44.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename deployment 06/13/23 03:12:44.216
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:12:44.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:12:44.242
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 06/13/23 03:12:44.269
    Jun 13 03:12:44.269: INFO: Creating simple deployment test-deployment-7vjdj
    Jun 13 03:12:44.381: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 12, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 12, 44, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-deployment-7vjdj-777898ffcc\""}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 06/13/23 03:12:46.397
    Jun 13 03:12:46.405: INFO: Deployment test-deployment-7vjdj has Conditions: [{Available True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7vjdj-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 06/13/23 03:12:46.405
    Jun 13 03:12:46.424: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 12, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 12, 46, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 12, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 12, 44, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-7vjdj-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 06/13/23 03:12:46.424
    Jun 13 03:12:46.428: INFO: Observed &Deployment event: ADDED
    Jun 13 03:12:46.428: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7vjdj-777898ffcc"}
    Jun 13 03:12:46.428: INFO: Observed &Deployment event: MODIFIED
    Jun 13 03:12:46.428: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7vjdj-777898ffcc"}
    Jun 13 03:12:46.428: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 13 03:12:46.429: INFO: Observed &Deployment event: MODIFIED
    Jun 13 03:12:46.429: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 13 03:12:46.429: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7vjdj-777898ffcc" is progressing.}
    Jun 13 03:12:46.429: INFO: Observed &Deployment event: MODIFIED
    Jun 13 03:12:46.429: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 13 03:12:46.429: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7vjdj-777898ffcc" has successfully progressed.}
    Jun 13 03:12:46.430: INFO: Observed &Deployment event: MODIFIED
    Jun 13 03:12:46.430: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 13 03:12:46.430: INFO: Observed Deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7vjdj-777898ffcc" has successfully progressed.}
    Jun 13 03:12:46.430: INFO: Found Deployment test-deployment-7vjdj in namespace deployment-8079 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 13 03:12:46.430: INFO: Deployment test-deployment-7vjdj has an updated status
    STEP: patching the Statefulset Status 06/13/23 03:12:46.43
    Jun 13 03:12:46.430: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun 13 03:12:46.439: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 06/13/23 03:12:46.439
    Jun 13 03:12:46.442: INFO: Observed &Deployment event: ADDED
    Jun 13 03:12:46.442: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7vjdj-777898ffcc"}
    Jun 13 03:12:46.442: INFO: Observed &Deployment event: MODIFIED
    Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-7vjdj-777898ffcc"}
    Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 13 03:12:46.443: INFO: Observed &Deployment event: MODIFIED
    Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:44 +0000 UTC 2023-06-13 03:12:44 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-7vjdj-777898ffcc" is progressing.}
    Jun 13 03:12:46.443: INFO: Observed &Deployment event: MODIFIED
    Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7vjdj-777898ffcc" has successfully progressed.}
    Jun 13 03:12:46.443: INFO: Observed &Deployment event: MODIFIED
    Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:46 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-13 03:12:46 +0000 UTC 2023-06-13 03:12:44 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-7vjdj-777898ffcc" has successfully progressed.}
    Jun 13 03:12:46.443: INFO: Observed deployment test-deployment-7vjdj in namespace deployment-8079 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 13 03:12:46.444: INFO: Observed &Deployment event: MODIFIED
    Jun 13 03:12:46.444: INFO: Found deployment test-deployment-7vjdj in namespace deployment-8079 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jun 13 03:12:46.444: INFO: Deployment test-deployment-7vjdj has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 13 03:12:46.449: INFO: Deployment "test-deployment-7vjdj":
    &Deployment{ObjectMeta:{test-deployment-7vjdj  deployment-8079  68a8273f-105e-4a84-b875-f2be0428760a 31768 1 2023-06-13 03:12:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-13 03:12:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-13 03:12:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-13 03:12:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037179b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-7vjdj-777898ffcc",LastUpdateTime:2023-06-13 03:12:46 +0000 UTC,LastTransitionTime:2023-06-13 03:12:46 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 13 03:12:46.454: INFO: New ReplicaSet "test-deployment-7vjdj-777898ffcc" of Deployment "test-deployment-7vjdj":
    &ReplicaSet{ObjectMeta:{test-deployment-7vjdj-777898ffcc  deployment-8079  d252fe9a-e23b-4a46-af9e-f37c2277d4fb 31762 1 2023-06-13 03:12:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-7vjdj 68a8273f-105e-4a84-b875-f2be0428760a 0xc003717da0 0xc003717da1}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:12:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"68a8273f-105e-4a84-b875-f2be0428760a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:12:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003717e48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 03:12:46.460: INFO: Pod "test-deployment-7vjdj-777898ffcc-pb9rd" is available:
    &Pod{ObjectMeta:{test-deployment-7vjdj-777898ffcc-pb9rd test-deployment-7vjdj-777898ffcc- deployment-8079  7aa90e53-97d7-4941-b942-2ae74ee990f3 31761 0 2023-06-13 03:12:44 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:203240cfbf78f24f07b65e99fdf3a51de31b897e5e6eb10dbfd01e2fe8e227ae cni.projectcalico.org/podIP:172.16.172.20/32 cni.projectcalico.org/podIPs:172.16.172.20/32] [{apps/v1 ReplicaSet test-deployment-7vjdj-777898ffcc d252fe9a-e23b-4a46-af9e-f37c2277d4fb 0xc0019e3d00 0xc0019e3d01}] [] [{kube-controller-manager Update v1 2023-06-13 03:12:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d252fe9a-e23b-4a46-af9e-f37c2277d4fb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:12:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:12:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wgttv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wgttv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:12:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:12:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:12:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.20,StartTime:2023-06-13 03:12:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:12:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://87e0dad0049a15948054cec3e89f6bd5fead9a989c2b62e554de6fd8ef9a50c1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 13 03:12:46.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8079" for this suite. 06/13/23 03:12:46.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:12:46.483
Jun 13 03:12:46.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename ingress 06/13/23 03:12:46.484
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:12:46.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:12:46.514
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 06/13/23 03:12:46.52
STEP: getting /apis/networking.k8s.io 06/13/23 03:12:46.527
STEP: getting /apis/networking.k8s.iov1 06/13/23 03:12:46.529
STEP: creating 06/13/23 03:12:46.532
STEP: getting 06/13/23 03:12:46.561
STEP: listing 06/13/23 03:12:46.567
STEP: watching 06/13/23 03:12:46.574
Jun 13 03:12:46.574: INFO: starting watch
STEP: cluster-wide listing 06/13/23 03:12:46.577
STEP: cluster-wide watching 06/13/23 03:12:46.584
Jun 13 03:12:46.584: INFO: starting watch
STEP: patching 06/13/23 03:12:46.586
STEP: updating 06/13/23 03:12:46.596
Jun 13 03:12:46.613: INFO: waiting for watch events with expected annotations
Jun 13 03:12:46.614: INFO: saw patched and updated annotations
STEP: patching /status 06/13/23 03:12:46.614
STEP: updating /status 06/13/23 03:12:46.629
STEP: get /status 06/13/23 03:12:46.647
STEP: deleting 06/13/23 03:12:46.652
STEP: deleting a collection 06/13/23 03:12:46.676
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jun 13 03:12:46.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-6615" for this suite. 06/13/23 03:12:46.718
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":179,"skipped":3630,"failed":0}
------------------------------
• [0.248 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:12:46.483
    Jun 13 03:12:46.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename ingress 06/13/23 03:12:46.484
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:12:46.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:12:46.514
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 06/13/23 03:12:46.52
    STEP: getting /apis/networking.k8s.io 06/13/23 03:12:46.527
    STEP: getting /apis/networking.k8s.iov1 06/13/23 03:12:46.529
    STEP: creating 06/13/23 03:12:46.532
    STEP: getting 06/13/23 03:12:46.561
    STEP: listing 06/13/23 03:12:46.567
    STEP: watching 06/13/23 03:12:46.574
    Jun 13 03:12:46.574: INFO: starting watch
    STEP: cluster-wide listing 06/13/23 03:12:46.577
    STEP: cluster-wide watching 06/13/23 03:12:46.584
    Jun 13 03:12:46.584: INFO: starting watch
    STEP: patching 06/13/23 03:12:46.586
    STEP: updating 06/13/23 03:12:46.596
    Jun 13 03:12:46.613: INFO: waiting for watch events with expected annotations
    Jun 13 03:12:46.614: INFO: saw patched and updated annotations
    STEP: patching /status 06/13/23 03:12:46.614
    STEP: updating /status 06/13/23 03:12:46.629
    STEP: get /status 06/13/23 03:12:46.647
    STEP: deleting 06/13/23 03:12:46.652
    STEP: deleting a collection 06/13/23 03:12:46.676
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jun 13 03:12:46.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-6615" for this suite. 06/13/23 03:12:46.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:12:46.732
Jun 13 03:12:46.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 03:12:46.733
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:12:46.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:12:46.759
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 06/13/23 03:12:46.764
Jun 13 03:12:46.781: INFO: Waiting up to 5m0s for pod "pod-51008210-806b-4eab-ac8c-4a2d1d33224a" in namespace "emptydir-2487" to be "Succeeded or Failed"
Jun 13 03:12:46.795: INFO: Pod "pod-51008210-806b-4eab-ac8c-4a2d1d33224a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.724784ms
Jun 13 03:12:48.806: INFO: Pod "pod-51008210-806b-4eab-ac8c-4a2d1d33224a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024940228s
Jun 13 03:12:50.803: INFO: Pod "pod-51008210-806b-4eab-ac8c-4a2d1d33224a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022352838s
STEP: Saw pod success 06/13/23 03:12:50.803
Jun 13 03:12:50.803: INFO: Pod "pod-51008210-806b-4eab-ac8c-4a2d1d33224a" satisfied condition "Succeeded or Failed"
Jun 13 03:12:50.810: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-51008210-806b-4eab-ac8c-4a2d1d33224a container test-container: <nil>
STEP: delete the pod 06/13/23 03:12:50.835
Jun 13 03:12:50.861: INFO: Waiting for pod pod-51008210-806b-4eab-ac8c-4a2d1d33224a to disappear
Jun 13 03:12:50.868: INFO: Pod pod-51008210-806b-4eab-ac8c-4a2d1d33224a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 03:12:50.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2487" for this suite. 06/13/23 03:12:50.88
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":180,"skipped":3646,"failed":0}
------------------------------
• [4.174 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:12:46.732
    Jun 13 03:12:46.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 03:12:46.733
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:12:46.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:12:46.759
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 06/13/23 03:12:46.764
    Jun 13 03:12:46.781: INFO: Waiting up to 5m0s for pod "pod-51008210-806b-4eab-ac8c-4a2d1d33224a" in namespace "emptydir-2487" to be "Succeeded or Failed"
    Jun 13 03:12:46.795: INFO: Pod "pod-51008210-806b-4eab-ac8c-4a2d1d33224a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.724784ms
    Jun 13 03:12:48.806: INFO: Pod "pod-51008210-806b-4eab-ac8c-4a2d1d33224a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024940228s
    Jun 13 03:12:50.803: INFO: Pod "pod-51008210-806b-4eab-ac8c-4a2d1d33224a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022352838s
    STEP: Saw pod success 06/13/23 03:12:50.803
    Jun 13 03:12:50.803: INFO: Pod "pod-51008210-806b-4eab-ac8c-4a2d1d33224a" satisfied condition "Succeeded or Failed"
    Jun 13 03:12:50.810: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-51008210-806b-4eab-ac8c-4a2d1d33224a container test-container: <nil>
    STEP: delete the pod 06/13/23 03:12:50.835
    Jun 13 03:12:50.861: INFO: Waiting for pod pod-51008210-806b-4eab-ac8c-4a2d1d33224a to disappear
    Jun 13 03:12:50.868: INFO: Pod pod-51008210-806b-4eab-ac8c-4a2d1d33224a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 03:12:50.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2487" for this suite. 06/13/23 03:12:50.88
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:12:50.906
Jun 13 03:12:50.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename statefulset 06/13/23 03:12:50.908
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:12:50.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:12:50.941
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2735 06/13/23 03:12:50.948
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-2735 06/13/23 03:12:50.963
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2735 06/13/23 03:12:50.975
Jun 13 03:12:50.989: INFO: Found 0 stateful pods, waiting for 1
Jun 13 03:13:01.001: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 06/13/23 03:13:01.001
Jun 13 03:13:01.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 13 03:13:01.342: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 13 03:13:01.342: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 13 03:13:01.343: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 13 03:13:01.350: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 13 03:13:11.358: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 13 03:13:11.358: INFO: Waiting for statefulset status.replicas updated to 0
Jun 13 03:13:11.412: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Jun 13 03:13:11.412: INFO: ss-0  sks-test-v1-25-9-workergroup-469fm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:12:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:12:51 +0000 UTC  }]
Jun 13 03:13:11.412: INFO: 
Jun 13 03:13:11.412: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 13 03:13:12.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990194724s
Jun 13 03:13:13.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984020426s
Jun 13 03:13:14.431: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977774117s
Jun 13 03:13:15.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970990827s
Jun 13 03:13:16.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96417065s
Jun 13 03:13:17.478: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.941200486s
Jun 13 03:13:18.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.924864393s
Jun 13 03:13:19.498: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.912336603s
Jun 13 03:13:20.505: INFO: Verifying statefulset ss doesn't scale past 3 for another 904.215649ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2735 06/13/23 03:13:21.505
Jun 13 03:13:21.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 13 03:13:21.774: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 13 03:13:21.774: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 13 03:13:21.774: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 13 03:13:21.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 13 03:13:22.029: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 13 03:13:22.029: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 13 03:13:22.029: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 13 03:13:22.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 13 03:13:22.225: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 13 03:13:22.225: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 13 03:13:22.225: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 13 03:13:22.231: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jun 13 03:13:32.241: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 03:13:32.241: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 03:13:32.241: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 06/13/23 03:13:32.241
Jun 13 03:13:32.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 13 03:13:32.430: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 13 03:13:32.430: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 13 03:13:32.430: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 13 03:13:32.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 13 03:13:32.595: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 13 03:13:32.595: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 13 03:13:32.595: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 13 03:13:32.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 13 03:13:32.786: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 13 03:13:32.786: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 13 03:13:32.786: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 13 03:13:32.786: INFO: Waiting for statefulset status.replicas updated to 0
Jun 13 03:13:32.794: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 13 03:13:42.809: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 13 03:13:42.809: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 13 03:13:42.809: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 13 03:13:42.834: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Jun 13 03:13:42.834: INFO: ss-0  sks-test-v1-25-9-workergroup-469fm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:12:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:12:51 +0000 UTC  }]
Jun 13 03:13:42.834: INFO: ss-1  sks-test-v1-25-9-workergroup-2q6k2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  }]
Jun 13 03:13:42.834: INFO: ss-2  sks-test-v1-25-9-workergroup-l5gcd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  }]
Jun 13 03:13:42.834: INFO: 
Jun 13 03:13:42.834: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 13 03:13:43.842: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
Jun 13 03:13:43.842: INFO: ss-1  sks-test-v1-25-9-workergroup-2q6k2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  }]
Jun 13 03:13:43.842: INFO: ss-2  sks-test-v1-25-9-workergroup-l5gcd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  }]
Jun 13 03:13:43.842: INFO: 
Jun 13 03:13:43.842: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 13 03:13:44.849: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.984212387s
Jun 13 03:13:45.857: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.977528823s
Jun 13 03:13:46.863: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.969565061s
Jun 13 03:13:47.870: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.9635881s
Jun 13 03:13:48.876: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.956560559s
Jun 13 03:13:49.887: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.949916927s
Jun 13 03:13:50.893: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.939588075s
Jun 13 03:13:51.901: INFO: Verifying statefulset ss doesn't scale past 0 for another 932.878801ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2735 06/13/23 03:13:52.901
Jun 13 03:13:52.907: INFO: Scaling statefulset ss to 0
Jun 13 03:13:52.925: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 13 03:13:52.932: INFO: Deleting all statefulset in ns statefulset-2735
Jun 13 03:13:52.938: INFO: Scaling statefulset ss to 0
Jun 13 03:13:52.959: INFO: Waiting for statefulset status.replicas updated to 0
Jun 13 03:13:52.970: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 13 03:13:53.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2735" for this suite. 06/13/23 03:13:53.015
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":181,"skipped":3649,"failed":0}
------------------------------
• [SLOW TEST] [62.124 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:12:50.906
    Jun 13 03:12:50.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename statefulset 06/13/23 03:12:50.908
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:12:50.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:12:50.941
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2735 06/13/23 03:12:50.948
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-2735 06/13/23 03:12:50.963
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2735 06/13/23 03:12:50.975
    Jun 13 03:12:50.989: INFO: Found 0 stateful pods, waiting for 1
    Jun 13 03:13:01.001: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 06/13/23 03:13:01.001
    Jun 13 03:13:01.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 13 03:13:01.342: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 13 03:13:01.342: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 13 03:13:01.343: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 13 03:13:01.350: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jun 13 03:13:11.358: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 13 03:13:11.358: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 13 03:13:11.412: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
    Jun 13 03:13:11.412: INFO: ss-0  sks-test-v1-25-9-workergroup-469fm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:12:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:01 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:12:51 +0000 UTC  }]
    Jun 13 03:13:11.412: INFO: 
    Jun 13 03:13:11.412: INFO: StatefulSet ss has not reached scale 3, at 1
    Jun 13 03:13:12.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990194724s
    Jun 13 03:13:13.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984020426s
    Jun 13 03:13:14.431: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977774117s
    Jun 13 03:13:15.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970990827s
    Jun 13 03:13:16.461: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96417065s
    Jun 13 03:13:17.478: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.941200486s
    Jun 13 03:13:18.489: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.924864393s
    Jun 13 03:13:19.498: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.912336603s
    Jun 13 03:13:20.505: INFO: Verifying statefulset ss doesn't scale past 3 for another 904.215649ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2735 06/13/23 03:13:21.505
    Jun 13 03:13:21.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 13 03:13:21.774: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 13 03:13:21.774: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 13 03:13:21.774: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 13 03:13:21.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 13 03:13:22.029: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jun 13 03:13:22.029: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 13 03:13:22.029: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 13 03:13:22.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 13 03:13:22.225: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jun 13 03:13:22.225: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 13 03:13:22.225: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 13 03:13:22.231: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jun 13 03:13:32.241: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 03:13:32.241: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 03:13:32.241: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 06/13/23 03:13:32.241
    Jun 13 03:13:32.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 13 03:13:32.430: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 13 03:13:32.430: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 13 03:13:32.430: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 13 03:13:32.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 13 03:13:32.595: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 13 03:13:32.595: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 13 03:13:32.595: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 13 03:13:32.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-2735 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 13 03:13:32.786: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 13 03:13:32.786: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 13 03:13:32.786: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 13 03:13:32.786: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 13 03:13:32.794: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jun 13 03:13:42.809: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 13 03:13:42.809: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jun 13 03:13:42.809: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jun 13 03:13:42.834: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
    Jun 13 03:13:42.834: INFO: ss-0  sks-test-v1-25-9-workergroup-469fm  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:12:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:33 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:12:51 +0000 UTC  }]
    Jun 13 03:13:42.834: INFO: ss-1  sks-test-v1-25-9-workergroup-2q6k2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  }]
    Jun 13 03:13:42.834: INFO: ss-2  sks-test-v1-25-9-workergroup-l5gcd  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  }]
    Jun 13 03:13:42.834: INFO: 
    Jun 13 03:13:42.834: INFO: StatefulSet ss has not reached scale 0, at 3
    Jun 13 03:13:43.842: INFO: POD   NODE                                PHASE    GRACE  CONDITIONS
    Jun 13 03:13:43.842: INFO: ss-1  sks-test-v1-25-9-workergroup-2q6k2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  }]
    Jun 13 03:13:43.842: INFO: ss-2  sks-test-v1-25-9-workergroup-l5gcd  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:32 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:13:11 +0000 UTC  }]
    Jun 13 03:13:43.842: INFO: 
    Jun 13 03:13:43.842: INFO: StatefulSet ss has not reached scale 0, at 2
    Jun 13 03:13:44.849: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.984212387s
    Jun 13 03:13:45.857: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.977528823s
    Jun 13 03:13:46.863: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.969565061s
    Jun 13 03:13:47.870: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.9635881s
    Jun 13 03:13:48.876: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.956560559s
    Jun 13 03:13:49.887: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.949916927s
    Jun 13 03:13:50.893: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.939588075s
    Jun 13 03:13:51.901: INFO: Verifying statefulset ss doesn't scale past 0 for another 932.878801ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2735 06/13/23 03:13:52.901
    Jun 13 03:13:52.907: INFO: Scaling statefulset ss to 0
    Jun 13 03:13:52.925: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 13 03:13:52.932: INFO: Deleting all statefulset in ns statefulset-2735
    Jun 13 03:13:52.938: INFO: Scaling statefulset ss to 0
    Jun 13 03:13:52.959: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 13 03:13:52.970: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 13 03:13:53.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2735" for this suite. 06/13/23 03:13:53.015
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:13:53.031
Jun 13 03:13:53.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubelet-test 06/13/23 03:13:53.033
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:13:53.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:13:53.059
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 13 03:13:57.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9475" for this suite. 06/13/23 03:13:57.103
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":182,"skipped":3652,"failed":0}
------------------------------
• [4.085 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:13:53.031
    Jun 13 03:13:53.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubelet-test 06/13/23 03:13:53.033
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:13:53.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:13:53.059
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 13 03:13:57.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-9475" for this suite. 06/13/23 03:13:57.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:13:57.118
Jun 13 03:13:57.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename csistoragecapacity 06/13/23 03:13:57.119
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:13:57.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:13:57.144
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 06/13/23 03:13:57.149
STEP: getting /apis/storage.k8s.io 06/13/23 03:13:57.154
STEP: getting /apis/storage.k8s.io/v1 06/13/23 03:13:57.157
STEP: creating 06/13/23 03:13:57.16
STEP: watching 06/13/23 03:13:57.188
Jun 13 03:13:57.188: INFO: starting watch
STEP: getting 06/13/23 03:13:57.203
STEP: listing in namespace 06/13/23 03:13:57.208
STEP: listing across namespaces 06/13/23 03:13:57.215
STEP: patching 06/13/23 03:13:57.226
STEP: updating 06/13/23 03:13:57.234
Jun 13 03:13:57.242: INFO: waiting for watch events with expected annotations in namespace
Jun 13 03:13:57.242: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 06/13/23 03:13:57.242
STEP: deleting a collection 06/13/23 03:13:57.264
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jun 13 03:13:57.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-6664" for this suite. 06/13/23 03:13:57.302
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":183,"skipped":3694,"failed":0}
------------------------------
• [0.193 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:13:57.118
    Jun 13 03:13:57.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename csistoragecapacity 06/13/23 03:13:57.119
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:13:57.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:13:57.144
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 06/13/23 03:13:57.149
    STEP: getting /apis/storage.k8s.io 06/13/23 03:13:57.154
    STEP: getting /apis/storage.k8s.io/v1 06/13/23 03:13:57.157
    STEP: creating 06/13/23 03:13:57.16
    STEP: watching 06/13/23 03:13:57.188
    Jun 13 03:13:57.188: INFO: starting watch
    STEP: getting 06/13/23 03:13:57.203
    STEP: listing in namespace 06/13/23 03:13:57.208
    STEP: listing across namespaces 06/13/23 03:13:57.215
    STEP: patching 06/13/23 03:13:57.226
    STEP: updating 06/13/23 03:13:57.234
    Jun 13 03:13:57.242: INFO: waiting for watch events with expected annotations in namespace
    Jun 13 03:13:57.242: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 06/13/23 03:13:57.242
    STEP: deleting a collection 06/13/23 03:13:57.264
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jun 13 03:13:57.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-6664" for this suite. 06/13/23 03:13:57.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:13:57.312
Jun 13 03:13:57.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename sched-pred 06/13/23 03:13:57.314
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:13:57.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:13:57.337
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 13 03:13:57.342: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 13 03:13:57.358: INFO: Waiting for terminating namespaces to be deleted...
Jun 13 03:13:57.364: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-2q6k2 before test
Jun 13 03:13:57.384: INFO: calico-node-xb8kf from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
Jun 13 03:13:57.384: INFO: 	Container calico-node ready: true, restart count 0
Jun 13 03:13:57.384: INFO: calico-typha-79dfdd7d65-w95lk from calico-system started at 2023-06-13 02:04:34 +0000 UTC (1 container statuses recorded)
Jun 13 03:13:57.384: INFO: 	Container calico-typha ready: true, restart count 0
Jun 13 03:13:57.384: INFO: csi-node-driver-vkf24 from calico-system started at 2023-06-13 02:05:19 +0000 UTC (2 container statuses recorded)
Jun 13 03:13:57.384: INFO: 	Container calico-csi ready: true, restart count 0
Jun 13 03:13:57.384: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun 13 03:13:57.384: INFO: kube-proxy-g2qcx from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
Jun 13 03:13:57.384: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 13 03:13:57.384: INFO: smtx-elf-csi-driver-node-plugin-gqbkk from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
Jun 13 03:13:57.384: INFO: 	Container csi-driver ready: true, restart count 0
Jun 13 03:13:57.384: INFO: 	Container driver-registrar ready: true, restart count 0
Jun 13 03:13:57.384: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 13 03:13:57.384: INFO: sonobuoy from sonobuoy started at 2023-06-13 02:14:11 +0000 UTC (1 container statuses recorded)
Jun 13 03:13:57.384: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 13 03:13:57.384: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-f9dsl from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:13:57.384: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 03:13:57.384: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 13 03:13:57.384: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-469fm before test
Jun 13 03:13:57.399: INFO: calico-node-lhmmz from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
Jun 13 03:13:57.399: INFO: 	Container calico-node ready: true, restart count 0
Jun 13 03:13:57.399: INFO: csi-node-driver-jrfzt from calico-system started at 2023-06-13 02:04:55 +0000 UTC (2 container statuses recorded)
Jun 13 03:13:57.399: INFO: 	Container calico-csi ready: true, restart count 0
Jun 13 03:13:57.399: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun 13 03:13:57.399: INFO: kube-proxy-fn2dq from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
Jun 13 03:13:57.399: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 13 03:13:57.399: INFO: snapshot-controller-76c6888c5-9m5td from kube-system started at 2023-06-13 02:04:55 +0000 UTC (1 container statuses recorded)
Jun 13 03:13:57.399: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 13 03:13:57.399: INFO: bin-falsedc6cba46-bfc9-4971-919e-dcbe83186526 from kubelet-test-9475 started at 2023-06-13 03:13:53 +0000 UTC (1 container statuses recorded)
Jun 13 03:13:57.399: INFO: 	Container bin-falsedc6cba46-bfc9-4971-919e-dcbe83186526 ready: false, restart count 0
Jun 13 03:13:57.399: INFO: smtx-elf-csi-driver-node-plugin-h8k7m from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
Jun 13 03:13:57.399: INFO: 	Container csi-driver ready: true, restart count 0
Jun 13 03:13:57.399: INFO: 	Container driver-registrar ready: true, restart count 0
Jun 13 03:13:57.399: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 13 03:13:57.399: INFO: tigera-operator-6f49bd984-r7pkz from sks-system started at 2023-06-13 02:04:29 +0000 UTC (1 container statuses recorded)
Jun 13 03:13:57.399: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 13 03:13:57.399: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-hcvsr from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:13:57.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 03:13:57.399: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 13 03:13:57.399: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-l5gcd before test
Jun 13 03:13:57.433: INFO: calico-node-fptww from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
Jun 13 03:13:57.433: INFO: 	Container calico-node ready: true, restart count 0
Jun 13 03:13:57.433: INFO: calico-typha-79dfdd7d65-nrkt5 from calico-system started at 2023-06-13 02:04:43 +0000 UTC (1 container statuses recorded)
Jun 13 03:13:57.433: INFO: 	Container calico-typha ready: true, restart count 0
Jun 13 03:13:57.433: INFO: csi-node-driver-66bp4 from calico-system started at 2023-06-13 02:05:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:13:57.433: INFO: 	Container calico-csi ready: true, restart count 0
Jun 13 03:13:57.433: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun 13 03:13:57.433: INFO: kube-proxy-669th from kube-system started at 2023-06-13 02:03:17 +0000 UTC (1 container statuses recorded)
Jun 13 03:13:57.433: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 13 03:13:57.433: INFO: smtx-elf-csi-driver-node-plugin-dtt2k from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
Jun 13 03:13:57.433: INFO: 	Container csi-driver ready: true, restart count 0
Jun 13 03:13:57.433: INFO: 	Container driver-registrar ready: true, restart count 0
Jun 13 03:13:57.433: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 13 03:13:57.433: INFO: sonobuoy-e2e-job-246c2b6dcb314696 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:13:57.433: INFO: 	Container e2e ready: true, restart count 0
Jun 13 03:13:57.433: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 03:13:57.433: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-bvbp2 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:13:57.433: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 03:13:57.433: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/13/23 03:13:57.433
Jun 13 03:13:57.446: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6348" to be "running"
Jun 13 03:13:57.454: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.245519ms
Jun 13 03:13:59.461: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015509527s
Jun 13 03:13:59.461: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/13/23 03:13:59.467
STEP: Trying to apply a random label on the found node. 06/13/23 03:13:59.508
STEP: verifying the node has the label kubernetes.io/e2e-c785b12c-73ec-4856-ad4b-188d5c401617 42 06/13/23 03:13:59.524
STEP: Trying to relaunch the pod, now with labels. 06/13/23 03:13:59.53
Jun 13 03:13:59.540: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6348" to be "not pending"
Jun 13 03:13:59.548: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 7.414569ms
Jun 13 03:14:01.557: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.017000935s
Jun 13 03:14:01.557: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-c785b12c-73ec-4856-ad4b-188d5c401617 off the node sks-test-v1-25-9-workergroup-469fm 06/13/23 03:14:01.564
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c785b12c-73ec-4856-ad4b-188d5c401617 06/13/23 03:14:01.59
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:14:01.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6348" for this suite. 06/13/23 03:14:01.606
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":184,"skipped":3710,"failed":0}
------------------------------
• [4.345 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:13:57.312
    Jun 13 03:13:57.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename sched-pred 06/13/23 03:13:57.314
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:13:57.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:13:57.337
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jun 13 03:13:57.342: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 13 03:13:57.358: INFO: Waiting for terminating namespaces to be deleted...
    Jun 13 03:13:57.364: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-2q6k2 before test
    Jun 13 03:13:57.384: INFO: calico-node-xb8kf from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
    Jun 13 03:13:57.384: INFO: 	Container calico-node ready: true, restart count 0
    Jun 13 03:13:57.384: INFO: calico-typha-79dfdd7d65-w95lk from calico-system started at 2023-06-13 02:04:34 +0000 UTC (1 container statuses recorded)
    Jun 13 03:13:57.384: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 13 03:13:57.384: INFO: csi-node-driver-vkf24 from calico-system started at 2023-06-13 02:05:19 +0000 UTC (2 container statuses recorded)
    Jun 13 03:13:57.384: INFO: 	Container calico-csi ready: true, restart count 0
    Jun 13 03:13:57.384: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun 13 03:13:57.384: INFO: kube-proxy-g2qcx from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
    Jun 13 03:13:57.384: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 13 03:13:57.384: INFO: smtx-elf-csi-driver-node-plugin-gqbkk from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
    Jun 13 03:13:57.384: INFO: 	Container csi-driver ready: true, restart count 0
    Jun 13 03:13:57.384: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun 13 03:13:57.384: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 13 03:13:57.384: INFO: sonobuoy from sonobuoy started at 2023-06-13 02:14:11 +0000 UTC (1 container statuses recorded)
    Jun 13 03:13:57.384: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 13 03:13:57.384: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-f9dsl from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:13:57.384: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 03:13:57.384: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 13 03:13:57.384: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-469fm before test
    Jun 13 03:13:57.399: INFO: calico-node-lhmmz from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
    Jun 13 03:13:57.399: INFO: 	Container calico-node ready: true, restart count 0
    Jun 13 03:13:57.399: INFO: csi-node-driver-jrfzt from calico-system started at 2023-06-13 02:04:55 +0000 UTC (2 container statuses recorded)
    Jun 13 03:13:57.399: INFO: 	Container calico-csi ready: true, restart count 0
    Jun 13 03:13:57.399: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun 13 03:13:57.399: INFO: kube-proxy-fn2dq from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
    Jun 13 03:13:57.399: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 13 03:13:57.399: INFO: snapshot-controller-76c6888c5-9m5td from kube-system started at 2023-06-13 02:04:55 +0000 UTC (1 container statuses recorded)
    Jun 13 03:13:57.399: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun 13 03:13:57.399: INFO: bin-falsedc6cba46-bfc9-4971-919e-dcbe83186526 from kubelet-test-9475 started at 2023-06-13 03:13:53 +0000 UTC (1 container statuses recorded)
    Jun 13 03:13:57.399: INFO: 	Container bin-falsedc6cba46-bfc9-4971-919e-dcbe83186526 ready: false, restart count 0
    Jun 13 03:13:57.399: INFO: smtx-elf-csi-driver-node-plugin-h8k7m from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
    Jun 13 03:13:57.399: INFO: 	Container csi-driver ready: true, restart count 0
    Jun 13 03:13:57.399: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun 13 03:13:57.399: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 13 03:13:57.399: INFO: tigera-operator-6f49bd984-r7pkz from sks-system started at 2023-06-13 02:04:29 +0000 UTC (1 container statuses recorded)
    Jun 13 03:13:57.399: INFO: 	Container tigera-operator ready: true, restart count 0
    Jun 13 03:13:57.399: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-hcvsr from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:13:57.399: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 03:13:57.399: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 13 03:13:57.399: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-l5gcd before test
    Jun 13 03:13:57.433: INFO: calico-node-fptww from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
    Jun 13 03:13:57.433: INFO: 	Container calico-node ready: true, restart count 0
    Jun 13 03:13:57.433: INFO: calico-typha-79dfdd7d65-nrkt5 from calico-system started at 2023-06-13 02:04:43 +0000 UTC (1 container statuses recorded)
    Jun 13 03:13:57.433: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 13 03:13:57.433: INFO: csi-node-driver-66bp4 from calico-system started at 2023-06-13 02:05:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:13:57.433: INFO: 	Container calico-csi ready: true, restart count 0
    Jun 13 03:13:57.433: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun 13 03:13:57.433: INFO: kube-proxy-669th from kube-system started at 2023-06-13 02:03:17 +0000 UTC (1 container statuses recorded)
    Jun 13 03:13:57.433: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 13 03:13:57.433: INFO: smtx-elf-csi-driver-node-plugin-dtt2k from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
    Jun 13 03:13:57.433: INFO: 	Container csi-driver ready: true, restart count 0
    Jun 13 03:13:57.433: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun 13 03:13:57.433: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 13 03:13:57.433: INFO: sonobuoy-e2e-job-246c2b6dcb314696 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:13:57.433: INFO: 	Container e2e ready: true, restart count 0
    Jun 13 03:13:57.433: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 03:13:57.433: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-bvbp2 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:13:57.433: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 03:13:57.433: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/13/23 03:13:57.433
    Jun 13 03:13:57.446: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-6348" to be "running"
    Jun 13 03:13:57.454: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 8.245519ms
    Jun 13 03:13:59.461: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.015509527s
    Jun 13 03:13:59.461: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/13/23 03:13:59.467
    STEP: Trying to apply a random label on the found node. 06/13/23 03:13:59.508
    STEP: verifying the node has the label kubernetes.io/e2e-c785b12c-73ec-4856-ad4b-188d5c401617 42 06/13/23 03:13:59.524
    STEP: Trying to relaunch the pod, now with labels. 06/13/23 03:13:59.53
    Jun 13 03:13:59.540: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-6348" to be "not pending"
    Jun 13 03:13:59.548: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 7.414569ms
    Jun 13 03:14:01.557: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.017000935s
    Jun 13 03:14:01.557: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-c785b12c-73ec-4856-ad4b-188d5c401617 off the node sks-test-v1-25-9-workergroup-469fm 06/13/23 03:14:01.564
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-c785b12c-73ec-4856-ad4b-188d5c401617 06/13/23 03:14:01.59
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:14:01.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-6348" for this suite. 06/13/23 03:14:01.606
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:14:01.657
Jun 13 03:14:01.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:14:01.659
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:01.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:01.694
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394
STEP: creating a Service 06/13/23 03:14:01.713
STEP: watching for the Service to be added 06/13/23 03:14:01.74
Jun 13 03:14:01.746: INFO: Found Service test-service-v8k4j in namespace services-9848 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jun 13 03:14:01.746: INFO: Service test-service-v8k4j created
STEP: Getting /status 06/13/23 03:14:01.746
Jun 13 03:14:01.763: INFO: Service test-service-v8k4j has LoadBalancer: {[]}
STEP: patching the ServiceStatus 06/13/23 03:14:01.763
STEP: watching for the Service to be patched 06/13/23 03:14:01.775
Jun 13 03:14:01.777: INFO: observed Service test-service-v8k4j in namespace services-9848 with annotations: map[] & LoadBalancer: {[]}
Jun 13 03:14:01.777: INFO: Found Service test-service-v8k4j in namespace services-9848 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jun 13 03:14:01.777: INFO: Service test-service-v8k4j has service status patched
STEP: updating the ServiceStatus 06/13/23 03:14:01.777
Jun 13 03:14:01.803: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 06/13/23 03:14:01.804
Jun 13 03:14:01.808: INFO: Observed Service test-service-v8k4j in namespace services-9848 with annotations: map[] & Conditions: {[]}
Jun 13 03:14:01.808: INFO: Observed event: &Service{ObjectMeta:{test-service-v8k4j  services-9848  ca21208a-d63a-4c1d-9cfb-e589d2a96414 32447 0 2023-06-13 03:14:01 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-13 03:14:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-13 03:14:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.106.214.102,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.106.214.102],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jun 13 03:14:01.808: INFO: Found Service test-service-v8k4j in namespace services-9848 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 13 03:14:01.808: INFO: Service test-service-v8k4j has service status updated
STEP: patching the service 06/13/23 03:14:01.808
STEP: watching for the Service to be patched 06/13/23 03:14:01.832
Jun 13 03:14:01.838: INFO: observed Service test-service-v8k4j in namespace services-9848 with labels: map[test-service-static:true]
Jun 13 03:14:01.838: INFO: observed Service test-service-v8k4j in namespace services-9848 with labels: map[test-service-static:true]
Jun 13 03:14:01.838: INFO: observed Service test-service-v8k4j in namespace services-9848 with labels: map[test-service-static:true]
Jun 13 03:14:01.838: INFO: Found Service test-service-v8k4j in namespace services-9848 with labels: map[test-service:patched test-service-static:true]
Jun 13 03:14:01.838: INFO: Service test-service-v8k4j patched
STEP: deleting the service 06/13/23 03:14:01.838
STEP: watching for the Service to be deleted 06/13/23 03:14:01.878
Jun 13 03:14:01.882: INFO: Observed event: ADDED
Jun 13 03:14:01.882: INFO: Observed event: MODIFIED
Jun 13 03:14:01.882: INFO: Observed event: MODIFIED
Jun 13 03:14:01.882: INFO: Observed event: MODIFIED
Jun 13 03:14:01.882: INFO: Found Service test-service-v8k4j in namespace services-9848 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jun 13 03:14:01.882: INFO: Service test-service-v8k4j deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:14:01.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9848" for this suite. 06/13/23 03:14:01.893
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":185,"skipped":3711,"failed":0}
------------------------------
• [0.257 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:14:01.657
    Jun 13 03:14:01.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:14:01.659
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:01.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:01.694
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3394
    STEP: creating a Service 06/13/23 03:14:01.713
    STEP: watching for the Service to be added 06/13/23 03:14:01.74
    Jun 13 03:14:01.746: INFO: Found Service test-service-v8k4j in namespace services-9848 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jun 13 03:14:01.746: INFO: Service test-service-v8k4j created
    STEP: Getting /status 06/13/23 03:14:01.746
    Jun 13 03:14:01.763: INFO: Service test-service-v8k4j has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 06/13/23 03:14:01.763
    STEP: watching for the Service to be patched 06/13/23 03:14:01.775
    Jun 13 03:14:01.777: INFO: observed Service test-service-v8k4j in namespace services-9848 with annotations: map[] & LoadBalancer: {[]}
    Jun 13 03:14:01.777: INFO: Found Service test-service-v8k4j in namespace services-9848 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jun 13 03:14:01.777: INFO: Service test-service-v8k4j has service status patched
    STEP: updating the ServiceStatus 06/13/23 03:14:01.777
    Jun 13 03:14:01.803: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 06/13/23 03:14:01.804
    Jun 13 03:14:01.808: INFO: Observed Service test-service-v8k4j in namespace services-9848 with annotations: map[] & Conditions: {[]}
    Jun 13 03:14:01.808: INFO: Observed event: &Service{ObjectMeta:{test-service-v8k4j  services-9848  ca21208a-d63a-4c1d-9cfb-e589d2a96414 32447 0 2023-06-13 03:14:01 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-13 03:14:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-13 03:14:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.106.214.102,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.106.214.102],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jun 13 03:14:01.808: INFO: Found Service test-service-v8k4j in namespace services-9848 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 13 03:14:01.808: INFO: Service test-service-v8k4j has service status updated
    STEP: patching the service 06/13/23 03:14:01.808
    STEP: watching for the Service to be patched 06/13/23 03:14:01.832
    Jun 13 03:14:01.838: INFO: observed Service test-service-v8k4j in namespace services-9848 with labels: map[test-service-static:true]
    Jun 13 03:14:01.838: INFO: observed Service test-service-v8k4j in namespace services-9848 with labels: map[test-service-static:true]
    Jun 13 03:14:01.838: INFO: observed Service test-service-v8k4j in namespace services-9848 with labels: map[test-service-static:true]
    Jun 13 03:14:01.838: INFO: Found Service test-service-v8k4j in namespace services-9848 with labels: map[test-service:patched test-service-static:true]
    Jun 13 03:14:01.838: INFO: Service test-service-v8k4j patched
    STEP: deleting the service 06/13/23 03:14:01.838
    STEP: watching for the Service to be deleted 06/13/23 03:14:01.878
    Jun 13 03:14:01.882: INFO: Observed event: ADDED
    Jun 13 03:14:01.882: INFO: Observed event: MODIFIED
    Jun 13 03:14:01.882: INFO: Observed event: MODIFIED
    Jun 13 03:14:01.882: INFO: Observed event: MODIFIED
    Jun 13 03:14:01.882: INFO: Found Service test-service-v8k4j in namespace services-9848 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jun 13 03:14:01.882: INFO: Service test-service-v8k4j deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:14:01.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9848" for this suite. 06/13/23 03:14:01.893
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:14:01.914
Jun 13 03:14:01.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename dns 06/13/23 03:14:01.916
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:01.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:01.954
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 06/13/23 03:14:01.964
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 06/13/23 03:14:01.965
STEP: creating a pod to probe DNS 06/13/23 03:14:01.965
STEP: submitting the pod to kubernetes 06/13/23 03:14:01.965
Jun 13 03:14:01.981: INFO: Waiting up to 15m0s for pod "dns-test-998c0d34-9816-4eb9-9bde-3fb7bcc591d1" in namespace "dns-7614" to be "running"
Jun 13 03:14:01.989: INFO: Pod "dns-test-998c0d34-9816-4eb9-9bde-3fb7bcc591d1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.120671ms
Jun 13 03:14:03.996: INFO: Pod "dns-test-998c0d34-9816-4eb9-9bde-3fb7bcc591d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014597442s
Jun 13 03:14:03.996: INFO: Pod "dns-test-998c0d34-9816-4eb9-9bde-3fb7bcc591d1" satisfied condition "running"
STEP: retrieving the pod 06/13/23 03:14:03.996
STEP: looking for the results for each expected name from probers 06/13/23 03:14:04.002
Jun 13 03:14:04.028: INFO: DNS probes using dns-7614/dns-test-998c0d34-9816-4eb9-9bde-3fb7bcc591d1 succeeded

STEP: deleting the pod 06/13/23 03:14:04.029
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 13 03:14:04.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7614" for this suite. 06/13/23 03:14:04.055
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":186,"skipped":3719,"failed":0}
------------------------------
• [2.151 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:14:01.914
    Jun 13 03:14:01.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename dns 06/13/23 03:14:01.916
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:01.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:01.954
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     06/13/23 03:14:01.964
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     06/13/23 03:14:01.965
    STEP: creating a pod to probe DNS 06/13/23 03:14:01.965
    STEP: submitting the pod to kubernetes 06/13/23 03:14:01.965
    Jun 13 03:14:01.981: INFO: Waiting up to 15m0s for pod "dns-test-998c0d34-9816-4eb9-9bde-3fb7bcc591d1" in namespace "dns-7614" to be "running"
    Jun 13 03:14:01.989: INFO: Pod "dns-test-998c0d34-9816-4eb9-9bde-3fb7bcc591d1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.120671ms
    Jun 13 03:14:03.996: INFO: Pod "dns-test-998c0d34-9816-4eb9-9bde-3fb7bcc591d1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014597442s
    Jun 13 03:14:03.996: INFO: Pod "dns-test-998c0d34-9816-4eb9-9bde-3fb7bcc591d1" satisfied condition "running"
    STEP: retrieving the pod 06/13/23 03:14:03.996
    STEP: looking for the results for each expected name from probers 06/13/23 03:14:04.002
    Jun 13 03:14:04.028: INFO: DNS probes using dns-7614/dns-test-998c0d34-9816-4eb9-9bde-3fb7bcc591d1 succeeded

    STEP: deleting the pod 06/13/23 03:14:04.029
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 13 03:14:04.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7614" for this suite. 06/13/23 03:14:04.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:14:04.069
Jun 13 03:14:04.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:14:04.07
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:04.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:04.097
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 06/13/23 03:14:04.103
Jun 13 03:14:04.118: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc" in namespace "downward-api-1623" to be "Succeeded or Failed"
Jun 13 03:14:04.129: INFO: Pod "downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.756239ms
Jun 13 03:14:06.137: INFO: Pod "downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019143697s
Jun 13 03:14:08.135: INFO: Pod "downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01747409s
STEP: Saw pod success 06/13/23 03:14:08.135
Jun 13 03:14:08.136: INFO: Pod "downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc" satisfied condition "Succeeded or Failed"
Jun 13 03:14:08.140: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc container client-container: <nil>
STEP: delete the pod 06/13/23 03:14:08.162
Jun 13 03:14:08.180: INFO: Waiting for pod downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc to disappear
Jun 13 03:14:08.191: INFO: Pod downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 13 03:14:08.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1623" for this suite. 06/13/23 03:14:08.202
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":187,"skipped":3745,"failed":0}
------------------------------
• [4.145 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:14:04.069
    Jun 13 03:14:04.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:14:04.07
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:04.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:04.097
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 06/13/23 03:14:04.103
    Jun 13 03:14:04.118: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc" in namespace "downward-api-1623" to be "Succeeded or Failed"
    Jun 13 03:14:04.129: INFO: Pod "downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.756239ms
    Jun 13 03:14:06.137: INFO: Pod "downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019143697s
    Jun 13 03:14:08.135: INFO: Pod "downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01747409s
    STEP: Saw pod success 06/13/23 03:14:08.135
    Jun 13 03:14:08.136: INFO: Pod "downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc" satisfied condition "Succeeded or Failed"
    Jun 13 03:14:08.140: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc container client-container: <nil>
    STEP: delete the pod 06/13/23 03:14:08.162
    Jun 13 03:14:08.180: INFO: Waiting for pod downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc to disappear
    Jun 13 03:14:08.191: INFO: Pod downwardapi-volume-22003918-ced0-43d5-ac81-999280223edc no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 13 03:14:08.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1623" for this suite. 06/13/23 03:14:08.202
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:14:08.216
Jun 13 03:14:08.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 03:14:08.217
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:08.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:08.244
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-4b7d49d0-98b2-48ed-a686-f6d0115a6f7e 06/13/23 03:14:08.249
STEP: Creating a pod to test consume secrets 06/13/23 03:14:08.259
Jun 13 03:14:08.272: INFO: Waiting up to 5m0s for pod "pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743" in namespace "secrets-5673" to be "Succeeded or Failed"
Jun 13 03:14:08.277: INFO: Pod "pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743": Phase="Pending", Reason="", readiness=false. Elapsed: 5.634441ms
Jun 13 03:14:10.286: INFO: Pod "pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013848172s
Jun 13 03:14:12.284: INFO: Pod "pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012519277s
STEP: Saw pod success 06/13/23 03:14:12.284
Jun 13 03:14:12.284: INFO: Pod "pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743" satisfied condition "Succeeded or Failed"
Jun 13 03:14:12.290: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743 container secret-volume-test: <nil>
STEP: delete the pod 06/13/23 03:14:12.302
Jun 13 03:14:12.316: INFO: Waiting for pod pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743 to disappear
Jun 13 03:14:12.320: INFO: Pod pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 13 03:14:12.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5673" for this suite. 06/13/23 03:14:12.328
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":188,"skipped":3749,"failed":0}
------------------------------
• [4.122 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:14:08.216
    Jun 13 03:14:08.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 03:14:08.217
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:08.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:08.244
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-4b7d49d0-98b2-48ed-a686-f6d0115a6f7e 06/13/23 03:14:08.249
    STEP: Creating a pod to test consume secrets 06/13/23 03:14:08.259
    Jun 13 03:14:08.272: INFO: Waiting up to 5m0s for pod "pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743" in namespace "secrets-5673" to be "Succeeded or Failed"
    Jun 13 03:14:08.277: INFO: Pod "pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743": Phase="Pending", Reason="", readiness=false. Elapsed: 5.634441ms
    Jun 13 03:14:10.286: INFO: Pod "pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013848172s
    Jun 13 03:14:12.284: INFO: Pod "pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012519277s
    STEP: Saw pod success 06/13/23 03:14:12.284
    Jun 13 03:14:12.284: INFO: Pod "pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743" satisfied condition "Succeeded or Failed"
    Jun 13 03:14:12.290: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743 container secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:14:12.302
    Jun 13 03:14:12.316: INFO: Waiting for pod pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743 to disappear
    Jun 13 03:14:12.320: INFO: Pod pod-secrets-6bd9c4d2-70ec-4b41-a46e-2f8539bd8743 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 03:14:12.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5673" for this suite. 06/13/23 03:14:12.328
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:14:12.338
Jun 13 03:14:12.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-lifecycle-hook 06/13/23 03:14:12.339
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:12.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:12.363
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 06/13/23 03:14:12.378
Jun 13 03:14:12.393: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1634" to be "running and ready"
Jun 13 03:14:12.400: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.000517ms
Jun 13 03:14:12.400: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:14:14.408: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014926375s
Jun 13 03:14:14.408: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 13 03:14:14.408: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 06/13/23 03:14:14.413
Jun 13 03:14:14.421: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1634" to be "running and ready"
Jun 13 03:14:14.427: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.260861ms
Jun 13 03:14:14.427: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:14:16.440: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018067217s
Jun 13 03:14:16.440: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jun 13 03:14:16.440: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 06/13/23 03:14:16.447
STEP: delete the pod with lifecycle hook 06/13/23 03:14:16.475
Jun 13 03:14:16.494: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 13 03:14:16.500: INFO: Pod pod-with-poststart-http-hook still exists
Jun 13 03:14:18.501: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 13 03:14:18.513: INFO: Pod pod-with-poststart-http-hook still exists
Jun 13 03:14:20.504: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 13 03:14:20.530: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jun 13 03:14:20.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1634" for this suite. 06/13/23 03:14:20.539
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":189,"skipped":3751,"failed":0}
------------------------------
• [SLOW TEST] [8.215 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:14:12.338
    Jun 13 03:14:12.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/13/23 03:14:12.339
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:12.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:12.363
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 06/13/23 03:14:12.378
    Jun 13 03:14:12.393: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1634" to be "running and ready"
    Jun 13 03:14:12.400: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 7.000517ms
    Jun 13 03:14:12.400: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:14:14.408: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.014926375s
    Jun 13 03:14:14.408: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 13 03:14:14.408: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 06/13/23 03:14:14.413
    Jun 13 03:14:14.421: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1634" to be "running and ready"
    Jun 13 03:14:14.427: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.260861ms
    Jun 13 03:14:14.427: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:14:16.440: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.018067217s
    Jun 13 03:14:16.440: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jun 13 03:14:16.440: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 06/13/23 03:14:16.447
    STEP: delete the pod with lifecycle hook 06/13/23 03:14:16.475
    Jun 13 03:14:16.494: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun 13 03:14:16.500: INFO: Pod pod-with-poststart-http-hook still exists
    Jun 13 03:14:18.501: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun 13 03:14:18.513: INFO: Pod pod-with-poststart-http-hook still exists
    Jun 13 03:14:20.504: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun 13 03:14:20.530: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jun 13 03:14:20.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1634" for this suite. 06/13/23 03:14:20.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:14:20.554
Jun 13 03:14:20.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:14:20.556
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:20.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:20.6
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-69454055-a2eb-40e0-8244-a3faa430ab93 06/13/23 03:14:20.606
STEP: Creating a pod to test consume configMaps 06/13/23 03:14:20.625
Jun 13 03:14:20.666: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a" in namespace "projected-6760" to be "Succeeded or Failed"
Jun 13 03:14:20.689: INFO: Pod "pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.210969ms
Jun 13 03:14:22.696: INFO: Pod "pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029985965s
Jun 13 03:14:24.697: INFO: Pod "pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03174737s
STEP: Saw pod success 06/13/23 03:14:24.698
Jun 13 03:14:24.698: INFO: Pod "pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a" satisfied condition "Succeeded or Failed"
Jun 13 03:14:24.704: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a container agnhost-container: <nil>
STEP: delete the pod 06/13/23 03:14:24.715
Jun 13 03:14:24.731: INFO: Waiting for pod pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a to disappear
Jun 13 03:14:24.739: INFO: Pod pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 13 03:14:24.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6760" for this suite. 06/13/23 03:14:24.748
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":190,"skipped":3760,"failed":0}
------------------------------
• [4.206 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:14:20.554
    Jun 13 03:14:20.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:14:20.556
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:20.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:20.6
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-69454055-a2eb-40e0-8244-a3faa430ab93 06/13/23 03:14:20.606
    STEP: Creating a pod to test consume configMaps 06/13/23 03:14:20.625
    Jun 13 03:14:20.666: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a" in namespace "projected-6760" to be "Succeeded or Failed"
    Jun 13 03:14:20.689: INFO: Pod "pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a": Phase="Pending", Reason="", readiness=false. Elapsed: 23.210969ms
    Jun 13 03:14:22.696: INFO: Pod "pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029985965s
    Jun 13 03:14:24.697: INFO: Pod "pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03174737s
    STEP: Saw pod success 06/13/23 03:14:24.698
    Jun 13 03:14:24.698: INFO: Pod "pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a" satisfied condition "Succeeded or Failed"
    Jun 13 03:14:24.704: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 03:14:24.715
    Jun 13 03:14:24.731: INFO: Waiting for pod pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a to disappear
    Jun 13 03:14:24.739: INFO: Pod pod-projected-configmaps-14f87b57-ff07-49cf-9f1c-85194750045a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 13 03:14:24.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6760" for this suite. 06/13/23 03:14:24.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:14:24.761
Jun 13 03:14:24.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 03:14:24.763
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:24.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:24.794
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 06/13/23 03:14:24.8
Jun 13 03:14:24.801: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-787 proxy --unix-socket=/tmp/kubectl-proxy-unix3616505586/test'
STEP: retrieving proxy /api/ output 06/13/23 03:14:24.869
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 03:14:24.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-787" for this suite. 06/13/23 03:14:24.879
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":191,"skipped":3787,"failed":0}
------------------------------
• [0.136 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:14:24.761
    Jun 13 03:14:24.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 03:14:24.763
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:24.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:24.794
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 06/13/23 03:14:24.8
    Jun 13 03:14:24.801: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-787 proxy --unix-socket=/tmp/kubectl-proxy-unix3616505586/test'
    STEP: retrieving proxy /api/ output 06/13/23 03:14:24.869
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 03:14:24.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-787" for this suite. 06/13/23 03:14:24.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:14:24.898
Jun 13 03:14:24.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename containers 06/13/23 03:14:24.9
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:24.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:24.935
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 06/13/23 03:14:24.94
Jun 13 03:14:24.954: INFO: Waiting up to 5m0s for pod "client-containers-de631dec-8bee-4d4b-8b07-483585e9262d" in namespace "containers-8007" to be "Succeeded or Failed"
Jun 13 03:14:24.961: INFO: Pod "client-containers-de631dec-8bee-4d4b-8b07-483585e9262d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.844192ms
Jun 13 03:14:27.005: INFO: Pod "client-containers-de631dec-8bee-4d4b-8b07-483585e9262d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050869871s
Jun 13 03:14:28.969: INFO: Pod "client-containers-de631dec-8bee-4d4b-8b07-483585e9262d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015193183s
Jun 13 03:14:30.968: INFO: Pod "client-containers-de631dec-8bee-4d4b-8b07-483585e9262d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014006103s
STEP: Saw pod success 06/13/23 03:14:30.968
Jun 13 03:14:30.968: INFO: Pod "client-containers-de631dec-8bee-4d4b-8b07-483585e9262d" satisfied condition "Succeeded or Failed"
Jun 13 03:14:30.973: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod client-containers-de631dec-8bee-4d4b-8b07-483585e9262d container agnhost-container: <nil>
STEP: delete the pod 06/13/23 03:14:30.983
Jun 13 03:14:31.000: INFO: Waiting for pod client-containers-de631dec-8bee-4d4b-8b07-483585e9262d to disappear
Jun 13 03:14:31.005: INFO: Pod client-containers-de631dec-8bee-4d4b-8b07-483585e9262d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jun 13 03:14:31.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8007" for this suite. 06/13/23 03:14:31.012
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":192,"skipped":3794,"failed":0}
------------------------------
• [SLOW TEST] [6.122 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:14:24.898
    Jun 13 03:14:24.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename containers 06/13/23 03:14:24.9
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:24.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:24.935
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 06/13/23 03:14:24.94
    Jun 13 03:14:24.954: INFO: Waiting up to 5m0s for pod "client-containers-de631dec-8bee-4d4b-8b07-483585e9262d" in namespace "containers-8007" to be "Succeeded or Failed"
    Jun 13 03:14:24.961: INFO: Pod "client-containers-de631dec-8bee-4d4b-8b07-483585e9262d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.844192ms
    Jun 13 03:14:27.005: INFO: Pod "client-containers-de631dec-8bee-4d4b-8b07-483585e9262d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050869871s
    Jun 13 03:14:28.969: INFO: Pod "client-containers-de631dec-8bee-4d4b-8b07-483585e9262d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015193183s
    Jun 13 03:14:30.968: INFO: Pod "client-containers-de631dec-8bee-4d4b-8b07-483585e9262d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.014006103s
    STEP: Saw pod success 06/13/23 03:14:30.968
    Jun 13 03:14:30.968: INFO: Pod "client-containers-de631dec-8bee-4d4b-8b07-483585e9262d" satisfied condition "Succeeded or Failed"
    Jun 13 03:14:30.973: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod client-containers-de631dec-8bee-4d4b-8b07-483585e9262d container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 03:14:30.983
    Jun 13 03:14:31.000: INFO: Waiting for pod client-containers-de631dec-8bee-4d4b-8b07-483585e9262d to disappear
    Jun 13 03:14:31.005: INFO: Pod client-containers-de631dec-8bee-4d4b-8b07-483585e9262d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jun 13 03:14:31.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8007" for this suite. 06/13/23 03:14:31.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:14:31.021
Jun 13 03:14:31.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename watch 06/13/23 03:14:31.022
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:31.041
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:31.046
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 06/13/23 03:14:31.051
STEP: creating a new configmap 06/13/23 03:14:31.054
STEP: modifying the configmap once 06/13/23 03:14:31.062
STEP: closing the watch once it receives two notifications 06/13/23 03:14:31.075
Jun 13 03:14:31.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6092  d82811c1-1a97-4d51-b96c-79bdf729bb15 32827 0 2023-06-13 03:14:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-13 03:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 03:14:31.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6092  d82811c1-1a97-4d51-b96c-79bdf729bb15 32828 0 2023-06-13 03:14:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-13 03:14:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 06/13/23 03:14:31.076
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 06/13/23 03:14:31.099
STEP: deleting the configmap 06/13/23 03:14:31.102
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 06/13/23 03:14:31.112
Jun 13 03:14:31.112: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6092  d82811c1-1a97-4d51-b96c-79bdf729bb15 32829 0 2023-06-13 03:14:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-13 03:14:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 03:14:31.112: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6092  d82811c1-1a97-4d51-b96c-79bdf729bb15 32830 0 2023-06-13 03:14:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-13 03:14:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 13 03:14:31.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6092" for this suite. 06/13/23 03:14:31.12
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":193,"skipped":3814,"failed":0}
------------------------------
• [0.115 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:14:31.021
    Jun 13 03:14:31.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename watch 06/13/23 03:14:31.022
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:31.041
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:31.046
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 06/13/23 03:14:31.051
    STEP: creating a new configmap 06/13/23 03:14:31.054
    STEP: modifying the configmap once 06/13/23 03:14:31.062
    STEP: closing the watch once it receives two notifications 06/13/23 03:14:31.075
    Jun 13 03:14:31.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6092  d82811c1-1a97-4d51-b96c-79bdf729bb15 32827 0 2023-06-13 03:14:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-13 03:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 03:14:31.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6092  d82811c1-1a97-4d51-b96c-79bdf729bb15 32828 0 2023-06-13 03:14:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-13 03:14:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 06/13/23 03:14:31.076
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 06/13/23 03:14:31.099
    STEP: deleting the configmap 06/13/23 03:14:31.102
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 06/13/23 03:14:31.112
    Jun 13 03:14:31.112: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6092  d82811c1-1a97-4d51-b96c-79bdf729bb15 32829 0 2023-06-13 03:14:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-13 03:14:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 03:14:31.112: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6092  d82811c1-1a97-4d51-b96c-79bdf729bb15 32830 0 2023-06-13 03:14:31 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-13 03:14:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 13 03:14:31.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6092" for this suite. 06/13/23 03:14:31.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:14:31.137
Jun 13 03:14:31.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename sched-pred 06/13/23 03:14:31.138
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:31.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:31.166
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 13 03:14:31.170: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 13 03:14:31.188: INFO: Waiting for terminating namespaces to be deleted...
Jun 13 03:14:31.195: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-2q6k2 before test
Jun 13 03:14:31.210: INFO: calico-node-xb8kf from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
Jun 13 03:14:31.210: INFO: 	Container calico-node ready: true, restart count 0
Jun 13 03:14:31.210: INFO: calico-typha-79dfdd7d65-w95lk from calico-system started at 2023-06-13 02:04:34 +0000 UTC (1 container statuses recorded)
Jun 13 03:14:31.210: INFO: 	Container calico-typha ready: true, restart count 0
Jun 13 03:14:31.210: INFO: csi-node-driver-vkf24 from calico-system started at 2023-06-13 02:05:19 +0000 UTC (2 container statuses recorded)
Jun 13 03:14:31.210: INFO: 	Container calico-csi ready: true, restart count 0
Jun 13 03:14:31.210: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun 13 03:14:31.210: INFO: kube-proxy-g2qcx from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
Jun 13 03:14:31.210: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 13 03:14:31.210: INFO: smtx-elf-csi-driver-node-plugin-gqbkk from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
Jun 13 03:14:31.210: INFO: 	Container csi-driver ready: true, restart count 0
Jun 13 03:14:31.210: INFO: 	Container driver-registrar ready: true, restart count 0
Jun 13 03:14:31.210: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 13 03:14:31.210: INFO: sonobuoy from sonobuoy started at 2023-06-13 02:14:11 +0000 UTC (1 container statuses recorded)
Jun 13 03:14:31.210: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 13 03:14:31.210: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-f9dsl from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:14:31.210: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 03:14:31.210: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 13 03:14:31.210: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-469fm before test
Jun 13 03:14:31.225: INFO: calico-node-lhmmz from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
Jun 13 03:14:31.225: INFO: 	Container calico-node ready: true, restart count 0
Jun 13 03:14:31.225: INFO: csi-node-driver-jrfzt from calico-system started at 2023-06-13 02:04:55 +0000 UTC (2 container statuses recorded)
Jun 13 03:14:31.225: INFO: 	Container calico-csi ready: true, restart count 0
Jun 13 03:14:31.225: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun 13 03:14:31.225: INFO: kube-proxy-fn2dq from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
Jun 13 03:14:31.225: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 13 03:14:31.225: INFO: snapshot-controller-76c6888c5-9m5td from kube-system started at 2023-06-13 02:04:55 +0000 UTC (1 container statuses recorded)
Jun 13 03:14:31.225: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 13 03:14:31.225: INFO: smtx-elf-csi-driver-node-plugin-h8k7m from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
Jun 13 03:14:31.225: INFO: 	Container csi-driver ready: true, restart count 0
Jun 13 03:14:31.225: INFO: 	Container driver-registrar ready: true, restart count 0
Jun 13 03:14:31.225: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 13 03:14:31.225: INFO: tigera-operator-6f49bd984-r7pkz from sks-system started at 2023-06-13 02:04:29 +0000 UTC (1 container statuses recorded)
Jun 13 03:14:31.225: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 13 03:14:31.225: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-hcvsr from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:14:31.225: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 03:14:31.225: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 13 03:14:31.225: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-l5gcd before test
Jun 13 03:14:31.238: INFO: calico-node-fptww from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
Jun 13 03:14:31.238: INFO: 	Container calico-node ready: true, restart count 0
Jun 13 03:14:31.238: INFO: calico-typha-79dfdd7d65-nrkt5 from calico-system started at 2023-06-13 02:04:43 +0000 UTC (1 container statuses recorded)
Jun 13 03:14:31.238: INFO: 	Container calico-typha ready: true, restart count 0
Jun 13 03:14:31.238: INFO: csi-node-driver-66bp4 from calico-system started at 2023-06-13 02:05:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:14:31.238: INFO: 	Container calico-csi ready: true, restart count 0
Jun 13 03:14:31.238: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun 13 03:14:31.238: INFO: kube-proxy-669th from kube-system started at 2023-06-13 02:03:17 +0000 UTC (1 container statuses recorded)
Jun 13 03:14:31.238: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 13 03:14:31.238: INFO: smtx-elf-csi-driver-node-plugin-dtt2k from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
Jun 13 03:14:31.238: INFO: 	Container csi-driver ready: true, restart count 0
Jun 13 03:14:31.238: INFO: 	Container driver-registrar ready: true, restart count 0
Jun 13 03:14:31.238: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 13 03:14:31.238: INFO: sonobuoy-e2e-job-246c2b6dcb314696 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:14:31.238: INFO: 	Container e2e ready: true, restart count 0
Jun 13 03:14:31.238: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 03:14:31.238: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-bvbp2 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:14:31.238: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 03:14:31.238: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node sks-test-v1-25-9-workergroup-2q6k2 06/13/23 03:14:31.274
STEP: verifying the node has the label node sks-test-v1-25-9-workergroup-469fm 06/13/23 03:14:31.3
STEP: verifying the node has the label node sks-test-v1-25-9-workergroup-l5gcd 06/13/23 03:14:31.319
Jun 13 03:14:31.349: INFO: Pod calico-node-fptww requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-l5gcd
Jun 13 03:14:31.349: INFO: Pod calico-node-lhmmz requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-469fm
Jun 13 03:14:31.349: INFO: Pod calico-node-xb8kf requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-2q6k2
Jun 13 03:14:31.349: INFO: Pod calico-typha-79dfdd7d65-nrkt5 requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-l5gcd
Jun 13 03:14:31.349: INFO: Pod calico-typha-79dfdd7d65-w95lk requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-2q6k2
Jun 13 03:14:31.349: INFO: Pod csi-node-driver-66bp4 requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-l5gcd
Jun 13 03:14:31.349: INFO: Pod csi-node-driver-jrfzt requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-469fm
Jun 13 03:14:31.349: INFO: Pod csi-node-driver-vkf24 requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-2q6k2
Jun 13 03:14:31.349: INFO: Pod kube-proxy-669th requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-l5gcd
Jun 13 03:14:31.349: INFO: Pod kube-proxy-fn2dq requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-469fm
Jun 13 03:14:31.349: INFO: Pod kube-proxy-g2qcx requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-2q6k2
Jun 13 03:14:31.349: INFO: Pod snapshot-controller-76c6888c5-9m5td requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-469fm
Jun 13 03:14:31.349: INFO: Pod smtx-elf-csi-driver-node-plugin-dtt2k requesting resource cpu=210m on Node sks-test-v1-25-9-workergroup-l5gcd
Jun 13 03:14:31.349: INFO: Pod smtx-elf-csi-driver-node-plugin-gqbkk requesting resource cpu=210m on Node sks-test-v1-25-9-workergroup-2q6k2
Jun 13 03:14:31.349: INFO: Pod smtx-elf-csi-driver-node-plugin-h8k7m requesting resource cpu=210m on Node sks-test-v1-25-9-workergroup-469fm
Jun 13 03:14:31.349: INFO: Pod tigera-operator-6f49bd984-r7pkz requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-469fm
Jun 13 03:14:31.349: INFO: Pod sonobuoy requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-2q6k2
Jun 13 03:14:31.349: INFO: Pod sonobuoy-e2e-job-246c2b6dcb314696 requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-l5gcd
Jun 13 03:14:31.349: INFO: Pod sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-bvbp2 requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-l5gcd
Jun 13 03:14:31.349: INFO: Pod sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-f9dsl requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-2q6k2
Jun 13 03:14:31.349: INFO: Pod sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-hcvsr requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-469fm
STEP: Starting Pods to consume most of the cluster CPU. 06/13/23 03:14:31.349
Jun 13 03:14:31.349: INFO: Creating a pod which consumes cpu=1253m on Node sks-test-v1-25-9-workergroup-2q6k2
Jun 13 03:14:31.364: INFO: Creating a pod which consumes cpu=1253m on Node sks-test-v1-25-9-workergroup-469fm
Jun 13 03:14:31.381: INFO: Creating a pod which consumes cpu=1253m on Node sks-test-v1-25-9-workergroup-l5gcd
Jun 13 03:14:31.393: INFO: Waiting up to 5m0s for pod "filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854" in namespace "sched-pred-2123" to be "running"
Jun 13 03:14:31.408: INFO: Pod "filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854": Phase="Pending", Reason="", readiness=false. Elapsed: 14.634585ms
Jun 13 03:14:33.415: INFO: Pod "filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854": Phase="Running", Reason="", readiness=true. Elapsed: 2.021801519s
Jun 13 03:14:33.415: INFO: Pod "filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854" satisfied condition "running"
Jun 13 03:14:33.415: INFO: Waiting up to 5m0s for pod "filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5" in namespace "sched-pred-2123" to be "running"
Jun 13 03:14:33.422: INFO: Pod "filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5": Phase="Running", Reason="", readiness=true. Elapsed: 6.753711ms
Jun 13 03:14:33.422: INFO: Pod "filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5" satisfied condition "running"
Jun 13 03:14:33.422: INFO: Waiting up to 5m0s for pod "filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e" in namespace "sched-pred-2123" to be "running"
Jun 13 03:14:33.428: INFO: Pod "filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e": Phase="Running", Reason="", readiness=true. Elapsed: 6.164335ms
Jun 13 03:14:33.428: INFO: Pod "filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 06/13/23 03:14:33.428
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854.17681979cd76e710], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2123/filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854 to sks-test-v1-25-9-workergroup-2q6k2] 06/13/23 03:14:33.434
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854.17681979fddb2e5f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/13/23 03:14:33.435
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854.1768197a005363be], Reason = [Created], Message = [Created container filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854] 06/13/23 03:14:33.435
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854.1768197a07dbdfca], Reason = [Started], Message = [Started container filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854] 06/13/23 03:14:33.435
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5.17681979ce19317f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2123/filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5 to sks-test-v1-25-9-workergroup-469fm] 06/13/23 03:14:33.435
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5.1768197a023166a4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/13/23 03:14:33.435
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5.1768197a05089904], Reason = [Created], Message = [Created container filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5] 06/13/23 03:14:33.435
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5.1768197a0dc05a5a], Reason = [Started], Message = [Started container filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5] 06/13/23 03:14:33.435
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e.17681979cecf8835], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2123/filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e to sks-test-v1-25-9-workergroup-l5gcd] 06/13/23 03:14:33.435
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e.1768197a01977147], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/13/23 03:14:33.435
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e.1768197a043bff0b], Reason = [Created], Message = [Created container filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e] 06/13/23 03:14:33.435
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e.1768197a0d93cf84], Reason = [Started], Message = [Started container filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e] 06/13/23 03:14:33.435
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1768197a48d92230], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 06/13/23 03:14:33.458
STEP: removing the label node off the node sks-test-v1-25-9-workergroup-469fm 06/13/23 03:14:34.458
STEP: verifying the node doesn't have the label node 06/13/23 03:14:34.481
STEP: removing the label node off the node sks-test-v1-25-9-workergroup-l5gcd 06/13/23 03:14:34.486
STEP: verifying the node doesn't have the label node 06/13/23 03:14:34.508
STEP: removing the label node off the node sks-test-v1-25-9-workergroup-2q6k2 06/13/23 03:14:34.526
STEP: verifying the node doesn't have the label node 06/13/23 03:14:34.55
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:14:34.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2123" for this suite. 06/13/23 03:14:34.57
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":194,"skipped":3823,"failed":0}
------------------------------
• [3.448 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:14:31.137
    Jun 13 03:14:31.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename sched-pred 06/13/23 03:14:31.138
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:31.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:31.166
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jun 13 03:14:31.170: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 13 03:14:31.188: INFO: Waiting for terminating namespaces to be deleted...
    Jun 13 03:14:31.195: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-2q6k2 before test
    Jun 13 03:14:31.210: INFO: calico-node-xb8kf from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
    Jun 13 03:14:31.210: INFO: 	Container calico-node ready: true, restart count 0
    Jun 13 03:14:31.210: INFO: calico-typha-79dfdd7d65-w95lk from calico-system started at 2023-06-13 02:04:34 +0000 UTC (1 container statuses recorded)
    Jun 13 03:14:31.210: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 13 03:14:31.210: INFO: csi-node-driver-vkf24 from calico-system started at 2023-06-13 02:05:19 +0000 UTC (2 container statuses recorded)
    Jun 13 03:14:31.210: INFO: 	Container calico-csi ready: true, restart count 0
    Jun 13 03:14:31.210: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun 13 03:14:31.210: INFO: kube-proxy-g2qcx from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
    Jun 13 03:14:31.210: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 13 03:14:31.210: INFO: smtx-elf-csi-driver-node-plugin-gqbkk from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
    Jun 13 03:14:31.210: INFO: 	Container csi-driver ready: true, restart count 0
    Jun 13 03:14:31.210: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun 13 03:14:31.210: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 13 03:14:31.210: INFO: sonobuoy from sonobuoy started at 2023-06-13 02:14:11 +0000 UTC (1 container statuses recorded)
    Jun 13 03:14:31.210: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 13 03:14:31.210: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-f9dsl from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:14:31.210: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 03:14:31.210: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 13 03:14:31.210: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-469fm before test
    Jun 13 03:14:31.225: INFO: calico-node-lhmmz from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
    Jun 13 03:14:31.225: INFO: 	Container calico-node ready: true, restart count 0
    Jun 13 03:14:31.225: INFO: csi-node-driver-jrfzt from calico-system started at 2023-06-13 02:04:55 +0000 UTC (2 container statuses recorded)
    Jun 13 03:14:31.225: INFO: 	Container calico-csi ready: true, restart count 0
    Jun 13 03:14:31.225: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun 13 03:14:31.225: INFO: kube-proxy-fn2dq from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
    Jun 13 03:14:31.225: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 13 03:14:31.225: INFO: snapshot-controller-76c6888c5-9m5td from kube-system started at 2023-06-13 02:04:55 +0000 UTC (1 container statuses recorded)
    Jun 13 03:14:31.225: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun 13 03:14:31.225: INFO: smtx-elf-csi-driver-node-plugin-h8k7m from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
    Jun 13 03:14:31.225: INFO: 	Container csi-driver ready: true, restart count 0
    Jun 13 03:14:31.225: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun 13 03:14:31.225: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 13 03:14:31.225: INFO: tigera-operator-6f49bd984-r7pkz from sks-system started at 2023-06-13 02:04:29 +0000 UTC (1 container statuses recorded)
    Jun 13 03:14:31.225: INFO: 	Container tigera-operator ready: true, restart count 0
    Jun 13 03:14:31.225: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-hcvsr from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:14:31.225: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 03:14:31.225: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 13 03:14:31.225: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-l5gcd before test
    Jun 13 03:14:31.238: INFO: calico-node-fptww from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
    Jun 13 03:14:31.238: INFO: 	Container calico-node ready: true, restart count 0
    Jun 13 03:14:31.238: INFO: calico-typha-79dfdd7d65-nrkt5 from calico-system started at 2023-06-13 02:04:43 +0000 UTC (1 container statuses recorded)
    Jun 13 03:14:31.238: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 13 03:14:31.238: INFO: csi-node-driver-66bp4 from calico-system started at 2023-06-13 02:05:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:14:31.238: INFO: 	Container calico-csi ready: true, restart count 0
    Jun 13 03:14:31.238: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun 13 03:14:31.238: INFO: kube-proxy-669th from kube-system started at 2023-06-13 02:03:17 +0000 UTC (1 container statuses recorded)
    Jun 13 03:14:31.238: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 13 03:14:31.238: INFO: smtx-elf-csi-driver-node-plugin-dtt2k from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
    Jun 13 03:14:31.238: INFO: 	Container csi-driver ready: true, restart count 0
    Jun 13 03:14:31.238: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun 13 03:14:31.238: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 13 03:14:31.238: INFO: sonobuoy-e2e-job-246c2b6dcb314696 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:14:31.238: INFO: 	Container e2e ready: true, restart count 0
    Jun 13 03:14:31.238: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 03:14:31.238: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-bvbp2 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:14:31.238: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 03:14:31.238: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node sks-test-v1-25-9-workergroup-2q6k2 06/13/23 03:14:31.274
    STEP: verifying the node has the label node sks-test-v1-25-9-workergroup-469fm 06/13/23 03:14:31.3
    STEP: verifying the node has the label node sks-test-v1-25-9-workergroup-l5gcd 06/13/23 03:14:31.319
    Jun 13 03:14:31.349: INFO: Pod calico-node-fptww requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-l5gcd
    Jun 13 03:14:31.349: INFO: Pod calico-node-lhmmz requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-469fm
    Jun 13 03:14:31.349: INFO: Pod calico-node-xb8kf requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-2q6k2
    Jun 13 03:14:31.349: INFO: Pod calico-typha-79dfdd7d65-nrkt5 requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-l5gcd
    Jun 13 03:14:31.349: INFO: Pod calico-typha-79dfdd7d65-w95lk requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-2q6k2
    Jun 13 03:14:31.349: INFO: Pod csi-node-driver-66bp4 requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-l5gcd
    Jun 13 03:14:31.349: INFO: Pod csi-node-driver-jrfzt requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-469fm
    Jun 13 03:14:31.349: INFO: Pod csi-node-driver-vkf24 requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-2q6k2
    Jun 13 03:14:31.349: INFO: Pod kube-proxy-669th requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-l5gcd
    Jun 13 03:14:31.349: INFO: Pod kube-proxy-fn2dq requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-469fm
    Jun 13 03:14:31.349: INFO: Pod kube-proxy-g2qcx requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-2q6k2
    Jun 13 03:14:31.349: INFO: Pod snapshot-controller-76c6888c5-9m5td requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-469fm
    Jun 13 03:14:31.349: INFO: Pod smtx-elf-csi-driver-node-plugin-dtt2k requesting resource cpu=210m on Node sks-test-v1-25-9-workergroup-l5gcd
    Jun 13 03:14:31.349: INFO: Pod smtx-elf-csi-driver-node-plugin-gqbkk requesting resource cpu=210m on Node sks-test-v1-25-9-workergroup-2q6k2
    Jun 13 03:14:31.349: INFO: Pod smtx-elf-csi-driver-node-plugin-h8k7m requesting resource cpu=210m on Node sks-test-v1-25-9-workergroup-469fm
    Jun 13 03:14:31.349: INFO: Pod tigera-operator-6f49bd984-r7pkz requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-469fm
    Jun 13 03:14:31.349: INFO: Pod sonobuoy requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-2q6k2
    Jun 13 03:14:31.349: INFO: Pod sonobuoy-e2e-job-246c2b6dcb314696 requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-l5gcd
    Jun 13 03:14:31.349: INFO: Pod sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-bvbp2 requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-l5gcd
    Jun 13 03:14:31.349: INFO: Pod sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-f9dsl requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-2q6k2
    Jun 13 03:14:31.349: INFO: Pod sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-hcvsr requesting resource cpu=0m on Node sks-test-v1-25-9-workergroup-469fm
    STEP: Starting Pods to consume most of the cluster CPU. 06/13/23 03:14:31.349
    Jun 13 03:14:31.349: INFO: Creating a pod which consumes cpu=1253m on Node sks-test-v1-25-9-workergroup-2q6k2
    Jun 13 03:14:31.364: INFO: Creating a pod which consumes cpu=1253m on Node sks-test-v1-25-9-workergroup-469fm
    Jun 13 03:14:31.381: INFO: Creating a pod which consumes cpu=1253m on Node sks-test-v1-25-9-workergroup-l5gcd
    Jun 13 03:14:31.393: INFO: Waiting up to 5m0s for pod "filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854" in namespace "sched-pred-2123" to be "running"
    Jun 13 03:14:31.408: INFO: Pod "filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854": Phase="Pending", Reason="", readiness=false. Elapsed: 14.634585ms
    Jun 13 03:14:33.415: INFO: Pod "filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854": Phase="Running", Reason="", readiness=true. Elapsed: 2.021801519s
    Jun 13 03:14:33.415: INFO: Pod "filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854" satisfied condition "running"
    Jun 13 03:14:33.415: INFO: Waiting up to 5m0s for pod "filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5" in namespace "sched-pred-2123" to be "running"
    Jun 13 03:14:33.422: INFO: Pod "filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5": Phase="Running", Reason="", readiness=true. Elapsed: 6.753711ms
    Jun 13 03:14:33.422: INFO: Pod "filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5" satisfied condition "running"
    Jun 13 03:14:33.422: INFO: Waiting up to 5m0s for pod "filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e" in namespace "sched-pred-2123" to be "running"
    Jun 13 03:14:33.428: INFO: Pod "filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e": Phase="Running", Reason="", readiness=true. Elapsed: 6.164335ms
    Jun 13 03:14:33.428: INFO: Pod "filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 06/13/23 03:14:33.428
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854.17681979cd76e710], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2123/filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854 to sks-test-v1-25-9-workergroup-2q6k2] 06/13/23 03:14:33.434
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854.17681979fddb2e5f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/13/23 03:14:33.435
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854.1768197a005363be], Reason = [Created], Message = [Created container filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854] 06/13/23 03:14:33.435
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854.1768197a07dbdfca], Reason = [Started], Message = [Started container filler-pod-7a9d4e52-6b53-4ac5-ad0d-afc6d3aef854] 06/13/23 03:14:33.435
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5.17681979ce19317f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2123/filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5 to sks-test-v1-25-9-workergroup-469fm] 06/13/23 03:14:33.435
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5.1768197a023166a4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/13/23 03:14:33.435
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5.1768197a05089904], Reason = [Created], Message = [Created container filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5] 06/13/23 03:14:33.435
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5.1768197a0dc05a5a], Reason = [Started], Message = [Started container filler-pod-94835352-4ad2-46fb-95c5-5cf4a88125d5] 06/13/23 03:14:33.435
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e.17681979cecf8835], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2123/filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e to sks-test-v1-25-9-workergroup-l5gcd] 06/13/23 03:14:33.435
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e.1768197a01977147], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/13/23 03:14:33.435
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e.1768197a043bff0b], Reason = [Created], Message = [Created container filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e] 06/13/23 03:14:33.435
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e.1768197a0d93cf84], Reason = [Started], Message = [Started container filler-pod-bceac414-bd16-4fd0-9093-011bd3ffd37e] 06/13/23 03:14:33.435
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1768197a48d92230], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 06/13/23 03:14:33.458
    STEP: removing the label node off the node sks-test-v1-25-9-workergroup-469fm 06/13/23 03:14:34.458
    STEP: verifying the node doesn't have the label node 06/13/23 03:14:34.481
    STEP: removing the label node off the node sks-test-v1-25-9-workergroup-l5gcd 06/13/23 03:14:34.486
    STEP: verifying the node doesn't have the label node 06/13/23 03:14:34.508
    STEP: removing the label node off the node sks-test-v1-25-9-workergroup-2q6k2 06/13/23 03:14:34.526
    STEP: verifying the node doesn't have the label node 06/13/23 03:14:34.55
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:14:34.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-2123" for this suite. 06/13/23 03:14:34.57
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:14:34.586
Jun 13 03:14:34.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 03:14:34.587
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:34.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:34.63
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 06/13/23 03:14:34.638
Jun 13 03:14:34.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 create -f -'
Jun 13 03:14:36.234: INFO: stderr: ""
Jun 13 03:14:36.234: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/13/23 03:14:36.234
Jun 13 03:14:36.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 13 03:14:36.340: INFO: stderr: ""
Jun 13 03:14:36.340: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
Jun 13 03:14:36.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:14:36.550: INFO: stderr: ""
Jun 13 03:14:36.550: INFO: stdout: ""
Jun 13 03:14:36.550: INFO: update-demo-nautilus-nznqp is created but not running
Jun 13 03:14:41.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 13 03:14:41.660: INFO: stderr: ""
Jun 13 03:14:41.660: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
Jun 13 03:14:41.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:14:41.801: INFO: stderr: ""
Jun 13 03:14:41.801: INFO: stdout: ""
Jun 13 03:14:41.801: INFO: update-demo-nautilus-nznqp is created but not running
Jun 13 03:14:46.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 13 03:14:46.930: INFO: stderr: ""
Jun 13 03:14:46.930: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
Jun 13 03:14:46.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:14:47.055: INFO: stderr: ""
Jun 13 03:14:47.055: INFO: stdout: ""
Jun 13 03:14:47.055: INFO: update-demo-nautilus-nznqp is created but not running
Jun 13 03:14:52.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 13 03:14:52.164: INFO: stderr: ""
Jun 13 03:14:52.164: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
Jun 13 03:14:52.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:14:52.266: INFO: stderr: ""
Jun 13 03:14:52.266: INFO: stdout: ""
Jun 13 03:14:52.266: INFO: update-demo-nautilus-nznqp is created but not running
Jun 13 03:14:57.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 13 03:14:57.397: INFO: stderr: ""
Jun 13 03:14:57.397: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
Jun 13 03:14:57.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:14:57.532: INFO: stderr: ""
Jun 13 03:14:57.532: INFO: stdout: ""
Jun 13 03:14:57.532: INFO: update-demo-nautilus-nznqp is created but not running
Jun 13 03:15:02.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 13 03:15:02.675: INFO: stderr: ""
Jun 13 03:15:02.675: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
Jun 13 03:15:02.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:15:02.818: INFO: stderr: ""
Jun 13 03:15:02.818: INFO: stdout: "true"
Jun 13 03:15:02.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 13 03:15:02.938: INFO: stderr: ""
Jun 13 03:15:02.938: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 13 03:15:02.938: INFO: validating pod update-demo-nautilus-nznqp
Jun 13 03:15:02.950: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 13 03:15:02.950: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 13 03:15:02.950: INFO: update-demo-nautilus-nznqp is verified up and running
Jun 13 03:15:02.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-wdnq5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:15:03.086: INFO: stderr: ""
Jun 13 03:15:03.086: INFO: stdout: "true"
Jun 13 03:15:03.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-wdnq5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 13 03:15:03.236: INFO: stderr: ""
Jun 13 03:15:03.236: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 13 03:15:03.236: INFO: validating pod update-demo-nautilus-wdnq5
Jun 13 03:15:03.246: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 13 03:15:03.246: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 13 03:15:03.246: INFO: update-demo-nautilus-wdnq5 is verified up and running
STEP: scaling down the replication controller 06/13/23 03:15:03.246
Jun 13 03:15:03.248: INFO: scanned /root for discovery docs: <nil>
Jun 13 03:15:03.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun 13 03:15:03.585: INFO: stderr: ""
Jun 13 03:15:03.585: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/13/23 03:15:03.585
Jun 13 03:15:03.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 13 03:15:03.706: INFO: stderr: ""
Jun 13 03:15:03.706: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
STEP: Replicas for name=update-demo: expected=1 actual=2 06/13/23 03:15:03.706
Jun 13 03:15:08.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 13 03:15:08.802: INFO: stderr: ""
Jun 13 03:15:08.802: INFO: stdout: "update-demo-nautilus-nznqp "
Jun 13 03:15:08.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:15:08.896: INFO: stderr: ""
Jun 13 03:15:08.896: INFO: stdout: "true"
Jun 13 03:15:08.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 13 03:15:08.993: INFO: stderr: ""
Jun 13 03:15:08.993: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 13 03:15:08.993: INFO: validating pod update-demo-nautilus-nznqp
Jun 13 03:15:09.001: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 13 03:15:09.001: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 13 03:15:09.001: INFO: update-demo-nautilus-nznqp is verified up and running
STEP: scaling up the replication controller 06/13/23 03:15:09.001
Jun 13 03:15:09.003: INFO: scanned /root for discovery docs: <nil>
Jun 13 03:15:09.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun 13 03:15:10.138: INFO: stderr: ""
Jun 13 03:15:10.139: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/13/23 03:15:10.139
Jun 13 03:15:10.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 13 03:15:10.240: INFO: stderr: ""
Jun 13 03:15:10.240: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-zkrps "
Jun 13 03:15:10.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:15:10.323: INFO: stderr: ""
Jun 13 03:15:10.323: INFO: stdout: "true"
Jun 13 03:15:10.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 13 03:15:10.406: INFO: stderr: ""
Jun 13 03:15:10.406: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 13 03:15:10.406: INFO: validating pod update-demo-nautilus-nznqp
Jun 13 03:15:10.414: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 13 03:15:10.414: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 13 03:15:10.414: INFO: update-demo-nautilus-nznqp is verified up and running
Jun 13 03:15:10.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-zkrps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:15:10.517: INFO: stderr: ""
Jun 13 03:15:10.517: INFO: stdout: ""
Jun 13 03:15:10.517: INFO: update-demo-nautilus-zkrps is created but not running
Jun 13 03:15:15.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 13 03:15:15.625: INFO: stderr: ""
Jun 13 03:15:15.625: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-zkrps "
Jun 13 03:15:15.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:15:15.724: INFO: stderr: ""
Jun 13 03:15:15.724: INFO: stdout: "true"
Jun 13 03:15:15.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 13 03:15:15.826: INFO: stderr: ""
Jun 13 03:15:15.826: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 13 03:15:15.826: INFO: validating pod update-demo-nautilus-nznqp
Jun 13 03:15:15.833: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 13 03:15:15.833: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 13 03:15:15.833: INFO: update-demo-nautilus-nznqp is verified up and running
Jun 13 03:15:15.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-zkrps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:15:15.931: INFO: stderr: ""
Jun 13 03:15:15.931: INFO: stdout: "true"
Jun 13 03:15:15.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-zkrps -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 13 03:15:16.169: INFO: stderr: ""
Jun 13 03:15:16.169: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 13 03:15:16.169: INFO: validating pod update-demo-nautilus-zkrps
Jun 13 03:15:16.187: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 13 03:15:16.187: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 13 03:15:16.187: INFO: update-demo-nautilus-zkrps is verified up and running
STEP: using delete to clean up resources 06/13/23 03:15:16.187
Jun 13 03:15:16.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 delete --grace-period=0 --force -f -'
Jun 13 03:15:16.328: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 13 03:15:16.329: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 13 03:15:16.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get rc,svc -l name=update-demo --no-headers'
Jun 13 03:15:16.502: INFO: stderr: "No resources found in kubectl-7740 namespace.\n"
Jun 13 03:15:16.502: INFO: stdout: ""
Jun 13 03:15:16.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 13 03:15:16.730: INFO: stderr: ""
Jun 13 03:15:16.730: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 03:15:16.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7740" for this suite. 06/13/23 03:15:16.741
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":195,"skipped":3836,"failed":0}
------------------------------
• [SLOW TEST] [42.172 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:14:34.586
    Jun 13 03:14:34.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 03:14:34.587
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:14:34.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:14:34.63
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 06/13/23 03:14:34.638
    Jun 13 03:14:34.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 create -f -'
    Jun 13 03:14:36.234: INFO: stderr: ""
    Jun 13 03:14:36.234: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/13/23 03:14:36.234
    Jun 13 03:14:36.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 13 03:14:36.340: INFO: stderr: ""
    Jun 13 03:14:36.340: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
    Jun 13 03:14:36.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:14:36.550: INFO: stderr: ""
    Jun 13 03:14:36.550: INFO: stdout: ""
    Jun 13 03:14:36.550: INFO: update-demo-nautilus-nznqp is created but not running
    Jun 13 03:14:41.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 13 03:14:41.660: INFO: stderr: ""
    Jun 13 03:14:41.660: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
    Jun 13 03:14:41.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:14:41.801: INFO: stderr: ""
    Jun 13 03:14:41.801: INFO: stdout: ""
    Jun 13 03:14:41.801: INFO: update-demo-nautilus-nznqp is created but not running
    Jun 13 03:14:46.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 13 03:14:46.930: INFO: stderr: ""
    Jun 13 03:14:46.930: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
    Jun 13 03:14:46.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:14:47.055: INFO: stderr: ""
    Jun 13 03:14:47.055: INFO: stdout: ""
    Jun 13 03:14:47.055: INFO: update-demo-nautilus-nznqp is created but not running
    Jun 13 03:14:52.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 13 03:14:52.164: INFO: stderr: ""
    Jun 13 03:14:52.164: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
    Jun 13 03:14:52.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:14:52.266: INFO: stderr: ""
    Jun 13 03:14:52.266: INFO: stdout: ""
    Jun 13 03:14:52.266: INFO: update-demo-nautilus-nznqp is created but not running
    Jun 13 03:14:57.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 13 03:14:57.397: INFO: stderr: ""
    Jun 13 03:14:57.397: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
    Jun 13 03:14:57.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:14:57.532: INFO: stderr: ""
    Jun 13 03:14:57.532: INFO: stdout: ""
    Jun 13 03:14:57.532: INFO: update-demo-nautilus-nznqp is created but not running
    Jun 13 03:15:02.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 13 03:15:02.675: INFO: stderr: ""
    Jun 13 03:15:02.675: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
    Jun 13 03:15:02.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:15:02.818: INFO: stderr: ""
    Jun 13 03:15:02.818: INFO: stdout: "true"
    Jun 13 03:15:02.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 13 03:15:02.938: INFO: stderr: ""
    Jun 13 03:15:02.938: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 13 03:15:02.938: INFO: validating pod update-demo-nautilus-nznqp
    Jun 13 03:15:02.950: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 13 03:15:02.950: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 13 03:15:02.950: INFO: update-demo-nautilus-nznqp is verified up and running
    Jun 13 03:15:02.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-wdnq5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:15:03.086: INFO: stderr: ""
    Jun 13 03:15:03.086: INFO: stdout: "true"
    Jun 13 03:15:03.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-wdnq5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 13 03:15:03.236: INFO: stderr: ""
    Jun 13 03:15:03.236: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 13 03:15:03.236: INFO: validating pod update-demo-nautilus-wdnq5
    Jun 13 03:15:03.246: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 13 03:15:03.246: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 13 03:15:03.246: INFO: update-demo-nautilus-wdnq5 is verified up and running
    STEP: scaling down the replication controller 06/13/23 03:15:03.246
    Jun 13 03:15:03.248: INFO: scanned /root for discovery docs: <nil>
    Jun 13 03:15:03.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jun 13 03:15:03.585: INFO: stderr: ""
    Jun 13 03:15:03.585: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/13/23 03:15:03.585
    Jun 13 03:15:03.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 13 03:15:03.706: INFO: stderr: ""
    Jun 13 03:15:03.706: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-wdnq5 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 06/13/23 03:15:03.706
    Jun 13 03:15:08.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 13 03:15:08.802: INFO: stderr: ""
    Jun 13 03:15:08.802: INFO: stdout: "update-demo-nautilus-nznqp "
    Jun 13 03:15:08.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:15:08.896: INFO: stderr: ""
    Jun 13 03:15:08.896: INFO: stdout: "true"
    Jun 13 03:15:08.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 13 03:15:08.993: INFO: stderr: ""
    Jun 13 03:15:08.993: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 13 03:15:08.993: INFO: validating pod update-demo-nautilus-nznqp
    Jun 13 03:15:09.001: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 13 03:15:09.001: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 13 03:15:09.001: INFO: update-demo-nautilus-nznqp is verified up and running
    STEP: scaling up the replication controller 06/13/23 03:15:09.001
    Jun 13 03:15:09.003: INFO: scanned /root for discovery docs: <nil>
    Jun 13 03:15:09.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jun 13 03:15:10.138: INFO: stderr: ""
    Jun 13 03:15:10.139: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/13/23 03:15:10.139
    Jun 13 03:15:10.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 13 03:15:10.240: INFO: stderr: ""
    Jun 13 03:15:10.240: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-zkrps "
    Jun 13 03:15:10.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:15:10.323: INFO: stderr: ""
    Jun 13 03:15:10.323: INFO: stdout: "true"
    Jun 13 03:15:10.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 13 03:15:10.406: INFO: stderr: ""
    Jun 13 03:15:10.406: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 13 03:15:10.406: INFO: validating pod update-demo-nautilus-nznqp
    Jun 13 03:15:10.414: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 13 03:15:10.414: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 13 03:15:10.414: INFO: update-demo-nautilus-nznqp is verified up and running
    Jun 13 03:15:10.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-zkrps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:15:10.517: INFO: stderr: ""
    Jun 13 03:15:10.517: INFO: stdout: ""
    Jun 13 03:15:10.517: INFO: update-demo-nautilus-zkrps is created but not running
    Jun 13 03:15:15.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 13 03:15:15.625: INFO: stderr: ""
    Jun 13 03:15:15.625: INFO: stdout: "update-demo-nautilus-nznqp update-demo-nautilus-zkrps "
    Jun 13 03:15:15.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:15:15.724: INFO: stderr: ""
    Jun 13 03:15:15.724: INFO: stdout: "true"
    Jun 13 03:15:15.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-nznqp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 13 03:15:15.826: INFO: stderr: ""
    Jun 13 03:15:15.826: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 13 03:15:15.826: INFO: validating pod update-demo-nautilus-nznqp
    Jun 13 03:15:15.833: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 13 03:15:15.833: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 13 03:15:15.833: INFO: update-demo-nautilus-nznqp is verified up and running
    Jun 13 03:15:15.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-zkrps -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:15:15.931: INFO: stderr: ""
    Jun 13 03:15:15.931: INFO: stdout: "true"
    Jun 13 03:15:15.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods update-demo-nautilus-zkrps -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 13 03:15:16.169: INFO: stderr: ""
    Jun 13 03:15:16.169: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 13 03:15:16.169: INFO: validating pod update-demo-nautilus-zkrps
    Jun 13 03:15:16.187: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 13 03:15:16.187: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 13 03:15:16.187: INFO: update-demo-nautilus-zkrps is verified up and running
    STEP: using delete to clean up resources 06/13/23 03:15:16.187
    Jun 13 03:15:16.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 delete --grace-period=0 --force -f -'
    Jun 13 03:15:16.328: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 13 03:15:16.329: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jun 13 03:15:16.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get rc,svc -l name=update-demo --no-headers'
    Jun 13 03:15:16.502: INFO: stderr: "No resources found in kubectl-7740 namespace.\n"
    Jun 13 03:15:16.502: INFO: stdout: ""
    Jun 13 03:15:16.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-7740 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun 13 03:15:16.730: INFO: stderr: ""
    Jun 13 03:15:16.730: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 03:15:16.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7740" for this suite. 06/13/23 03:15:16.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:15:16.762
Jun 13 03:15:16.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 03:15:16.764
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:15:16.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:15:16.951
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 06/13/23 03:15:16.959
Jun 13 03:15:17.012: INFO: Waiting up to 5m0s for pod "pod-8a12640f-88bc-4298-8784-9115db52760a" in namespace "emptydir-4279" to be "Succeeded or Failed"
Jun 13 03:15:17.030: INFO: Pod "pod-8a12640f-88bc-4298-8784-9115db52760a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.053798ms
Jun 13 03:15:19.037: INFO: Pod "pod-8a12640f-88bc-4298-8784-9115db52760a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024911881s
Jun 13 03:15:21.039: INFO: Pod "pod-8a12640f-88bc-4298-8784-9115db52760a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026625523s
STEP: Saw pod success 06/13/23 03:15:21.039
Jun 13 03:15:21.039: INFO: Pod "pod-8a12640f-88bc-4298-8784-9115db52760a" satisfied condition "Succeeded or Failed"
Jun 13 03:15:21.045: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-8a12640f-88bc-4298-8784-9115db52760a container test-container: <nil>
STEP: delete the pod 06/13/23 03:15:21.057
Jun 13 03:15:21.077: INFO: Waiting for pod pod-8a12640f-88bc-4298-8784-9115db52760a to disappear
Jun 13 03:15:21.084: INFO: Pod pod-8a12640f-88bc-4298-8784-9115db52760a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 03:15:21.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4279" for this suite. 06/13/23 03:15:21.097
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":196,"skipped":3866,"failed":0}
------------------------------
• [4.351 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:15:16.762
    Jun 13 03:15:16.762: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 03:15:16.764
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:15:16.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:15:16.951
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 06/13/23 03:15:16.959
    Jun 13 03:15:17.012: INFO: Waiting up to 5m0s for pod "pod-8a12640f-88bc-4298-8784-9115db52760a" in namespace "emptydir-4279" to be "Succeeded or Failed"
    Jun 13 03:15:17.030: INFO: Pod "pod-8a12640f-88bc-4298-8784-9115db52760a": Phase="Pending", Reason="", readiness=false. Elapsed: 18.053798ms
    Jun 13 03:15:19.037: INFO: Pod "pod-8a12640f-88bc-4298-8784-9115db52760a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024911881s
    Jun 13 03:15:21.039: INFO: Pod "pod-8a12640f-88bc-4298-8784-9115db52760a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026625523s
    STEP: Saw pod success 06/13/23 03:15:21.039
    Jun 13 03:15:21.039: INFO: Pod "pod-8a12640f-88bc-4298-8784-9115db52760a" satisfied condition "Succeeded or Failed"
    Jun 13 03:15:21.045: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-8a12640f-88bc-4298-8784-9115db52760a container test-container: <nil>
    STEP: delete the pod 06/13/23 03:15:21.057
    Jun 13 03:15:21.077: INFO: Waiting for pod pod-8a12640f-88bc-4298-8784-9115db52760a to disappear
    Jun 13 03:15:21.084: INFO: Pod pod-8a12640f-88bc-4298-8784-9115db52760a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 03:15:21.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4279" for this suite. 06/13/23 03:15:21.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:15:21.114
Jun 13 03:15:21.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:15:21.115
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:15:21.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:15:21.145
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620
STEP: creating a collection of services 06/13/23 03:15:21.154
Jun 13 03:15:21.154: INFO: Creating e2e-svc-a-b5wbz
Jun 13 03:15:21.179: INFO: Creating e2e-svc-b-2bggx
Jun 13 03:15:21.221: INFO: Creating e2e-svc-c-chgqg
STEP: deleting service collection 06/13/23 03:15:21.284
Jun 13 03:15:21.422: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:15:21.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2537" for this suite. 06/13/23 03:15:21.433
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":197,"skipped":3900,"failed":0}
------------------------------
• [0.377 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:15:21.114
    Jun 13 03:15:21.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:15:21.115
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:15:21.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:15:21.145
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3620
    STEP: creating a collection of services 06/13/23 03:15:21.154
    Jun 13 03:15:21.154: INFO: Creating e2e-svc-a-b5wbz
    Jun 13 03:15:21.179: INFO: Creating e2e-svc-b-2bggx
    Jun 13 03:15:21.221: INFO: Creating e2e-svc-c-chgqg
    STEP: deleting service collection 06/13/23 03:15:21.284
    Jun 13 03:15:21.422: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:15:21.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2537" for this suite. 06/13/23 03:15:21.433
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:15:21.492
Jun 13 03:15:21.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename statefulset 06/13/23 03:15:21.493
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:15:21.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:15:21.539
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-449 06/13/23 03:15:21.547
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 06/13/23 03:15:21.564
Jun 13 03:15:21.580: INFO: Found 0 stateful pods, waiting for 3
Jun 13 03:15:31.589: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 03:15:31.589: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 03:15:31.589: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 03:15:31.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-449 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 13 03:15:31.838: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 13 03:15:31.838: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 13 03:15:31.838: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 06/13/23 03:15:41.873
Jun 13 03:15:41.905: INFO: Updating stateful set ss2
STEP: Creating a new revision 06/13/23 03:15:41.905
STEP: Updating Pods in reverse ordinal order 06/13/23 03:15:51.936
Jun 13 03:15:51.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-449 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 13 03:15:52.149: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 13 03:15:52.149: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 13 03:15:52.149: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 13 03:16:02.193: INFO: Waiting for StatefulSet statefulset-449/ss2 to complete update
Jun 13 03:16:02.193: INFO: Waiting for Pod statefulset-449/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jun 13 03:16:02.193: INFO: Waiting for Pod statefulset-449/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jun 13 03:16:12.210: INFO: Waiting for StatefulSet statefulset-449/ss2 to complete update
Jun 13 03:16:12.210: INFO: Waiting for Pod statefulset-449/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jun 13 03:16:22.219: INFO: Waiting for StatefulSet statefulset-449/ss2 to complete update
Jun 13 03:16:22.219: INFO: Waiting for Pod statefulset-449/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jun 13 03:16:32.216: INFO: Waiting for StatefulSet statefulset-449/ss2 to complete update
STEP: Rolling back to a previous revision 06/13/23 03:16:42.213
Jun 13 03:16:42.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-449 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 13 03:16:42.490: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 13 03:16:42.490: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 13 03:16:42.490: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 13 03:16:52.569: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 06/13/23 03:17:02.606
Jun 13 03:17:02.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-449 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 13 03:17:02.848: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 13 03:17:02.848: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 13 03:17:02.848: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 13 03:17:12.896: INFO: Waiting for StatefulSet statefulset-449/ss2 to complete update
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 13 03:17:22.910: INFO: Deleting all statefulset in ns statefulset-449
Jun 13 03:17:22.920: INFO: Scaling statefulset ss2 to 0
Jun 13 03:17:32.954: INFO: Waiting for statefulset status.replicas updated to 0
Jun 13 03:17:32.961: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 13 03:17:32.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-449" for this suite. 06/13/23 03:17:32.999
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":198,"skipped":3908,"failed":0}
------------------------------
• [SLOW TEST] [131.556 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:15:21.492
    Jun 13 03:15:21.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename statefulset 06/13/23 03:15:21.493
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:15:21.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:15:21.539
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-449 06/13/23 03:15:21.547
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 06/13/23 03:15:21.564
    Jun 13 03:15:21.580: INFO: Found 0 stateful pods, waiting for 3
    Jun 13 03:15:31.589: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 03:15:31.589: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 03:15:31.589: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 03:15:31.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-449 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 13 03:15:31.838: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 13 03:15:31.838: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 13 03:15:31.838: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 06/13/23 03:15:41.873
    Jun 13 03:15:41.905: INFO: Updating stateful set ss2
    STEP: Creating a new revision 06/13/23 03:15:41.905
    STEP: Updating Pods in reverse ordinal order 06/13/23 03:15:51.936
    Jun 13 03:15:51.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-449 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 13 03:15:52.149: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 13 03:15:52.149: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 13 03:15:52.149: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 13 03:16:02.193: INFO: Waiting for StatefulSet statefulset-449/ss2 to complete update
    Jun 13 03:16:02.193: INFO: Waiting for Pod statefulset-449/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jun 13 03:16:02.193: INFO: Waiting for Pod statefulset-449/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jun 13 03:16:12.210: INFO: Waiting for StatefulSet statefulset-449/ss2 to complete update
    Jun 13 03:16:12.210: INFO: Waiting for Pod statefulset-449/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jun 13 03:16:22.219: INFO: Waiting for StatefulSet statefulset-449/ss2 to complete update
    Jun 13 03:16:22.219: INFO: Waiting for Pod statefulset-449/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jun 13 03:16:32.216: INFO: Waiting for StatefulSet statefulset-449/ss2 to complete update
    STEP: Rolling back to a previous revision 06/13/23 03:16:42.213
    Jun 13 03:16:42.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-449 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 13 03:16:42.490: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 13 03:16:42.490: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 13 03:16:42.490: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 13 03:16:52.569: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 06/13/23 03:17:02.606
    Jun 13 03:17:02.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=statefulset-449 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 13 03:17:02.848: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 13 03:17:02.848: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 13 03:17:02.848: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 13 03:17:12.896: INFO: Waiting for StatefulSet statefulset-449/ss2 to complete update
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 13 03:17:22.910: INFO: Deleting all statefulset in ns statefulset-449
    Jun 13 03:17:22.920: INFO: Scaling statefulset ss2 to 0
    Jun 13 03:17:32.954: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 13 03:17:32.961: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 13 03:17:32.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-449" for this suite. 06/13/23 03:17:32.999
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:17:33.049
Jun 13 03:17:33.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename job 06/13/23 03:17:33.051
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:17:33.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:17:33.114
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 06/13/23 03:17:33.12
STEP: Ensuring active pods == parallelism 06/13/23 03:17:33.137
STEP: delete a job 06/13/23 03:17:37.15
STEP: deleting Job.batch foo in namespace job-180, will wait for the garbage collector to delete the pods 06/13/23 03:17:37.15
Jun 13 03:17:37.224: INFO: Deleting Job.batch foo took: 15.439603ms
Jun 13 03:17:37.325: INFO: Terminating Job.batch foo pods took: 100.683324ms
STEP: Ensuring job was deleted 06/13/23 03:18:08.926
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 13 03:18:08.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-180" for this suite. 06/13/23 03:18:08.956
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":199,"skipped":3920,"failed":0}
------------------------------
• [SLOW TEST] [35.926 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:17:33.049
    Jun 13 03:17:33.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename job 06/13/23 03:17:33.051
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:17:33.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:17:33.114
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 06/13/23 03:17:33.12
    STEP: Ensuring active pods == parallelism 06/13/23 03:17:33.137
    STEP: delete a job 06/13/23 03:17:37.15
    STEP: deleting Job.batch foo in namespace job-180, will wait for the garbage collector to delete the pods 06/13/23 03:17:37.15
    Jun 13 03:17:37.224: INFO: Deleting Job.batch foo took: 15.439603ms
    Jun 13 03:17:37.325: INFO: Terminating Job.batch foo pods took: 100.683324ms
    STEP: Ensuring job was deleted 06/13/23 03:18:08.926
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 13 03:18:08.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-180" for this suite. 06/13/23 03:18:08.956
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:18:08.976
Jun 13 03:18:08.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename crd-webhook 06/13/23 03:18:08.978
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:09.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:09.03
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 06/13/23 03:18:09.04
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/13/23 03:18:09.339
STEP: Deploying the custom resource conversion webhook pod 06/13/23 03:18:09.351
STEP: Wait for the deployment to be ready 06/13/23 03:18:09.385
Jun 13 03:18:09.403: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 13 03:18:11.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 18, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 18, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 18, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 18, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/13/23 03:18:13.766
STEP: Verifying the service has paired with the endpoint 06/13/23 03:18:13.812
Jun 13 03:18:14.813: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jun 13 03:18:14.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Creating a v1 custom resource 06/13/23 03:18:17.479
STEP: v2 custom resource should be converted 06/13/23 03:18:17.492
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:18:18.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-2414" for this suite. 06/13/23 03:18:18.042
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":200,"skipped":3935,"failed":0}
------------------------------
• [SLOW TEST] [9.188 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:18:08.976
    Jun 13 03:18:08.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename crd-webhook 06/13/23 03:18:08.978
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:09.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:09.03
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 06/13/23 03:18:09.04
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/13/23 03:18:09.339
    STEP: Deploying the custom resource conversion webhook pod 06/13/23 03:18:09.351
    STEP: Wait for the deployment to be ready 06/13/23 03:18:09.385
    Jun 13 03:18:09.403: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jun 13 03:18:11.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 18, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 18, 9, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 18, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 18, 9, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/13/23 03:18:13.766
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:18:13.812
    Jun 13 03:18:14.813: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jun 13 03:18:14.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Creating a v1 custom resource 06/13/23 03:18:17.479
    STEP: v2 custom resource should be converted 06/13/23 03:18:17.492
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:18:18.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-2414" for this suite. 06/13/23 03:18:18.042
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:18:18.164
Jun 13 03:18:18.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename deployment 06/13/23 03:18:18.165
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:18.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:18.243
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jun 13 03:18:18.276: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 13 03:18:23.288: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/13/23 03:18:23.288
Jun 13 03:18:23.288: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 06/13/23 03:18:23.309
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 13 03:18:23.359: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8440  54d8f6ad-3c05-41a9-85f3-5b5deca68186 34629 1 2023-06-13 03:18:23 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-13 03:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f4b7b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetCreated,Message:Created new replica set "test-cleanup-deployment-69cb9c5497",LastUpdateTime:2023-06-13 03:18:23 +0000 UTC,LastTransitionTime:2023-06-13 03:18:23 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 13 03:18:23.370: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-8440  46161f59-0b5a-42cc-945f-c78d4107bb0e 34628 1 2023-06-13 03:18:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 54d8f6ad-3c05-41a9-85f3-5b5deca68186 0xc003f4bc77 0xc003f4bc78}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54d8f6ad-3c05-41a9-85f3-5b5deca68186\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f4bd08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 13 03:18:23.370: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun 13 03:18:23.371: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8440  1e4a414e-23ea-435d-ae95-4aa7cea87966 34627 1 2023-06-13 03:18:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 54d8f6ad-3c05-41a9-85f3-5b5deca68186 0xc003f4bb37 0xc003f4bb38}] [] [{e2e.test Update apps/v1 2023-06-13 03:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:18:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-13 03:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"54d8f6ad-3c05-41a9-85f3-5b5deca68186\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f4bbf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 13 03:18:23.385: INFO: Pod "test-cleanup-controller-kbhjc" is available:
&Pod{ObjectMeta:{test-cleanup-controller-kbhjc test-cleanup-controller- deployment-8440  c0a89735-5947-4fdc-afe0-f90028ea18de 34610 0 2023-06-13 03:18:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:0bcc34bb9d9bedcc4429e113d0bb41562fe9aafe68a319682ec3112db2455103 cni.projectcalico.org/podIP:172.16.172.52/32 cni.projectcalico.org/podIPs:172.16.172.52/32] [{apps/v1 ReplicaSet test-cleanup-controller 1e4a414e-23ea-435d-ae95-4aa7cea87966 0xc003fc3037 0xc003fc3038}] [] [{kube-controller-manager Update v1 2023-06-13 03:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e4a414e-23ea-435d-ae95-4aa7cea87966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:18:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kd5vg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kd5vg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:18:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.52,StartTime:2023-06-13 03:18:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:18:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bda9e8e0c6a0949ea615bef78553372366a1b163bfc7dba6097a47e4786a6006,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:18:23.385: INFO: Pod "test-cleanup-deployment-69cb9c5497-fhc27" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-fhc27 test-cleanup-deployment-69cb9c5497- deployment-8440  3efa136e-daa3-47b3-acdf-99f1300993fa 34632 0 2023-06-13 03:18:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 46161f59-0b5a-42cc-945f-c78d4107bb0e 0xc003fc3257 0xc003fc3258}] [] [{kube-controller-manager Update v1 2023-06-13 03:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46161f59-0b5a-42cc-945f-c78d4107bb0e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fpvmj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fpvmj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 13 03:18:23.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8440" for this suite. 06/13/23 03:18:23.407
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":201,"skipped":3950,"failed":0}
------------------------------
• [SLOW TEST] [5.256 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:18:18.164
    Jun 13 03:18:18.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename deployment 06/13/23 03:18:18.165
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:18.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:18.243
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jun 13 03:18:18.276: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jun 13 03:18:23.288: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/13/23 03:18:23.288
    Jun 13 03:18:23.288: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 06/13/23 03:18:23.309
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 13 03:18:23.359: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8440  54d8f6ad-3c05-41a9-85f3-5b5deca68186 34629 1 2023-06-13 03:18:23 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-13 03:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f4b7b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetCreated,Message:Created new replica set "test-cleanup-deployment-69cb9c5497",LastUpdateTime:2023-06-13 03:18:23 +0000 UTC,LastTransitionTime:2023-06-13 03:18:23 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jun 13 03:18:23.370: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-8440  46161f59-0b5a-42cc-945f-c78d4107bb0e 34628 1 2023-06-13 03:18:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 54d8f6ad-3c05-41a9-85f3-5b5deca68186 0xc003f4bc77 0xc003f4bc78}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54d8f6ad-3c05-41a9-85f3-5b5deca68186\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f4bd08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 03:18:23.370: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jun 13 03:18:23.371: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-8440  1e4a414e-23ea-435d-ae95-4aa7cea87966 34627 1 2023-06-13 03:18:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 54d8f6ad-3c05-41a9-85f3-5b5deca68186 0xc003f4bb37 0xc003f4bb38}] [] [{e2e.test Update apps/v1 2023-06-13 03:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:18:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-06-13 03:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"54d8f6ad-3c05-41a9-85f3-5b5deca68186\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f4bbf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 03:18:23.385: INFO: Pod "test-cleanup-controller-kbhjc" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-kbhjc test-cleanup-controller- deployment-8440  c0a89735-5947-4fdc-afe0-f90028ea18de 34610 0 2023-06-13 03:18:18 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:0bcc34bb9d9bedcc4429e113d0bb41562fe9aafe68a319682ec3112db2455103 cni.projectcalico.org/podIP:172.16.172.52/32 cni.projectcalico.org/podIPs:172.16.172.52/32] [{apps/v1 ReplicaSet test-cleanup-controller 1e4a414e-23ea-435d-ae95-4aa7cea87966 0xc003fc3037 0xc003fc3038}] [] [{kube-controller-manager Update v1 2023-06-13 03:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e4a414e-23ea-435d-ae95-4aa7cea87966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:18:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kd5vg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kd5vg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:18:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.52,StartTime:2023-06-13 03:18:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:18:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bda9e8e0c6a0949ea615bef78553372366a1b163bfc7dba6097a47e4786a6006,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.52,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:18:23.385: INFO: Pod "test-cleanup-deployment-69cb9c5497-fhc27" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-fhc27 test-cleanup-deployment-69cb9c5497- deployment-8440  3efa136e-daa3-47b3-acdf-99f1300993fa 34632 0 2023-06-13 03:18:23 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 46161f59-0b5a-42cc-945f-c78d4107bb0e 0xc003fc3257 0xc003fc3258}] [] [{kube-controller-manager Update v1 2023-06-13 03:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46161f59-0b5a-42cc-945f-c78d4107bb0e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fpvmj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fpvmj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:18:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 13 03:18:23.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8440" for this suite. 06/13/23 03:18:23.407
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:18:23.421
Jun 13 03:18:23.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 03:18:23.422
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:23.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:23.49
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 03:18:23.543
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:18:24.1
STEP: Deploying the webhook pod 06/13/23 03:18:24.108
STEP: Wait for the deployment to be ready 06/13/23 03:18:24.129
Jun 13 03:18:24.175: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 13 03:18:26.205: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 18, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 18, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 18, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 18, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/13/23 03:18:28.213
STEP: Verifying the service has paired with the endpoint 06/13/23 03:18:28.256
Jun 13 03:18:29.257: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 06/13/23 03:18:29.274
STEP: Updating a mutating webhook configuration's rules to not include the create operation 06/13/23 03:18:29.307
STEP: Creating a configMap that should not be mutated 06/13/23 03:18:29.357
STEP: Patching a mutating webhook configuration's rules to include the create operation 06/13/23 03:18:29.375
STEP: Creating a configMap that should be mutated 06/13/23 03:18:29.386
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:18:29.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4838" for this suite. 06/13/23 03:18:29.431
STEP: Destroying namespace "webhook-4838-markers" for this suite. 06/13/23 03:18:29.44
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":202,"skipped":3951,"failed":0}
------------------------------
• [SLOW TEST] [6.117 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:18:23.421
    Jun 13 03:18:23.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 03:18:23.422
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:23.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:23.49
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 03:18:23.543
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:18:24.1
    STEP: Deploying the webhook pod 06/13/23 03:18:24.108
    STEP: Wait for the deployment to be ready 06/13/23 03:18:24.129
    Jun 13 03:18:24.175: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 13 03:18:26.205: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 18, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 18, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 18, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 18, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/13/23 03:18:28.213
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:18:28.256
    Jun 13 03:18:29.257: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 06/13/23 03:18:29.274
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 06/13/23 03:18:29.307
    STEP: Creating a configMap that should not be mutated 06/13/23 03:18:29.357
    STEP: Patching a mutating webhook configuration's rules to include the create operation 06/13/23 03:18:29.375
    STEP: Creating a configMap that should be mutated 06/13/23 03:18:29.386
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:18:29.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4838" for this suite. 06/13/23 03:18:29.431
    STEP: Destroying namespace "webhook-4838-markers" for this suite. 06/13/23 03:18:29.44
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:18:29.539
Jun 13 03:18:29.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename prestop 06/13/23 03:18:29.54
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:29.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:29.583
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-6726 06/13/23 03:18:29.6
STEP: Waiting for pods to come up. 06/13/23 03:18:29.619
Jun 13 03:18:29.619: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6726" to be "running"
Jun 13 03:18:29.628: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 9.024731ms
Jun 13 03:18:31.665: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046747119s
Jun 13 03:18:33.636: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.017217978s
Jun 13 03:18:33.636: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-6726 06/13/23 03:18:33.646
Jun 13 03:18:33.665: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6726" to be "running"
Jun 13 03:18:33.681: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 15.308505ms
Jun 13 03:18:35.702: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.036640881s
Jun 13 03:18:35.702: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 06/13/23 03:18:35.702
Jun 13 03:18:40.734: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 06/13/23 03:18:40.735
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jun 13 03:18:40.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6726" for this suite. 06/13/23 03:18:40.764
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":203,"skipped":3952,"failed":0}
------------------------------
• [SLOW TEST] [11.234 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:18:29.539
    Jun 13 03:18:29.539: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename prestop 06/13/23 03:18:29.54
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:29.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:29.583
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-6726 06/13/23 03:18:29.6
    STEP: Waiting for pods to come up. 06/13/23 03:18:29.619
    Jun 13 03:18:29.619: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-6726" to be "running"
    Jun 13 03:18:29.628: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 9.024731ms
    Jun 13 03:18:31.665: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046747119s
    Jun 13 03:18:33.636: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.017217978s
    Jun 13 03:18:33.636: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-6726 06/13/23 03:18:33.646
    Jun 13 03:18:33.665: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-6726" to be "running"
    Jun 13 03:18:33.681: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 15.308505ms
    Jun 13 03:18:35.702: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.036640881s
    Jun 13 03:18:35.702: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 06/13/23 03:18:35.702
    Jun 13 03:18:40.734: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 06/13/23 03:18:40.735
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jun 13 03:18:40.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-6726" for this suite. 06/13/23 03:18:40.764
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:18:40.773
Jun 13 03:18:40.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename security-context 06/13/23 03:18:40.776
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:40.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:40.804
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/13/23 03:18:40.81
Jun 13 03:18:40.823: INFO: Waiting up to 5m0s for pod "security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3" in namespace "security-context-2076" to be "Succeeded or Failed"
Jun 13 03:18:40.829: INFO: Pod "security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.757599ms
Jun 13 03:18:42.835: INFO: Pod "security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012099817s
Jun 13 03:18:44.847: INFO: Pod "security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024189878s
STEP: Saw pod success 06/13/23 03:18:44.847
Jun 13 03:18:44.847: INFO: Pod "security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3" satisfied condition "Succeeded or Failed"
Jun 13 03:18:44.855: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3 container test-container: <nil>
STEP: delete the pod 06/13/23 03:18:44.887
Jun 13 03:18:44.906: INFO: Waiting for pod security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3 to disappear
Jun 13 03:18:44.914: INFO: Pod security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 13 03:18:44.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2076" for this suite. 06/13/23 03:18:44.925
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":204,"skipped":3956,"failed":0}
------------------------------
• [4.172 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:18:40.773
    Jun 13 03:18:40.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename security-context 06/13/23 03:18:40.776
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:40.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:40.804
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/13/23 03:18:40.81
    Jun 13 03:18:40.823: INFO: Waiting up to 5m0s for pod "security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3" in namespace "security-context-2076" to be "Succeeded or Failed"
    Jun 13 03:18:40.829: INFO: Pod "security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.757599ms
    Jun 13 03:18:42.835: INFO: Pod "security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012099817s
    Jun 13 03:18:44.847: INFO: Pod "security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024189878s
    STEP: Saw pod success 06/13/23 03:18:44.847
    Jun 13 03:18:44.847: INFO: Pod "security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3" satisfied condition "Succeeded or Failed"
    Jun 13 03:18:44.855: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3 container test-container: <nil>
    STEP: delete the pod 06/13/23 03:18:44.887
    Jun 13 03:18:44.906: INFO: Waiting for pod security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3 to disappear
    Jun 13 03:18:44.914: INFO: Pod security-context-b29908b8-a62a-4bab-a3b2-9d9ecb381eb3 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 13 03:18:44.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-2076" for this suite. 06/13/23 03:18:44.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:18:44.946
Jun 13 03:18:44.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename daemonsets 06/13/23 03:18:44.948
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:44.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:44.986
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jun 13 03:18:45.062: INFO: Create a RollingUpdate DaemonSet
Jun 13 03:18:45.077: INFO: Check that daemon pods launch on every node of the cluster
Jun 13 03:18:45.092: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:45.092: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:45.092: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:45.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:18:45.104: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 03:18:46.131: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:46.131: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:46.131: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:46.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:18:46.139: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 03:18:47.112: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:47.112: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:47.112: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:47.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 13 03:18:47.119: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
Jun 13 03:18:48.115: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:48.115: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:48.115: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:48.122: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 13 03:18:48.122: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jun 13 03:18:48.122: INFO: Update the DaemonSet to trigger a rollout
Jun 13 03:18:48.150: INFO: Updating DaemonSet daemon-set
Jun 13 03:18:50.272: INFO: Roll back the DaemonSet before rollout is complete
Jun 13 03:18:50.291: INFO: Updating DaemonSet daemon-set
Jun 13 03:18:50.291: INFO: Make sure DaemonSet rollback is complete
Jun 13 03:18:50.303: INFO: Wrong image for pod: daemon-set-2cns6. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jun 13 03:18:50.303: INFO: Pod daemon-set-2cns6 is not available
Jun 13 03:18:50.316: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:50.316: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:50.316: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:51.353: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:51.353: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:51.353: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:52.342: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:52.342: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:52.342: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:53.374: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:53.375: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:53.375: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:54.447: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:54.447: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:54.447: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:55.333: INFO: Pod daemon-set-bp96g is not available
Jun 13 03:18:55.342: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:55.343: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:18:55.343: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/13/23 03:18:55.359
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1963, will wait for the garbage collector to delete the pods 06/13/23 03:18:55.359
Jun 13 03:18:55.433: INFO: Deleting DaemonSet.extensions daemon-set took: 17.331997ms
Jun 13 03:18:55.839: INFO: Terminating DaemonSet.extensions daemon-set pods took: 405.285382ms
Jun 13 03:18:57.748: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:18:57.748: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 13 03:18:57.761: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35089"},"items":null}

Jun 13 03:18:57.769: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35089"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:18:57.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1963" for this suite. 06/13/23 03:18:57.825
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":205,"skipped":3968,"failed":0}
------------------------------
• [SLOW TEST] [12.892 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:18:44.946
    Jun 13 03:18:44.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename daemonsets 06/13/23 03:18:44.948
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:44.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:44.986
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jun 13 03:18:45.062: INFO: Create a RollingUpdate DaemonSet
    Jun 13 03:18:45.077: INFO: Check that daemon pods launch on every node of the cluster
    Jun 13 03:18:45.092: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:45.092: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:45.092: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:45.104: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:18:45.104: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 03:18:46.131: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:46.131: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:46.131: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:46.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:18:46.139: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 03:18:47.112: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:47.112: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:47.112: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:47.119: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 13 03:18:47.119: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
    Jun 13 03:18:48.115: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:48.115: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:48.115: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:48.122: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 13 03:18:48.122: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jun 13 03:18:48.122: INFO: Update the DaemonSet to trigger a rollout
    Jun 13 03:18:48.150: INFO: Updating DaemonSet daemon-set
    Jun 13 03:18:50.272: INFO: Roll back the DaemonSet before rollout is complete
    Jun 13 03:18:50.291: INFO: Updating DaemonSet daemon-set
    Jun 13 03:18:50.291: INFO: Make sure DaemonSet rollback is complete
    Jun 13 03:18:50.303: INFO: Wrong image for pod: daemon-set-2cns6. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jun 13 03:18:50.303: INFO: Pod daemon-set-2cns6 is not available
    Jun 13 03:18:50.316: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:50.316: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:50.316: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:51.353: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:51.353: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:51.353: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:52.342: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:52.342: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:52.342: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:53.374: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:53.375: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:53.375: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:54.447: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:54.447: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:54.447: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:55.333: INFO: Pod daemon-set-bp96g is not available
    Jun 13 03:18:55.342: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:55.343: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:18:55.343: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/13/23 03:18:55.359
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1963, will wait for the garbage collector to delete the pods 06/13/23 03:18:55.359
    Jun 13 03:18:55.433: INFO: Deleting DaemonSet.extensions daemon-set took: 17.331997ms
    Jun 13 03:18:55.839: INFO: Terminating DaemonSet.extensions daemon-set pods took: 405.285382ms
    Jun 13 03:18:57.748: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:18:57.748: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 13 03:18:57.761: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"35089"},"items":null}

    Jun 13 03:18:57.769: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"35089"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:18:57.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1963" for this suite. 06/13/23 03:18:57.825
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:18:57.838
Jun 13 03:18:57.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename sched-preemption 06/13/23 03:18:57.84
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:57.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:57.874
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 13 03:18:57.909: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 13 03:19:57.993: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 06/13/23 03:19:57.999
Jun 13 03:19:58.083: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun 13 03:19:58.112: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun 13 03:19:58.147: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun 13 03:19:58.163: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun 13 03:19:58.207: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun 13 03:19:58.232: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 06/13/23 03:19:58.232
Jun 13 03:19:58.232: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6172" to be "running"
Jun 13 03:19:58.253: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 20.423452ms
Jun 13 03:20:00.261: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.028443307s
Jun 13 03:20:00.261: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jun 13 03:20:00.261: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6172" to be "running"
Jun 13 03:20:00.269: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.364097ms
Jun 13 03:20:00.269: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 13 03:20:00.269: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6172" to be "running"
Jun 13 03:20:00.275: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.050988ms
Jun 13 03:20:00.275: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 13 03:20:00.275: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6172" to be "running"
Jun 13 03:20:00.282: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.200043ms
Jun 13 03:20:00.282: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 13 03:20:00.282: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-6172" to be "running"
Jun 13 03:20:00.289: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.030495ms
Jun 13 03:20:00.289: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 13 03:20:00.289: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-6172" to be "running"
Jun 13 03:20:00.294: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.6212ms
Jun 13 03:20:00.294: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 06/13/23 03:20:00.294
Jun 13 03:20:00.304: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6172" to be "running"
Jun 13 03:20:00.316: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.270522ms
Jun 13 03:20:02.325: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020642952s
Jun 13 03:20:04.325: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020444811s
Jun 13 03:20:06.325: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.02137829s
Jun 13 03:20:06.326: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:20:06.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6172" for this suite. 06/13/23 03:20:06.384
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":206,"skipped":3970,"failed":0}
------------------------------
• [SLOW TEST] [68.770 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:18:57.838
    Jun 13 03:18:57.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename sched-preemption 06/13/23 03:18:57.84
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:18:57.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:18:57.874
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jun 13 03:18:57.909: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 13 03:19:57.993: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 06/13/23 03:19:57.999
    Jun 13 03:19:58.083: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jun 13 03:19:58.112: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jun 13 03:19:58.147: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jun 13 03:19:58.163: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jun 13 03:19:58.207: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jun 13 03:19:58.232: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 06/13/23 03:19:58.232
    Jun 13 03:19:58.232: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6172" to be "running"
    Jun 13 03:19:58.253: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 20.423452ms
    Jun 13 03:20:00.261: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.028443307s
    Jun 13 03:20:00.261: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jun 13 03:20:00.261: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6172" to be "running"
    Jun 13 03:20:00.269: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.364097ms
    Jun 13 03:20:00.269: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 13 03:20:00.269: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6172" to be "running"
    Jun 13 03:20:00.275: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.050988ms
    Jun 13 03:20:00.275: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 13 03:20:00.275: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6172" to be "running"
    Jun 13 03:20:00.282: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.200043ms
    Jun 13 03:20:00.282: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 13 03:20:00.282: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-6172" to be "running"
    Jun 13 03:20:00.289: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 7.030495ms
    Jun 13 03:20:00.289: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 13 03:20:00.289: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-6172" to be "running"
    Jun 13 03:20:00.294: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.6212ms
    Jun 13 03:20:00.294: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 06/13/23 03:20:00.294
    Jun 13 03:20:00.304: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6172" to be "running"
    Jun 13 03:20:00.316: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.270522ms
    Jun 13 03:20:02.325: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020642952s
    Jun 13 03:20:04.325: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020444811s
    Jun 13 03:20:06.325: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.02137829s
    Jun 13 03:20:06.326: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:20:06.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6172" for this suite. 06/13/23 03:20:06.384
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:20:06.611
Jun 13 03:20:06.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename subpath 06/13/23 03:20:06.613
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:20:06.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:20:06.644
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/13/23 03:20:06.649
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-8brf 06/13/23 03:20:06.667
STEP: Creating a pod to test atomic-volume-subpath 06/13/23 03:20:06.667
Jun 13 03:20:06.685: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8brf" in namespace "subpath-2637" to be "Succeeded or Failed"
Jun 13 03:20:06.693: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.659566ms
Jun 13 03:20:08.702: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017652332s
Jun 13 03:20:10.701: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 4.016620939s
Jun 13 03:20:12.709: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 6.023760529s
Jun 13 03:20:14.705: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 8.020360811s
Jun 13 03:20:16.701: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 10.01639508s
Jun 13 03:20:18.700: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 12.01558138s
Jun 13 03:20:20.703: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 14.018382871s
Jun 13 03:20:22.700: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 16.015691712s
Jun 13 03:20:24.700: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 18.015735754s
Jun 13 03:20:26.701: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 20.016240573s
Jun 13 03:20:28.701: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 22.016162672s
Jun 13 03:20:30.705: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=false. Elapsed: 24.020527686s
Jun 13 03:20:32.703: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.018343236s
STEP: Saw pod success 06/13/23 03:20:32.703
Jun 13 03:20:32.703: INFO: Pod "pod-subpath-test-projected-8brf" satisfied condition "Succeeded or Failed"
Jun 13 03:20:32.723: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-subpath-test-projected-8brf container test-container-subpath-projected-8brf: <nil>
STEP: delete the pod 06/13/23 03:20:32.763
Jun 13 03:20:32.793: INFO: Waiting for pod pod-subpath-test-projected-8brf to disappear
Jun 13 03:20:32.804: INFO: Pod pod-subpath-test-projected-8brf no longer exists
STEP: Deleting pod pod-subpath-test-projected-8brf 06/13/23 03:20:32.804
Jun 13 03:20:32.804: INFO: Deleting pod "pod-subpath-test-projected-8brf" in namespace "subpath-2637"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 13 03:20:32.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2637" for this suite. 06/13/23 03:20:32.82
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":207,"skipped":3992,"failed":0}
------------------------------
• [SLOW TEST] [26.224 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:20:06.611
    Jun 13 03:20:06.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename subpath 06/13/23 03:20:06.613
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:20:06.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:20:06.644
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/13/23 03:20:06.649
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-8brf 06/13/23 03:20:06.667
    STEP: Creating a pod to test atomic-volume-subpath 06/13/23 03:20:06.667
    Jun 13 03:20:06.685: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8brf" in namespace "subpath-2637" to be "Succeeded or Failed"
    Jun 13 03:20:06.693: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.659566ms
    Jun 13 03:20:08.702: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017652332s
    Jun 13 03:20:10.701: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 4.016620939s
    Jun 13 03:20:12.709: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 6.023760529s
    Jun 13 03:20:14.705: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 8.020360811s
    Jun 13 03:20:16.701: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 10.01639508s
    Jun 13 03:20:18.700: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 12.01558138s
    Jun 13 03:20:20.703: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 14.018382871s
    Jun 13 03:20:22.700: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 16.015691712s
    Jun 13 03:20:24.700: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 18.015735754s
    Jun 13 03:20:26.701: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 20.016240573s
    Jun 13 03:20:28.701: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=true. Elapsed: 22.016162672s
    Jun 13 03:20:30.705: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Running", Reason="", readiness=false. Elapsed: 24.020527686s
    Jun 13 03:20:32.703: INFO: Pod "pod-subpath-test-projected-8brf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.018343236s
    STEP: Saw pod success 06/13/23 03:20:32.703
    Jun 13 03:20:32.703: INFO: Pod "pod-subpath-test-projected-8brf" satisfied condition "Succeeded or Failed"
    Jun 13 03:20:32.723: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-subpath-test-projected-8brf container test-container-subpath-projected-8brf: <nil>
    STEP: delete the pod 06/13/23 03:20:32.763
    Jun 13 03:20:32.793: INFO: Waiting for pod pod-subpath-test-projected-8brf to disappear
    Jun 13 03:20:32.804: INFO: Pod pod-subpath-test-projected-8brf no longer exists
    STEP: Deleting pod pod-subpath-test-projected-8brf 06/13/23 03:20:32.804
    Jun 13 03:20:32.804: INFO: Deleting pod "pod-subpath-test-projected-8brf" in namespace "subpath-2637"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 13 03:20:32.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2637" for this suite. 06/13/23 03:20:32.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:20:32.835
Jun 13 03:20:32.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename replication-controller 06/13/23 03:20:32.837
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:20:32.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:20:32.861
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 06/13/23 03:20:32.866
Jun 13 03:20:32.880: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2781" to be "running and ready"
Jun 13 03:20:32.889: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 8.677423ms
Jun 13 03:20:32.889: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:20:34.897: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017156475s
Jun 13 03:20:34.897: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:20:36.895: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.015231177s
Jun 13 03:20:36.896: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jun 13 03:20:36.896: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 06/13/23 03:20:36.902
STEP: Then the orphan pod is adopted 06/13/23 03:20:36.912
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 13 03:20:37.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2781" for this suite. 06/13/23 03:20:37.935
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":208,"skipped":3997,"failed":0}
------------------------------
• [SLOW TEST] [5.116 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:20:32.835
    Jun 13 03:20:32.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename replication-controller 06/13/23 03:20:32.837
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:20:32.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:20:32.861
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 06/13/23 03:20:32.866
    Jun 13 03:20:32.880: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2781" to be "running and ready"
    Jun 13 03:20:32.889: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 8.677423ms
    Jun 13 03:20:32.889: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:20:34.897: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017156475s
    Jun 13 03:20:34.897: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:20:36.895: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 4.015231177s
    Jun 13 03:20:36.896: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jun 13 03:20:36.896: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 06/13/23 03:20:36.902
    STEP: Then the orphan pod is adopted 06/13/23 03:20:36.912
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 13 03:20:37.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2781" for this suite. 06/13/23 03:20:37.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:20:37.951
Jun 13 03:20:37.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 03:20:37.953
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:20:37.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:20:37.983
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 06/13/23 03:20:37.99
Jun 13 03:20:37.990: INFO: namespace kubectl-3397
Jun 13 03:20:37.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3397 create -f -'
Jun 13 03:20:39.451: INFO: stderr: ""
Jun 13 03:20:39.451: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/13/23 03:20:39.451
Jun 13 03:20:40.462: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 13 03:20:40.462: INFO: Found 0 / 1
Jun 13 03:20:41.468: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 13 03:20:41.468: INFO: Found 1 / 1
Jun 13 03:20:41.468: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 13 03:20:41.477: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 13 03:20:41.477: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 13 03:20:41.477: INFO: wait on agnhost-primary startup in kubectl-3397 
Jun 13 03:20:41.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3397 logs agnhost-primary-nswsn agnhost-primary'
Jun 13 03:20:41.596: INFO: stderr: ""
Jun 13 03:20:41.596: INFO: stdout: "Paused\n"
STEP: exposing RC 06/13/23 03:20:41.596
Jun 13 03:20:41.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3397 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun 13 03:20:41.770: INFO: stderr: ""
Jun 13 03:20:41.770: INFO: stdout: "service/rm2 exposed\n"
Jun 13 03:20:41.776: INFO: Service rm2 in namespace kubectl-3397 found.
STEP: exposing service 06/13/23 03:20:43.838
Jun 13 03:20:43.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3397 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun 13 03:20:44.109: INFO: stderr: ""
Jun 13 03:20:44.109: INFO: stdout: "service/rm3 exposed\n"
Jun 13 03:20:44.179: INFO: Service rm3 in namespace kubectl-3397 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 03:20:46.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3397" for this suite. 06/13/23 03:20:46.214
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":209,"skipped":4005,"failed":0}
------------------------------
• [SLOW TEST] [8.281 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:20:37.951
    Jun 13 03:20:37.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 03:20:37.953
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:20:37.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:20:37.983
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 06/13/23 03:20:37.99
    Jun 13 03:20:37.990: INFO: namespace kubectl-3397
    Jun 13 03:20:37.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3397 create -f -'
    Jun 13 03:20:39.451: INFO: stderr: ""
    Jun 13 03:20:39.451: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/13/23 03:20:39.451
    Jun 13 03:20:40.462: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 13 03:20:40.462: INFO: Found 0 / 1
    Jun 13 03:20:41.468: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 13 03:20:41.468: INFO: Found 1 / 1
    Jun 13 03:20:41.468: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jun 13 03:20:41.477: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 13 03:20:41.477: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun 13 03:20:41.477: INFO: wait on agnhost-primary startup in kubectl-3397 
    Jun 13 03:20:41.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3397 logs agnhost-primary-nswsn agnhost-primary'
    Jun 13 03:20:41.596: INFO: stderr: ""
    Jun 13 03:20:41.596: INFO: stdout: "Paused\n"
    STEP: exposing RC 06/13/23 03:20:41.596
    Jun 13 03:20:41.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3397 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jun 13 03:20:41.770: INFO: stderr: ""
    Jun 13 03:20:41.770: INFO: stdout: "service/rm2 exposed\n"
    Jun 13 03:20:41.776: INFO: Service rm2 in namespace kubectl-3397 found.
    STEP: exposing service 06/13/23 03:20:43.838
    Jun 13 03:20:43.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3397 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jun 13 03:20:44.109: INFO: stderr: ""
    Jun 13 03:20:44.109: INFO: stdout: "service/rm3 exposed\n"
    Jun 13 03:20:44.179: INFO: Service rm3 in namespace kubectl-3397 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 03:20:46.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3397" for this suite. 06/13/23 03:20:46.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:20:46.233
Jun 13 03:20:46.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename svcaccounts 06/13/23 03:20:46.235
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:20:46.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:20:46.288
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jun 13 03:20:46.334: INFO: Waiting up to 5m0s for pod "pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37" in namespace "svcaccounts-612" to be "running"
Jun 13 03:20:46.345: INFO: Pod "pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37": Phase="Pending", Reason="", readiness=false. Elapsed: 10.736882ms
Jun 13 03:20:48.360: INFO: Pod "pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37": Phase="Running", Reason="", readiness=true. Elapsed: 2.026164691s
Jun 13 03:20:48.360: INFO: Pod "pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37" satisfied condition "running"
STEP: reading a file in the container 06/13/23 03:20:48.36
Jun 13 03:20:48.360: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-612 pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 06/13/23 03:20:48.56
Jun 13 03:20:48.560: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-612 pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 06/13/23 03:20:48.767
Jun 13 03:20:48.768: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-612 pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jun 13 03:20:49.020: INFO: Got root ca configmap in namespace "svcaccounts-612"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 13 03:20:49.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-612" for this suite. 06/13/23 03:20:49.042
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":210,"skipped":4020,"failed":0}
------------------------------
• [2.822 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:20:46.233
    Jun 13 03:20:46.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename svcaccounts 06/13/23 03:20:46.235
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:20:46.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:20:46.288
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jun 13 03:20:46.334: INFO: Waiting up to 5m0s for pod "pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37" in namespace "svcaccounts-612" to be "running"
    Jun 13 03:20:46.345: INFO: Pod "pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37": Phase="Pending", Reason="", readiness=false. Elapsed: 10.736882ms
    Jun 13 03:20:48.360: INFO: Pod "pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37": Phase="Running", Reason="", readiness=true. Elapsed: 2.026164691s
    Jun 13 03:20:48.360: INFO: Pod "pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37" satisfied condition "running"
    STEP: reading a file in the container 06/13/23 03:20:48.36
    Jun 13 03:20:48.360: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-612 pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 06/13/23 03:20:48.56
    Jun 13 03:20:48.560: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-612 pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 06/13/23 03:20:48.767
    Jun 13 03:20:48.768: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-612 pod-service-account-dad8a56d-b3b6-45d4-96ac-b71c4c02be37 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jun 13 03:20:49.020: INFO: Got root ca configmap in namespace "svcaccounts-612"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 13 03:20:49.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-612" for this suite. 06/13/23 03:20:49.042
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:20:49.055
Jun 13 03:20:49.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename daemonsets 06/13/23 03:20:49.057
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:20:49.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:20:49.091
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 06/13/23 03:20:49.147
STEP: Check that daemon pods launch on every node of the cluster. 06/13/23 03:20:49.16
Jun 13 03:20:49.181: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:49.181: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:49.181: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:49.190: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:20:49.190: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 03:20:50.205: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:50.205: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:50.205: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:50.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:20:50.216: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 03:20:51.200: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:51.200: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:51.200: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:51.208: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 13 03:20:51.208: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
Jun 13 03:20:52.217: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:52.217: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:52.217: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:52.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 13 03:20:52.222: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 06/13/23 03:20:52.227
Jun 13 03:20:52.293: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:52.293: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:52.293: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:52.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 13 03:20:52.301: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
Jun 13 03:20:53.311: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:53.311: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:53.311: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:53.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 13 03:20:53.320: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
Jun 13 03:20:54.389: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:54.389: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:54.389: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:54.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 13 03:20:54.394: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
Jun 13 03:20:55.311: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:55.311: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:55.311: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:20:55.318: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 13 03:20:55.318: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 06/13/23 03:20:55.318
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/13/23 03:20:55.328
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9153, will wait for the garbage collector to delete the pods 06/13/23 03:20:55.328
Jun 13 03:20:55.402: INFO: Deleting DaemonSet.extensions daemon-set took: 17.116698ms
Jun 13 03:20:55.504: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.967958ms
Jun 13 03:20:57.411: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:20:57.411: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 13 03:20:57.423: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36055"},"items":null}

Jun 13 03:20:57.430: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36055"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:20:57.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9153" for this suite. 06/13/23 03:20:57.485
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":211,"skipped":4022,"failed":0}
------------------------------
• [SLOW TEST] [8.453 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:20:49.055
    Jun 13 03:20:49.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename daemonsets 06/13/23 03:20:49.057
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:20:49.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:20:49.091
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 06/13/23 03:20:49.147
    STEP: Check that daemon pods launch on every node of the cluster. 06/13/23 03:20:49.16
    Jun 13 03:20:49.181: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:49.181: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:49.181: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:49.190: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:20:49.190: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 03:20:50.205: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:50.205: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:50.205: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:50.216: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:20:50.216: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 03:20:51.200: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:51.200: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:51.200: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:51.208: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 13 03:20:51.208: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
    Jun 13 03:20:52.217: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:52.217: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:52.217: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:52.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 13 03:20:52.222: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 06/13/23 03:20:52.227
    Jun 13 03:20:52.293: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:52.293: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:52.293: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:52.301: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 13 03:20:52.301: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
    Jun 13 03:20:53.311: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:53.311: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:53.311: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:53.320: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 13 03:20:53.320: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
    Jun 13 03:20:54.389: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:54.389: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:54.389: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:54.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 13 03:20:54.394: INFO: Node sks-test-v1-25-9-workergroup-469fm is running 0 daemon pod, expected 1
    Jun 13 03:20:55.311: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:55.311: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:55.311: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:20:55.318: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 13 03:20:55.318: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 06/13/23 03:20:55.318
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/13/23 03:20:55.328
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9153, will wait for the garbage collector to delete the pods 06/13/23 03:20:55.328
    Jun 13 03:20:55.402: INFO: Deleting DaemonSet.extensions daemon-set took: 17.116698ms
    Jun 13 03:20:55.504: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.967958ms
    Jun 13 03:20:57.411: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:20:57.411: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 13 03:20:57.423: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36055"},"items":null}

    Jun 13 03:20:57.430: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36055"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:20:57.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9153" for this suite. 06/13/23 03:20:57.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:20:57.51
Jun 13 03:20:57.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:20:57.511
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:20:57.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:20:57.556
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-4288 06/13/23 03:20:57.564
STEP: creating service affinity-clusterip in namespace services-4288 06/13/23 03:20:57.564
STEP: creating replication controller affinity-clusterip in namespace services-4288 06/13/23 03:20:57.6
I0613 03:20:57.617596      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4288, replica count: 3
I0613 03:21:00.668071      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 13 03:21:00.682: INFO: Creating new exec pod
Jun 13 03:21:00.691: INFO: Waiting up to 5m0s for pod "execpod-affinitysrfng" in namespace "services-4288" to be "running"
Jun 13 03:21:00.703: INFO: Pod "execpod-affinitysrfng": Phase="Pending", Reason="", readiness=false. Elapsed: 12.11467ms
Jun 13 03:21:02.709: INFO: Pod "execpod-affinitysrfng": Phase="Running", Reason="", readiness=true. Elapsed: 2.017593316s
Jun 13 03:21:02.709: INFO: Pod "execpod-affinitysrfng" satisfied condition "running"
Jun 13 03:21:03.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4288 exec execpod-affinitysrfng -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jun 13 03:21:03.983: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun 13 03:21:03.983: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 03:21:03.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4288 exec execpod-affinitysrfng -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.243.197 80'
Jun 13 03:21:04.228: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.243.197 80\nConnection to 10.108.243.197 80 port [tcp/http] succeeded!\n"
Jun 13 03:21:04.228: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 03:21:04.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4288 exec execpod-affinitysrfng -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.243.197:80/ ; done'
Jun 13 03:21:04.633: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n"
Jun 13 03:21:04.633: INFO: stdout: "\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh"
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
Jun 13 03:21:04.633: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4288, will wait for the garbage collector to delete the pods 06/13/23 03:21:04.92
Jun 13 03:21:05.005: INFO: Deleting ReplicationController affinity-clusterip took: 13.153811ms
Jun 13 03:21:05.105: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.265732ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:21:07.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4288" for this suite. 06/13/23 03:21:07.558
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":212,"skipped":4042,"failed":0}
------------------------------
• [SLOW TEST] [10.063 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:20:57.51
    Jun 13 03:20:57.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:20:57.511
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:20:57.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:20:57.556
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-4288 06/13/23 03:20:57.564
    STEP: creating service affinity-clusterip in namespace services-4288 06/13/23 03:20:57.564
    STEP: creating replication controller affinity-clusterip in namespace services-4288 06/13/23 03:20:57.6
    I0613 03:20:57.617596      18 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4288, replica count: 3
    I0613 03:21:00.668071      18 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 13 03:21:00.682: INFO: Creating new exec pod
    Jun 13 03:21:00.691: INFO: Waiting up to 5m0s for pod "execpod-affinitysrfng" in namespace "services-4288" to be "running"
    Jun 13 03:21:00.703: INFO: Pod "execpod-affinitysrfng": Phase="Pending", Reason="", readiness=false. Elapsed: 12.11467ms
    Jun 13 03:21:02.709: INFO: Pod "execpod-affinitysrfng": Phase="Running", Reason="", readiness=true. Elapsed: 2.017593316s
    Jun 13 03:21:02.709: INFO: Pod "execpod-affinitysrfng" satisfied condition "running"
    Jun 13 03:21:03.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4288 exec execpod-affinitysrfng -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jun 13 03:21:03.983: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jun 13 03:21:03.983: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 03:21:03.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4288 exec execpod-affinitysrfng -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.243.197 80'
    Jun 13 03:21:04.228: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.243.197 80\nConnection to 10.108.243.197 80 port [tcp/http] succeeded!\n"
    Jun 13 03:21:04.228: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 03:21:04.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4288 exec execpod-affinitysrfng -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.243.197:80/ ; done'
    Jun 13 03:21:04.633: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.243.197:80/\n"
    Jun 13 03:21:04.633: INFO: stdout: "\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh\naffinity-clusterip-lkssh"
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Received response from host: affinity-clusterip-lkssh
    Jun 13 03:21:04.633: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-4288, will wait for the garbage collector to delete the pods 06/13/23 03:21:04.92
    Jun 13 03:21:05.005: INFO: Deleting ReplicationController affinity-clusterip took: 13.153811ms
    Jun 13 03:21:05.105: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.265732ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:21:07.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4288" for this suite. 06/13/23 03:21:07.558
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:21:07.574
Jun 13 03:21:07.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 03:21:07.576
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:07.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:07.609
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 03:21:07.651
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:21:08.068
STEP: Deploying the webhook pod 06/13/23 03:21:08.096
STEP: Wait for the deployment to be ready 06/13/23 03:21:08.14
Jun 13 03:21:08.171: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 13 03:21:10.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 21, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 21, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 21, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 21, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/13/23 03:21:12.196
STEP: Verifying the service has paired with the endpoint 06/13/23 03:21:12.219
Jun 13 03:21:13.220: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/13/23 03:21:13.228
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/13/23 03:21:13.258
STEP: Creating a dummy validating-webhook-configuration object 06/13/23 03:21:13.286
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 06/13/23 03:21:13.309
STEP: Creating a dummy mutating-webhook-configuration object 06/13/23 03:21:13.321
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 06/13/23 03:21:13.339
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:21:13.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2140" for this suite. 06/13/23 03:21:13.383
STEP: Destroying namespace "webhook-2140-markers" for this suite. 06/13/23 03:21:13.403
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":213,"skipped":4048,"failed":0}
------------------------------
• [SLOW TEST] [5.948 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:21:07.574
    Jun 13 03:21:07.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 03:21:07.576
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:07.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:07.609
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 03:21:07.651
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:21:08.068
    STEP: Deploying the webhook pod 06/13/23 03:21:08.096
    STEP: Wait for the deployment to be ready 06/13/23 03:21:08.14
    Jun 13 03:21:08.171: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 13 03:21:10.190: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 21, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 21, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 21, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 21, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/13/23 03:21:12.196
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:21:12.219
    Jun 13 03:21:13.220: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/13/23 03:21:13.228
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/13/23 03:21:13.258
    STEP: Creating a dummy validating-webhook-configuration object 06/13/23 03:21:13.286
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 06/13/23 03:21:13.309
    STEP: Creating a dummy mutating-webhook-configuration object 06/13/23 03:21:13.321
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 06/13/23 03:21:13.339
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:21:13.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2140" for this suite. 06/13/23 03:21:13.383
    STEP: Destroying namespace "webhook-2140-markers" for this suite. 06/13/23 03:21:13.403
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:21:13.524
Jun 13 03:21:13.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename security-context 06/13/23 03:21:13.525
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:13.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:13.557
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/13/23 03:21:13.564
Jun 13 03:21:13.588: INFO: Waiting up to 5m0s for pod "security-context-1cbb9f7d-3d90-432b-8967-90287dcea826" in namespace "security-context-2621" to be "Succeeded or Failed"
Jun 13 03:21:13.598: INFO: Pod "security-context-1cbb9f7d-3d90-432b-8967-90287dcea826": Phase="Pending", Reason="", readiness=false. Elapsed: 9.471153ms
Jun 13 03:21:15.611: INFO: Pod "security-context-1cbb9f7d-3d90-432b-8967-90287dcea826": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022690749s
Jun 13 03:21:17.608: INFO: Pod "security-context-1cbb9f7d-3d90-432b-8967-90287dcea826": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019570507s
Jun 13 03:21:19.607: INFO: Pod "security-context-1cbb9f7d-3d90-432b-8967-90287dcea826": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018213288s
STEP: Saw pod success 06/13/23 03:21:19.607
Jun 13 03:21:19.607: INFO: Pod "security-context-1cbb9f7d-3d90-432b-8967-90287dcea826" satisfied condition "Succeeded or Failed"
Jun 13 03:21:19.616: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod security-context-1cbb9f7d-3d90-432b-8967-90287dcea826 container test-container: <nil>
STEP: delete the pod 06/13/23 03:21:19.631
Jun 13 03:21:19.659: INFO: Waiting for pod security-context-1cbb9f7d-3d90-432b-8967-90287dcea826 to disappear
Jun 13 03:21:19.667: INFO: Pod security-context-1cbb9f7d-3d90-432b-8967-90287dcea826 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 13 03:21:19.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2621" for this suite. 06/13/23 03:21:19.684
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":214,"skipped":4076,"failed":0}
------------------------------
• [SLOW TEST] [6.189 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:21:13.524
    Jun 13 03:21:13.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename security-context 06/13/23 03:21:13.525
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:13.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:13.557
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/13/23 03:21:13.564
    Jun 13 03:21:13.588: INFO: Waiting up to 5m0s for pod "security-context-1cbb9f7d-3d90-432b-8967-90287dcea826" in namespace "security-context-2621" to be "Succeeded or Failed"
    Jun 13 03:21:13.598: INFO: Pod "security-context-1cbb9f7d-3d90-432b-8967-90287dcea826": Phase="Pending", Reason="", readiness=false. Elapsed: 9.471153ms
    Jun 13 03:21:15.611: INFO: Pod "security-context-1cbb9f7d-3d90-432b-8967-90287dcea826": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022690749s
    Jun 13 03:21:17.608: INFO: Pod "security-context-1cbb9f7d-3d90-432b-8967-90287dcea826": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019570507s
    Jun 13 03:21:19.607: INFO: Pod "security-context-1cbb9f7d-3d90-432b-8967-90287dcea826": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018213288s
    STEP: Saw pod success 06/13/23 03:21:19.607
    Jun 13 03:21:19.607: INFO: Pod "security-context-1cbb9f7d-3d90-432b-8967-90287dcea826" satisfied condition "Succeeded or Failed"
    Jun 13 03:21:19.616: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod security-context-1cbb9f7d-3d90-432b-8967-90287dcea826 container test-container: <nil>
    STEP: delete the pod 06/13/23 03:21:19.631
    Jun 13 03:21:19.659: INFO: Waiting for pod security-context-1cbb9f7d-3d90-432b-8967-90287dcea826 to disappear
    Jun 13 03:21:19.667: INFO: Pod security-context-1cbb9f7d-3d90-432b-8967-90287dcea826 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 13 03:21:19.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-2621" for this suite. 06/13/23 03:21:19.684
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:21:19.714
Jun 13 03:21:19.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 03:21:19.715
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:19.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:19.755
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-d10ce96b-87fa-4e6f-927f-7e83ac0dbc65 06/13/23 03:21:19.785
STEP: Creating configMap with name cm-test-opt-upd-6736c85c-0824-453c-8da8-1a9216b58ba9 06/13/23 03:21:19.796
STEP: Creating the pod 06/13/23 03:21:19.809
Jun 13 03:21:19.827: INFO: Waiting up to 5m0s for pod "pod-configmaps-46a710f3-6799-42ab-b299-df91298d0107" in namespace "configmap-2083" to be "running and ready"
Jun 13 03:21:19.835: INFO: Pod "pod-configmaps-46a710f3-6799-42ab-b299-df91298d0107": Phase="Pending", Reason="", readiness=false. Elapsed: 8.743723ms
Jun 13 03:21:19.835: INFO: The phase of Pod pod-configmaps-46a710f3-6799-42ab-b299-df91298d0107 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:21:21.848: INFO: Pod "pod-configmaps-46a710f3-6799-42ab-b299-df91298d0107": Phase="Running", Reason="", readiness=true. Elapsed: 2.021707429s
Jun 13 03:21:21.848: INFO: The phase of Pod pod-configmaps-46a710f3-6799-42ab-b299-df91298d0107 is Running (Ready = true)
Jun 13 03:21:21.848: INFO: Pod "pod-configmaps-46a710f3-6799-42ab-b299-df91298d0107" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-d10ce96b-87fa-4e6f-927f-7e83ac0dbc65 06/13/23 03:21:21.916
STEP: Updating configmap cm-test-opt-upd-6736c85c-0824-453c-8da8-1a9216b58ba9 06/13/23 03:21:21.939
STEP: Creating configMap with name cm-test-opt-create-dfa83b3c-4da2-4a30-82b8-975b3353e5a3 06/13/23 03:21:21.95
STEP: waiting to observe update in volume 06/13/23 03:21:21.968
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 03:21:24.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2083" for this suite. 06/13/23 03:21:24.05
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":215,"skipped":4089,"failed":0}
------------------------------
• [4.356 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:21:19.714
    Jun 13 03:21:19.714: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 03:21:19.715
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:19.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:19.755
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-d10ce96b-87fa-4e6f-927f-7e83ac0dbc65 06/13/23 03:21:19.785
    STEP: Creating configMap with name cm-test-opt-upd-6736c85c-0824-453c-8da8-1a9216b58ba9 06/13/23 03:21:19.796
    STEP: Creating the pod 06/13/23 03:21:19.809
    Jun 13 03:21:19.827: INFO: Waiting up to 5m0s for pod "pod-configmaps-46a710f3-6799-42ab-b299-df91298d0107" in namespace "configmap-2083" to be "running and ready"
    Jun 13 03:21:19.835: INFO: Pod "pod-configmaps-46a710f3-6799-42ab-b299-df91298d0107": Phase="Pending", Reason="", readiness=false. Elapsed: 8.743723ms
    Jun 13 03:21:19.835: INFO: The phase of Pod pod-configmaps-46a710f3-6799-42ab-b299-df91298d0107 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:21:21.848: INFO: Pod "pod-configmaps-46a710f3-6799-42ab-b299-df91298d0107": Phase="Running", Reason="", readiness=true. Elapsed: 2.021707429s
    Jun 13 03:21:21.848: INFO: The phase of Pod pod-configmaps-46a710f3-6799-42ab-b299-df91298d0107 is Running (Ready = true)
    Jun 13 03:21:21.848: INFO: Pod "pod-configmaps-46a710f3-6799-42ab-b299-df91298d0107" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-d10ce96b-87fa-4e6f-927f-7e83ac0dbc65 06/13/23 03:21:21.916
    STEP: Updating configmap cm-test-opt-upd-6736c85c-0824-453c-8da8-1a9216b58ba9 06/13/23 03:21:21.939
    STEP: Creating configMap with name cm-test-opt-create-dfa83b3c-4da2-4a30-82b8-975b3353e5a3 06/13/23 03:21:21.95
    STEP: waiting to observe update in volume 06/13/23 03:21:21.968
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 03:21:24.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2083" for this suite. 06/13/23 03:21:24.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:21:24.077
Jun 13 03:21:24.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename endpointslice 06/13/23 03:21:24.079
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:24.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:24.134
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 06/13/23 03:21:30.392
STEP: referencing matching pods with named port 06/13/23 03:21:35.56
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 06/13/23 03:21:40.65
STEP: recreating EndpointSlices after they've been deleted 06/13/23 03:21:45.733
Jun 13 03:21:45.922: INFO: EndpointSlice for Service endpointslice-8116/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jun 13 03:21:55.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8116" for this suite. 06/13/23 03:21:55.984
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":216,"skipped":4108,"failed":0}
------------------------------
• [SLOW TEST] [32.083 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:21:24.077
    Jun 13 03:21:24.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename endpointslice 06/13/23 03:21:24.079
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:24.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:24.134
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 06/13/23 03:21:30.392
    STEP: referencing matching pods with named port 06/13/23 03:21:35.56
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 06/13/23 03:21:40.65
    STEP: recreating EndpointSlices after they've been deleted 06/13/23 03:21:45.733
    Jun 13 03:21:45.922: INFO: EndpointSlice for Service endpointslice-8116/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jun 13 03:21:55.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8116" for this suite. 06/13/23 03:21:55.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:21:56.161
Jun 13 03:21:56.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 03:21:56.164
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:56.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:56.246
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 06/13/23 03:21:56.256
Jun 13 03:21:56.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5243 cluster-info'
Jun 13 03:21:56.344: INFO: stderr: ""
Jun 13 03:21:56.344: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 03:21:56.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5243" for this suite. 06/13/23 03:21:56.353
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":217,"skipped":4113,"failed":0}
------------------------------
• [0.244 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:21:56.161
    Jun 13 03:21:56.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 03:21:56.164
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:56.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:56.246
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 06/13/23 03:21:56.256
    Jun 13 03:21:56.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5243 cluster-info'
    Jun 13 03:21:56.344: INFO: stderr: ""
    Jun 13 03:21:56.344: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 03:21:56.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5243" for this suite. 06/13/23 03:21:56.353
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:21:56.406
Jun 13 03:21:56.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename ingressclass 06/13/23 03:21:56.407
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:56.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:56.453
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 06/13/23 03:21:56.46
STEP: getting /apis/networking.k8s.io 06/13/23 03:21:56.467
STEP: getting /apis/networking.k8s.iov1 06/13/23 03:21:56.481
STEP: creating 06/13/23 03:21:56.486
STEP: getting 06/13/23 03:21:56.525
STEP: listing 06/13/23 03:21:56.535
STEP: watching 06/13/23 03:21:56.54
Jun 13 03:21:56.540: INFO: starting watch
STEP: patching 06/13/23 03:21:56.546
STEP: updating 06/13/23 03:21:56.555
Jun 13 03:21:56.566: INFO: waiting for watch events with expected annotations
Jun 13 03:21:56.566: INFO: saw patched and updated annotations
STEP: deleting 06/13/23 03:21:56.566
STEP: deleting a collection 06/13/23 03:21:56.62
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jun 13 03:21:56.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-8536" for this suite. 06/13/23 03:21:56.673
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":218,"skipped":4116,"failed":0}
------------------------------
• [0.284 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:21:56.406
    Jun 13 03:21:56.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename ingressclass 06/13/23 03:21:56.407
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:56.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:56.453
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 06/13/23 03:21:56.46
    STEP: getting /apis/networking.k8s.io 06/13/23 03:21:56.467
    STEP: getting /apis/networking.k8s.iov1 06/13/23 03:21:56.481
    STEP: creating 06/13/23 03:21:56.486
    STEP: getting 06/13/23 03:21:56.525
    STEP: listing 06/13/23 03:21:56.535
    STEP: watching 06/13/23 03:21:56.54
    Jun 13 03:21:56.540: INFO: starting watch
    STEP: patching 06/13/23 03:21:56.546
    STEP: updating 06/13/23 03:21:56.555
    Jun 13 03:21:56.566: INFO: waiting for watch events with expected annotations
    Jun 13 03:21:56.566: INFO: saw patched and updated annotations
    STEP: deleting 06/13/23 03:21:56.566
    STEP: deleting a collection 06/13/23 03:21:56.62
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jun 13 03:21:56.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-8536" for this suite. 06/13/23 03:21:56.673
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:21:56.691
Jun 13 03:21:56.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 03:21:56.692
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:56.718
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:56.727
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-d77d28a1-5223-4180-82a8-6df0d9ac6bd1 06/13/23 03:21:56.732
STEP: Creating a pod to test consume secrets 06/13/23 03:21:56.752
Jun 13 03:21:56.775: INFO: Waiting up to 5m0s for pod "pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b" in namespace "secrets-5174" to be "Succeeded or Failed"
Jun 13 03:21:56.790: INFO: Pod "pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.547794ms
Jun 13 03:21:58.797: INFO: Pod "pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02223672s
Jun 13 03:22:00.800: INFO: Pod "pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024860843s
STEP: Saw pod success 06/13/23 03:22:00.8
Jun 13 03:22:00.800: INFO: Pod "pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b" satisfied condition "Succeeded or Failed"
Jun 13 03:22:00.814: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b container secret-env-test: <nil>
STEP: delete the pod 06/13/23 03:22:00.834
Jun 13 03:22:00.866: INFO: Waiting for pod pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b to disappear
Jun 13 03:22:00.876: INFO: Pod pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jun 13 03:22:00.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5174" for this suite. 06/13/23 03:22:00.887
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":219,"skipped":4121,"failed":0}
------------------------------
• [4.217 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:21:56.691
    Jun 13 03:21:56.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 03:21:56.692
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:21:56.718
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:21:56.727
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-d77d28a1-5223-4180-82a8-6df0d9ac6bd1 06/13/23 03:21:56.732
    STEP: Creating a pod to test consume secrets 06/13/23 03:21:56.752
    Jun 13 03:21:56.775: INFO: Waiting up to 5m0s for pod "pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b" in namespace "secrets-5174" to be "Succeeded or Failed"
    Jun 13 03:21:56.790: INFO: Pod "pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.547794ms
    Jun 13 03:21:58.797: INFO: Pod "pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02223672s
    Jun 13 03:22:00.800: INFO: Pod "pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024860843s
    STEP: Saw pod success 06/13/23 03:22:00.8
    Jun 13 03:22:00.800: INFO: Pod "pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b" satisfied condition "Succeeded or Failed"
    Jun 13 03:22:00.814: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b container secret-env-test: <nil>
    STEP: delete the pod 06/13/23 03:22:00.834
    Jun 13 03:22:00.866: INFO: Waiting for pod pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b to disappear
    Jun 13 03:22:00.876: INFO: Pod pod-secrets-f680c079-06b1-4e90-a469-a0c8a548e71b no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 03:22:00.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5174" for this suite. 06/13/23 03:22:00.887
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:22:00.908
Jun 13 03:22:00.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename sched-preemption 06/13/23 03:22:00.91
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:22:00.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:22:00.951
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 13 03:22:01.149: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 13 03:23:01.258: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:23:01.265
Jun 13 03:23:01.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename sched-preemption-path 06/13/23 03:23:01.268
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:01.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:01.317
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jun 13 03:23:01.361: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jun 13 03:23:01.376: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jun 13 03:23:01.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9770" for this suite. 06/13/23 03:23:01.457
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:23:01.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5910" for this suite. 06/13/23 03:23:01.532
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":220,"skipped":4123,"failed":0}
------------------------------
• [SLOW TEST] [60.892 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:22:00.908
    Jun 13 03:22:00.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename sched-preemption 06/13/23 03:22:00.91
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:22:00.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:22:00.951
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jun 13 03:22:01.149: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 13 03:23:01.258: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:23:01.265
    Jun 13 03:23:01.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename sched-preemption-path 06/13/23 03:23:01.268
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:01.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:01.317
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jun 13 03:23:01.361: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jun 13 03:23:01.376: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jun 13 03:23:01.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-9770" for this suite. 06/13/23 03:23:01.457
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:23:01.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5910" for this suite. 06/13/23 03:23:01.532
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:23:01.801
Jun 13 03:23:01.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename svcaccounts 06/13/23 03:23:01.803
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:01.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:01.843
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 06/13/23 03:23:01.855
STEP: watching for the ServiceAccount to be added 06/13/23 03:23:01.878
STEP: patching the ServiceAccount 06/13/23 03:23:01.882
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 06/13/23 03:23:01.899
STEP: deleting the ServiceAccount 06/13/23 03:23:01.905
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 13 03:23:01.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5139" for this suite. 06/13/23 03:23:01.974
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":221,"skipped":4136,"failed":0}
------------------------------
• [0.198 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:23:01.801
    Jun 13 03:23:01.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename svcaccounts 06/13/23 03:23:01.803
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:01.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:01.843
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 06/13/23 03:23:01.855
    STEP: watching for the ServiceAccount to be added 06/13/23 03:23:01.878
    STEP: patching the ServiceAccount 06/13/23 03:23:01.882
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 06/13/23 03:23:01.899
    STEP: deleting the ServiceAccount 06/13/23 03:23:01.905
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 13 03:23:01.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5139" for this suite. 06/13/23 03:23:01.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:23:02
Jun 13 03:23:02.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir-wrapper 06/13/23 03:23:02.001
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:02.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:02.063
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jun 13 03:23:02.169: INFO: Waiting up to 5m0s for pod "pod-secrets-17b89589-734f-4fcd-afdf-49f9af9917b1" in namespace "emptydir-wrapper-1533" to be "running and ready"
Jun 13 03:23:02.198: INFO: Pod "pod-secrets-17b89589-734f-4fcd-afdf-49f9af9917b1": Phase="Pending", Reason="", readiness=false. Elapsed: 29.157738ms
Jun 13 03:23:02.198: INFO: The phase of Pod pod-secrets-17b89589-734f-4fcd-afdf-49f9af9917b1 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:23:04.207: INFO: Pod "pod-secrets-17b89589-734f-4fcd-afdf-49f9af9917b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.037932353s
Jun 13 03:23:04.207: INFO: The phase of Pod pod-secrets-17b89589-734f-4fcd-afdf-49f9af9917b1 is Running (Ready = true)
Jun 13 03:23:04.207: INFO: Pod "pod-secrets-17b89589-734f-4fcd-afdf-49f9af9917b1" satisfied condition "running and ready"
STEP: Cleaning up the secret 06/13/23 03:23:04.214
STEP: Cleaning up the configmap 06/13/23 03:23:04.233
STEP: Cleaning up the pod 06/13/23 03:23:04.244
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jun 13 03:23:04.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1533" for this suite. 06/13/23 03:23:04.271
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":222,"skipped":4145,"failed":0}
------------------------------
• [2.283 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:23:02
    Jun 13 03:23:02.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir-wrapper 06/13/23 03:23:02.001
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:02.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:02.063
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jun 13 03:23:02.169: INFO: Waiting up to 5m0s for pod "pod-secrets-17b89589-734f-4fcd-afdf-49f9af9917b1" in namespace "emptydir-wrapper-1533" to be "running and ready"
    Jun 13 03:23:02.198: INFO: Pod "pod-secrets-17b89589-734f-4fcd-afdf-49f9af9917b1": Phase="Pending", Reason="", readiness=false. Elapsed: 29.157738ms
    Jun 13 03:23:02.198: INFO: The phase of Pod pod-secrets-17b89589-734f-4fcd-afdf-49f9af9917b1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:23:04.207: INFO: Pod "pod-secrets-17b89589-734f-4fcd-afdf-49f9af9917b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.037932353s
    Jun 13 03:23:04.207: INFO: The phase of Pod pod-secrets-17b89589-734f-4fcd-afdf-49f9af9917b1 is Running (Ready = true)
    Jun 13 03:23:04.207: INFO: Pod "pod-secrets-17b89589-734f-4fcd-afdf-49f9af9917b1" satisfied condition "running and ready"
    STEP: Cleaning up the secret 06/13/23 03:23:04.214
    STEP: Cleaning up the configmap 06/13/23 03:23:04.233
    STEP: Cleaning up the pod 06/13/23 03:23:04.244
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jun 13 03:23:04.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-1533" for this suite. 06/13/23 03:23:04.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:23:04.283
Jun 13 03:23:04.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 03:23:04.285
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:04.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:04.322
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 03:23:04.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-673" for this suite. 06/13/23 03:23:04.487
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":223,"skipped":4151,"failed":0}
------------------------------
• [0.215 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:23:04.283
    Jun 13 03:23:04.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 03:23:04.285
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:04.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:04.322
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 03:23:04.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-673" for this suite. 06/13/23 03:23:04.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:23:04.5
Jun 13 03:23:04.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:23:04.501
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:04.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:04.533
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 06/13/23 03:23:04.539
Jun 13 03:23:04.554: INFO: Waiting up to 5m0s for pod "labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f" in namespace "downward-api-2392" to be "running and ready"
Jun 13 03:23:04.566: INFO: Pod "labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.509562ms
Jun 13 03:23:04.566: INFO: The phase of Pod labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:23:06.574: INFO: Pod "labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.020393418s
Jun 13 03:23:06.574: INFO: The phase of Pod labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f is Running (Ready = true)
Jun 13 03:23:06.574: INFO: Pod "labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f" satisfied condition "running and ready"
Jun 13 03:23:07.191: INFO: Successfully updated pod "labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 13 03:23:11.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2392" for this suite. 06/13/23 03:23:11.252
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":224,"skipped":4168,"failed":0}
------------------------------
• [SLOW TEST] [6.762 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:23:04.5
    Jun 13 03:23:04.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:23:04.501
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:04.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:04.533
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 06/13/23 03:23:04.539
    Jun 13 03:23:04.554: INFO: Waiting up to 5m0s for pod "labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f" in namespace "downward-api-2392" to be "running and ready"
    Jun 13 03:23:04.566: INFO: Pod "labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f": Phase="Pending", Reason="", readiness=false. Elapsed: 12.509562ms
    Jun 13 03:23:04.566: INFO: The phase of Pod labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:23:06.574: INFO: Pod "labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f": Phase="Running", Reason="", readiness=true. Elapsed: 2.020393418s
    Jun 13 03:23:06.574: INFO: The phase of Pod labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f is Running (Ready = true)
    Jun 13 03:23:06.574: INFO: Pod "labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f" satisfied condition "running and ready"
    Jun 13 03:23:07.191: INFO: Successfully updated pod "labelsupdateb8c7dafb-7481-4f15-96e9-a40c5423de1f"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 13 03:23:11.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2392" for this suite. 06/13/23 03:23:11.252
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:23:11.263
Jun 13 03:23:11.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:23:11.263
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:11.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:11.29
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210
STEP: creating an Endpoint 06/13/23 03:23:11.307
STEP: waiting for available Endpoint 06/13/23 03:23:11.316
STEP: listing all Endpoints 06/13/23 03:23:11.319
STEP: updating the Endpoint 06/13/23 03:23:11.326
STEP: fetching the Endpoint 06/13/23 03:23:11.34
STEP: patching the Endpoint 06/13/23 03:23:11.348
STEP: fetching the Endpoint 06/13/23 03:23:11.367
STEP: deleting the Endpoint by Collection 06/13/23 03:23:11.374
STEP: waiting for Endpoint deletion 06/13/23 03:23:11.396
STEP: fetching the Endpoint 06/13/23 03:23:11.4
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:23:11.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7695" for this suite. 06/13/23 03:23:11.418
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":225,"skipped":4171,"failed":0}
------------------------------
• [0.168 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:23:11.263
    Jun 13 03:23:11.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:23:11.263
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:11.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:11.29
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3210
    STEP: creating an Endpoint 06/13/23 03:23:11.307
    STEP: waiting for available Endpoint 06/13/23 03:23:11.316
    STEP: listing all Endpoints 06/13/23 03:23:11.319
    STEP: updating the Endpoint 06/13/23 03:23:11.326
    STEP: fetching the Endpoint 06/13/23 03:23:11.34
    STEP: patching the Endpoint 06/13/23 03:23:11.348
    STEP: fetching the Endpoint 06/13/23 03:23:11.367
    STEP: deleting the Endpoint by Collection 06/13/23 03:23:11.374
    STEP: waiting for Endpoint deletion 06/13/23 03:23:11.396
    STEP: fetching the Endpoint 06/13/23 03:23:11.4
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:23:11.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7695" for this suite. 06/13/23 03:23:11.418
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:23:11.43
Jun 13 03:23:11.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename replication-controller 06/13/23 03:23:11.432
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:11.467
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:11.479
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd 06/13/23 03:23:11.487
Jun 13 03:23:11.513: INFO: Pod name my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd: Found 0 pods out of 1
Jun 13 03:23:16.519: INFO: Pod name my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd: Found 1 pods out of 1
Jun 13 03:23:16.519: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd" are running
Jun 13 03:23:16.519: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd-qxnxk" in namespace "replication-controller-608" to be "running"
Jun 13 03:23:16.532: INFO: Pod "my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd-qxnxk": Phase="Running", Reason="", readiness=true. Elapsed: 13.21558ms
Jun 13 03:23:16.533: INFO: Pod "my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd-qxnxk" satisfied condition "running"
Jun 13 03:23:16.533: INFO: Pod "my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd-qxnxk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:23:11 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:23:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:23:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:23:11 +0000 UTC Reason: Message:}])
Jun 13 03:23:16.533: INFO: Trying to dial the pod
Jun 13 03:23:21.558: INFO: Controller my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd: Got expected result from replica 1 [my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd-qxnxk]: "my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd-qxnxk", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 13 03:23:21.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-608" for this suite. 06/13/23 03:23:21.567
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":226,"skipped":4172,"failed":0}
------------------------------
• [SLOW TEST] [10.150 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:23:11.43
    Jun 13 03:23:11.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename replication-controller 06/13/23 03:23:11.432
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:11.467
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:11.479
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd 06/13/23 03:23:11.487
    Jun 13 03:23:11.513: INFO: Pod name my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd: Found 0 pods out of 1
    Jun 13 03:23:16.519: INFO: Pod name my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd: Found 1 pods out of 1
    Jun 13 03:23:16.519: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd" are running
    Jun 13 03:23:16.519: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd-qxnxk" in namespace "replication-controller-608" to be "running"
    Jun 13 03:23:16.532: INFO: Pod "my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd-qxnxk": Phase="Running", Reason="", readiness=true. Elapsed: 13.21558ms
    Jun 13 03:23:16.533: INFO: Pod "my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd-qxnxk" satisfied condition "running"
    Jun 13 03:23:16.533: INFO: Pod "my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd-qxnxk" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:23:11 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:23:13 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:23:13 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-13 03:23:11 +0000 UTC Reason: Message:}])
    Jun 13 03:23:16.533: INFO: Trying to dial the pod
    Jun 13 03:23:21.558: INFO: Controller my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd: Got expected result from replica 1 [my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd-qxnxk]: "my-hostname-basic-4278377a-9572-4daf-a31e-422f5cb091cd-qxnxk", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 13 03:23:21.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-608" for this suite. 06/13/23 03:23:21.567
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:23:21.583
Jun 13 03:23:21.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename resourcequota 06/13/23 03:23:21.585
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:21.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:21.609
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 06/13/23 03:23:21.615
STEP: Ensuring ResourceQuota status is calculated 06/13/23 03:23:21.623
STEP: Creating a ResourceQuota with not best effort scope 06/13/23 03:23:23.632
STEP: Ensuring ResourceQuota status is calculated 06/13/23 03:23:23.65
STEP: Creating a best-effort pod 06/13/23 03:23:25.658
STEP: Ensuring resource quota with best effort scope captures the pod usage 06/13/23 03:23:25.685
STEP: Ensuring resource quota with not best effort ignored the pod usage 06/13/23 03:23:27.831
STEP: Deleting the pod 06/13/23 03:23:29.837
STEP: Ensuring resource quota status released the pod usage 06/13/23 03:23:29.856
STEP: Creating a not best-effort pod 06/13/23 03:23:31.863
STEP: Ensuring resource quota with not best effort scope captures the pod usage 06/13/23 03:23:31.879
STEP: Ensuring resource quota with best effort scope ignored the pod usage 06/13/23 03:23:33.888
STEP: Deleting the pod 06/13/23 03:23:35.9
STEP: Ensuring resource quota status released the pod usage 06/13/23 03:23:35.96
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 13 03:23:38.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1593" for this suite. 06/13/23 03:23:38.102
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":227,"skipped":4235,"failed":0}
------------------------------
• [SLOW TEST] [16.531 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:23:21.583
    Jun 13 03:23:21.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename resourcequota 06/13/23 03:23:21.585
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:21.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:21.609
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 06/13/23 03:23:21.615
    STEP: Ensuring ResourceQuota status is calculated 06/13/23 03:23:21.623
    STEP: Creating a ResourceQuota with not best effort scope 06/13/23 03:23:23.632
    STEP: Ensuring ResourceQuota status is calculated 06/13/23 03:23:23.65
    STEP: Creating a best-effort pod 06/13/23 03:23:25.658
    STEP: Ensuring resource quota with best effort scope captures the pod usage 06/13/23 03:23:25.685
    STEP: Ensuring resource quota with not best effort ignored the pod usage 06/13/23 03:23:27.831
    STEP: Deleting the pod 06/13/23 03:23:29.837
    STEP: Ensuring resource quota status released the pod usage 06/13/23 03:23:29.856
    STEP: Creating a not best-effort pod 06/13/23 03:23:31.863
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 06/13/23 03:23:31.879
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 06/13/23 03:23:33.888
    STEP: Deleting the pod 06/13/23 03:23:35.9
    STEP: Ensuring resource quota status released the pod usage 06/13/23 03:23:35.96
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 13 03:23:38.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1593" for this suite. 06/13/23 03:23:38.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:23:38.116
Jun 13 03:23:38.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename lease-test 06/13/23 03:23:38.117
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:38.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:38.158
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jun 13 03:23:38.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-4236" for this suite. 06/13/23 03:23:38.357
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":228,"skipped":4251,"failed":0}
------------------------------
• [0.293 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:23:38.116
    Jun 13 03:23:38.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename lease-test 06/13/23 03:23:38.117
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:38.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:38.158
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jun 13 03:23:38.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-4236" for this suite. 06/13/23 03:23:38.357
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:23:38.409
Jun 13 03:23:38.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 03:23:38.41
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:38.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:38.636
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-475b6d6a-f204-41fa-9768-83ddbd9064fe 06/13/23 03:23:38.646
STEP: Creating a pod to test consume configMaps 06/13/23 03:23:38.659
Jun 13 03:23:38.681: INFO: Waiting up to 5m0s for pod "pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e" in namespace "configmap-3555" to be "Succeeded or Failed"
Jun 13 03:23:38.700: INFO: Pod "pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.561767ms
Jun 13 03:23:40.707: INFO: Pod "pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02609054s
Jun 13 03:23:42.707: INFO: Pod "pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025563628s
STEP: Saw pod success 06/13/23 03:23:42.707
Jun 13 03:23:42.707: INFO: Pod "pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e" satisfied condition "Succeeded or Failed"
Jun 13 03:23:42.714: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e container configmap-volume-test: <nil>
STEP: delete the pod 06/13/23 03:23:42.742
Jun 13 03:23:42.758: INFO: Waiting for pod pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e to disappear
Jun 13 03:23:42.764: INFO: Pod pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 03:23:42.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3555" for this suite. 06/13/23 03:23:42.779
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":229,"skipped":4262,"failed":0}
------------------------------
• [4.380 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:23:38.409
    Jun 13 03:23:38.409: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 03:23:38.41
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:38.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:38.636
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-475b6d6a-f204-41fa-9768-83ddbd9064fe 06/13/23 03:23:38.646
    STEP: Creating a pod to test consume configMaps 06/13/23 03:23:38.659
    Jun 13 03:23:38.681: INFO: Waiting up to 5m0s for pod "pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e" in namespace "configmap-3555" to be "Succeeded or Failed"
    Jun 13 03:23:38.700: INFO: Pod "pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 18.561767ms
    Jun 13 03:23:40.707: INFO: Pod "pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02609054s
    Jun 13 03:23:42.707: INFO: Pod "pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025563628s
    STEP: Saw pod success 06/13/23 03:23:42.707
    Jun 13 03:23:42.707: INFO: Pod "pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e" satisfied condition "Succeeded or Failed"
    Jun 13 03:23:42.714: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e container configmap-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:23:42.742
    Jun 13 03:23:42.758: INFO: Waiting for pod pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e to disappear
    Jun 13 03:23:42.764: INFO: Pod pod-configmaps-7dc8fb63-68ce-47fa-8a38-9249c37e0e6e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 03:23:42.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3555" for this suite. 06/13/23 03:23:42.779
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:23:42.789
Jun 13 03:23:42.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pods 06/13/23 03:23:42.79
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:42.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:42.819
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 06/13/23 03:23:42.823
Jun 13 03:23:42.836: INFO: Waiting up to 5m0s for pod "pod-n8bq8" in namespace "pods-3188" to be "running"
Jun 13 03:23:42.845: INFO: Pod "pod-n8bq8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.618659ms
Jun 13 03:23:44.852: INFO: Pod "pod-n8bq8": Phase="Running", Reason="", readiness=true. Elapsed: 2.015818196s
Jun 13 03:23:44.852: INFO: Pod "pod-n8bq8" satisfied condition "running"
STEP: patching /status 06/13/23 03:23:44.852
Jun 13 03:23:44.889: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 13 03:23:44.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3188" for this suite. 06/13/23 03:23:44.899
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":230,"skipped":4263,"failed":0}
------------------------------
• [2.128 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:23:42.789
    Jun 13 03:23:42.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pods 06/13/23 03:23:42.79
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:42.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:42.819
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 06/13/23 03:23:42.823
    Jun 13 03:23:42.836: INFO: Waiting up to 5m0s for pod "pod-n8bq8" in namespace "pods-3188" to be "running"
    Jun 13 03:23:42.845: INFO: Pod "pod-n8bq8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.618659ms
    Jun 13 03:23:44.852: INFO: Pod "pod-n8bq8": Phase="Running", Reason="", readiness=true. Elapsed: 2.015818196s
    Jun 13 03:23:44.852: INFO: Pod "pod-n8bq8" satisfied condition "running"
    STEP: patching /status 06/13/23 03:23:44.852
    Jun 13 03:23:44.889: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 13 03:23:44.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3188" for this suite. 06/13/23 03:23:44.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:23:44.919
Jun 13 03:23:44.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 03:23:44.92
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:45.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:45.014
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jun 13 03:23:45.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/13/23 03:23:51.581
Jun 13 03:23:51.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-9760 --namespace=crd-publish-openapi-9760 create -f -'
Jun 13 03:23:52.861: INFO: stderr: ""
Jun 13 03:23:52.861: INFO: stdout: "e2e-test-crd-publish-openapi-8424-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 13 03:23:52.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-9760 --namespace=crd-publish-openapi-9760 delete e2e-test-crd-publish-openapi-8424-crds test-cr'
Jun 13 03:23:53.028: INFO: stderr: ""
Jun 13 03:23:53.028: INFO: stdout: "e2e-test-crd-publish-openapi-8424-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 13 03:23:53.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-9760 --namespace=crd-publish-openapi-9760 apply -f -'
Jun 13 03:23:53.441: INFO: stderr: ""
Jun 13 03:23:53.441: INFO: stdout: "e2e-test-crd-publish-openapi-8424-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 13 03:23:53.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-9760 --namespace=crd-publish-openapi-9760 delete e2e-test-crd-publish-openapi-8424-crds test-cr'
Jun 13 03:23:53.602: INFO: stderr: ""
Jun 13 03:23:53.602: INFO: stdout: "e2e-test-crd-publish-openapi-8424-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 06/13/23 03:23:53.602
Jun 13 03:23:53.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-9760 explain e2e-test-crd-publish-openapi-8424-crds'
Jun 13 03:23:54.050: INFO: stderr: ""
Jun 13 03:23:54.050: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8424-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:24:00.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9760" for this suite. 06/13/23 03:24:00.672
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":231,"skipped":4305,"failed":0}
------------------------------
• [SLOW TEST] [15.780 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:23:44.919
    Jun 13 03:23:44.919: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 03:23:44.92
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:23:45.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:23:45.014
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jun 13 03:23:45.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/13/23 03:23:51.581
    Jun 13 03:23:51.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-9760 --namespace=crd-publish-openapi-9760 create -f -'
    Jun 13 03:23:52.861: INFO: stderr: ""
    Jun 13 03:23:52.861: INFO: stdout: "e2e-test-crd-publish-openapi-8424-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jun 13 03:23:52.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-9760 --namespace=crd-publish-openapi-9760 delete e2e-test-crd-publish-openapi-8424-crds test-cr'
    Jun 13 03:23:53.028: INFO: stderr: ""
    Jun 13 03:23:53.028: INFO: stdout: "e2e-test-crd-publish-openapi-8424-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jun 13 03:23:53.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-9760 --namespace=crd-publish-openapi-9760 apply -f -'
    Jun 13 03:23:53.441: INFO: stderr: ""
    Jun 13 03:23:53.441: INFO: stdout: "e2e-test-crd-publish-openapi-8424-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jun 13 03:23:53.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-9760 --namespace=crd-publish-openapi-9760 delete e2e-test-crd-publish-openapi-8424-crds test-cr'
    Jun 13 03:23:53.602: INFO: stderr: ""
    Jun 13 03:23:53.602: INFO: stdout: "e2e-test-crd-publish-openapi-8424-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 06/13/23 03:23:53.602
    Jun 13 03:23:53.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-9760 explain e2e-test-crd-publish-openapi-8424-crds'
    Jun 13 03:23:54.050: INFO: stderr: ""
    Jun 13 03:23:54.050: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8424-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:24:00.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9760" for this suite. 06/13/23 03:24:00.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:24:00.701
Jun 13 03:24:00.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename hostport 06/13/23 03:24:00.702
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:24:00.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:24:00.757
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 06/13/23 03:24:00.78
Jun 13 03:24:00.822: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6083" to be "running and ready"
Jun 13 03:24:00.830: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117683ms
Jun 13 03:24:00.831: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:24:02.858: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.035430772s
Jun 13 03:24:02.858: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun 13 03:24:02.858: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.255.64.104 on the node which pod1 resides and expect scheduled 06/13/23 03:24:02.858
Jun 13 03:24:02.872: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6083" to be "running and ready"
Jun 13 03:24:02.886: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.792779ms
Jun 13 03:24:02.886: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:24:04.896: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.023579523s
Jun 13 03:24:04.896: INFO: The phase of Pod pod2 is Running (Ready = false)
Jun 13 03:24:06.896: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.023451915s
Jun 13 03:24:06.896: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun 13 03:24:06.896: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.255.64.104 but use UDP protocol on the node which pod2 resides 06/13/23 03:24:06.896
Jun 13 03:24:06.911: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6083" to be "running and ready"
Jun 13 03:24:06.925: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.241534ms
Jun 13 03:24:06.925: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:24:08.947: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.036077681s
Jun 13 03:24:08.947: INFO: The phase of Pod pod3 is Running (Ready = true)
Jun 13 03:24:08.947: INFO: Pod "pod3" satisfied condition "running and ready"
Jun 13 03:24:08.980: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6083" to be "running and ready"
Jun 13 03:24:09.002: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 22.253226ms
Jun 13 03:24:09.003: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:24:11.013: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.032479092s
Jun 13 03:24:11.013: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jun 13 03:24:11.013: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 06/13/23 03:24:11.019
Jun 13 03:24:11.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.255.64.104 http://127.0.0.1:54323/hostname] Namespace:hostport-6083 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:24:11.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:24:11.020: INFO: ExecWithOptions: Clientset creation
Jun 13 03:24:11.021: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6083/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.255.64.104+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.255.64.104, port: 54323 06/13/23 03:24:11.137
Jun 13 03:24:11.137: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.255.64.104:54323/hostname] Namespace:hostport-6083 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:24:11.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:24:11.138: INFO: ExecWithOptions: Clientset creation
Jun 13 03:24:11.138: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6083/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.255.64.104%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.255.64.104, port: 54323 UDP 06/13/23 03:24:11.259
Jun 13 03:24:11.259: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.255.64.104 54323] Namespace:hostport-6083 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:24:11.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:24:11.259: INFO: ExecWithOptions: Clientset creation
Jun 13 03:24:11.259: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6083/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.255.64.104+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jun 13 03:24:16.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-6083" for this suite. 06/13/23 03:24:16.391
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":232,"skipped":4358,"failed":0}
------------------------------
• [SLOW TEST] [15.723 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:24:00.701
    Jun 13 03:24:00.701: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename hostport 06/13/23 03:24:00.702
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:24:00.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:24:00.757
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 06/13/23 03:24:00.78
    Jun 13 03:24:00.822: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6083" to be "running and ready"
    Jun 13 03:24:00.830: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.117683ms
    Jun 13 03:24:00.831: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:24:02.858: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.035430772s
    Jun 13 03:24:02.858: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun 13 03:24:02.858: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.255.64.104 on the node which pod1 resides and expect scheduled 06/13/23 03:24:02.858
    Jun 13 03:24:02.872: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6083" to be "running and ready"
    Jun 13 03:24:02.886: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.792779ms
    Jun 13 03:24:02.886: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:24:04.896: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.023579523s
    Jun 13 03:24:04.896: INFO: The phase of Pod pod2 is Running (Ready = false)
    Jun 13 03:24:06.896: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.023451915s
    Jun 13 03:24:06.896: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun 13 03:24:06.896: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.255.64.104 but use UDP protocol on the node which pod2 resides 06/13/23 03:24:06.896
    Jun 13 03:24:06.911: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6083" to be "running and ready"
    Jun 13 03:24:06.925: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 14.241534ms
    Jun 13 03:24:06.925: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:24:08.947: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.036077681s
    Jun 13 03:24:08.947: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jun 13 03:24:08.947: INFO: Pod "pod3" satisfied condition "running and ready"
    Jun 13 03:24:08.980: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6083" to be "running and ready"
    Jun 13 03:24:09.002: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 22.253226ms
    Jun 13 03:24:09.003: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:24:11.013: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.032479092s
    Jun 13 03:24:11.013: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jun 13 03:24:11.013: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 06/13/23 03:24:11.019
    Jun 13 03:24:11.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.255.64.104 http://127.0.0.1:54323/hostname] Namespace:hostport-6083 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:24:11.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:24:11.020: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:24:11.021: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6083/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.255.64.104+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.255.64.104, port: 54323 06/13/23 03:24:11.137
    Jun 13 03:24:11.137: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.255.64.104:54323/hostname] Namespace:hostport-6083 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:24:11.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:24:11.138: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:24:11.138: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6083/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.255.64.104%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.255.64.104, port: 54323 UDP 06/13/23 03:24:11.259
    Jun 13 03:24:11.259: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.255.64.104 54323] Namespace:hostport-6083 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:24:11.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:24:11.259: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:24:11.259: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6083/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.255.64.104+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jun 13 03:24:16.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-6083" for this suite. 06/13/23 03:24:16.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:24:16.426
Jun 13 03:24:16.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 03:24:16.428
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:24:16.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:24:16.464
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 06/13/23 03:24:16.472
Jun 13 03:24:16.496: INFO: Waiting up to 5m0s for pod "pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b" in namespace "emptydir-5985" to be "Succeeded or Failed"
Jun 13 03:24:16.504: INFO: Pod "pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.656817ms
Jun 13 03:24:18.520: INFO: Pod "pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024216467s
Jun 13 03:24:20.513: INFO: Pod "pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017549194s
STEP: Saw pod success 06/13/23 03:24:20.513
Jun 13 03:24:20.514: INFO: Pod "pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b" satisfied condition "Succeeded or Failed"
Jun 13 03:24:20.521: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b container test-container: <nil>
STEP: delete the pod 06/13/23 03:24:20.553
Jun 13 03:24:20.581: INFO: Waiting for pod pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b to disappear
Jun 13 03:24:20.589: INFO: Pod pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 03:24:20.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5985" for this suite. 06/13/23 03:24:20.601
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":233,"skipped":4364,"failed":0}
------------------------------
• [4.199 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:24:16.426
    Jun 13 03:24:16.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 03:24:16.428
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:24:16.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:24:16.464
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 06/13/23 03:24:16.472
    Jun 13 03:24:16.496: INFO: Waiting up to 5m0s for pod "pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b" in namespace "emptydir-5985" to be "Succeeded or Failed"
    Jun 13 03:24:16.504: INFO: Pod "pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.656817ms
    Jun 13 03:24:18.520: INFO: Pod "pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024216467s
    Jun 13 03:24:20.513: INFO: Pod "pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017549194s
    STEP: Saw pod success 06/13/23 03:24:20.513
    Jun 13 03:24:20.514: INFO: Pod "pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b" satisfied condition "Succeeded or Failed"
    Jun 13 03:24:20.521: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b container test-container: <nil>
    STEP: delete the pod 06/13/23 03:24:20.553
    Jun 13 03:24:20.581: INFO: Waiting for pod pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b to disappear
    Jun 13 03:24:20.589: INFO: Pod pod-ffb6c72e-6a43-45f4-bb21-2c8791f2347b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 03:24:20.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5985" for this suite. 06/13/23 03:24:20.601
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:24:20.625
Jun 13 03:24:20.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename cronjob 06/13/23 03:24:20.627
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:24:20.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:24:20.668
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 06/13/23 03:24:20.68
STEP: Ensuring more than one job is running at a time 06/13/23 03:24:20.696
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 06/13/23 03:26:00.714
STEP: Removing cronjob 06/13/23 03:26:00.722
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 13 03:26:00.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9226" for this suite. 06/13/23 03:26:00.756
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":234,"skipped":4364,"failed":0}
------------------------------
• [SLOW TEST] [100.156 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:24:20.625
    Jun 13 03:24:20.625: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename cronjob 06/13/23 03:24:20.627
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:24:20.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:24:20.668
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 06/13/23 03:24:20.68
    STEP: Ensuring more than one job is running at a time 06/13/23 03:24:20.696
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 06/13/23 03:26:00.714
    STEP: Removing cronjob 06/13/23 03:26:00.722
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 13 03:26:00.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9226" for this suite. 06/13/23 03:26:00.756
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:26:00.781
Jun 13 03:26:00.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 03:26:00.784
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:00.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:00.841
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 03:26:00.908
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:26:01.384
STEP: Deploying the webhook pod 06/13/23 03:26:01.41
STEP: Wait for the deployment to be ready 06/13/23 03:26:01.479
Jun 13 03:26:01.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 26, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 26, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/13/23 03:26:03.598
STEP: Verifying the service has paired with the endpoint 06/13/23 03:26:03.737
Jun 13 03:26:04.738: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 06/13/23 03:26:04.943
STEP: Creating a configMap that should be mutated 06/13/23 03:26:04.983
STEP: Deleting the collection of validation webhooks 06/13/23 03:26:05.085
STEP: Creating a configMap that should not be mutated 06/13/23 03:26:05.25
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:26:05.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2425" for this suite. 06/13/23 03:26:05.3
STEP: Destroying namespace "webhook-2425-markers" for this suite. 06/13/23 03:26:05.385
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":235,"skipped":4367,"failed":0}
------------------------------
• [4.804 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:26:00.781
    Jun 13 03:26:00.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 03:26:00.784
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:00.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:00.841
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 03:26:00.908
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:26:01.384
    STEP: Deploying the webhook pod 06/13/23 03:26:01.41
    STEP: Wait for the deployment to be ready 06/13/23 03:26:01.479
    Jun 13 03:26:01.570: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 26, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 26, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 26, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/13/23 03:26:03.598
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:26:03.737
    Jun 13 03:26:04.738: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 06/13/23 03:26:04.943
    STEP: Creating a configMap that should be mutated 06/13/23 03:26:04.983
    STEP: Deleting the collection of validation webhooks 06/13/23 03:26:05.085
    STEP: Creating a configMap that should not be mutated 06/13/23 03:26:05.25
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:26:05.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2425" for this suite. 06/13/23 03:26:05.3
    STEP: Destroying namespace "webhook-2425-markers" for this suite. 06/13/23 03:26:05.385
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:26:05.587
Jun 13 03:26:05.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename watch 06/13/23 03:26:05.588
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:05.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:05.666
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 06/13/23 03:26:05.679
STEP: creating a watch on configmaps with label B 06/13/23 03:26:05.691
STEP: creating a watch on configmaps with label A or B 06/13/23 03:26:05.695
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 06/13/23 03:26:05.699
Jun 13 03:26:05.712: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38272 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 03:26:05.712: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38272 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 06/13/23 03:26:05.712
Jun 13 03:26:05.751: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38274 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 03:26:05.751: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38274 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 06/13/23 03:26:05.751
Jun 13 03:26:05.782: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38275 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 03:26:05.782: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38275 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 06/13/23 03:26:05.782
Jun 13 03:26:05.804: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38276 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 03:26:05.804: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38276 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 06/13/23 03:26:05.804
Jun 13 03:26:05.816: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4278  f716f95a-dc73-4dca-bbb3-dce804f49b8c 38277 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 03:26:05.818: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4278  f716f95a-dc73-4dca-bbb3-dce804f49b8c 38277 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 06/13/23 03:26:15.819
Jun 13 03:26:15.841: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4278  f716f95a-dc73-4dca-bbb3-dce804f49b8c 38348 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 03:26:15.842: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4278  f716f95a-dc73-4dca-bbb3-dce804f49b8c 38348 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 13 03:26:25.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4278" for this suite. 06/13/23 03:26:25.858
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":236,"skipped":4388,"failed":0}
------------------------------
• [SLOW TEST] [20.291 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:26:05.587
    Jun 13 03:26:05.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename watch 06/13/23 03:26:05.588
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:05.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:05.666
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 06/13/23 03:26:05.679
    STEP: creating a watch on configmaps with label B 06/13/23 03:26:05.691
    STEP: creating a watch on configmaps with label A or B 06/13/23 03:26:05.695
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 06/13/23 03:26:05.699
    Jun 13 03:26:05.712: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38272 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 03:26:05.712: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38272 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 06/13/23 03:26:05.712
    Jun 13 03:26:05.751: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38274 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 03:26:05.751: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38274 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 06/13/23 03:26:05.751
    Jun 13 03:26:05.782: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38275 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 03:26:05.782: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38275 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 06/13/23 03:26:05.782
    Jun 13 03:26:05.804: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38276 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 03:26:05.804: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4278  71b688ad-8f92-4d05-a186-db690e683e99 38276 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 06/13/23 03:26:05.804
    Jun 13 03:26:05.816: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4278  f716f95a-dc73-4dca-bbb3-dce804f49b8c 38277 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 03:26:05.818: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4278  f716f95a-dc73-4dca-bbb3-dce804f49b8c 38277 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 06/13/23 03:26:15.819
    Jun 13 03:26:15.841: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4278  f716f95a-dc73-4dca-bbb3-dce804f49b8c 38348 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 03:26:15.842: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4278  f716f95a-dc73-4dca-bbb3-dce804f49b8c 38348 0 2023-06-13 03:26:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-13 03:26:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 13 03:26:25.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-4278" for this suite. 06/13/23 03:26:25.858
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:26:25.878
Jun 13 03:26:25.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 03:26:25.879
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:25.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:25.936
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 06/13/23 03:26:25.95
Jun 13 03:26:25.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 13 03:26:26.064: INFO: stderr: ""
Jun 13 03:26:26.064: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 06/13/23 03:26:26.064
Jun 13 03:26:26.064: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 13 03:26:26.064: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4794" to be "running and ready, or succeeded"
Jun 13 03:26:26.081: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.601776ms
Jun 13 03:26:26.081: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'sks-test-v1-25-9-workergroup-469fm' to be 'Running' but was 'Pending'
Jun 13 03:26:28.090: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.025550615s
Jun 13 03:26:28.090: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 13 03:26:28.090: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 06/13/23 03:26:28.09
Jun 13 03:26:28.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 logs logs-generator logs-generator'
Jun 13 03:26:28.202: INFO: stderr: ""
Jun 13 03:26:28.203: INFO: stdout: "I0613 03:26:27.186703       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/nw2 445\nI0613 03:26:27.387321       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/c8db 226\nI0613 03:26:27.587638       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/hzc 428\nI0613 03:26:27.787133       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/td9 423\nI0613 03:26:27.991155       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/7f96 278\nI0613 03:26:28.187545       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/x59s 416\n"
STEP: limiting log lines 06/13/23 03:26:28.203
Jun 13 03:26:28.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 logs logs-generator logs-generator --tail=1'
Jun 13 03:26:28.299: INFO: stderr: ""
Jun 13 03:26:28.299: INFO: stdout: "I0613 03:26:28.187545       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/x59s 416\n"
Jun 13 03:26:28.299: INFO: got output "I0613 03:26:28.187545       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/x59s 416\n"
STEP: limiting log bytes 06/13/23 03:26:28.299
Jun 13 03:26:28.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 logs logs-generator logs-generator --limit-bytes=1'
Jun 13 03:26:28.401: INFO: stderr: ""
Jun 13 03:26:28.401: INFO: stdout: "I"
Jun 13 03:26:28.401: INFO: got output "I"
STEP: exposing timestamps 06/13/23 03:26:28.401
Jun 13 03:26:28.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 logs logs-generator logs-generator --tail=1 --timestamps'
Jun 13 03:26:28.497: INFO: stderr: ""
Jun 13 03:26:28.497: INFO: stdout: "2023-06-13T11:26:28.387443338+08:00 I0613 03:26:28.387261       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/rc2c 475\n"
Jun 13 03:26:28.497: INFO: got output "2023-06-13T11:26:28.387443338+08:00 I0613 03:26:28.387261       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/rc2c 475\n"
STEP: restricting to a time range 06/13/23 03:26:28.497
Jun 13 03:26:30.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 logs logs-generator logs-generator --since=1s'
Jun 13 03:26:31.161: INFO: stderr: ""
Jun 13 03:26:31.161: INFO: stdout: "I0613 03:26:30.187456       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/kvgv 387\nI0613 03:26:30.387659       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/mps 482\nI0613 03:26:30.587843       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/2vbh 441\nI0613 03:26:30.787262       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/tqv 509\nI0613 03:26:30.991070       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/ndv 220\n"
Jun 13 03:26:31.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 logs logs-generator logs-generator --since=24h'
Jun 13 03:26:31.296: INFO: stderr: ""
Jun 13 03:26:31.296: INFO: stdout: "I0613 03:26:27.186703       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/nw2 445\nI0613 03:26:27.387321       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/c8db 226\nI0613 03:26:27.587638       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/hzc 428\nI0613 03:26:27.787133       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/td9 423\nI0613 03:26:27.991155       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/7f96 278\nI0613 03:26:28.187545       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/x59s 416\nI0613 03:26:28.387261       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/rc2c 475\nI0613 03:26:28.587894       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/mv7 338\nI0613 03:26:28.787251       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/plpq 419\nI0613 03:26:28.989152       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/m57 301\nI0613 03:26:29.187505       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/b8s 353\nI0613 03:26:29.386835       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/qq9 261\nI0613 03:26:29.587206       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/l9p 254\nI0613 03:26:29.786856       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/c48x 595\nI0613 03:26:29.987882       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/9lg4 319\nI0613 03:26:30.187456       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/kvgv 387\nI0613 03:26:30.387659       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/mps 482\nI0613 03:26:30.587843       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/2vbh 441\nI0613 03:26:30.787262       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/tqv 509\nI0613 03:26:30.991070       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/ndv 220\nI0613 03:26:31.187664       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/5tq 440\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jun 13 03:26:31.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 delete pod logs-generator'
Jun 13 03:26:32.634: INFO: stderr: ""
Jun 13 03:26:32.634: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 03:26:32.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4794" for this suite. 06/13/23 03:26:32.645
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":237,"skipped":4391,"failed":0}
------------------------------
• [SLOW TEST] [6.783 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:26:25.878
    Jun 13 03:26:25.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 03:26:25.879
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:25.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:25.936
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 06/13/23 03:26:25.95
    Jun 13 03:26:25.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jun 13 03:26:26.064: INFO: stderr: ""
    Jun 13 03:26:26.064: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 06/13/23 03:26:26.064
    Jun 13 03:26:26.064: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jun 13 03:26:26.064: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4794" to be "running and ready, or succeeded"
    Jun 13 03:26:26.081: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 16.601776ms
    Jun 13 03:26:26.081: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'sks-test-v1-25-9-workergroup-469fm' to be 'Running' but was 'Pending'
    Jun 13 03:26:28.090: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.025550615s
    Jun 13 03:26:28.090: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jun 13 03:26:28.090: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 06/13/23 03:26:28.09
    Jun 13 03:26:28.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 logs logs-generator logs-generator'
    Jun 13 03:26:28.202: INFO: stderr: ""
    Jun 13 03:26:28.203: INFO: stdout: "I0613 03:26:27.186703       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/nw2 445\nI0613 03:26:27.387321       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/c8db 226\nI0613 03:26:27.587638       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/hzc 428\nI0613 03:26:27.787133       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/td9 423\nI0613 03:26:27.991155       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/7f96 278\nI0613 03:26:28.187545       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/x59s 416\n"
    STEP: limiting log lines 06/13/23 03:26:28.203
    Jun 13 03:26:28.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 logs logs-generator logs-generator --tail=1'
    Jun 13 03:26:28.299: INFO: stderr: ""
    Jun 13 03:26:28.299: INFO: stdout: "I0613 03:26:28.187545       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/x59s 416\n"
    Jun 13 03:26:28.299: INFO: got output "I0613 03:26:28.187545       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/x59s 416\n"
    STEP: limiting log bytes 06/13/23 03:26:28.299
    Jun 13 03:26:28.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 logs logs-generator logs-generator --limit-bytes=1'
    Jun 13 03:26:28.401: INFO: stderr: ""
    Jun 13 03:26:28.401: INFO: stdout: "I"
    Jun 13 03:26:28.401: INFO: got output "I"
    STEP: exposing timestamps 06/13/23 03:26:28.401
    Jun 13 03:26:28.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 logs logs-generator logs-generator --tail=1 --timestamps'
    Jun 13 03:26:28.497: INFO: stderr: ""
    Jun 13 03:26:28.497: INFO: stdout: "2023-06-13T11:26:28.387443338+08:00 I0613 03:26:28.387261       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/rc2c 475\n"
    Jun 13 03:26:28.497: INFO: got output "2023-06-13T11:26:28.387443338+08:00 I0613 03:26:28.387261       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/rc2c 475\n"
    STEP: restricting to a time range 06/13/23 03:26:28.497
    Jun 13 03:26:30.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 logs logs-generator logs-generator --since=1s'
    Jun 13 03:26:31.161: INFO: stderr: ""
    Jun 13 03:26:31.161: INFO: stdout: "I0613 03:26:30.187456       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/kvgv 387\nI0613 03:26:30.387659       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/mps 482\nI0613 03:26:30.587843       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/2vbh 441\nI0613 03:26:30.787262       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/tqv 509\nI0613 03:26:30.991070       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/ndv 220\n"
    Jun 13 03:26:31.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 logs logs-generator logs-generator --since=24h'
    Jun 13 03:26:31.296: INFO: stderr: ""
    Jun 13 03:26:31.296: INFO: stdout: "I0613 03:26:27.186703       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/nw2 445\nI0613 03:26:27.387321       1 logs_generator.go:76] 1 GET /api/v1/namespaces/kube-system/pods/c8db 226\nI0613 03:26:27.587638       1 logs_generator.go:76] 2 GET /api/v1/namespaces/ns/pods/hzc 428\nI0613 03:26:27.787133       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/td9 423\nI0613 03:26:27.991155       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/7f96 278\nI0613 03:26:28.187545       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/x59s 416\nI0613 03:26:28.387261       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/rc2c 475\nI0613 03:26:28.587894       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/mv7 338\nI0613 03:26:28.787251       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/plpq 419\nI0613 03:26:28.989152       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/m57 301\nI0613 03:26:29.187505       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/ns/pods/b8s 353\nI0613 03:26:29.386835       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/qq9 261\nI0613 03:26:29.587206       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/l9p 254\nI0613 03:26:29.786856       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/c48x 595\nI0613 03:26:29.987882       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/9lg4 319\nI0613 03:26:30.187456       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/kvgv 387\nI0613 03:26:30.387659       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/mps 482\nI0613 03:26:30.587843       1 logs_generator.go:76] 17 POST /api/v1/namespaces/default/pods/2vbh 441\nI0613 03:26:30.787262       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/tqv 509\nI0613 03:26:30.991070       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/ndv 220\nI0613 03:26:31.187664       1 logs_generator.go:76] 20 POST /api/v1/namespaces/ns/pods/5tq 440\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jun 13 03:26:31.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-4794 delete pod logs-generator'
    Jun 13 03:26:32.634: INFO: stderr: ""
    Jun 13 03:26:32.634: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 03:26:32.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4794" for this suite. 06/13/23 03:26:32.645
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:26:32.662
Jun 13 03:26:32.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename tables 06/13/23 03:26:32.663
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:32.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:32.71
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jun 13 03:26:32.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-8209" for this suite. 06/13/23 03:26:32.782
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":238,"skipped":4395,"failed":0}
------------------------------
• [0.149 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:26:32.662
    Jun 13 03:26:32.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename tables 06/13/23 03:26:32.663
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:32.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:32.71
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jun 13 03:26:32.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-8209" for this suite. 06/13/23 03:26:32.782
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:26:32.812
Jun 13 03:26:32.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename containers 06/13/23 03:26:32.813
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:32.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:32.855
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 06/13/23 03:26:32.864
Jun 13 03:26:32.887: INFO: Waiting up to 5m0s for pod "client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5" in namespace "containers-7302" to be "Succeeded or Failed"
Jun 13 03:26:32.903: INFO: Pod "client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.113409ms
Jun 13 03:26:34.915: INFO: Pod "client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5": Phase="Running", Reason="", readiness=true. Elapsed: 2.028351382s
Jun 13 03:26:36.915: INFO: Pod "client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5": Phase="Running", Reason="", readiness=false. Elapsed: 4.027514384s
Jun 13 03:26:38.916: INFO: Pod "client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028843108s
STEP: Saw pod success 06/13/23 03:26:38.916
Jun 13 03:26:38.916: INFO: Pod "client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5" satisfied condition "Succeeded or Failed"
Jun 13 03:26:38.933: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5 container agnhost-container: <nil>
STEP: delete the pod 06/13/23 03:26:39.027
Jun 13 03:26:39.081: INFO: Waiting for pod client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5 to disappear
Jun 13 03:26:39.092: INFO: Pod client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jun 13 03:26:39.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7302" for this suite. 06/13/23 03:26:39.109
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":239,"skipped":4395,"failed":0}
------------------------------
• [SLOW TEST] [6.362 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:26:32.812
    Jun 13 03:26:32.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename containers 06/13/23 03:26:32.813
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:32.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:32.855
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 06/13/23 03:26:32.864
    Jun 13 03:26:32.887: INFO: Waiting up to 5m0s for pod "client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5" in namespace "containers-7302" to be "Succeeded or Failed"
    Jun 13 03:26:32.903: INFO: Pod "client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.113409ms
    Jun 13 03:26:34.915: INFO: Pod "client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5": Phase="Running", Reason="", readiness=true. Elapsed: 2.028351382s
    Jun 13 03:26:36.915: INFO: Pod "client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5": Phase="Running", Reason="", readiness=false. Elapsed: 4.027514384s
    Jun 13 03:26:38.916: INFO: Pod "client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028843108s
    STEP: Saw pod success 06/13/23 03:26:38.916
    Jun 13 03:26:38.916: INFO: Pod "client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5" satisfied condition "Succeeded or Failed"
    Jun 13 03:26:38.933: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5 container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 03:26:39.027
    Jun 13 03:26:39.081: INFO: Waiting for pod client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5 to disappear
    Jun 13 03:26:39.092: INFO: Pod client-containers-e0d6f95e-6844-42fc-ba84-ee29cbcecab5 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jun 13 03:26:39.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7302" for this suite. 06/13/23 03:26:39.109
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:26:39.176
Jun 13 03:26:39.176: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename deployment 06/13/23 03:26:39.177
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:39.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:39.234
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 06/13/23 03:26:39.262
STEP: waiting for Deployment to be created 06/13/23 03:26:39.276
STEP: waiting for all Replicas to be Ready 06/13/23 03:26:39.28
Jun 13 03:26:39.284: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 13 03:26:39.284: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 13 03:26:39.311: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 13 03:26:39.311: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 13 03:26:39.337: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 13 03:26:39.337: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 13 03:26:39.390: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 13 03:26:39.390: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 13 03:26:40.713: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 13 03:26:40.713: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 13 03:26:41.329: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 06/13/23 03:26:41.329
W0613 03:26:41.358244      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun 13 03:26:41.361: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 06/13/23 03:26:41.361
Jun 13 03:26:41.364: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
Jun 13 03:26:41.364: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
Jun 13 03:26:41.364: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
Jun 13 03:26:41.364: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:41.389: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:41.389: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:41.432: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:41.432: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:41.477: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:41.478: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:41.489: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
Jun 13 03:26:41.489: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
Jun 13 03:26:43.346: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:43.346: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:43.504: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
STEP: listing Deployments 06/13/23 03:26:43.504
Jun 13 03:26:43.526: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 06/13/23 03:26:43.526
Jun 13 03:26:43.567: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 06/13/23 03:26:43.567
Jun 13 03:26:43.635: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 13 03:26:43.635: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 13 03:26:43.686: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 13 03:26:43.780: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 13 03:26:43.780: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 13 03:26:43.802: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 13 03:26:45.758: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 13 03:26:45.785: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 13 03:26:45.826: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 13 03:26:45.876: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 13 03:26:47.292: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 06/13/23 03:26:47.338
STEP: fetching the DeploymentStatus 06/13/23 03:26:47.367
Jun 13 03:26:47.381: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
Jun 13 03:26:47.381: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
Jun 13 03:26:47.381: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 3
STEP: deleting the Deployment 06/13/23 03:26:47.382
Jun 13 03:26:47.416: INFO: observed event type MODIFIED
Jun 13 03:26:47.416: INFO: observed event type MODIFIED
Jun 13 03:26:47.416: INFO: observed event type MODIFIED
Jun 13 03:26:47.417: INFO: observed event type MODIFIED
Jun 13 03:26:47.417: INFO: observed event type MODIFIED
Jun 13 03:26:47.417: INFO: observed event type MODIFIED
Jun 13 03:26:47.417: INFO: observed event type MODIFIED
Jun 13 03:26:47.417: INFO: observed event type MODIFIED
Jun 13 03:26:47.417: INFO: observed event type MODIFIED
Jun 13 03:26:47.417: INFO: observed event type MODIFIED
Jun 13 03:26:47.417: INFO: observed event type MODIFIED
Jun 13 03:26:47.417: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 13 03:26:47.432: INFO: Log out all the ReplicaSets if there is no deployment created
Jun 13 03:26:47.445: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-3700  a92cb069-226d-4814-9857-7b1a2ee621ef 38735 4 2023-06-13 03:26:41 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d51347e0-4b77-4772-b5d6-3728940023f1 0xc0041601d7 0xc0041601d8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:26:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d51347e0-4b77-4772-b5d6-3728940023f1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:26:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004160260 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun 13 03:26:47.459: INFO: pod: "test-deployment-54cc775c4b-9ks64":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-9ks64 test-deployment-54cc775c4b- deployment-3700  dfdda97b-3e33-451c-bbf3-ef5af6658678 38731 0 2023-06-13 03:26:41 +0000 UTC 2023-06-13 03:26:48 +0000 UTC 0xc003fc3fc8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:f821dd916876b773967daba7d2b15f28a20a4cc6ed634826d7855e81370d9664 cni.projectcalico.org/podIP:172.16.172.4/32 cni.projectcalico.org/podIPs:172.16.172.4/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a92cb069-226d-4814-9857-7b1a2ee621ef 0xc002c7e017 0xc002c7e018}] [] [{kube-controller-manager Update v1 2023-06-13 03:26:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a92cb069-226d-4814-9857-7b1a2ee621ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:26:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:26:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcdjd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcdjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.4,StartTime:2023-06-13 03:26:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:26:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://cdb7c17e5451c61e5443ad7bdad5ab5f880a97ef657d2aa8020cec2a90b85ddb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 13 03:26:47.460: INFO: pod: "test-deployment-54cc775c4b-9kt6q":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-9kt6q test-deployment-54cc775c4b- deployment-3700  5f31fa82-3ff6-401d-a213-e5f3fb74c5b5 38725 0 2023-06-13 03:26:43 +0000 UTC 2023-06-13 03:26:46 +0000 UTC 0xc002c7e1f0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:da01ffa16654a81fae5e885a734178335005bd3dd57120618f6071fc6bf4937f cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a92cb069-226d-4814-9857-7b1a2ee621ef 0xc002c7e227 0xc002c7e228}] [] [{kube-controller-manager Update v1 2023-06-13 03:26:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a92cb069-226d-4814-9857-7b1a2ee621ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:26:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.156.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:26:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2ml9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2ml9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.28.156.222,StartTime:2023-06-13 03:26:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:26:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://5361c8d295c412431841a31257aeb898dffcf09acaa46fc710e21923e702b1cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.156.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 13 03:26:47.460: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-3700  b52b7a2d-d45a-4dd9-b953-d586ac14308a 38727 2 2023-06-13 03:26:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment d51347e0-4b77-4772-b5d6-3728940023f1 0xc0041602c7 0xc0041602c8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d51347e0-4b77-4772-b5d6-3728940023f1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:26:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004160350 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jun 13 03:26:47.478: INFO: pod: "test-deployment-7c7d8d58c8-292rz":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-292rz test-deployment-7c7d8d58c8- deployment-3700  006978fa-2b20-48c7-a5b0-2ddbc65f2305 38689 0 2023-06-13 03:26:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:913002535b934d2cb4f80f07339414c658709a5667c114ea8c436ffab62c0e43 cni.projectcalico.org/podIP:172.30.77.183/32 cni.projectcalico.org/podIPs:172.30.77.183/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b52b7a2d-d45a-4dd9-b953-d586ac14308a 0xc004160717 0xc004160718}] [] [{kube-controller-manager Update v1 2023-06-13 03:26:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b52b7a2d-d45a-4dd9-b953-d586ac14308a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:26:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2blt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2blt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.77.183,StartTime:2023-06-13 03:26:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:26:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b066d2d32593f36f22cae041a82829389e43cd9011b91e7a1a2de124323cae2c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.77.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 13 03:26:47.479: INFO: pod: "test-deployment-7c7d8d58c8-s58rn":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-s58rn test-deployment-7c7d8d58c8- deployment-3700  6c6b0a00-67a6-48bc-9b29-bdeea0cb7faf 38726 0 2023-06-13 03:26:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:2ad86aa52ce1c83fc851bf4e32d4a32f7fa521f17f22cbe42471b9b2ff3be3b7 cni.projectcalico.org/podIP:172.16.172.23/32 cni.projectcalico.org/podIPs:172.16.172.23/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b52b7a2d-d45a-4dd9-b953-d586ac14308a 0xc004160937 0xc004160938}] [] [{kube-controller-manager Update v1 2023-06-13 03:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b52b7a2d-d45a-4dd9-b953-d586ac14308a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:26:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nxt2t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nxt2t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.23,StartTime:2023-06-13 03:26:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:26:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://08455745a1070ff690e0b65602d4c0f6d780e5d78b92c0dd546cf4f404caa019,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 13 03:26:47.479: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-3700  86efaa52-4f59-4ad9-8f46-8c99585d9cea 38617 3 2023-06-13 03:26:39 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d51347e0-4b77-4772-b5d6-3728940023f1 0xc0041603b7 0xc0041603b8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:26:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d51347e0-4b77-4772-b5d6-3728940023f1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:26:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004160440 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 13 03:26:47.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3700" for this suite. 06/13/23 03:26:47.519
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":240,"skipped":4419,"failed":0}
------------------------------
• [SLOW TEST] [8.365 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:26:39.176
    Jun 13 03:26:39.176: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename deployment 06/13/23 03:26:39.177
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:39.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:39.234
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 06/13/23 03:26:39.262
    STEP: waiting for Deployment to be created 06/13/23 03:26:39.276
    STEP: waiting for all Replicas to be Ready 06/13/23 03:26:39.28
    Jun 13 03:26:39.284: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 13 03:26:39.284: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 13 03:26:39.311: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 13 03:26:39.311: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 13 03:26:39.337: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 13 03:26:39.337: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 13 03:26:39.390: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 13 03:26:39.390: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 13 03:26:40.713: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jun 13 03:26:40.713: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jun 13 03:26:41.329: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 06/13/23 03:26:41.329
    W0613 03:26:41.358244      18 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun 13 03:26:41.361: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 06/13/23 03:26:41.361
    Jun 13 03:26:41.364: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
    Jun 13 03:26:41.364: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
    Jun 13 03:26:41.364: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
    Jun 13 03:26:41.364: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
    Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
    Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
    Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
    Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 0
    Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
    Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
    Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:41.365: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:41.389: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:41.389: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:41.432: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:41.432: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:41.477: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:41.478: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:41.489: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
    Jun 13 03:26:41.489: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
    Jun 13 03:26:43.346: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:43.346: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:43.504: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
    STEP: listing Deployments 06/13/23 03:26:43.504
    Jun 13 03:26:43.526: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 06/13/23 03:26:43.526
    Jun 13 03:26:43.567: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 06/13/23 03:26:43.567
    Jun 13 03:26:43.635: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 13 03:26:43.635: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 13 03:26:43.686: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 13 03:26:43.780: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 13 03:26:43.780: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 13 03:26:43.802: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 13 03:26:45.758: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 13 03:26:45.785: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 13 03:26:45.826: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 13 03:26:45.876: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 13 03:26:47.292: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 06/13/23 03:26:47.338
    STEP: fetching the DeploymentStatus 06/13/23 03:26:47.367
    Jun 13 03:26:47.381: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
    Jun 13 03:26:47.381: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
    Jun 13 03:26:47.381: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
    Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
    Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
    Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 1
    Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 2
    Jun 13 03:26:47.382: INFO: observed Deployment test-deployment in namespace deployment-3700 with ReadyReplicas 3
    STEP: deleting the Deployment 06/13/23 03:26:47.382
    Jun 13 03:26:47.416: INFO: observed event type MODIFIED
    Jun 13 03:26:47.416: INFO: observed event type MODIFIED
    Jun 13 03:26:47.416: INFO: observed event type MODIFIED
    Jun 13 03:26:47.417: INFO: observed event type MODIFIED
    Jun 13 03:26:47.417: INFO: observed event type MODIFIED
    Jun 13 03:26:47.417: INFO: observed event type MODIFIED
    Jun 13 03:26:47.417: INFO: observed event type MODIFIED
    Jun 13 03:26:47.417: INFO: observed event type MODIFIED
    Jun 13 03:26:47.417: INFO: observed event type MODIFIED
    Jun 13 03:26:47.417: INFO: observed event type MODIFIED
    Jun 13 03:26:47.417: INFO: observed event type MODIFIED
    Jun 13 03:26:47.417: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 13 03:26:47.432: INFO: Log out all the ReplicaSets if there is no deployment created
    Jun 13 03:26:47.445: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-3700  a92cb069-226d-4814-9857-7b1a2ee621ef 38735 4 2023-06-13 03:26:41 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d51347e0-4b77-4772-b5d6-3728940023f1 0xc0041601d7 0xc0041601d8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:26:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d51347e0-4b77-4772-b5d6-3728940023f1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:26:47 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004160260 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jun 13 03:26:47.459: INFO: pod: "test-deployment-54cc775c4b-9ks64":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-9ks64 test-deployment-54cc775c4b- deployment-3700  dfdda97b-3e33-451c-bbf3-ef5af6658678 38731 0 2023-06-13 03:26:41 +0000 UTC 2023-06-13 03:26:48 +0000 UTC 0xc003fc3fc8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:f821dd916876b773967daba7d2b15f28a20a4cc6ed634826d7855e81370d9664 cni.projectcalico.org/podIP:172.16.172.4/32 cni.projectcalico.org/podIPs:172.16.172.4/32] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a92cb069-226d-4814-9857-7b1a2ee621ef 0xc002c7e017 0xc002c7e018}] [] [{kube-controller-manager Update v1 2023-06-13 03:26:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a92cb069-226d-4814-9857-7b1a2ee621ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:26:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:26:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.4\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rcdjd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rcdjd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.4,StartTime:2023-06-13 03:26:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:26:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://cdb7c17e5451c61e5443ad7bdad5ab5f880a97ef657d2aa8020cec2a90b85ddb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.4,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun 13 03:26:47.460: INFO: pod: "test-deployment-54cc775c4b-9kt6q":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-9kt6q test-deployment-54cc775c4b- deployment-3700  5f31fa82-3ff6-401d-a213-e5f3fb74c5b5 38725 0 2023-06-13 03:26:43 +0000 UTC 2023-06-13 03:26:46 +0000 UTC 0xc002c7e1f0 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:da01ffa16654a81fae5e885a734178335005bd3dd57120618f6071fc6bf4937f cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs:] [{apps/v1 ReplicaSet test-deployment-54cc775c4b a92cb069-226d-4814-9857-7b1a2ee621ef 0xc002c7e227 0xc002c7e228}] [] [{kube-controller-manager Update v1 2023-06-13 03:26:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a92cb069-226d-4814-9857-7b1a2ee621ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:26:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.156.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:26:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h2ml9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h2ml9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.28.156.222,StartTime:2023-06-13 03:26:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:26:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://5361c8d295c412431841a31257aeb898dffcf09acaa46fc710e21923e702b1cc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.156.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun 13 03:26:47.460: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-3700  b52b7a2d-d45a-4dd9-b953-d586ac14308a 38727 2 2023-06-13 03:26:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment d51347e0-4b77-4772-b5d6-3728940023f1 0xc0041602c7 0xc0041602c8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d51347e0-4b77-4772-b5d6-3728940023f1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:26:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004160350 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jun 13 03:26:47.478: INFO: pod: "test-deployment-7c7d8d58c8-292rz":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-292rz test-deployment-7c7d8d58c8- deployment-3700  006978fa-2b20-48c7-a5b0-2ddbc65f2305 38689 0 2023-06-13 03:26:43 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:913002535b934d2cb4f80f07339414c658709a5667c114ea8c436ffab62c0e43 cni.projectcalico.org/podIP:172.30.77.183/32 cni.projectcalico.org/podIPs:172.30.77.183/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b52b7a2d-d45a-4dd9-b953-d586ac14308a 0xc004160717 0xc004160718}] [] [{kube-controller-manager Update v1 2023-06-13 03:26:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b52b7a2d-d45a-4dd9-b953-d586ac14308a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:26:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:26:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2blt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2blt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.77.183,StartTime:2023-06-13 03:26:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:26:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://b066d2d32593f36f22cae041a82829389e43cd9011b91e7a1a2de124323cae2c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.77.183,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun 13 03:26:47.479: INFO: pod: "test-deployment-7c7d8d58c8-s58rn":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-s58rn test-deployment-7c7d8d58c8- deployment-3700  6c6b0a00-67a6-48bc-9b29-bdeea0cb7faf 38726 0 2023-06-13 03:26:45 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:2ad86aa52ce1c83fc851bf4e32d4a32f7fa521f17f22cbe42471b9b2ff3be3b7 cni.projectcalico.org/podIP:172.16.172.23/32 cni.projectcalico.org/podIPs:172.16.172.23/32] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 b52b7a2d-d45a-4dd9-b953-d586ac14308a 0xc004160937 0xc004160938}] [] [{kube-controller-manager Update v1 2023-06-13 03:26:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b52b7a2d-d45a-4dd9-b953-d586ac14308a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:26:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:26:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nxt2t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nxt2t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:26:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.23,StartTime:2023-06-13 03:26:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:26:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://08455745a1070ff690e0b65602d4c0f6d780e5d78b92c0dd546cf4f404caa019,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.23,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun 13 03:26:47.479: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-3700  86efaa52-4f59-4ad9-8f46-8c99585d9cea 38617 3 2023-06-13 03:26:39 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d51347e0-4b77-4772-b5d6-3728940023f1 0xc0041603b7 0xc0041603b8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:26:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d51347e0-4b77-4772-b5d6-3728940023f1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:26:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004160440 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 13 03:26:47.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3700" for this suite. 06/13/23 03:26:47.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:26:47.542
Jun 13 03:26:47.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 03:26:47.543
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:47.599
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:47.607
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 06/13/23 03:26:47.617
STEP: fetching the ConfigMap 06/13/23 03:26:47.651
STEP: patching the ConfigMap 06/13/23 03:26:47.666
STEP: listing all ConfigMaps in all namespaces with a label selector 06/13/23 03:26:47.689
STEP: deleting the ConfigMap by collection with a label selector 06/13/23 03:26:47.704
STEP: listing all ConfigMaps in test namespace 06/13/23 03:26:47.762
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 03:26:47.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7998" for this suite. 06/13/23 03:26:47.816
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":241,"skipped":4455,"failed":0}
------------------------------
• [0.320 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:26:47.542
    Jun 13 03:26:47.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 03:26:47.543
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:47.599
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:47.607
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 06/13/23 03:26:47.617
    STEP: fetching the ConfigMap 06/13/23 03:26:47.651
    STEP: patching the ConfigMap 06/13/23 03:26:47.666
    STEP: listing all ConfigMaps in all namespaces with a label selector 06/13/23 03:26:47.689
    STEP: deleting the ConfigMap by collection with a label selector 06/13/23 03:26:47.704
    STEP: listing all ConfigMaps in test namespace 06/13/23 03:26:47.762
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 03:26:47.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7998" for this suite. 06/13/23 03:26:47.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:26:47.863
Jun 13 03:26:47.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename daemonsets 06/13/23 03:26:47.865
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:48.022
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jun 13 03:26:48.121: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 06/13/23 03:26:48.139
Jun 13 03:26:48.159: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:26:48.159: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 06/13/23 03:26:48.159
Jun 13 03:26:48.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:26:48.256: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
Jun 13 03:26:49.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:26:49.267: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
Jun 13 03:26:50.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 13 03:26:50.281: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 06/13/23 03:26:50.307
Jun 13 03:26:50.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 13 03:26:50.363: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jun 13 03:26:51.371: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:26:51.371: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 06/13/23 03:26:51.371
Jun 13 03:26:51.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:26:51.415: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
Jun 13 03:26:52.430: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:26:52.430: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
Jun 13 03:26:53.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:26:53.426: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
Jun 13 03:26:54.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:26:54.438: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
Jun 13 03:26:55.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 13 03:26:55.445: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/13/23 03:26:55.475
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2561, will wait for the garbage collector to delete the pods 06/13/23 03:26:55.475
Jun 13 03:26:55.621: INFO: Deleting DaemonSet.extensions daemon-set took: 74.763954ms
Jun 13 03:26:55.725: INFO: Terminating DaemonSet.extensions daemon-set pods took: 104.789376ms
Jun 13 03:26:58.035: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:26:58.035: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 13 03:26:58.050: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38937"},"items":null}

Jun 13 03:26:58.060: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38937"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:26:58.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2561" for this suite. 06/13/23 03:26:58.184
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":242,"skipped":4468,"failed":0}
------------------------------
• [SLOW TEST] [10.363 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:26:47.863
    Jun 13 03:26:47.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename daemonsets 06/13/23 03:26:47.865
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:48.022
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jun 13 03:26:48.121: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 06/13/23 03:26:48.139
    Jun 13 03:26:48.159: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:26:48.159: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 06/13/23 03:26:48.159
    Jun 13 03:26:48.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:26:48.256: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
    Jun 13 03:26:49.267: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:26:49.267: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
    Jun 13 03:26:50.280: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 13 03:26:50.281: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 06/13/23 03:26:50.307
    Jun 13 03:26:50.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 13 03:26:50.363: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jun 13 03:26:51.371: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:26:51.371: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 06/13/23 03:26:51.371
    Jun 13 03:26:51.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:26:51.415: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
    Jun 13 03:26:52.430: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:26:52.430: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
    Jun 13 03:26:53.426: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:26:53.426: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
    Jun 13 03:26:54.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:26:54.438: INFO: Node sks-test-v1-25-9-workergroup-l5gcd is running 0 daemon pod, expected 1
    Jun 13 03:26:55.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 13 03:26:55.445: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/13/23 03:26:55.475
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2561, will wait for the garbage collector to delete the pods 06/13/23 03:26:55.475
    Jun 13 03:26:55.621: INFO: Deleting DaemonSet.extensions daemon-set took: 74.763954ms
    Jun 13 03:26:55.725: INFO: Terminating DaemonSet.extensions daemon-set pods took: 104.789376ms
    Jun 13 03:26:58.035: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:26:58.035: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 13 03:26:58.050: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38937"},"items":null}

    Jun 13 03:26:58.060: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38937"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:26:58.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2561" for this suite. 06/13/23 03:26:58.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:26:58.227
Jun 13 03:26:58.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-runtime 06/13/23 03:26:58.228
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:58.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:58.285
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 06/13/23 03:26:58.3
STEP: wait for the container to reach Succeeded 06/13/23 03:26:58.325
STEP: get the container status 06/13/23 03:27:02.377
STEP: the container should be terminated 06/13/23 03:27:02.386
STEP: the termination message should be set 06/13/23 03:27:02.386
Jun 13 03:27:02.386: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 06/13/23 03:27:02.386
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 13 03:27:02.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1767" for this suite. 06/13/23 03:27:02.445
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":243,"skipped":4477,"failed":0}
------------------------------
• [4.234 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:26:58.227
    Jun 13 03:26:58.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-runtime 06/13/23 03:26:58.228
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:26:58.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:26:58.285
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 06/13/23 03:26:58.3
    STEP: wait for the container to reach Succeeded 06/13/23 03:26:58.325
    STEP: get the container status 06/13/23 03:27:02.377
    STEP: the container should be terminated 06/13/23 03:27:02.386
    STEP: the termination message should be set 06/13/23 03:27:02.386
    Jun 13 03:27:02.386: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 06/13/23 03:27:02.386
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 13 03:27:02.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1767" for this suite. 06/13/23 03:27:02.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:27:02.462
Jun 13 03:27:02.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:27:02.463
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:27:02.505
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:27:02.514
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216
STEP: creating service in namespace services-7772 06/13/23 03:27:02.524
STEP: creating service affinity-nodeport-transition in namespace services-7772 06/13/23 03:27:02.524
STEP: creating replication controller affinity-nodeport-transition in namespace services-7772 06/13/23 03:27:02.577
I0613 03:27:02.594939      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7772, replica count: 3
I0613 03:27:05.645724      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 13 03:27:05.692: INFO: Creating new exec pod
Jun 13 03:27:05.718: INFO: Waiting up to 5m0s for pod "execpod-affinityj8f7s" in namespace "services-7772" to be "running"
Jun 13 03:27:05.728: INFO: Pod "execpod-affinityj8f7s": Phase="Pending", Reason="", readiness=false. Elapsed: 10.346872ms
Jun 13 03:27:07.768: INFO: Pod "execpod-affinityj8f7s": Phase="Running", Reason="", readiness=true. Elapsed: 2.049774403s
Jun 13 03:27:07.768: INFO: Pod "execpod-affinityj8f7s" satisfied condition "running"
Jun 13 03:27:08.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-7772 exec execpod-affinityj8f7s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jun 13 03:27:09.016: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun 13 03:27:09.016: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 03:27:09.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-7772 exec execpod-affinityj8f7s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.242.103 80'
Jun 13 03:27:09.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.242.103 80\nConnection to 10.101.242.103 80 port [tcp/http] succeeded!\n"
Jun 13 03:27:09.217: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 03:27:09.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-7772 exec execpod-affinityj8f7s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30425'
Jun 13 03:27:09.446: INFO: stderr: "+ + nc -v -t -w 2 10.255.64.102 30425\necho hostName\nConnection to 10.255.64.102 30425 port [tcp/*] succeeded!\n"
Jun 13 03:27:09.446: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 03:27:09.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-7772 exec execpod-affinityj8f7s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.103 30425'
Jun 13 03:27:09.689: INFO: stderr: "+ + nc -v -t -w 2 10.255.64.103 30425\necho hostName\nConnection to 10.255.64.103 30425 port [tcp/*] succeeded!\n"
Jun 13 03:27:09.689: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 03:27:09.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-7772 exec execpod-affinityj8f7s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.64.103:30425/ ; done'
Jun 13 03:27:10.109: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n"
Jun 13 03:27:10.109: INFO: stdout: "\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-47p6j\naffinity-nodeport-transition-ggr65\naffinity-nodeport-transition-ggr65\naffinity-nodeport-transition-ggr65\naffinity-nodeport-transition-47p6j\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-47p6j\naffinity-nodeport-transition-47p6j\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-47p6j\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-ggr65"
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-47p6j
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-ggr65
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-ggr65
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-ggr65
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-47p6j
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-47p6j
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-47p6j
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-47p6j
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-ggr65
Jun 13 03:27:10.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-7772 exec execpod-affinityj8f7s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.64.103:30425/ ; done'
Jun 13 03:27:10.585: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n"
Jun 13 03:27:10.586: INFO: stdout: "\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw"
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
Jun 13 03:27:10.586: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7772, will wait for the garbage collector to delete the pods 06/13/23 03:27:10.628
Jun 13 03:27:10.703: INFO: Deleting ReplicationController affinity-nodeport-transition took: 15.773468ms
Jun 13 03:27:10.803: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.866069ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:27:13.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7772" for this suite. 06/13/23 03:27:13.592
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":244,"skipped":4488,"failed":0}
------------------------------
• [SLOW TEST] [11.150 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:27:02.462
    Jun 13 03:27:02.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:27:02.463
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:27:02.505
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:27:02.514
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2216
    STEP: creating service in namespace services-7772 06/13/23 03:27:02.524
    STEP: creating service affinity-nodeport-transition in namespace services-7772 06/13/23 03:27:02.524
    STEP: creating replication controller affinity-nodeport-transition in namespace services-7772 06/13/23 03:27:02.577
    I0613 03:27:02.594939      18 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-7772, replica count: 3
    I0613 03:27:05.645724      18 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 13 03:27:05.692: INFO: Creating new exec pod
    Jun 13 03:27:05.718: INFO: Waiting up to 5m0s for pod "execpod-affinityj8f7s" in namespace "services-7772" to be "running"
    Jun 13 03:27:05.728: INFO: Pod "execpod-affinityj8f7s": Phase="Pending", Reason="", readiness=false. Elapsed: 10.346872ms
    Jun 13 03:27:07.768: INFO: Pod "execpod-affinityj8f7s": Phase="Running", Reason="", readiness=true. Elapsed: 2.049774403s
    Jun 13 03:27:07.768: INFO: Pod "execpod-affinityj8f7s" satisfied condition "running"
    Jun 13 03:27:08.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-7772 exec execpod-affinityj8f7s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jun 13 03:27:09.016: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jun 13 03:27:09.016: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 03:27:09.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-7772 exec execpod-affinityj8f7s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.242.103 80'
    Jun 13 03:27:09.217: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.242.103 80\nConnection to 10.101.242.103 80 port [tcp/http] succeeded!\n"
    Jun 13 03:27:09.217: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 03:27:09.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-7772 exec execpod-affinityj8f7s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30425'
    Jun 13 03:27:09.446: INFO: stderr: "+ + nc -v -t -w 2 10.255.64.102 30425\necho hostName\nConnection to 10.255.64.102 30425 port [tcp/*] succeeded!\n"
    Jun 13 03:27:09.446: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 03:27:09.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-7772 exec execpod-affinityj8f7s -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.103 30425'
    Jun 13 03:27:09.689: INFO: stderr: "+ + nc -v -t -w 2 10.255.64.103 30425\necho hostName\nConnection to 10.255.64.103 30425 port [tcp/*] succeeded!\n"
    Jun 13 03:27:09.689: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 03:27:09.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-7772 exec execpod-affinityj8f7s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.64.103:30425/ ; done'
    Jun 13 03:27:10.109: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n"
    Jun 13 03:27:10.109: INFO: stdout: "\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-47p6j\naffinity-nodeport-transition-ggr65\naffinity-nodeport-transition-ggr65\naffinity-nodeport-transition-ggr65\naffinity-nodeport-transition-47p6j\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-47p6j\naffinity-nodeport-transition-47p6j\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-47p6j\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-ggr65"
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-47p6j
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-ggr65
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-ggr65
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-ggr65
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-47p6j
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-47p6j
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-47p6j
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-47p6j
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.109: INFO: Received response from host: affinity-nodeport-transition-ggr65
    Jun 13 03:27:10.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-7772 exec execpod-affinityj8f7s -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.255.64.103:30425/ ; done'
    Jun 13 03:27:10.585: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.255.64.103:30425/\n"
    Jun 13 03:27:10.586: INFO: stdout: "\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw\naffinity-nodeport-transition-g4xfw"
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Received response from host: affinity-nodeport-transition-g4xfw
    Jun 13 03:27:10.586: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7772, will wait for the garbage collector to delete the pods 06/13/23 03:27:10.628
    Jun 13 03:27:10.703: INFO: Deleting ReplicationController affinity-nodeport-transition took: 15.773468ms
    Jun 13 03:27:10.803: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.866069ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:27:13.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7772" for this suite. 06/13/23 03:27:13.592
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:27:13.613
Jun 13 03:27:13.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 03:27:13.614
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:27:13.662
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:27:13.683
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-d32d8a1e-11a1-4142-adcc-5ce207b1a530 06/13/23 03:27:13.694
STEP: Creating a pod to test consume configMaps 06/13/23 03:27:13.712
Jun 13 03:27:13.747: INFO: Waiting up to 5m0s for pod "pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3" in namespace "configmap-1568" to be "Succeeded or Failed"
Jun 13 03:27:13.775: INFO: Pod "pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3": Phase="Pending", Reason="", readiness=false. Elapsed: 28.101296ms
Jun 13 03:27:15.818: INFO: Pod "pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070858886s
Jun 13 03:27:17.812: INFO: Pod "pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064536034s
Jun 13 03:27:19.786: INFO: Pod "pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038611198s
STEP: Saw pod success 06/13/23 03:27:19.786
Jun 13 03:27:19.786: INFO: Pod "pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3" satisfied condition "Succeeded or Failed"
Jun 13 03:27:19.799: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3 container agnhost-container: <nil>
STEP: delete the pod 06/13/23 03:27:19.822
Jun 13 03:27:19.874: INFO: Waiting for pod pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3 to disappear
Jun 13 03:27:19.893: INFO: Pod pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 03:27:19.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1568" for this suite. 06/13/23 03:27:19.908
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":245,"skipped":4496,"failed":0}
------------------------------
• [SLOW TEST] [6.354 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:27:13.613
    Jun 13 03:27:13.613: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 03:27:13.614
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:27:13.662
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:27:13.683
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-d32d8a1e-11a1-4142-adcc-5ce207b1a530 06/13/23 03:27:13.694
    STEP: Creating a pod to test consume configMaps 06/13/23 03:27:13.712
    Jun 13 03:27:13.747: INFO: Waiting up to 5m0s for pod "pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3" in namespace "configmap-1568" to be "Succeeded or Failed"
    Jun 13 03:27:13.775: INFO: Pod "pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3": Phase="Pending", Reason="", readiness=false. Elapsed: 28.101296ms
    Jun 13 03:27:15.818: INFO: Pod "pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.070858886s
    Jun 13 03:27:17.812: INFO: Pod "pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064536034s
    Jun 13 03:27:19.786: INFO: Pod "pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038611198s
    STEP: Saw pod success 06/13/23 03:27:19.786
    Jun 13 03:27:19.786: INFO: Pod "pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3" satisfied condition "Succeeded or Failed"
    Jun 13 03:27:19.799: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3 container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 03:27:19.822
    Jun 13 03:27:19.874: INFO: Waiting for pod pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3 to disappear
    Jun 13 03:27:19.893: INFO: Pod pod-configmaps-71c26f0f-b7ec-4a40-99dd-319203c28df3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 03:27:19.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1568" for this suite. 06/13/23 03:27:19.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:27:19.971
Jun 13 03:27:19.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:27:19.972
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:27:20.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:27:20.044
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-bf1a70c4-4dc1-4dc7-aa52-f58afb5a291b 06/13/23 03:27:20.055
STEP: Creating a pod to test consume secrets 06/13/23 03:27:20.071
Jun 13 03:27:20.102: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751" in namespace "projected-7417" to be "Succeeded or Failed"
Jun 13 03:27:20.127: INFO: Pod "pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751": Phase="Pending", Reason="", readiness=false. Elapsed: 24.260182ms
Jun 13 03:27:22.363: INFO: Pod "pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260678157s
Jun 13 03:27:24.146: INFO: Pod "pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044026844s
Jun 13 03:27:26.136: INFO: Pod "pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03404534s
STEP: Saw pod success 06/13/23 03:27:26.136
Jun 13 03:27:26.137: INFO: Pod "pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751" satisfied condition "Succeeded or Failed"
Jun 13 03:27:26.146: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/13/23 03:27:26.17
Jun 13 03:27:26.210: INFO: Waiting for pod pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751 to disappear
Jun 13 03:27:26.219: INFO: Pod pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 13 03:27:26.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7417" for this suite. 06/13/23 03:27:26.233
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":246,"skipped":4502,"failed":0}
------------------------------
• [SLOW TEST] [6.292 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:27:19.971
    Jun 13 03:27:19.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:27:19.972
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:27:20.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:27:20.044
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-bf1a70c4-4dc1-4dc7-aa52-f58afb5a291b 06/13/23 03:27:20.055
    STEP: Creating a pod to test consume secrets 06/13/23 03:27:20.071
    Jun 13 03:27:20.102: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751" in namespace "projected-7417" to be "Succeeded or Failed"
    Jun 13 03:27:20.127: INFO: Pod "pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751": Phase="Pending", Reason="", readiness=false. Elapsed: 24.260182ms
    Jun 13 03:27:22.363: INFO: Pod "pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260678157s
    Jun 13 03:27:24.146: INFO: Pod "pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044026844s
    Jun 13 03:27:26.136: INFO: Pod "pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03404534s
    STEP: Saw pod success 06/13/23 03:27:26.136
    Jun 13 03:27:26.137: INFO: Pod "pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751" satisfied condition "Succeeded or Failed"
    Jun 13 03:27:26.146: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:27:26.17
    Jun 13 03:27:26.210: INFO: Waiting for pod pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751 to disappear
    Jun 13 03:27:26.219: INFO: Pod pod-projected-secrets-791238cc-d799-4f68-be65-6e0fc901a751 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 13 03:27:26.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7417" for this suite. 06/13/23 03:27:26.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:27:26.264
Jun 13 03:27:26.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename statefulset 06/13/23 03:27:26.265
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:27:26.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:27:26.322
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7272 06/13/23 03:27:26.331
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 06/13/23 03:27:26.342
Jun 13 03:27:26.380: INFO: Found 0 stateful pods, waiting for 3
Jun 13 03:27:36.393: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 03:27:36.393: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 03:27:36.393: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 06/13/23 03:27:36.419
Jun 13 03:27:36.458: INFO: Updating stateful set ss2
STEP: Creating a new revision 06/13/23 03:27:36.458
STEP: Not applying an update when the partition is greater than the number of replicas 06/13/23 03:27:46.505
STEP: Performing a canary update 06/13/23 03:27:46.506
Jun 13 03:27:46.538: INFO: Updating stateful set ss2
Jun 13 03:27:46.583: INFO: Waiting for Pod statefulset-7272/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 06/13/23 03:27:56.612
Jun 13 03:27:56.758: INFO: Found 2 stateful pods, waiting for 3
Jun 13 03:28:06.776: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 03:28:06.776: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 13 03:28:06.776: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 06/13/23 03:28:06.799
Jun 13 03:28:06.838: INFO: Updating stateful set ss2
Jun 13 03:28:06.880: INFO: Waiting for Pod statefulset-7272/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jun 13 03:28:16.957: INFO: Updating stateful set ss2
Jun 13 03:28:17.046: INFO: Waiting for StatefulSet statefulset-7272/ss2 to complete update
Jun 13 03:28:17.046: INFO: Waiting for Pod statefulset-7272/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 13 03:28:27.064: INFO: Deleting all statefulset in ns statefulset-7272
Jun 13 03:28:27.073: INFO: Scaling statefulset ss2 to 0
Jun 13 03:28:37.125: INFO: Waiting for statefulset status.replicas updated to 0
Jun 13 03:28:37.133: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 13 03:28:37.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7272" for this suite. 06/13/23 03:28:37.182
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":247,"skipped":4526,"failed":0}
------------------------------
• [SLOW TEST] [70.940 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:27:26.264
    Jun 13 03:27:26.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename statefulset 06/13/23 03:27:26.265
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:27:26.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:27:26.322
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7272 06/13/23 03:27:26.331
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 06/13/23 03:27:26.342
    Jun 13 03:27:26.380: INFO: Found 0 stateful pods, waiting for 3
    Jun 13 03:27:36.393: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 03:27:36.393: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 03:27:36.393: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 06/13/23 03:27:36.419
    Jun 13 03:27:36.458: INFO: Updating stateful set ss2
    STEP: Creating a new revision 06/13/23 03:27:36.458
    STEP: Not applying an update when the partition is greater than the number of replicas 06/13/23 03:27:46.505
    STEP: Performing a canary update 06/13/23 03:27:46.506
    Jun 13 03:27:46.538: INFO: Updating stateful set ss2
    Jun 13 03:27:46.583: INFO: Waiting for Pod statefulset-7272/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 06/13/23 03:27:56.612
    Jun 13 03:27:56.758: INFO: Found 2 stateful pods, waiting for 3
    Jun 13 03:28:06.776: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 03:28:06.776: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 13 03:28:06.776: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 06/13/23 03:28:06.799
    Jun 13 03:28:06.838: INFO: Updating stateful set ss2
    Jun 13 03:28:06.880: INFO: Waiting for Pod statefulset-7272/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jun 13 03:28:16.957: INFO: Updating stateful set ss2
    Jun 13 03:28:17.046: INFO: Waiting for StatefulSet statefulset-7272/ss2 to complete update
    Jun 13 03:28:17.046: INFO: Waiting for Pod statefulset-7272/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 13 03:28:27.064: INFO: Deleting all statefulset in ns statefulset-7272
    Jun 13 03:28:27.073: INFO: Scaling statefulset ss2 to 0
    Jun 13 03:28:37.125: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 13 03:28:37.133: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 13 03:28:37.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7272" for this suite. 06/13/23 03:28:37.182
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:28:37.206
Jun 13 03:28:37.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename security-context-test 06/13/23 03:28:37.208
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:28:37.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:28:37.259
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jun 13 03:28:37.286: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b367b20e-d47b-48e2-aefc-6385ad691c2b" in namespace "security-context-test-7761" to be "Succeeded or Failed"
Jun 13 03:28:37.301: INFO: Pod "busybox-privileged-false-b367b20e-d47b-48e2-aefc-6385ad691c2b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.537812ms
Jun 13 03:28:39.308: INFO: Pod "busybox-privileged-false-b367b20e-d47b-48e2-aefc-6385ad691c2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022289918s
Jun 13 03:28:41.310: INFO: Pod "busybox-privileged-false-b367b20e-d47b-48e2-aefc-6385ad691c2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024205028s
Jun 13 03:28:41.310: INFO: Pod "busybox-privileged-false-b367b20e-d47b-48e2-aefc-6385ad691c2b" satisfied condition "Succeeded or Failed"
Jun 13 03:28:41.326: INFO: Got logs for pod "busybox-privileged-false-b367b20e-d47b-48e2-aefc-6385ad691c2b": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 13 03:28:41.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7761" for this suite. 06/13/23 03:28:41.338
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":248,"skipped":4572,"failed":0}
------------------------------
• [4.146 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:28:37.206
    Jun 13 03:28:37.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename security-context-test 06/13/23 03:28:37.208
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:28:37.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:28:37.259
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jun 13 03:28:37.286: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-b367b20e-d47b-48e2-aefc-6385ad691c2b" in namespace "security-context-test-7761" to be "Succeeded or Failed"
    Jun 13 03:28:37.301: INFO: Pod "busybox-privileged-false-b367b20e-d47b-48e2-aefc-6385ad691c2b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.537812ms
    Jun 13 03:28:39.308: INFO: Pod "busybox-privileged-false-b367b20e-d47b-48e2-aefc-6385ad691c2b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022289918s
    Jun 13 03:28:41.310: INFO: Pod "busybox-privileged-false-b367b20e-d47b-48e2-aefc-6385ad691c2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024205028s
    Jun 13 03:28:41.310: INFO: Pod "busybox-privileged-false-b367b20e-d47b-48e2-aefc-6385ad691c2b" satisfied condition "Succeeded or Failed"
    Jun 13 03:28:41.326: INFO: Got logs for pod "busybox-privileged-false-b367b20e-d47b-48e2-aefc-6385ad691c2b": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 13 03:28:41.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7761" for this suite. 06/13/23 03:28:41.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:28:41.355
Jun 13 03:28:41.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:28:41.356
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:28:41.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:28:41.407
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4925 06/13/23 03:28:41.415
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/13/23 03:28:41.49
STEP: creating service externalsvc in namespace services-4925 06/13/23 03:28:41.49
STEP: creating replication controller externalsvc in namespace services-4925 06/13/23 03:28:41.646
I0613 03:28:41.722602      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4925, replica count: 2
I0613 03:28:44.773882      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 06/13/23 03:28:44.782
Jun 13 03:28:44.846: INFO: Creating new exec pod
Jun 13 03:28:44.869: INFO: Waiting up to 5m0s for pod "execpodck5bs" in namespace "services-4925" to be "running"
Jun 13 03:28:44.882: INFO: Pod "execpodck5bs": Phase="Pending", Reason="", readiness=false. Elapsed: 13.245011ms
Jun 13 03:28:46.889: INFO: Pod "execpodck5bs": Phase="Running", Reason="", readiness=true. Elapsed: 2.020047072s
Jun 13 03:28:46.889: INFO: Pod "execpodck5bs" satisfied condition "running"
Jun 13 03:28:46.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4925 exec execpodck5bs -- /bin/sh -x -c nslookup nodeport-service.services-4925.svc.cluster.local'
Jun 13 03:28:47.117: INFO: stderr: "+ nslookup nodeport-service.services-4925.svc.cluster.local\n"
Jun 13 03:28:47.117: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4925.svc.cluster.local\tcanonical name = externalsvc.services-4925.svc.cluster.local.\nName:\texternalsvc.services-4925.svc.cluster.local\nAddress: 10.97.149.145\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4925, will wait for the garbage collector to delete the pods 06/13/23 03:28:47.117
Jun 13 03:28:47.257: INFO: Deleting ReplicationController externalsvc took: 82.333706ms
Jun 13 03:28:47.357: INFO: Terminating ReplicationController externalsvc pods took: 100.155561ms
Jun 13 03:28:49.908: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:28:49.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4925" for this suite. 06/13/23 03:28:49.973
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":249,"skipped":4587,"failed":0}
------------------------------
• [SLOW TEST] [8.635 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:28:41.355
    Jun 13 03:28:41.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:28:41.356
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:28:41.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:28:41.407
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-4925 06/13/23 03:28:41.415
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/13/23 03:28:41.49
    STEP: creating service externalsvc in namespace services-4925 06/13/23 03:28:41.49
    STEP: creating replication controller externalsvc in namespace services-4925 06/13/23 03:28:41.646
    I0613 03:28:41.722602      18 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4925, replica count: 2
    I0613 03:28:44.773882      18 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 06/13/23 03:28:44.782
    Jun 13 03:28:44.846: INFO: Creating new exec pod
    Jun 13 03:28:44.869: INFO: Waiting up to 5m0s for pod "execpodck5bs" in namespace "services-4925" to be "running"
    Jun 13 03:28:44.882: INFO: Pod "execpodck5bs": Phase="Pending", Reason="", readiness=false. Elapsed: 13.245011ms
    Jun 13 03:28:46.889: INFO: Pod "execpodck5bs": Phase="Running", Reason="", readiness=true. Elapsed: 2.020047072s
    Jun 13 03:28:46.889: INFO: Pod "execpodck5bs" satisfied condition "running"
    Jun 13 03:28:46.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4925 exec execpodck5bs -- /bin/sh -x -c nslookup nodeport-service.services-4925.svc.cluster.local'
    Jun 13 03:28:47.117: INFO: stderr: "+ nslookup nodeport-service.services-4925.svc.cluster.local\n"
    Jun 13 03:28:47.117: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4925.svc.cluster.local\tcanonical name = externalsvc.services-4925.svc.cluster.local.\nName:\texternalsvc.services-4925.svc.cluster.local\nAddress: 10.97.149.145\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-4925, will wait for the garbage collector to delete the pods 06/13/23 03:28:47.117
    Jun 13 03:28:47.257: INFO: Deleting ReplicationController externalsvc took: 82.333706ms
    Jun 13 03:28:47.357: INFO: Terminating ReplicationController externalsvc pods took: 100.155561ms
    Jun 13 03:28:49.908: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:28:49.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4925" for this suite. 06/13/23 03:28:49.973
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:28:49.99
Jun 13 03:28:49.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:28:49.992
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:28:50.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:28:50.028
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-436f4843-4b23-4fd6-a8ca-532b1497dcb8 06/13/23 03:28:50.045
STEP: Creating secret with name s-test-opt-upd-1ea90417-1f14-410b-81f3-677fc3c3eda4 06/13/23 03:28:50.057
STEP: Creating the pod 06/13/23 03:28:50.068
Jun 13 03:28:50.086: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0c5f9402-4393-4768-a399-d9726cae355a" in namespace "projected-6674" to be "running and ready"
Jun 13 03:28:50.097: INFO: Pod "pod-projected-secrets-0c5f9402-4393-4768-a399-d9726cae355a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.440844ms
Jun 13 03:28:50.097: INFO: The phase of Pod pod-projected-secrets-0c5f9402-4393-4768-a399-d9726cae355a is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:28:52.108: INFO: Pod "pod-projected-secrets-0c5f9402-4393-4768-a399-d9726cae355a": Phase="Running", Reason="", readiness=true. Elapsed: 2.02139775s
Jun 13 03:28:52.108: INFO: The phase of Pod pod-projected-secrets-0c5f9402-4393-4768-a399-d9726cae355a is Running (Ready = true)
Jun 13 03:28:52.108: INFO: Pod "pod-projected-secrets-0c5f9402-4393-4768-a399-d9726cae355a" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-436f4843-4b23-4fd6-a8ca-532b1497dcb8 06/13/23 03:28:52.177
STEP: Updating secret s-test-opt-upd-1ea90417-1f14-410b-81f3-677fc3c3eda4 06/13/23 03:28:52.197
STEP: Creating secret with name s-test-opt-create-f764b2c7-ebfa-4739-8ded-a324d9c2a270 06/13/23 03:28:52.21
STEP: waiting to observe update in volume 06/13/23 03:28:52.226
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 13 03:28:54.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6674" for this suite. 06/13/23 03:28:54.302
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":250,"skipped":4587,"failed":0}
------------------------------
• [4.338 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:28:49.99
    Jun 13 03:28:49.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:28:49.992
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:28:50.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:28:50.028
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-436f4843-4b23-4fd6-a8ca-532b1497dcb8 06/13/23 03:28:50.045
    STEP: Creating secret with name s-test-opt-upd-1ea90417-1f14-410b-81f3-677fc3c3eda4 06/13/23 03:28:50.057
    STEP: Creating the pod 06/13/23 03:28:50.068
    Jun 13 03:28:50.086: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0c5f9402-4393-4768-a399-d9726cae355a" in namespace "projected-6674" to be "running and ready"
    Jun 13 03:28:50.097: INFO: Pod "pod-projected-secrets-0c5f9402-4393-4768-a399-d9726cae355a": Phase="Pending", Reason="", readiness=false. Elapsed: 10.440844ms
    Jun 13 03:28:50.097: INFO: The phase of Pod pod-projected-secrets-0c5f9402-4393-4768-a399-d9726cae355a is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:28:52.108: INFO: Pod "pod-projected-secrets-0c5f9402-4393-4768-a399-d9726cae355a": Phase="Running", Reason="", readiness=true. Elapsed: 2.02139775s
    Jun 13 03:28:52.108: INFO: The phase of Pod pod-projected-secrets-0c5f9402-4393-4768-a399-d9726cae355a is Running (Ready = true)
    Jun 13 03:28:52.108: INFO: Pod "pod-projected-secrets-0c5f9402-4393-4768-a399-d9726cae355a" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-436f4843-4b23-4fd6-a8ca-532b1497dcb8 06/13/23 03:28:52.177
    STEP: Updating secret s-test-opt-upd-1ea90417-1f14-410b-81f3-677fc3c3eda4 06/13/23 03:28:52.197
    STEP: Creating secret with name s-test-opt-create-f764b2c7-ebfa-4739-8ded-a324d9c2a270 06/13/23 03:28:52.21
    STEP: waiting to observe update in volume 06/13/23 03:28:52.226
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 13 03:28:54.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6674" for this suite. 06/13/23 03:28:54.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:28:54.331
Jun 13 03:28:54.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 03:28:54.333
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:28:54.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:28:54.384
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-9fe8c231-a4a7-4f31-9069-5b648ac8fb6b 06/13/23 03:28:54.39
STEP: Creating a pod to test consume secrets 06/13/23 03:28:54.405
Jun 13 03:28:54.421: INFO: Waiting up to 5m0s for pod "pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91" in namespace "secrets-2602" to be "Succeeded or Failed"
Jun 13 03:28:54.428: INFO: Pod "pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026491ms
Jun 13 03:28:56.438: INFO: Pod "pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017217403s
Jun 13 03:28:58.439: INFO: Pod "pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01790746s
STEP: Saw pod success 06/13/23 03:28:58.439
Jun 13 03:28:58.439: INFO: Pod "pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91" satisfied condition "Succeeded or Failed"
Jun 13 03:28:58.451: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91 container secret-volume-test: <nil>
STEP: delete the pod 06/13/23 03:28:58.474
Jun 13 03:28:58.513: INFO: Waiting for pod pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91 to disappear
Jun 13 03:28:58.526: INFO: Pod pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 13 03:28:58.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2602" for this suite. 06/13/23 03:28:58.555
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":251,"skipped":4630,"failed":0}
------------------------------
• [4.250 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:28:54.331
    Jun 13 03:28:54.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 03:28:54.333
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:28:54.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:28:54.384
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-9fe8c231-a4a7-4f31-9069-5b648ac8fb6b 06/13/23 03:28:54.39
    STEP: Creating a pod to test consume secrets 06/13/23 03:28:54.405
    Jun 13 03:28:54.421: INFO: Waiting up to 5m0s for pod "pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91" in namespace "secrets-2602" to be "Succeeded or Failed"
    Jun 13 03:28:54.428: INFO: Pod "pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91": Phase="Pending", Reason="", readiness=false. Elapsed: 7.026491ms
    Jun 13 03:28:56.438: INFO: Pod "pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017217403s
    Jun 13 03:28:58.439: INFO: Pod "pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01790746s
    STEP: Saw pod success 06/13/23 03:28:58.439
    Jun 13 03:28:58.439: INFO: Pod "pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91" satisfied condition "Succeeded or Failed"
    Jun 13 03:28:58.451: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91 container secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:28:58.474
    Jun 13 03:28:58.513: INFO: Waiting for pod pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91 to disappear
    Jun 13 03:28:58.526: INFO: Pod pod-secrets-9793bb6b-14c1-4ca3-ab16-bdf768e4bd91 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 03:28:58.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2602" for this suite. 06/13/23 03:28:58.555
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:28:58.581
Jun 13 03:28:58.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename var-expansion 06/13/23 03:28:58.583
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:28:58.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:28:58.646
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 06/13/23 03:28:58.653
Jun 13 03:28:58.687: INFO: Waiting up to 2m0s for pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4" in namespace "var-expansion-5692" to be "running"
Jun 13 03:28:58.709: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.998453ms
Jun 13 03:29:00.721: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033971161s
Jun 13 03:29:02.717: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03009237s
Jun 13 03:29:04.736: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048956614s
Jun 13 03:29:06.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031598563s
Jun 13 03:29:08.721: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033819767s
Jun 13 03:29:10.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033549137s
Jun 13 03:29:12.723: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.035799491s
Jun 13 03:29:14.716: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.029299934s
Jun 13 03:29:16.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.032276367s
Jun 13 03:29:18.732: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.045055361s
Jun 13 03:29:20.728: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.041316179s
Jun 13 03:29:22.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.031470043s
Jun 13 03:29:24.717: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.03040479s
Jun 13 03:29:26.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.032390691s
Jun 13 03:29:28.800: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 30.113295433s
Jun 13 03:29:30.721: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 32.034366706s
Jun 13 03:29:32.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 34.03118685s
Jun 13 03:29:34.728: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 36.040944227s
Jun 13 03:29:36.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 38.033399609s
Jun 13 03:29:38.716: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 40.029696624s
Jun 13 03:29:40.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 42.031991998s
Jun 13 03:29:42.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 44.033646466s
Jun 13 03:29:44.717: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 46.030655938s
Jun 13 03:29:46.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 48.031381985s
Jun 13 03:29:48.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 50.032272192s
Jun 13 03:29:50.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 52.032528864s
Jun 13 03:29:52.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 54.031101516s
Jun 13 03:29:54.723: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 56.036476377s
Jun 13 03:29:56.717: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 58.030562592s
Jun 13 03:29:58.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.033463747s
Jun 13 03:30:00.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.033701395s
Jun 13 03:30:02.725: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.037724748s
Jun 13 03:30:04.772: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.084950695s
Jun 13 03:30:06.730: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.043013063s
Jun 13 03:30:08.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.032118841s
Jun 13 03:30:10.722: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.035260888s
Jun 13 03:30:12.721: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.033732704s
Jun 13 03:30:14.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.032841646s
Jun 13 03:30:16.730: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.043384715s
Jun 13 03:30:18.722: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.03546877s
Jun 13 03:30:20.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.032146579s
Jun 13 03:30:22.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.031265845s
Jun 13 03:30:24.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.031423909s
Jun 13 03:30:26.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.033336954s
Jun 13 03:30:28.724: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.037543997s
Jun 13 03:30:30.717: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.030607615s
Jun 13 03:30:32.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.031867458s
Jun 13 03:30:34.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.031477216s
Jun 13 03:30:36.728: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.041254661s
Jun 13 03:30:38.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.031203928s
Jun 13 03:30:40.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.032655072s
Jun 13 03:30:42.716: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.029650068s
Jun 13 03:30:44.715: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.028211194s
Jun 13 03:30:46.725: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.038559529s
Jun 13 03:30:48.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.031391336s
Jun 13 03:30:50.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.032008577s
Jun 13 03:30:52.716: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.029602419s
Jun 13 03:30:54.716: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.029373774s
Jun 13 03:30:56.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.032112787s
Jun 13 03:30:58.722: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.035159721s
Jun 13 03:30:58.730: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.043447839s
STEP: updating the pod 06/13/23 03:30:58.73
Jun 13 03:30:59.284: INFO: Successfully updated pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4"
STEP: waiting for pod running 06/13/23 03:30:59.284
Jun 13 03:30:59.284: INFO: Waiting up to 2m0s for pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4" in namespace "var-expansion-5692" to be "running"
Jun 13 03:30:59.292: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.553463ms
Jun 13 03:31:01.303: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018979795s
Jun 13 03:31:01.303: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4" satisfied condition "running"
STEP: deleting the pod gracefully 06/13/23 03:31:01.303
Jun 13 03:31:01.303: INFO: Deleting pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4" in namespace "var-expansion-5692"
Jun 13 03:31:01.339: INFO: Wait up to 5m0s for pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 13 03:31:33.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5692" for this suite. 06/13/23 03:31:33.377
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":252,"skipped":4630,"failed":0}
------------------------------
• [SLOW TEST] [154.823 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:28:58.581
    Jun 13 03:28:58.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename var-expansion 06/13/23 03:28:58.583
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:28:58.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:28:58.646
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 06/13/23 03:28:58.653
    Jun 13 03:28:58.687: INFO: Waiting up to 2m0s for pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4" in namespace "var-expansion-5692" to be "running"
    Jun 13 03:28:58.709: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.998453ms
    Jun 13 03:29:00.721: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033971161s
    Jun 13 03:29:02.717: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03009237s
    Jun 13 03:29:04.736: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.048956614s
    Jun 13 03:29:06.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.031598563s
    Jun 13 03:29:08.721: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033819767s
    Jun 13 03:29:10.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033549137s
    Jun 13 03:29:12.723: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 14.035799491s
    Jun 13 03:29:14.716: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 16.029299934s
    Jun 13 03:29:16.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 18.032276367s
    Jun 13 03:29:18.732: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 20.045055361s
    Jun 13 03:29:20.728: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 22.041316179s
    Jun 13 03:29:22.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 24.031470043s
    Jun 13 03:29:24.717: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 26.03040479s
    Jun 13 03:29:26.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 28.032390691s
    Jun 13 03:29:28.800: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 30.113295433s
    Jun 13 03:29:30.721: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 32.034366706s
    Jun 13 03:29:32.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 34.03118685s
    Jun 13 03:29:34.728: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 36.040944227s
    Jun 13 03:29:36.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 38.033399609s
    Jun 13 03:29:38.716: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 40.029696624s
    Jun 13 03:29:40.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 42.031991998s
    Jun 13 03:29:42.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 44.033646466s
    Jun 13 03:29:44.717: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 46.030655938s
    Jun 13 03:29:46.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 48.031381985s
    Jun 13 03:29:48.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 50.032272192s
    Jun 13 03:29:50.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 52.032528864s
    Jun 13 03:29:52.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 54.031101516s
    Jun 13 03:29:54.723: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 56.036476377s
    Jun 13 03:29:56.717: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 58.030562592s
    Jun 13 03:29:58.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.033463747s
    Jun 13 03:30:00.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.033701395s
    Jun 13 03:30:02.725: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.037724748s
    Jun 13 03:30:04.772: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.084950695s
    Jun 13 03:30:06.730: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.043013063s
    Jun 13 03:30:08.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.032118841s
    Jun 13 03:30:10.722: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.035260888s
    Jun 13 03:30:12.721: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.033732704s
    Jun 13 03:30:14.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.032841646s
    Jun 13 03:30:16.730: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.043384715s
    Jun 13 03:30:18.722: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.03546877s
    Jun 13 03:30:20.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.032146579s
    Jun 13 03:30:22.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.031265845s
    Jun 13 03:30:24.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.031423909s
    Jun 13 03:30:26.720: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.033336954s
    Jun 13 03:30:28.724: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.037543997s
    Jun 13 03:30:30.717: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.030607615s
    Jun 13 03:30:32.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.031867458s
    Jun 13 03:30:34.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.031477216s
    Jun 13 03:30:36.728: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.041254661s
    Jun 13 03:30:38.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.031203928s
    Jun 13 03:30:40.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.032655072s
    Jun 13 03:30:42.716: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.029650068s
    Jun 13 03:30:44.715: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.028211194s
    Jun 13 03:30:46.725: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.038559529s
    Jun 13 03:30:48.718: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.031391336s
    Jun 13 03:30:50.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.032008577s
    Jun 13 03:30:52.716: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.029602419s
    Jun 13 03:30:54.716: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.029373774s
    Jun 13 03:30:56.719: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.032112787s
    Jun 13 03:30:58.722: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.035159721s
    Jun 13 03:30:58.730: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.043447839s
    STEP: updating the pod 06/13/23 03:30:58.73
    Jun 13 03:30:59.284: INFO: Successfully updated pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4"
    STEP: waiting for pod running 06/13/23 03:30:59.284
    Jun 13 03:30:59.284: INFO: Waiting up to 2m0s for pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4" in namespace "var-expansion-5692" to be "running"
    Jun 13 03:30:59.292: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.553463ms
    Jun 13 03:31:01.303: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.018979795s
    Jun 13 03:31:01.303: INFO: Pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4" satisfied condition "running"
    STEP: deleting the pod gracefully 06/13/23 03:31:01.303
    Jun 13 03:31:01.303: INFO: Deleting pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4" in namespace "var-expansion-5692"
    Jun 13 03:31:01.339: INFO: Wait up to 5m0s for pod "var-expansion-1d211706-a060-4e0f-b877-f8057dc7d9f4" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 13 03:31:33.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5692" for this suite. 06/13/23 03:31:33.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:31:33.407
Jun 13 03:31:33.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename subpath 06/13/23 03:31:33.408
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:31:33.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:31:33.451
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/13/23 03:31:33.457
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-wkvt 06/13/23 03:31:33.487
STEP: Creating a pod to test atomic-volume-subpath 06/13/23 03:31:33.487
Jun 13 03:31:33.524: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wkvt" in namespace "subpath-1525" to be "Succeeded or Failed"
Jun 13 03:31:33.534: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Pending", Reason="", readiness=false. Elapsed: 9.866277ms
Jun 13 03:31:35.542: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 2.018110602s
Jun 13 03:31:37.550: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 4.025795446s
Jun 13 03:31:39.542: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 6.018382649s
Jun 13 03:31:41.544: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 8.02053551s
Jun 13 03:31:43.544: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 10.020615369s
Jun 13 03:31:45.544: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 12.019748188s
Jun 13 03:31:47.549: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 14.024806646s
Jun 13 03:31:49.544: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 16.020291973s
Jun 13 03:31:51.552: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 18.028429575s
Jun 13 03:31:53.543: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 20.019147029s
Jun 13 03:31:55.548: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=false. Elapsed: 22.024589567s
Jun 13 03:31:57.545: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.021146432s
STEP: Saw pod success 06/13/23 03:31:57.545
Jun 13 03:31:57.545: INFO: Pod "pod-subpath-test-configmap-wkvt" satisfied condition "Succeeded or Failed"
Jun 13 03:31:57.556: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-subpath-test-configmap-wkvt container test-container-subpath-configmap-wkvt: <nil>
STEP: delete the pod 06/13/23 03:31:57.607
Jun 13 03:31:57.663: INFO: Waiting for pod pod-subpath-test-configmap-wkvt to disappear
Jun 13 03:31:57.674: INFO: Pod pod-subpath-test-configmap-wkvt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wkvt 06/13/23 03:31:57.674
Jun 13 03:31:57.674: INFO: Deleting pod "pod-subpath-test-configmap-wkvt" in namespace "subpath-1525"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 13 03:31:57.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1525" for this suite. 06/13/23 03:31:57.703
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":253,"skipped":4689,"failed":0}
------------------------------
• [SLOW TEST] [24.316 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:31:33.407
    Jun 13 03:31:33.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename subpath 06/13/23 03:31:33.408
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:31:33.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:31:33.451
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/13/23 03:31:33.457
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-wkvt 06/13/23 03:31:33.487
    STEP: Creating a pod to test atomic-volume-subpath 06/13/23 03:31:33.487
    Jun 13 03:31:33.524: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wkvt" in namespace "subpath-1525" to be "Succeeded or Failed"
    Jun 13 03:31:33.534: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Pending", Reason="", readiness=false. Elapsed: 9.866277ms
    Jun 13 03:31:35.542: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 2.018110602s
    Jun 13 03:31:37.550: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 4.025795446s
    Jun 13 03:31:39.542: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 6.018382649s
    Jun 13 03:31:41.544: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 8.02053551s
    Jun 13 03:31:43.544: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 10.020615369s
    Jun 13 03:31:45.544: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 12.019748188s
    Jun 13 03:31:47.549: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 14.024806646s
    Jun 13 03:31:49.544: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 16.020291973s
    Jun 13 03:31:51.552: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 18.028429575s
    Jun 13 03:31:53.543: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=true. Elapsed: 20.019147029s
    Jun 13 03:31:55.548: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Running", Reason="", readiness=false. Elapsed: 22.024589567s
    Jun 13 03:31:57.545: INFO: Pod "pod-subpath-test-configmap-wkvt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.021146432s
    STEP: Saw pod success 06/13/23 03:31:57.545
    Jun 13 03:31:57.545: INFO: Pod "pod-subpath-test-configmap-wkvt" satisfied condition "Succeeded or Failed"
    Jun 13 03:31:57.556: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-subpath-test-configmap-wkvt container test-container-subpath-configmap-wkvt: <nil>
    STEP: delete the pod 06/13/23 03:31:57.607
    Jun 13 03:31:57.663: INFO: Waiting for pod pod-subpath-test-configmap-wkvt to disappear
    Jun 13 03:31:57.674: INFO: Pod pod-subpath-test-configmap-wkvt no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-wkvt 06/13/23 03:31:57.674
    Jun 13 03:31:57.674: INFO: Deleting pod "pod-subpath-test-configmap-wkvt" in namespace "subpath-1525"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 13 03:31:57.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1525" for this suite. 06/13/23 03:31:57.703
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:31:57.723
Jun 13 03:31:57.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename daemonsets 06/13/23 03:31:57.724
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:31:57.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:31:57.776
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 06/13/23 03:31:57.856
STEP: Check that daemon pods launch on every node of the cluster. 06/13/23 03:31:57.916
Jun 13 03:31:57.940: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:31:57.940: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:31:57.940: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:31:57.966: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:31:57.966: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 03:31:59.012: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:31:59.013: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:31:59.013: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:31:59.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 13 03:31:59.023: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
Jun 13 03:31:59.992: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:31:59.992: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:31:59.992: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 13 03:32:00.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 13 03:32:00.003: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 06/13/23 03:32:00.012
STEP: DeleteCollection of the DaemonSets 06/13/23 03:32:00.025
STEP: Verify that ReplicaSets have been deleted 06/13/23 03:32:00.078
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jun 13 03:32:00.116: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41094"},"items":null}

Jun 13 03:32:00.172: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41094"},"items":[{"metadata":{"name":"daemon-set-88sdl","generateName":"daemon-set-","namespace":"daemonsets-2836","uid":"e3d871c1-9026-4bd2-9cae-718fb94ba905","resourceVersion":"41087","creationTimestamp":"2023-06-13T03:31:57Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3e7a907f3a51463b1a6b00fb3e03a167c9860c3fe2bd2924c49eed4a7348fb62","cni.projectcalico.org/podIP":"172.16.172.37/32","cni.projectcalico.org/podIPs":"172.16.172.37/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0a56db70-3259-4f5a-8321-bb2d6ab0720b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a56db70-3259-4f5a-8321-bb2d6ab0720b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pck5c","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pck5c","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"sks-test-v1-25-9-workergroup-2q6k2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["sks-test-v1-25-9-workergroup-2q6k2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:57Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:59Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:59Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:57Z"}],"hostIP":"10.255.64.103","podIP":"172.16.172.37","podIPs":[{"ip":"172.16.172.37"}],"startTime":"2023-06-13T03:31:57Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-13T03:31:58Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://b85442e7b220b3ee60a7da165d13dfd6b5ebc2c5561d39b96d000f54adf03591","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-92f56","generateName":"daemon-set-","namespace":"daemonsets-2836","uid":"2c898629-60e0-47a4-9c2c-d8f86c875bb5","resourceVersion":"41092","creationTimestamp":"2023-06-13T03:31:57Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"0c0e66b3649881a30c39b950e094a57a181e71c1f47653872c72c704137f4131","cni.projectcalico.org/podIP":"172.28.156.196/32","cni.projectcalico.org/podIPs":"172.28.156.196/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0a56db70-3259-4f5a-8321-bb2d6ab0720b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a56db70-3259-4f5a-8321-bb2d6ab0720b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.156.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jwpsf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jwpsf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"sks-test-v1-25-9-workergroup-l5gcd","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["sks-test-v1-25-9-workergroup-l5gcd"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:57Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:59Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:59Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:57Z"}],"hostIP":"10.255.64.104","podIP":"172.28.156.196","podIPs":[{"ip":"172.28.156.196"}],"startTime":"2023-06-13T03:31:57Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-13T03:31:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://3e813e9cf3c3f08cd27902cbd2a101ce508af8014817f29bc7e4a9f780d04ed7","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-slgl9","generateName":"daemon-set-","namespace":"daemonsets-2836","uid":"07e4f066-3eb0-46d5-bcf6-3b93fc9a3493","resourceVersion":"41089","creationTimestamp":"2023-06-13T03:31:57Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a5225fbfe244b6e425a319e267cd17e0d0147ef9b6cb3e1a35fed89fc95a7e70","cni.projectcalico.org/podIP":"172.30.77.146/32","cni.projectcalico.org/podIPs":"172.30.77.146/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0a56db70-3259-4f5a-8321-bb2d6ab0720b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a56db70-3259-4f5a-8321-bb2d6ab0720b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kl2fz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kl2fz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"sks-test-v1-25-9-workergroup-469fm","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["sks-test-v1-25-9-workergroup-469fm"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:57Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:59Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:59Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:57Z"}],"hostIP":"10.255.64.102","podIP":"172.30.77.146","podIPs":[{"ip":"172.30.77.146"}],"startTime":"2023-06-13T03:31:57Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-13T03:31:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://b44b8e3d99562f85fc34b3bcf076a6f59d1de2975201f4d6b356573a6e28bd0f","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:32:00.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2836" for this suite. 06/13/23 03:32:00.24
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":254,"skipped":4689,"failed":0}
------------------------------
• [2.566 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:31:57.723
    Jun 13 03:31:57.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename daemonsets 06/13/23 03:31:57.724
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:31:57.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:31:57.776
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 06/13/23 03:31:57.856
    STEP: Check that daemon pods launch on every node of the cluster. 06/13/23 03:31:57.916
    Jun 13 03:31:57.940: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:31:57.940: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:31:57.940: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:31:57.966: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:31:57.966: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 03:31:59.012: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:31:59.013: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:31:59.013: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:31:59.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 13 03:31:59.023: INFO: Node sks-test-v1-25-9-workergroup-2q6k2 is running 0 daemon pod, expected 1
    Jun 13 03:31:59.992: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-48kt5 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:31:59.992: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-dqc47 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:31:59.992: INFO: DaemonSet pods can't tolerate node sks-test-v1-25-9-controlplane-rmxsq with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jun 13 03:32:00.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 13 03:32:00.003: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 06/13/23 03:32:00.012
    STEP: DeleteCollection of the DaemonSets 06/13/23 03:32:00.025
    STEP: Verify that ReplicaSets have been deleted 06/13/23 03:32:00.078
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jun 13 03:32:00.116: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41094"},"items":null}

    Jun 13 03:32:00.172: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41094"},"items":[{"metadata":{"name":"daemon-set-88sdl","generateName":"daemon-set-","namespace":"daemonsets-2836","uid":"e3d871c1-9026-4bd2-9cae-718fb94ba905","resourceVersion":"41087","creationTimestamp":"2023-06-13T03:31:57Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3e7a907f3a51463b1a6b00fb3e03a167c9860c3fe2bd2924c49eed4a7348fb62","cni.projectcalico.org/podIP":"172.16.172.37/32","cni.projectcalico.org/podIPs":"172.16.172.37/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0a56db70-3259-4f5a-8321-bb2d6ab0720b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a56db70-3259-4f5a-8321-bb2d6ab0720b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-pck5c","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-pck5c","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"sks-test-v1-25-9-workergroup-2q6k2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["sks-test-v1-25-9-workergroup-2q6k2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:57Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:59Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:59Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:57Z"}],"hostIP":"10.255.64.103","podIP":"172.16.172.37","podIPs":[{"ip":"172.16.172.37"}],"startTime":"2023-06-13T03:31:57Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-13T03:31:58Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://b85442e7b220b3ee60a7da165d13dfd6b5ebc2c5561d39b96d000f54adf03591","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-92f56","generateName":"daemon-set-","namespace":"daemonsets-2836","uid":"2c898629-60e0-47a4-9c2c-d8f86c875bb5","resourceVersion":"41092","creationTimestamp":"2023-06-13T03:31:57Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"0c0e66b3649881a30c39b950e094a57a181e71c1f47653872c72c704137f4131","cni.projectcalico.org/podIP":"172.28.156.196/32","cni.projectcalico.org/podIPs":"172.28.156.196/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0a56db70-3259-4f5a-8321-bb2d6ab0720b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a56db70-3259-4f5a-8321-bb2d6ab0720b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.156.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jwpsf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jwpsf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"sks-test-v1-25-9-workergroup-l5gcd","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["sks-test-v1-25-9-workergroup-l5gcd"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:57Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:59Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:59Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:57Z"}],"hostIP":"10.255.64.104","podIP":"172.28.156.196","podIPs":[{"ip":"172.28.156.196"}],"startTime":"2023-06-13T03:31:57Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-13T03:31:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://3e813e9cf3c3f08cd27902cbd2a101ce508af8014817f29bc7e4a9f780d04ed7","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-slgl9","generateName":"daemon-set-","namespace":"daemonsets-2836","uid":"07e4f066-3eb0-46d5-bcf6-3b93fc9a3493","resourceVersion":"41089","creationTimestamp":"2023-06-13T03:31:57Z","labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"a5225fbfe244b6e425a319e267cd17e0d0147ef9b6cb3e1a35fed89fc95a7e70","cni.projectcalico.org/podIP":"172.30.77.146/32","cni.projectcalico.org/podIPs":"172.30.77.146/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0a56db70-3259-4f5a-8321-bb2d6ab0720b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:57Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a56db70-3259-4f5a-8321-bb2d6ab0720b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:58Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-13T03:31:59Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.146\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kl2fz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kl2fz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"sks-test-v1-25-9-workergroup-469fm","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["sks-test-v1-25-9-workergroup-469fm"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:57Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:59Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:59Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-13T03:31:57Z"}],"hostIP":"10.255.64.102","podIP":"172.30.77.146","podIPs":[{"ip":"172.30.77.146"}],"startTime":"2023-06-13T03:31:57Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-13T03:31:59Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://b44b8e3d99562f85fc34b3bcf076a6f59d1de2975201f4d6b356573a6e28bd0f","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:32:00.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2836" for this suite. 06/13/23 03:32:00.24
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:32:00.29
Jun 13 03:32:00.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:32:00.291
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:32:00.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:32:00.362
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2852 06/13/23 03:32:00.369
STEP: changing the ExternalName service to type=NodePort 06/13/23 03:32:00.401
STEP: creating replication controller externalname-service in namespace services-2852 06/13/23 03:32:00.523
I0613 03:32:00.568271      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2852, replica count: 2
I0613 03:32:03.620478      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 13 03:32:03.620: INFO: Creating new exec pod
Jun 13 03:32:03.650: INFO: Waiting up to 5m0s for pod "execpod7lgbk" in namespace "services-2852" to be "running"
Jun 13 03:32:03.668: INFO: Pod "execpod7lgbk": Phase="Pending", Reason="", readiness=false. Elapsed: 17.590126ms
Jun 13 03:32:05.681: INFO: Pod "execpod7lgbk": Phase="Running", Reason="", readiness=true. Elapsed: 2.030740888s
Jun 13 03:32:05.681: INFO: Pod "execpod7lgbk" satisfied condition "running"
Jun 13 03:32:06.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 13 03:32:06.947: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 13 03:32:06.947: INFO: stdout: "externalname-service-j2clb"
Jun 13 03:32:06.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.133.52 80'
Jun 13 03:32:07.136: INFO: stderr: "+ nc -v -t -w 2 10.103.133.52 80\n+ echo hostName\nConnection to 10.103.133.52 80 port [tcp/http] succeeded!\n"
Jun 13 03:32:07.136: INFO: stdout: ""
Jun 13 03:32:08.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.133.52 80'
Jun 13 03:32:08.320: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.133.52 80\nConnection to 10.103.133.52 80 port [tcp/http] succeeded!\n"
Jun 13 03:32:08.320: INFO: stdout: ""
Jun 13 03:32:09.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.133.52 80'
Jun 13 03:32:09.342: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.133.52 80\nConnection to 10.103.133.52 80 port [tcp/http] succeeded!\n"
Jun 13 03:32:09.342: INFO: stdout: "externalname-service-29h96"
Jun 13 03:32:09.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30092'
Jun 13 03:32:09.536: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 10.255.64.102 30092\nConnection to 10.255.64.102 30092 port [tcp/*] succeeded!\n"
Jun 13 03:32:09.536: INFO: stdout: ""
Jun 13 03:32:10.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30092'
Jun 13 03:32:10.734: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.102 30092\nConnection to 10.255.64.102 30092 port [tcp/*] succeeded!\n"
Jun 13 03:32:10.734: INFO: stdout: ""
Jun 13 03:32:11.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30092'
Jun 13 03:32:11.741: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.102 30092\nConnection to 10.255.64.102 30092 port [tcp/*] succeeded!\n"
Jun 13 03:32:11.741: INFO: stdout: ""
Jun 13 03:32:12.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30092'
Jun 13 03:32:12.736: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 10.255.64.102 30092\nConnection to 10.255.64.102 30092 port [tcp/*] succeeded!\n"
Jun 13 03:32:12.736: INFO: stdout: ""
Jun 13 03:32:13.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30092'
Jun 13 03:32:13.778: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.102 30092\nConnection to 10.255.64.102 30092 port [tcp/*] succeeded!\n"
Jun 13 03:32:13.778: INFO: stdout: ""
Jun 13 03:32:14.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30092'
Jun 13 03:32:14.734: INFO: stderr: "+ nc -v -t -w 2 10.255.64.102 30092\n+ echo hostName\nConnection to 10.255.64.102 30092 port [tcp/*] succeeded!\n"
Jun 13 03:32:14.735: INFO: stdout: "externalname-service-29h96"
Jun 13 03:32:14.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.104 30092'
Jun 13 03:32:14.944: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.104 30092\nConnection to 10.255.64.104 30092 port [tcp/*] succeeded!\n"
Jun 13 03:32:14.944: INFO: stdout: "externalname-service-29h96"
Jun 13 03:32:14.944: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:32:15.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2852" for this suite. 06/13/23 03:32:15.025
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":255,"skipped":4689,"failed":0}
------------------------------
• [SLOW TEST] [14.757 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:32:00.29
    Jun 13 03:32:00.290: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:32:00.291
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:32:00.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:32:00.362
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2852 06/13/23 03:32:00.369
    STEP: changing the ExternalName service to type=NodePort 06/13/23 03:32:00.401
    STEP: creating replication controller externalname-service in namespace services-2852 06/13/23 03:32:00.523
    I0613 03:32:00.568271      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2852, replica count: 2
    I0613 03:32:03.620478      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 13 03:32:03.620: INFO: Creating new exec pod
    Jun 13 03:32:03.650: INFO: Waiting up to 5m0s for pod "execpod7lgbk" in namespace "services-2852" to be "running"
    Jun 13 03:32:03.668: INFO: Pod "execpod7lgbk": Phase="Pending", Reason="", readiness=false. Elapsed: 17.590126ms
    Jun 13 03:32:05.681: INFO: Pod "execpod7lgbk": Phase="Running", Reason="", readiness=true. Elapsed: 2.030740888s
    Jun 13 03:32:05.681: INFO: Pod "execpod7lgbk" satisfied condition "running"
    Jun 13 03:32:06.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 13 03:32:06.947: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 13 03:32:06.947: INFO: stdout: "externalname-service-j2clb"
    Jun 13 03:32:06.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.133.52 80'
    Jun 13 03:32:07.136: INFO: stderr: "+ nc -v -t -w 2 10.103.133.52 80\n+ echo hostName\nConnection to 10.103.133.52 80 port [tcp/http] succeeded!\n"
    Jun 13 03:32:07.136: INFO: stdout: ""
    Jun 13 03:32:08.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.133.52 80'
    Jun 13 03:32:08.320: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.133.52 80\nConnection to 10.103.133.52 80 port [tcp/http] succeeded!\n"
    Jun 13 03:32:08.320: INFO: stdout: ""
    Jun 13 03:32:09.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.133.52 80'
    Jun 13 03:32:09.342: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.133.52 80\nConnection to 10.103.133.52 80 port [tcp/http] succeeded!\n"
    Jun 13 03:32:09.342: INFO: stdout: "externalname-service-29h96"
    Jun 13 03:32:09.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30092'
    Jun 13 03:32:09.536: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 10.255.64.102 30092\nConnection to 10.255.64.102 30092 port [tcp/*] succeeded!\n"
    Jun 13 03:32:09.536: INFO: stdout: ""
    Jun 13 03:32:10.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30092'
    Jun 13 03:32:10.734: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.102 30092\nConnection to 10.255.64.102 30092 port [tcp/*] succeeded!\n"
    Jun 13 03:32:10.734: INFO: stdout: ""
    Jun 13 03:32:11.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30092'
    Jun 13 03:32:11.741: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.102 30092\nConnection to 10.255.64.102 30092 port [tcp/*] succeeded!\n"
    Jun 13 03:32:11.741: INFO: stdout: ""
    Jun 13 03:32:12.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30092'
    Jun 13 03:32:12.736: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 10.255.64.102 30092\nConnection to 10.255.64.102 30092 port [tcp/*] succeeded!\n"
    Jun 13 03:32:12.736: INFO: stdout: ""
    Jun 13 03:32:13.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30092'
    Jun 13 03:32:13.778: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.102 30092\nConnection to 10.255.64.102 30092 port [tcp/*] succeeded!\n"
    Jun 13 03:32:13.778: INFO: stdout: ""
    Jun 13 03:32:14.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.102 30092'
    Jun 13 03:32:14.734: INFO: stderr: "+ nc -v -t -w 2 10.255.64.102 30092\n+ echo hostName\nConnection to 10.255.64.102 30092 port [tcp/*] succeeded!\n"
    Jun 13 03:32:14.735: INFO: stdout: "externalname-service-29h96"
    Jun 13 03:32:14.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-2852 exec execpod7lgbk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.104 30092'
    Jun 13 03:32:14.944: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.104 30092\nConnection to 10.255.64.104 30092 port [tcp/*] succeeded!\n"
    Jun 13 03:32:14.944: INFO: stdout: "externalname-service-29h96"
    Jun 13 03:32:14.944: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:32:15.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2852" for this suite. 06/13/23 03:32:15.025
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:32:15.048
Jun 13 03:32:15.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename crd-watch 06/13/23 03:32:15.05
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:32:15.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:32:15.101
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jun 13 03:32:15.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Creating first CR  06/13/23 03:32:17.72
Jun 13 03:32:17.741: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-13T03:32:17Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-13T03:32:17Z]] name:name1 resourceVersion:41293 uid:67cde944-84a6-4280-91b6-2bdd087c88dd] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 06/13/23 03:32:27.741
Jun 13 03:32:27.805: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-13T03:32:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-13T03:32:27Z]] name:name2 resourceVersion:41379 uid:7d54e043-9dc8-47d0-85bb-818944336f27] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 06/13/23 03:32:37.806
Jun 13 03:32:37.818: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-13T03:32:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-13T03:32:37Z]] name:name1 resourceVersion:41420 uid:67cde944-84a6-4280-91b6-2bdd087c88dd] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 06/13/23 03:32:47.818
Jun 13 03:32:48.002: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-13T03:32:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-13T03:32:47Z]] name:name2 resourceVersion:41460 uid:7d54e043-9dc8-47d0-85bb-818944336f27] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 06/13/23 03:32:58.003
Jun 13 03:32:58.023: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-13T03:32:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-13T03:32:37Z]] name:name1 resourceVersion:41496 uid:67cde944-84a6-4280-91b6-2bdd087c88dd] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 06/13/23 03:33:08.024
Jun 13 03:33:08.102: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-13T03:32:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-13T03:32:47Z]] name:name2 resourceVersion:41536 uid:7d54e043-9dc8-47d0-85bb-818944336f27] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:33:18.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-2297" for this suite. 06/13/23 03:33:18.806
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":256,"skipped":4711,"failed":0}
------------------------------
• [SLOW TEST] [63.851 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:32:15.048
    Jun 13 03:32:15.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename crd-watch 06/13/23 03:32:15.05
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:32:15.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:32:15.101
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jun 13 03:32:15.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Creating first CR  06/13/23 03:32:17.72
    Jun 13 03:32:17.741: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-13T03:32:17Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-13T03:32:17Z]] name:name1 resourceVersion:41293 uid:67cde944-84a6-4280-91b6-2bdd087c88dd] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 06/13/23 03:32:27.741
    Jun 13 03:32:27.805: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-13T03:32:27Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-13T03:32:27Z]] name:name2 resourceVersion:41379 uid:7d54e043-9dc8-47d0-85bb-818944336f27] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 06/13/23 03:32:37.806
    Jun 13 03:32:37.818: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-13T03:32:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-13T03:32:37Z]] name:name1 resourceVersion:41420 uid:67cde944-84a6-4280-91b6-2bdd087c88dd] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 06/13/23 03:32:47.818
    Jun 13 03:32:48.002: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-13T03:32:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-13T03:32:47Z]] name:name2 resourceVersion:41460 uid:7d54e043-9dc8-47d0-85bb-818944336f27] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 06/13/23 03:32:58.003
    Jun 13 03:32:58.023: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-13T03:32:17Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-13T03:32:37Z]] name:name1 resourceVersion:41496 uid:67cde944-84a6-4280-91b6-2bdd087c88dd] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 06/13/23 03:33:08.024
    Jun 13 03:33:08.102: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-13T03:32:27Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-13T03:32:47Z]] name:name2 resourceVersion:41536 uid:7d54e043-9dc8-47d0-85bb-818944336f27] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:33:18.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-2297" for this suite. 06/13/23 03:33:18.806
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:33:18.9
Jun 13 03:33:18.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 03:33:18.903
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:18.956
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:18.962
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jun 13 03:33:18.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 06/13/23 03:33:26.128
Jun 13 03:33:26.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 create -f -'
Jun 13 03:33:27.365: INFO: stderr: ""
Jun 13 03:33:27.365: INFO: stdout: "e2e-test-crd-publish-openapi-1621-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 13 03:33:27.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 delete e2e-test-crd-publish-openapi-1621-crds test-foo'
Jun 13 03:33:27.477: INFO: stderr: ""
Jun 13 03:33:27.477: INFO: stdout: "e2e-test-crd-publish-openapi-1621-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 13 03:33:27.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 apply -f -'
Jun 13 03:33:28.747: INFO: stderr: ""
Jun 13 03:33:28.747: INFO: stdout: "e2e-test-crd-publish-openapi-1621-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 13 03:33:28.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 delete e2e-test-crd-publish-openapi-1621-crds test-foo'
Jun 13 03:33:29.120: INFO: stderr: ""
Jun 13 03:33:29.120: INFO: stdout: "e2e-test-crd-publish-openapi-1621-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 06/13/23 03:33:29.12
Jun 13 03:33:29.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 create -f -'
Jun 13 03:33:30.157: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 06/13/23 03:33:30.157
Jun 13 03:33:30.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 create -f -'
Jun 13 03:33:30.505: INFO: rc: 1
Jun 13 03:33:30.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 apply -f -'
Jun 13 03:33:30.844: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 06/13/23 03:33:30.844
Jun 13 03:33:30.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 create -f -'
Jun 13 03:33:31.245: INFO: rc: 1
Jun 13 03:33:31.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 apply -f -'
Jun 13 03:33:31.629: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 06/13/23 03:33:31.629
Jun 13 03:33:31.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 explain e2e-test-crd-publish-openapi-1621-crds'
Jun 13 03:33:31.965: INFO: stderr: ""
Jun 13 03:33:31.966: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 06/13/23 03:33:31.966
Jun 13 03:33:31.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 explain e2e-test-crd-publish-openapi-1621-crds.metadata'
Jun 13 03:33:32.366: INFO: stderr: ""
Jun 13 03:33:32.366: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 13 03:33:32.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 explain e2e-test-crd-publish-openapi-1621-crds.spec'
Jun 13 03:33:32.711: INFO: stderr: ""
Jun 13 03:33:32.711: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 13 03:33:32.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 explain e2e-test-crd-publish-openapi-1621-crds.spec.bars'
Jun 13 03:33:33.142: INFO: stderr: ""
Jun 13 03:33:33.142: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 06/13/23 03:33:33.143
Jun 13 03:33:33.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 explain e2e-test-crd-publish-openapi-1621-crds.spec.bars2'
Jun 13 03:33:33.490: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:33:40.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-6241" for this suite. 06/13/23 03:33:40.156
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":257,"skipped":4713,"failed":0}
------------------------------
• [SLOW TEST] [21.275 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:33:18.9
    Jun 13 03:33:18.901: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 03:33:18.903
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:18.956
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:18.962
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jun 13 03:33:18.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 06/13/23 03:33:26.128
    Jun 13 03:33:26.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 create -f -'
    Jun 13 03:33:27.365: INFO: stderr: ""
    Jun 13 03:33:27.365: INFO: stdout: "e2e-test-crd-publish-openapi-1621-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jun 13 03:33:27.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 delete e2e-test-crd-publish-openapi-1621-crds test-foo'
    Jun 13 03:33:27.477: INFO: stderr: ""
    Jun 13 03:33:27.477: INFO: stdout: "e2e-test-crd-publish-openapi-1621-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jun 13 03:33:27.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 apply -f -'
    Jun 13 03:33:28.747: INFO: stderr: ""
    Jun 13 03:33:28.747: INFO: stdout: "e2e-test-crd-publish-openapi-1621-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jun 13 03:33:28.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 delete e2e-test-crd-publish-openapi-1621-crds test-foo'
    Jun 13 03:33:29.120: INFO: stderr: ""
    Jun 13 03:33:29.120: INFO: stdout: "e2e-test-crd-publish-openapi-1621-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 06/13/23 03:33:29.12
    Jun 13 03:33:29.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 create -f -'
    Jun 13 03:33:30.157: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 06/13/23 03:33:30.157
    Jun 13 03:33:30.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 create -f -'
    Jun 13 03:33:30.505: INFO: rc: 1
    Jun 13 03:33:30.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 apply -f -'
    Jun 13 03:33:30.844: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 06/13/23 03:33:30.844
    Jun 13 03:33:30.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 create -f -'
    Jun 13 03:33:31.245: INFO: rc: 1
    Jun 13 03:33:31.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 --namespace=crd-publish-openapi-6241 apply -f -'
    Jun 13 03:33:31.629: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 06/13/23 03:33:31.629
    Jun 13 03:33:31.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 explain e2e-test-crd-publish-openapi-1621-crds'
    Jun 13 03:33:31.965: INFO: stderr: ""
    Jun 13 03:33:31.966: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 06/13/23 03:33:31.966
    Jun 13 03:33:31.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 explain e2e-test-crd-publish-openapi-1621-crds.metadata'
    Jun 13 03:33:32.366: INFO: stderr: ""
    Jun 13 03:33:32.366: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jun 13 03:33:32.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 explain e2e-test-crd-publish-openapi-1621-crds.spec'
    Jun 13 03:33:32.711: INFO: stderr: ""
    Jun 13 03:33:32.711: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jun 13 03:33:32.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 explain e2e-test-crd-publish-openapi-1621-crds.spec.bars'
    Jun 13 03:33:33.142: INFO: stderr: ""
    Jun 13 03:33:33.142: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1621-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 06/13/23 03:33:33.143
    Jun 13 03:33:33.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-6241 explain e2e-test-crd-publish-openapi-1621-crds.spec.bars2'
    Jun 13 03:33:33.490: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:33:40.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-6241" for this suite. 06/13/23 03:33:40.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:33:40.176
Jun 13 03:33:40.176: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:33:40.178
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:40.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:40.223
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 06/13/23 03:33:40.229
Jun 13 03:33:40.248: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65" in namespace "projected-5069" to be "Succeeded or Failed"
Jun 13 03:33:40.255: INFO: Pod "downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65": Phase="Pending", Reason="", readiness=false. Elapsed: 7.732283ms
Jun 13 03:33:42.265: INFO: Pod "downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017175467s
Jun 13 03:33:44.263: INFO: Pod "downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015260705s
STEP: Saw pod success 06/13/23 03:33:44.263
Jun 13 03:33:44.263: INFO: Pod "downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65" satisfied condition "Succeeded or Failed"
Jun 13 03:33:44.273: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65 container client-container: <nil>
STEP: delete the pod 06/13/23 03:33:44.302
Jun 13 03:33:44.343: INFO: Waiting for pod downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65 to disappear
Jun 13 03:33:44.352: INFO: Pod downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 13 03:33:44.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5069" for this suite. 06/13/23 03:33:44.374
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":258,"skipped":4719,"failed":0}
------------------------------
• [4.220 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:33:40.176
    Jun 13 03:33:40.176: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:33:40.178
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:40.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:40.223
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 06/13/23 03:33:40.229
    Jun 13 03:33:40.248: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65" in namespace "projected-5069" to be "Succeeded or Failed"
    Jun 13 03:33:40.255: INFO: Pod "downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65": Phase="Pending", Reason="", readiness=false. Elapsed: 7.732283ms
    Jun 13 03:33:42.265: INFO: Pod "downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017175467s
    Jun 13 03:33:44.263: INFO: Pod "downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015260705s
    STEP: Saw pod success 06/13/23 03:33:44.263
    Jun 13 03:33:44.263: INFO: Pod "downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65" satisfied condition "Succeeded or Failed"
    Jun 13 03:33:44.273: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65 container client-container: <nil>
    STEP: delete the pod 06/13/23 03:33:44.302
    Jun 13 03:33:44.343: INFO: Waiting for pod downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65 to disappear
    Jun 13 03:33:44.352: INFO: Pod downwardapi-volume-ddec4395-5df9-41fb-9c73-ca9dca5c1b65 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 13 03:33:44.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5069" for this suite. 06/13/23 03:33:44.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:33:44.398
Jun 13 03:33:44.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename custom-resource-definition 06/13/23 03:33:44.399
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:44.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:44.454
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jun 13 03:33:44.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:33:45.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4574" for this suite. 06/13/23 03:33:45.17
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":259,"skipped":4764,"failed":0}
------------------------------
• [0.806 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:33:44.398
    Jun 13 03:33:44.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename custom-resource-definition 06/13/23 03:33:44.399
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:44.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:44.454
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jun 13 03:33:44.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:33:45.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4574" for this suite. 06/13/23 03:33:45.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:33:45.205
Jun 13 03:33:45.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename namespaces 06/13/23 03:33:45.206
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:45.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:45.254
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 06/13/23 03:33:45.261
Jun 13 03:33:45.271: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 06/13/23 03:33:45.271
Jun 13 03:33:45.284: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 06/13/23 03:33:45.284
Jun 13 03:33:45.306: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:33:45.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8174" for this suite. 06/13/23 03:33:45.316
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":260,"skipped":4775,"failed":0}
------------------------------
• [0.125 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:33:45.205
    Jun 13 03:33:45.205: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename namespaces 06/13/23 03:33:45.206
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:45.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:45.254
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 06/13/23 03:33:45.261
    Jun 13 03:33:45.271: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 06/13/23 03:33:45.271
    Jun 13 03:33:45.284: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 06/13/23 03:33:45.284
    Jun 13 03:33:45.306: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:33:45.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-8174" for this suite. 06/13/23 03:33:45.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:33:45.332
Jun 13 03:33:45.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename replication-controller 06/13/23 03:33:45.333
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:45.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:45.373
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 06/13/23 03:33:45.388
STEP: waiting for RC to be added 06/13/23 03:33:45.401
STEP: waiting for available Replicas 06/13/23 03:33:45.401
STEP: patching ReplicationController 06/13/23 03:33:46.756
STEP: waiting for RC to be modified 06/13/23 03:33:46.771
STEP: patching ReplicationController status 06/13/23 03:33:46.771
STEP: waiting for RC to be modified 06/13/23 03:33:46.784
STEP: waiting for available Replicas 06/13/23 03:33:46.784
STEP: fetching ReplicationController status 06/13/23 03:33:46.797
STEP: patching ReplicationController scale 06/13/23 03:33:46.805
STEP: waiting for RC to be modified 06/13/23 03:33:46.818
STEP: waiting for ReplicationController's scale to be the max amount 06/13/23 03:33:46.818
STEP: fetching ReplicationController; ensuring that it's patched 06/13/23 03:33:48.451
STEP: updating ReplicationController status 06/13/23 03:33:48.465
STEP: waiting for RC to be modified 06/13/23 03:33:48.493
STEP: listing all ReplicationControllers 06/13/23 03:33:48.494
STEP: checking that ReplicationController has expected values 06/13/23 03:33:48.532
STEP: deleting ReplicationControllers by collection 06/13/23 03:33:48.533
STEP: waiting for ReplicationController to have a DELETED watchEvent 06/13/23 03:33:48.613
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 13 03:33:48.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3304" for this suite. 06/13/23 03:33:48.835
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":261,"skipped":4808,"failed":0}
------------------------------
• [3.539 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:33:45.332
    Jun 13 03:33:45.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename replication-controller 06/13/23 03:33:45.333
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:45.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:45.373
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 06/13/23 03:33:45.388
    STEP: waiting for RC to be added 06/13/23 03:33:45.401
    STEP: waiting for available Replicas 06/13/23 03:33:45.401
    STEP: patching ReplicationController 06/13/23 03:33:46.756
    STEP: waiting for RC to be modified 06/13/23 03:33:46.771
    STEP: patching ReplicationController status 06/13/23 03:33:46.771
    STEP: waiting for RC to be modified 06/13/23 03:33:46.784
    STEP: waiting for available Replicas 06/13/23 03:33:46.784
    STEP: fetching ReplicationController status 06/13/23 03:33:46.797
    STEP: patching ReplicationController scale 06/13/23 03:33:46.805
    STEP: waiting for RC to be modified 06/13/23 03:33:46.818
    STEP: waiting for ReplicationController's scale to be the max amount 06/13/23 03:33:46.818
    STEP: fetching ReplicationController; ensuring that it's patched 06/13/23 03:33:48.451
    STEP: updating ReplicationController status 06/13/23 03:33:48.465
    STEP: waiting for RC to be modified 06/13/23 03:33:48.493
    STEP: listing all ReplicationControllers 06/13/23 03:33:48.494
    STEP: checking that ReplicationController has expected values 06/13/23 03:33:48.532
    STEP: deleting ReplicationControllers by collection 06/13/23 03:33:48.533
    STEP: waiting for ReplicationController to have a DELETED watchEvent 06/13/23 03:33:48.613
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 13 03:33:48.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3304" for this suite. 06/13/23 03:33:48.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:33:48.872
Jun 13 03:33:48.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 03:33:48.873
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:48.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:48.965
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-d30cee50-8d0a-4e0b-96cf-121d7954ff85 06/13/23 03:33:48.972
STEP: Creating a pod to test consume configMaps 06/13/23 03:33:49.011
Jun 13 03:33:49.071: INFO: Waiting up to 5m0s for pod "pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec" in namespace "configmap-6195" to be "Succeeded or Failed"
Jun 13 03:33:49.079: INFO: Pod "pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.886737ms
Jun 13 03:33:51.090: INFO: Pod "pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019626034s
Jun 13 03:33:53.088: INFO: Pod "pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01720762s
STEP: Saw pod success 06/13/23 03:33:53.088
Jun 13 03:33:53.088: INFO: Pod "pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec" satisfied condition "Succeeded or Failed"
Jun 13 03:33:53.102: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec container agnhost-container: <nil>
STEP: delete the pod 06/13/23 03:33:53.14
Jun 13 03:33:53.177: INFO: Waiting for pod pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec to disappear
Jun 13 03:33:53.186: INFO: Pod pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 03:33:53.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6195" for this suite. 06/13/23 03:33:53.199
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":262,"skipped":4816,"failed":0}
------------------------------
• [4.344 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:33:48.872
    Jun 13 03:33:48.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 03:33:48.873
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:48.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:48.965
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-d30cee50-8d0a-4e0b-96cf-121d7954ff85 06/13/23 03:33:48.972
    STEP: Creating a pod to test consume configMaps 06/13/23 03:33:49.011
    Jun 13 03:33:49.071: INFO: Waiting up to 5m0s for pod "pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec" in namespace "configmap-6195" to be "Succeeded or Failed"
    Jun 13 03:33:49.079: INFO: Pod "pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec": Phase="Pending", Reason="", readiness=false. Elapsed: 7.886737ms
    Jun 13 03:33:51.090: INFO: Pod "pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019626034s
    Jun 13 03:33:53.088: INFO: Pod "pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01720762s
    STEP: Saw pod success 06/13/23 03:33:53.088
    Jun 13 03:33:53.088: INFO: Pod "pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec" satisfied condition "Succeeded or Failed"
    Jun 13 03:33:53.102: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 03:33:53.14
    Jun 13 03:33:53.177: INFO: Waiting for pod pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec to disappear
    Jun 13 03:33:53.186: INFO: Pod pod-configmaps-d240bb6f-fa2b-4fa8-b852-63d71aca26ec no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 03:33:53.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6195" for this suite. 06/13/23 03:33:53.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:33:53.224
Jun 13 03:33:53.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 03:33:53.226
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:53.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:53.285
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 06/13/23 03:33:53.294
Jun 13 03:33:53.323: INFO: Waiting up to 5m0s for pod "pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c" in namespace "emptydir-7014" to be "Succeeded or Failed"
Jun 13 03:33:53.335: INFO: Pod "pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.90909ms
Jun 13 03:33:55.345: INFO: Pod "pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021462563s
Jun 13 03:33:57.344: INFO: Pod "pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020564971s
STEP: Saw pod success 06/13/23 03:33:57.344
Jun 13 03:33:57.344: INFO: Pod "pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c" satisfied condition "Succeeded or Failed"
Jun 13 03:33:57.351: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c container test-container: <nil>
STEP: delete the pod 06/13/23 03:33:57.368
Jun 13 03:33:57.394: INFO: Waiting for pod pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c to disappear
Jun 13 03:33:57.401: INFO: Pod pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 03:33:57.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7014" for this suite. 06/13/23 03:33:57.41
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":263,"skipped":4881,"failed":0}
------------------------------
• [4.207 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:33:53.224
    Jun 13 03:33:53.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 03:33:53.226
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:53.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:53.285
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 06/13/23 03:33:53.294
    Jun 13 03:33:53.323: INFO: Waiting up to 5m0s for pod "pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c" in namespace "emptydir-7014" to be "Succeeded or Failed"
    Jun 13 03:33:53.335: INFO: Pod "pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.90909ms
    Jun 13 03:33:55.345: INFO: Pod "pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021462563s
    Jun 13 03:33:57.344: INFO: Pod "pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020564971s
    STEP: Saw pod success 06/13/23 03:33:57.344
    Jun 13 03:33:57.344: INFO: Pod "pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c" satisfied condition "Succeeded or Failed"
    Jun 13 03:33:57.351: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c container test-container: <nil>
    STEP: delete the pod 06/13/23 03:33:57.368
    Jun 13 03:33:57.394: INFO: Waiting for pod pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c to disappear
    Jun 13 03:33:57.401: INFO: Pod pod-be13cbeb-27d9-470d-a3c6-5007999cfd3c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 03:33:57.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7014" for this suite. 06/13/23 03:33:57.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:33:57.431
Jun 13 03:33:57.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:33:57.433
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:57.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:57.468
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 06/13/23 03:33:57.474
Jun 13 03:33:57.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8" in namespace "downward-api-1284" to be "Succeeded or Failed"
Jun 13 03:33:57.508: INFO: Pod "downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.992896ms
Jun 13 03:33:59.518: INFO: Pod "downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022314825s
Jun 13 03:34:01.519: INFO: Pod "downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023893319s
STEP: Saw pod success 06/13/23 03:34:01.519
Jun 13 03:34:01.520: INFO: Pod "downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8" satisfied condition "Succeeded or Failed"
Jun 13 03:34:01.527: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8 container client-container: <nil>
STEP: delete the pod 06/13/23 03:34:01.542
Jun 13 03:34:01.568: INFO: Waiting for pod downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8 to disappear
Jun 13 03:34:01.575: INFO: Pod downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 13 03:34:01.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1284" for this suite. 06/13/23 03:34:01.586
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":264,"skipped":4887,"failed":0}
------------------------------
• [4.169 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:33:57.431
    Jun 13 03:33:57.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:33:57.433
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:33:57.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:33:57.468
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 06/13/23 03:33:57.474
    Jun 13 03:33:57.495: INFO: Waiting up to 5m0s for pod "downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8" in namespace "downward-api-1284" to be "Succeeded or Failed"
    Jun 13 03:33:57.508: INFO: Pod "downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.992896ms
    Jun 13 03:33:59.518: INFO: Pod "downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022314825s
    Jun 13 03:34:01.519: INFO: Pod "downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023893319s
    STEP: Saw pod success 06/13/23 03:34:01.519
    Jun 13 03:34:01.520: INFO: Pod "downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8" satisfied condition "Succeeded or Failed"
    Jun 13 03:34:01.527: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8 container client-container: <nil>
    STEP: delete the pod 06/13/23 03:34:01.542
    Jun 13 03:34:01.568: INFO: Waiting for pod downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8 to disappear
    Jun 13 03:34:01.575: INFO: Pod downwardapi-volume-16e78686-4ab2-46a5-8cab-009b4bd96cd8 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 13 03:34:01.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1284" for this suite. 06/13/23 03:34:01.586
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:34:01.602
Jun 13 03:34:01.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename var-expansion 06/13/23 03:34:01.603
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:01.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:01.647
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 06/13/23 03:34:01.653
Jun 13 03:34:01.683: INFO: Waiting up to 5m0s for pod "var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800" in namespace "var-expansion-474" to be "Succeeded or Failed"
Jun 13 03:34:01.699: INFO: Pod "var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800": Phase="Pending", Reason="", readiness=false. Elapsed: 15.699574ms
Jun 13 03:34:03.708: INFO: Pod "var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024531072s
Jun 13 03:34:05.714: INFO: Pod "var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030441588s
Jun 13 03:34:07.707: INFO: Pod "var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023639321s
STEP: Saw pod success 06/13/23 03:34:07.707
Jun 13 03:34:07.707: INFO: Pod "var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800" satisfied condition "Succeeded or Failed"
Jun 13 03:34:07.713: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800 container dapi-container: <nil>
STEP: delete the pod 06/13/23 03:34:07.734
Jun 13 03:34:07.761: INFO: Waiting for pod var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800 to disappear
Jun 13 03:34:07.768: INFO: Pod var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 13 03:34:07.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-474" for this suite. 06/13/23 03:34:07.777
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":265,"skipped":4909,"failed":0}
------------------------------
• [SLOW TEST] [6.190 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:34:01.602
    Jun 13 03:34:01.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename var-expansion 06/13/23 03:34:01.603
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:01.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:01.647
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 06/13/23 03:34:01.653
    Jun 13 03:34:01.683: INFO: Waiting up to 5m0s for pod "var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800" in namespace "var-expansion-474" to be "Succeeded or Failed"
    Jun 13 03:34:01.699: INFO: Pod "var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800": Phase="Pending", Reason="", readiness=false. Elapsed: 15.699574ms
    Jun 13 03:34:03.708: INFO: Pod "var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024531072s
    Jun 13 03:34:05.714: INFO: Pod "var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030441588s
    Jun 13 03:34:07.707: INFO: Pod "var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.023639321s
    STEP: Saw pod success 06/13/23 03:34:07.707
    Jun 13 03:34:07.707: INFO: Pod "var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800" satisfied condition "Succeeded or Failed"
    Jun 13 03:34:07.713: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800 container dapi-container: <nil>
    STEP: delete the pod 06/13/23 03:34:07.734
    Jun 13 03:34:07.761: INFO: Waiting for pod var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800 to disappear
    Jun 13 03:34:07.768: INFO: Pod var-expansion-7c2ea3ed-817b-4c57-b3a4-391cfe294800 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 13 03:34:07.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-474" for this suite. 06/13/23 03:34:07.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:34:07.793
Jun 13 03:34:07.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-runtime 06/13/23 03:34:07.795
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:07.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:07.83
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 06/13/23 03:34:07.852
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 06/13/23 03:34:26.067
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 06/13/23 03:34:26.075
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 06/13/23 03:34:26.09
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 06/13/23 03:34:26.09
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 06/13/23 03:34:26.136
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 06/13/23 03:34:29.175
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 06/13/23 03:34:31.2
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 06/13/23 03:34:31.216
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 06/13/23 03:34:31.216
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 06/13/23 03:34:31.299
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 06/13/23 03:34:32.319
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 06/13/23 03:34:35.354
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 06/13/23 03:34:35.366
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 06/13/23 03:34:35.366
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 13 03:34:35.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4602" for this suite. 06/13/23 03:34:35.434
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":266,"skipped":4916,"failed":0}
------------------------------
• [SLOW TEST] [27.691 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:34:07.793
    Jun 13 03:34:07.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-runtime 06/13/23 03:34:07.795
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:07.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:07.83
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 06/13/23 03:34:07.852
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 06/13/23 03:34:26.067
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 06/13/23 03:34:26.075
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 06/13/23 03:34:26.09
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 06/13/23 03:34:26.09
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 06/13/23 03:34:26.136
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 06/13/23 03:34:29.175
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 06/13/23 03:34:31.2
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 06/13/23 03:34:31.216
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 06/13/23 03:34:31.216
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 06/13/23 03:34:31.299
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 06/13/23 03:34:32.319
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 06/13/23 03:34:35.354
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 06/13/23 03:34:35.366
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 06/13/23 03:34:35.366
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 13 03:34:35.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-4602" for this suite. 06/13/23 03:34:35.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:34:35.485
Jun 13 03:34:35.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename replication-controller 06/13/23 03:34:35.487
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:35.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:35.587
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jun 13 03:34:35.594: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 06/13/23 03:34:36.704
STEP: Checking rc "condition-test" has the desired failure condition set 06/13/23 03:34:36.716
STEP: Scaling down rc "condition-test" to satisfy pod quota 06/13/23 03:34:37.735
Jun 13 03:34:37.758: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 06/13/23 03:34:37.758
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 13 03:34:37.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8050" for this suite. 06/13/23 03:34:37.778
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":267,"skipped":4938,"failed":0}
------------------------------
• [2.305 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:34:35.485
    Jun 13 03:34:35.485: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename replication-controller 06/13/23 03:34:35.487
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:35.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:35.587
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jun 13 03:34:35.594: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 06/13/23 03:34:36.704
    STEP: Checking rc "condition-test" has the desired failure condition set 06/13/23 03:34:36.716
    STEP: Scaling down rc "condition-test" to satisfy pod quota 06/13/23 03:34:37.735
    Jun 13 03:34:37.758: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 06/13/23 03:34:37.758
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 13 03:34:37.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8050" for this suite. 06/13/23 03:34:37.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:34:37.792
Jun 13 03:34:37.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename endpointslice 06/13/23 03:34:37.794
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:37.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:37.831
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jun 13 03:34:38.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7669" for this suite. 06/13/23 03:34:38.089
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":268,"skipped":4983,"failed":0}
------------------------------
• [0.397 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:34:37.792
    Jun 13 03:34:37.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename endpointslice 06/13/23 03:34:37.794
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:37.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:37.831
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jun 13 03:34:38.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7669" for this suite. 06/13/23 03:34:38.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:34:38.19
Jun 13 03:34:38.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pods 06/13/23 03:34:38.191
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:38.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:38.235
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 06/13/23 03:34:38.239
Jun 13 03:34:38.278: INFO: created test-pod-1
Jun 13 03:34:38.306: INFO: created test-pod-2
Jun 13 03:34:38.322: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 06/13/23 03:34:38.322
Jun 13 03:34:38.322: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-3972' to be running and ready
Jun 13 03:34:38.371: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 13 03:34:38.371: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 13 03:34:38.371: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 13 03:34:38.371: INFO: 0 / 3 pods in namespace 'pods-3972' are running and ready (0 seconds elapsed)
Jun 13 03:34:38.371: INFO: expected 0 pod replicas in namespace 'pods-3972', 0 are Running and Ready.
Jun 13 03:34:38.371: INFO: POD         NODE                                PHASE    GRACE  CONDITIONS
Jun 13 03:34:38.371: INFO: test-pod-1  sks-test-v1-25-9-workergroup-2q6k2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  }]
Jun 13 03:34:38.371: INFO: test-pod-2  sks-test-v1-25-9-workergroup-l5gcd  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  }]
Jun 13 03:34:38.371: INFO: test-pod-3  sks-test-v1-25-9-workergroup-469fm  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  }]
Jun 13 03:34:38.371: INFO: 
Jun 13 03:34:40.402: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 13 03:34:40.402: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 13 03:34:40.402: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 13 03:34:40.402: INFO: 0 / 3 pods in namespace 'pods-3972' are running and ready (2 seconds elapsed)
Jun 13 03:34:40.402: INFO: expected 0 pod replicas in namespace 'pods-3972', 0 are Running and Ready.
Jun 13 03:34:40.402: INFO: POD         NODE                                PHASE    GRACE  CONDITIONS
Jun 13 03:34:40.402: INFO: test-pod-1  sks-test-v1-25-9-workergroup-2q6k2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  }]
Jun 13 03:34:40.402: INFO: test-pod-2  sks-test-v1-25-9-workergroup-l5gcd  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  }]
Jun 13 03:34:40.402: INFO: test-pod-3  sks-test-v1-25-9-workergroup-469fm  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  }]
Jun 13 03:34:40.402: INFO: 
Jun 13 03:34:42.411: INFO: 3 / 3 pods in namespace 'pods-3972' are running and ready (4 seconds elapsed)
Jun 13 03:34:42.411: INFO: expected 0 pod replicas in namespace 'pods-3972', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 06/13/23 03:34:42.554
Jun 13 03:34:42.564: INFO: Pod quantity 3 is different from expected quantity 0
Jun 13 03:34:43.599: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 13 03:34:44.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3972" for this suite. 06/13/23 03:34:44.595
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":269,"skipped":4990,"failed":0}
------------------------------
• [SLOW TEST] [6.455 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:34:38.19
    Jun 13 03:34:38.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pods 06/13/23 03:34:38.191
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:38.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:38.235
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 06/13/23 03:34:38.239
    Jun 13 03:34:38.278: INFO: created test-pod-1
    Jun 13 03:34:38.306: INFO: created test-pod-2
    Jun 13 03:34:38.322: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 06/13/23 03:34:38.322
    Jun 13 03:34:38.322: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-3972' to be running and ready
    Jun 13 03:34:38.371: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 13 03:34:38.371: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 13 03:34:38.371: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 13 03:34:38.371: INFO: 0 / 3 pods in namespace 'pods-3972' are running and ready (0 seconds elapsed)
    Jun 13 03:34:38.371: INFO: expected 0 pod replicas in namespace 'pods-3972', 0 are Running and Ready.
    Jun 13 03:34:38.371: INFO: POD         NODE                                PHASE    GRACE  CONDITIONS
    Jun 13 03:34:38.371: INFO: test-pod-1  sks-test-v1-25-9-workergroup-2q6k2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  }]
    Jun 13 03:34:38.371: INFO: test-pod-2  sks-test-v1-25-9-workergroup-l5gcd  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  }]
    Jun 13 03:34:38.371: INFO: test-pod-3  sks-test-v1-25-9-workergroup-469fm  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  }]
    Jun 13 03:34:38.371: INFO: 
    Jun 13 03:34:40.402: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 13 03:34:40.402: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 13 03:34:40.402: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 13 03:34:40.402: INFO: 0 / 3 pods in namespace 'pods-3972' are running and ready (2 seconds elapsed)
    Jun 13 03:34:40.402: INFO: expected 0 pod replicas in namespace 'pods-3972', 0 are Running and Ready.
    Jun 13 03:34:40.402: INFO: POD         NODE                                PHASE    GRACE  CONDITIONS
    Jun 13 03:34:40.402: INFO: test-pod-1  sks-test-v1-25-9-workergroup-2q6k2  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  }]
    Jun 13 03:34:40.402: INFO: test-pod-2  sks-test-v1-25-9-workergroup-l5gcd  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  }]
    Jun 13 03:34:40.402: INFO: test-pod-3  sks-test-v1-25-9-workergroup-469fm  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-13 03:34:38 +0000 UTC  }]
    Jun 13 03:34:40.402: INFO: 
    Jun 13 03:34:42.411: INFO: 3 / 3 pods in namespace 'pods-3972' are running and ready (4 seconds elapsed)
    Jun 13 03:34:42.411: INFO: expected 0 pod replicas in namespace 'pods-3972', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 06/13/23 03:34:42.554
    Jun 13 03:34:42.564: INFO: Pod quantity 3 is different from expected quantity 0
    Jun 13 03:34:43.599: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 13 03:34:44.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3972" for this suite. 06/13/23 03:34:44.595
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:34:44.646
Jun 13 03:34:44.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:34:44.648
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:44.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:44.772
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-c5a4cf55-0a51-41e5-96e5-961fdd3076aa 06/13/23 03:34:44.78
STEP: Creating a pod to test consume configMaps 06/13/23 03:34:44.805
Jun 13 03:34:44.847: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06" in namespace "projected-5986" to be "Succeeded or Failed"
Jun 13 03:34:44.870: INFO: Pod "pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06": Phase="Pending", Reason="", readiness=false. Elapsed: 22.295626ms
Jun 13 03:34:46.877: INFO: Pod "pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029679802s
Jun 13 03:34:48.879: INFO: Pod "pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031517207s
STEP: Saw pod success 06/13/23 03:34:48.879
Jun 13 03:34:48.879: INFO: Pod "pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06" satisfied condition "Succeeded or Failed"
Jun 13 03:34:48.886: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06 container agnhost-container: <nil>
STEP: delete the pod 06/13/23 03:34:48.903
Jun 13 03:34:48.974: INFO: Waiting for pod pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06 to disappear
Jun 13 03:34:48.981: INFO: Pod pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 13 03:34:48.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5986" for this suite. 06/13/23 03:34:48.989
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":270,"skipped":4993,"failed":0}
------------------------------
• [4.365 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:34:44.646
    Jun 13 03:34:44.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:34:44.648
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:44.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:44.772
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-c5a4cf55-0a51-41e5-96e5-961fdd3076aa 06/13/23 03:34:44.78
    STEP: Creating a pod to test consume configMaps 06/13/23 03:34:44.805
    Jun 13 03:34:44.847: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06" in namespace "projected-5986" to be "Succeeded or Failed"
    Jun 13 03:34:44.870: INFO: Pod "pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06": Phase="Pending", Reason="", readiness=false. Elapsed: 22.295626ms
    Jun 13 03:34:46.877: INFO: Pod "pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029679802s
    Jun 13 03:34:48.879: INFO: Pod "pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031517207s
    STEP: Saw pod success 06/13/23 03:34:48.879
    Jun 13 03:34:48.879: INFO: Pod "pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06" satisfied condition "Succeeded or Failed"
    Jun 13 03:34:48.886: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06 container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 03:34:48.903
    Jun 13 03:34:48.974: INFO: Waiting for pod pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06 to disappear
    Jun 13 03:34:48.981: INFO: Pod pod-projected-configmaps-4c7e4651-7bf9-473c-a31e-c5e779d51d06 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 13 03:34:48.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5986" for this suite. 06/13/23 03:34:48.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:34:49.012
Jun 13 03:34:49.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 03:34:49.013
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:49.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:49.08
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-b20c3517-c2eb-478c-bbb4-c3e533fea81b 06/13/23 03:34:49.088
STEP: Creating a pod to test consume secrets 06/13/23 03:34:49.123
Jun 13 03:34:49.156: INFO: Waiting up to 5m0s for pod "pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37" in namespace "secrets-9564" to be "Succeeded or Failed"
Jun 13 03:34:49.162: INFO: Pod "pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37": Phase="Pending", Reason="", readiness=false. Elapsed: 5.41771ms
Jun 13 03:34:51.170: INFO: Pod "pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014135757s
Jun 13 03:34:53.170: INFO: Pod "pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014182064s
STEP: Saw pod success 06/13/23 03:34:53.17
Jun 13 03:34:53.171: INFO: Pod "pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37" satisfied condition "Succeeded or Failed"
Jun 13 03:34:53.183: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37 container secret-volume-test: <nil>
STEP: delete the pod 06/13/23 03:34:53.199
Jun 13 03:34:53.224: INFO: Waiting for pod pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37 to disappear
Jun 13 03:34:53.229: INFO: Pod pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 13 03:34:53.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9564" for this suite. 06/13/23 03:34:53.239
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":271,"skipped":5006,"failed":0}
------------------------------
• [4.240 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:34:49.012
    Jun 13 03:34:49.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 03:34:49.013
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:49.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:49.08
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-b20c3517-c2eb-478c-bbb4-c3e533fea81b 06/13/23 03:34:49.088
    STEP: Creating a pod to test consume secrets 06/13/23 03:34:49.123
    Jun 13 03:34:49.156: INFO: Waiting up to 5m0s for pod "pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37" in namespace "secrets-9564" to be "Succeeded or Failed"
    Jun 13 03:34:49.162: INFO: Pod "pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37": Phase="Pending", Reason="", readiness=false. Elapsed: 5.41771ms
    Jun 13 03:34:51.170: INFO: Pod "pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014135757s
    Jun 13 03:34:53.170: INFO: Pod "pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014182064s
    STEP: Saw pod success 06/13/23 03:34:53.17
    Jun 13 03:34:53.171: INFO: Pod "pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37" satisfied condition "Succeeded or Failed"
    Jun 13 03:34:53.183: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37 container secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:34:53.199
    Jun 13 03:34:53.224: INFO: Waiting for pod pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37 to disappear
    Jun 13 03:34:53.229: INFO: Pod pod-secrets-bdb41145-ac9d-4c91-bea5-21d9153cae37 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 03:34:53.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9564" for this suite. 06/13/23 03:34:53.239
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:34:53.252
Jun 13 03:34:53.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename custom-resource-definition 06/13/23 03:34:53.253
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:53.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:53.292
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jun 13 03:34:53.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:35:00.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2286" for this suite. 06/13/23 03:35:00.228
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":272,"skipped":5008,"failed":0}
------------------------------
• [SLOW TEST] [7.000 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:34:53.252
    Jun 13 03:34:53.252: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename custom-resource-definition 06/13/23 03:34:53.253
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:34:53.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:34:53.292
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jun 13 03:34:53.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:35:00.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2286" for this suite. 06/13/23 03:35:00.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:35:00.254
Jun 13 03:35:00.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 03:35:00.255
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:00.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:00.389
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 03:35:00.499
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:35:01.328
STEP: Deploying the webhook pod 06/13/23 03:35:01.345
STEP: Wait for the deployment to be ready 06/13/23 03:35:01.369
Jun 13 03:35:01.448: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/13/23 03:35:03.484
STEP: Verifying the service has paired with the endpoint 06/13/23 03:35:03.695
Jun 13 03:35:04.696: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jun 13 03:35:04.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-333-crds.webhook.example.com via the AdmissionRegistration API 06/13/23 03:35:05.225
STEP: Creating a custom resource that should be mutated by the webhook 06/13/23 03:35:05.253
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:35:07.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7238" for this suite. 06/13/23 03:35:07.906
STEP: Destroying namespace "webhook-7238-markers" for this suite. 06/13/23 03:35:07.923
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":273,"skipped":5042,"failed":0}
------------------------------
• [SLOW TEST] [7.828 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:35:00.254
    Jun 13 03:35:00.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 03:35:00.255
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:00.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:00.389
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 03:35:00.499
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:35:01.328
    STEP: Deploying the webhook pod 06/13/23 03:35:01.345
    STEP: Wait for the deployment to be ready 06/13/23 03:35:01.369
    Jun 13 03:35:01.448: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/13/23 03:35:03.484
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:35:03.695
    Jun 13 03:35:04.696: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jun 13 03:35:04.703: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-333-crds.webhook.example.com via the AdmissionRegistration API 06/13/23 03:35:05.225
    STEP: Creating a custom resource that should be mutated by the webhook 06/13/23 03:35:05.253
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:35:07.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7238" for this suite. 06/13/23 03:35:07.906
    STEP: Destroying namespace "webhook-7238-markers" for this suite. 06/13/23 03:35:07.923
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:35:08.084
Jun 13 03:35:08.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename proxy 06/13/23 03:35:08.085
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:08.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:08.13
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jun 13 03:35:08.137: INFO: Creating pod...
Jun 13 03:35:08.159: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-683" to be "running"
Jun 13 03:35:08.170: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 10.733903ms
Jun 13 03:35:10.179: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.019224218s
Jun 13 03:35:10.179: INFO: Pod "agnhost" satisfied condition "running"
Jun 13 03:35:10.179: INFO: Creating service...
Jun 13 03:35:10.317: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/DELETE
Jun 13 03:35:10.403: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 13 03:35:10.403: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/GET
Jun 13 03:35:10.412: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun 13 03:35:10.412: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/HEAD
Jun 13 03:35:10.420: INFO: http.Client request:HEAD | StatusCode:200
Jun 13 03:35:10.420: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/OPTIONS
Jun 13 03:35:10.429: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 13 03:35:10.429: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/PATCH
Jun 13 03:35:10.438: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 13 03:35:10.438: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/POST
Jun 13 03:35:10.446: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 13 03:35:10.446: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/PUT
Jun 13 03:35:10.453: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 13 03:35:10.453: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/DELETE
Jun 13 03:35:10.479: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 13 03:35:10.479: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/GET
Jun 13 03:35:10.560: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun 13 03:35:10.560: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/HEAD
Jun 13 03:35:10.673: INFO: http.Client request:HEAD | StatusCode:200
Jun 13 03:35:10.673: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/OPTIONS
Jun 13 03:35:10.687: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 13 03:35:10.687: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/PATCH
Jun 13 03:35:10.703: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 13 03:35:10.703: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/POST
Jun 13 03:35:10.715: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 13 03:35:10.715: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/PUT
Jun 13 03:35:10.726: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jun 13 03:35:10.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-683" for this suite. 06/13/23 03:35:10.736
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":274,"skipped":5077,"failed":0}
------------------------------
• [2.678 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:35:08.084
    Jun 13 03:35:08.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename proxy 06/13/23 03:35:08.085
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:08.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:08.13
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jun 13 03:35:08.137: INFO: Creating pod...
    Jun 13 03:35:08.159: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-683" to be "running"
    Jun 13 03:35:08.170: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 10.733903ms
    Jun 13 03:35:10.179: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.019224218s
    Jun 13 03:35:10.179: INFO: Pod "agnhost" satisfied condition "running"
    Jun 13 03:35:10.179: INFO: Creating service...
    Jun 13 03:35:10.317: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/DELETE
    Jun 13 03:35:10.403: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 13 03:35:10.403: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/GET
    Jun 13 03:35:10.412: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jun 13 03:35:10.412: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/HEAD
    Jun 13 03:35:10.420: INFO: http.Client request:HEAD | StatusCode:200
    Jun 13 03:35:10.420: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/OPTIONS
    Jun 13 03:35:10.429: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 13 03:35:10.429: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/PATCH
    Jun 13 03:35:10.438: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 13 03:35:10.438: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/POST
    Jun 13 03:35:10.446: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 13 03:35:10.446: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/pods/agnhost/proxy/some/path/with/PUT
    Jun 13 03:35:10.453: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun 13 03:35:10.453: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/DELETE
    Jun 13 03:35:10.479: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 13 03:35:10.479: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/GET
    Jun 13 03:35:10.560: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jun 13 03:35:10.560: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/HEAD
    Jun 13 03:35:10.673: INFO: http.Client request:HEAD | StatusCode:200
    Jun 13 03:35:10.673: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/OPTIONS
    Jun 13 03:35:10.687: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 13 03:35:10.687: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/PATCH
    Jun 13 03:35:10.703: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 13 03:35:10.703: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/POST
    Jun 13 03:35:10.715: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 13 03:35:10.715: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-683/services/test-service/proxy/some/path/with/PUT
    Jun 13 03:35:10.726: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jun 13 03:35:10.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-683" for this suite. 06/13/23 03:35:10.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:35:10.763
Jun 13 03:35:10.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 03:35:10.765
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:10.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:10.808
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/13/23 03:35:10.813
Jun 13 03:35:10.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-387 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun 13 03:35:10.930: INFO: stderr: ""
Jun 13 03:35:10.930: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 06/13/23 03:35:10.93
STEP: verifying the pod e2e-test-httpd-pod was created 06/13/23 03:35:15.983
Jun 13 03:35:15.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-387 get pod e2e-test-httpd-pod -o json'
Jun 13 03:35:16.154: INFO: stderr: ""
Jun 13 03:35:16.154: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"e16cb4677b4a38c44bb6bbdc96b1dd6cf92e12df0616bebcc996b8b3a415fe91\",\n            \"cni.projectcalico.org/podIP\": \"172.16.172.31/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.16.172.31/32\"\n        },\n        \"creationTimestamp\": \"2023-06-13T03:35:10Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-387\",\n        \"resourceVersion\": \"42800\",\n        \"uid\": \"60442d93-a9f0-4a0b-8687-f143a7975ff5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-hnrsv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"sks-test-v1-25-9-workergroup-2q6k2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-hnrsv\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-13T03:35:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-13T03:35:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-13T03:35:12Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-13T03:35:10Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://03641cf6c808ccaf3588c70e01ccea376268f025135d5068d04a96a1c91fb269\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-13T03:35:11Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.255.64.103\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.172.31\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.172.31\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-13T03:35:10Z\"\n    }\n}\n"
STEP: replace the image in the pod 06/13/23 03:35:16.154
Jun 13 03:35:16.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-387 replace -f -'
Jun 13 03:35:17.849: INFO: stderr: ""
Jun 13 03:35:17.849: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 06/13/23 03:35:17.849
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jun 13 03:35:17.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-387 delete pods e2e-test-httpd-pod'
Jun 13 03:35:20.519: INFO: stderr: ""
Jun 13 03:35:20.519: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 03:35:20.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-387" for this suite. 06/13/23 03:35:20.539
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":275,"skipped":5090,"failed":0}
------------------------------
• [SLOW TEST] [9.801 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:35:10.763
    Jun 13 03:35:10.763: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 03:35:10.765
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:10.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:10.808
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/13/23 03:35:10.813
    Jun 13 03:35:10.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-387 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jun 13 03:35:10.930: INFO: stderr: ""
    Jun 13 03:35:10.930: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 06/13/23 03:35:10.93
    STEP: verifying the pod e2e-test-httpd-pod was created 06/13/23 03:35:15.983
    Jun 13 03:35:15.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-387 get pod e2e-test-httpd-pod -o json'
    Jun 13 03:35:16.154: INFO: stderr: ""
    Jun 13 03:35:16.154: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"e16cb4677b4a38c44bb6bbdc96b1dd6cf92e12df0616bebcc996b8b3a415fe91\",\n            \"cni.projectcalico.org/podIP\": \"172.16.172.31/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.16.172.31/32\"\n        },\n        \"creationTimestamp\": \"2023-06-13T03:35:10Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-387\",\n        \"resourceVersion\": \"42800\",\n        \"uid\": \"60442d93-a9f0-4a0b-8687-f143a7975ff5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-hnrsv\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"sks-test-v1-25-9-workergroup-2q6k2\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-hnrsv\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-13T03:35:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-13T03:35:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-13T03:35:12Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-13T03:35:10Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://03641cf6c808ccaf3588c70e01ccea376268f025135d5068d04a96a1c91fb269\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-13T03:35:11Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.255.64.103\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.172.31\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.172.31\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-13T03:35:10Z\"\n    }\n}\n"
    STEP: replace the image in the pod 06/13/23 03:35:16.154
    Jun 13 03:35:16.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-387 replace -f -'
    Jun 13 03:35:17.849: INFO: stderr: ""
    Jun 13 03:35:17.849: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 06/13/23 03:35:17.849
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jun 13 03:35:17.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-387 delete pods e2e-test-httpd-pod'
    Jun 13 03:35:20.519: INFO: stderr: ""
    Jun 13 03:35:20.519: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 03:35:20.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-387" for this suite. 06/13/23 03:35:20.539
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:35:20.565
Jun 13 03:35:20.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 03:35:20.566
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:20.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:20.618
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 06/13/23 03:35:20.625
Jun 13 03:35:20.626: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun 13 03:35:20.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 create -f -'
Jun 13 03:35:20.985: INFO: stderr: ""
Jun 13 03:35:20.985: INFO: stdout: "service/agnhost-replica created\n"
Jun 13 03:35:20.986: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun 13 03:35:20.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 create -f -'
Jun 13 03:35:21.336: INFO: stderr: ""
Jun 13 03:35:21.336: INFO: stdout: "service/agnhost-primary created\n"
Jun 13 03:35:21.336: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 13 03:35:21.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 create -f -'
Jun 13 03:35:21.659: INFO: stderr: ""
Jun 13 03:35:21.659: INFO: stdout: "service/frontend created\n"
Jun 13 03:35:21.659: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 13 03:35:21.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 create -f -'
Jun 13 03:35:23.241: INFO: stderr: ""
Jun 13 03:35:23.241: INFO: stdout: "deployment.apps/frontend created\n"
Jun 13 03:35:23.241: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 13 03:35:23.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 create -f -'
Jun 13 03:35:23.564: INFO: stderr: ""
Jun 13 03:35:23.564: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun 13 03:35:23.564: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 13 03:35:23.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 create -f -'
Jun 13 03:35:23.986: INFO: stderr: ""
Jun 13 03:35:23.986: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 06/13/23 03:35:23.986
Jun 13 03:35:23.988: INFO: Waiting for all frontend pods to be Running.
Jun 13 03:35:29.040: INFO: Waiting for frontend to serve content.
Jun 13 03:35:29.072: INFO: Trying to add a new entry to the guestbook.
Jun 13 03:35:29.104: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 06/13/23 03:35:29.132
Jun 13 03:35:29.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 delete --grace-period=0 --force -f -'
Jun 13 03:35:29.414: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 13 03:35:29.414: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 06/13/23 03:35:29.414
Jun 13 03:35:29.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 delete --grace-period=0 --force -f -'
Jun 13 03:35:29.614: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 13 03:35:29.614: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 06/13/23 03:35:29.614
Jun 13 03:35:29.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 delete --grace-period=0 --force -f -'
Jun 13 03:35:29.776: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 13 03:35:29.776: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 06/13/23 03:35:29.776
Jun 13 03:35:29.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 delete --grace-period=0 --force -f -'
Jun 13 03:35:29.915: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 13 03:35:29.915: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 06/13/23 03:35:29.915
Jun 13 03:35:29.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 delete --grace-period=0 --force -f -'
Jun 13 03:35:30.048: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 13 03:35:30.049: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 06/13/23 03:35:30.05
Jun 13 03:35:30.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 delete --grace-period=0 --force -f -'
Jun 13 03:35:30.196: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 13 03:35:30.197: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 03:35:30.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5126" for this suite. 06/13/23 03:35:30.213
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":276,"skipped":5091,"failed":0}
------------------------------
• [SLOW TEST] [9.677 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:35:20.565
    Jun 13 03:35:20.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 03:35:20.566
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:20.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:20.618
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 06/13/23 03:35:20.625
    Jun 13 03:35:20.626: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jun 13 03:35:20.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 create -f -'
    Jun 13 03:35:20.985: INFO: stderr: ""
    Jun 13 03:35:20.985: INFO: stdout: "service/agnhost-replica created\n"
    Jun 13 03:35:20.986: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jun 13 03:35:20.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 create -f -'
    Jun 13 03:35:21.336: INFO: stderr: ""
    Jun 13 03:35:21.336: INFO: stdout: "service/agnhost-primary created\n"
    Jun 13 03:35:21.336: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jun 13 03:35:21.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 create -f -'
    Jun 13 03:35:21.659: INFO: stderr: ""
    Jun 13 03:35:21.659: INFO: stdout: "service/frontend created\n"
    Jun 13 03:35:21.659: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jun 13 03:35:21.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 create -f -'
    Jun 13 03:35:23.241: INFO: stderr: ""
    Jun 13 03:35:23.241: INFO: stdout: "deployment.apps/frontend created\n"
    Jun 13 03:35:23.241: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jun 13 03:35:23.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 create -f -'
    Jun 13 03:35:23.564: INFO: stderr: ""
    Jun 13 03:35:23.564: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jun 13 03:35:23.564: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jun 13 03:35:23.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 create -f -'
    Jun 13 03:35:23.986: INFO: stderr: ""
    Jun 13 03:35:23.986: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 06/13/23 03:35:23.986
    Jun 13 03:35:23.988: INFO: Waiting for all frontend pods to be Running.
    Jun 13 03:35:29.040: INFO: Waiting for frontend to serve content.
    Jun 13 03:35:29.072: INFO: Trying to add a new entry to the guestbook.
    Jun 13 03:35:29.104: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 06/13/23 03:35:29.132
    Jun 13 03:35:29.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 delete --grace-period=0 --force -f -'
    Jun 13 03:35:29.414: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 13 03:35:29.414: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 06/13/23 03:35:29.414
    Jun 13 03:35:29.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 delete --grace-period=0 --force -f -'
    Jun 13 03:35:29.614: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 13 03:35:29.614: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 06/13/23 03:35:29.614
    Jun 13 03:35:29.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 delete --grace-period=0 --force -f -'
    Jun 13 03:35:29.776: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 13 03:35:29.776: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 06/13/23 03:35:29.776
    Jun 13 03:35:29.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 delete --grace-period=0 --force -f -'
    Jun 13 03:35:29.915: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 13 03:35:29.915: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 06/13/23 03:35:29.915
    Jun 13 03:35:29.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 delete --grace-period=0 --force -f -'
    Jun 13 03:35:30.048: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 13 03:35:30.049: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 06/13/23 03:35:30.05
    Jun 13 03:35:30.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-5126 delete --grace-period=0 --force -f -'
    Jun 13 03:35:30.196: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 13 03:35:30.197: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 03:35:30.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5126" for this suite. 06/13/23 03:35:30.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:35:30.246
Jun 13 03:35:30.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename var-expansion 06/13/23 03:35:30.247
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:30.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:30.641
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jun 13 03:35:30.977: INFO: Waiting up to 2m0s for pod "var-expansion-a7c233ae-c98d-42c2-a8e6-3315c76803a7" in namespace "var-expansion-855" to be "container 0 failed with reason CreateContainerConfigError"
Jun 13 03:35:31.072: INFO: Pod "var-expansion-a7c233ae-c98d-42c2-a8e6-3315c76803a7": Phase="Pending", Reason="", readiness=false. Elapsed: 95.407779ms
Jun 13 03:35:33.103: INFO: Pod "var-expansion-a7c233ae-c98d-42c2-a8e6-3315c76803a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.125950946s
Jun 13 03:35:33.103: INFO: Pod "var-expansion-a7c233ae-c98d-42c2-a8e6-3315c76803a7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jun 13 03:35:33.103: INFO: Deleting pod "var-expansion-a7c233ae-c98d-42c2-a8e6-3315c76803a7" in namespace "var-expansion-855"
Jun 13 03:35:33.136: INFO: Wait up to 5m0s for pod "var-expansion-a7c233ae-c98d-42c2-a8e6-3315c76803a7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 13 03:35:37.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-855" for this suite. 06/13/23 03:35:37.176
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":277,"skipped":5157,"failed":0}
------------------------------
• [SLOW TEST] [6.948 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:35:30.246
    Jun 13 03:35:30.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename var-expansion 06/13/23 03:35:30.247
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:30.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:30.641
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jun 13 03:35:30.977: INFO: Waiting up to 2m0s for pod "var-expansion-a7c233ae-c98d-42c2-a8e6-3315c76803a7" in namespace "var-expansion-855" to be "container 0 failed with reason CreateContainerConfigError"
    Jun 13 03:35:31.072: INFO: Pod "var-expansion-a7c233ae-c98d-42c2-a8e6-3315c76803a7": Phase="Pending", Reason="", readiness=false. Elapsed: 95.407779ms
    Jun 13 03:35:33.103: INFO: Pod "var-expansion-a7c233ae-c98d-42c2-a8e6-3315c76803a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.125950946s
    Jun 13 03:35:33.103: INFO: Pod "var-expansion-a7c233ae-c98d-42c2-a8e6-3315c76803a7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jun 13 03:35:33.103: INFO: Deleting pod "var-expansion-a7c233ae-c98d-42c2-a8e6-3315c76803a7" in namespace "var-expansion-855"
    Jun 13 03:35:33.136: INFO: Wait up to 5m0s for pod "var-expansion-a7c233ae-c98d-42c2-a8e6-3315c76803a7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 13 03:35:37.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-855" for this suite. 06/13/23 03:35:37.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:35:37.195
Jun 13 03:35:37.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 03:35:37.196
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:37.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:37.233
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-9cc5bc4e-0c93-451c-b28b-e9d4f9b43b9f 06/13/23 03:35:37.239
STEP: Creating a pod to test consume configMaps 06/13/23 03:35:37.247
Jun 13 03:35:37.265: INFO: Waiting up to 5m0s for pod "pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6" in namespace "configmap-7438" to be "Succeeded or Failed"
Jun 13 03:35:37.275: INFO: Pod "pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.629708ms
Jun 13 03:35:39.284: INFO: Pod "pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01889068s
Jun 13 03:35:41.285: INFO: Pod "pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019783079s
STEP: Saw pod success 06/13/23 03:35:41.285
Jun 13 03:35:41.285: INFO: Pod "pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6" satisfied condition "Succeeded or Failed"
Jun 13 03:35:41.292: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6 container agnhost-container: <nil>
STEP: delete the pod 06/13/23 03:35:41.308
Jun 13 03:35:41.482: INFO: Waiting for pod pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6 to disappear
Jun 13 03:35:41.489: INFO: Pod pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 03:35:41.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7438" for this suite. 06/13/23 03:35:41.498
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":278,"skipped":5170,"failed":0}
------------------------------
• [4.315 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:35:37.195
    Jun 13 03:35:37.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 03:35:37.196
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:37.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:37.233
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-9cc5bc4e-0c93-451c-b28b-e9d4f9b43b9f 06/13/23 03:35:37.239
    STEP: Creating a pod to test consume configMaps 06/13/23 03:35:37.247
    Jun 13 03:35:37.265: INFO: Waiting up to 5m0s for pod "pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6" in namespace "configmap-7438" to be "Succeeded or Failed"
    Jun 13 03:35:37.275: INFO: Pod "pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.629708ms
    Jun 13 03:35:39.284: INFO: Pod "pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01889068s
    Jun 13 03:35:41.285: INFO: Pod "pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019783079s
    STEP: Saw pod success 06/13/23 03:35:41.285
    Jun 13 03:35:41.285: INFO: Pod "pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6" satisfied condition "Succeeded or Failed"
    Jun 13 03:35:41.292: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6 container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 03:35:41.308
    Jun 13 03:35:41.482: INFO: Waiting for pod pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6 to disappear
    Jun 13 03:35:41.489: INFO: Pod pod-configmaps-fd8aa415-db9b-4600-b129-45c5ba0ce8f6 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 03:35:41.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7438" for this suite. 06/13/23 03:35:41.498
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:35:41.51
Jun 13 03:35:41.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pods 06/13/23 03:35:41.512
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:41.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:41.543
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 06/13/23 03:35:41.548
STEP: submitting the pod to kubernetes 06/13/23 03:35:41.548
Jun 13 03:35:41.568: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf" in namespace "pods-9317" to be "running and ready"
Jun 13 03:35:41.576: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.24646ms
Jun 13 03:35:41.576: INFO: The phase of Pod pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:35:43.583: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.015806339s
Jun 13 03:35:43.583: INFO: The phase of Pod pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf is Running (Ready = true)
Jun 13 03:35:43.583: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 06/13/23 03:35:43.589
STEP: updating the pod 06/13/23 03:35:43.595
Jun 13 03:35:44.128: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf"
Jun 13 03:35:44.128: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf" in namespace "pods-9317" to be "terminated with reason DeadlineExceeded"
Jun 13 03:35:44.137: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf": Phase="Running", Reason="", readiness=true. Elapsed: 8.944856ms
Jun 13 03:35:46.145: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.017103937s
Jun 13 03:35:48.155: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf": Phase="Running", Reason="", readiness=false. Elapsed: 4.027062694s
Jun 13 03:35:50.148: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.019734824s
Jun 13 03:35:50.148: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 13 03:35:50.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9317" for this suite. 06/13/23 03:35:50.156
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":279,"skipped":5170,"failed":0}
------------------------------
• [SLOW TEST] [8.661 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:35:41.51
    Jun 13 03:35:41.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pods 06/13/23 03:35:41.512
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:41.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:41.543
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 06/13/23 03:35:41.548
    STEP: submitting the pod to kubernetes 06/13/23 03:35:41.548
    Jun 13 03:35:41.568: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf" in namespace "pods-9317" to be "running and ready"
    Jun 13 03:35:41.576: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.24646ms
    Jun 13 03:35:41.576: INFO: The phase of Pod pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:35:43.583: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.015806339s
    Jun 13 03:35:43.583: INFO: The phase of Pod pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf is Running (Ready = true)
    Jun 13 03:35:43.583: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 06/13/23 03:35:43.589
    STEP: updating the pod 06/13/23 03:35:43.595
    Jun 13 03:35:44.128: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf"
    Jun 13 03:35:44.128: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf" in namespace "pods-9317" to be "terminated with reason DeadlineExceeded"
    Jun 13 03:35:44.137: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf": Phase="Running", Reason="", readiness=true. Elapsed: 8.944856ms
    Jun 13 03:35:46.145: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.017103937s
    Jun 13 03:35:48.155: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf": Phase="Running", Reason="", readiness=false. Elapsed: 4.027062694s
    Jun 13 03:35:50.148: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.019734824s
    Jun 13 03:35:50.148: INFO: Pod "pod-update-activedeadlineseconds-ea5bd081-fcc7-4da5-914c-69933ffb00bf" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 13 03:35:50.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9317" for this suite. 06/13/23 03:35:50.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:35:50.173
Jun 13 03:35:50.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:35:50.174
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:50.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:50.21
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-b5acff11-8bc1-4568-bdd2-6c86c2a61f81 06/13/23 03:35:50.217
STEP: Creating a pod to test consume secrets 06/13/23 03:35:50.257
Jun 13 03:35:50.275: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f" in namespace "projected-5313" to be "Succeeded or Failed"
Jun 13 03:35:50.369: INFO: Pod "pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f": Phase="Pending", Reason="", readiness=false. Elapsed: 94.370091ms
Jun 13 03:35:52.378: INFO: Pod "pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103019094s
Jun 13 03:35:54.380: INFO: Pod "pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.104701436s
STEP: Saw pod success 06/13/23 03:35:54.38
Jun 13 03:35:54.380: INFO: Pod "pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f" satisfied condition "Succeeded or Failed"
Jun 13 03:35:54.388: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f container projected-secret-volume-test: <nil>
STEP: delete the pod 06/13/23 03:35:54.405
Jun 13 03:35:54.444: INFO: Waiting for pod pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f to disappear
Jun 13 03:35:54.454: INFO: Pod pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 13 03:35:54.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5313" for this suite. 06/13/23 03:35:54.465
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":280,"skipped":5189,"failed":0}
------------------------------
• [4.341 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:35:50.173
    Jun 13 03:35:50.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:35:50.174
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:50.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:50.21
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-b5acff11-8bc1-4568-bdd2-6c86c2a61f81 06/13/23 03:35:50.217
    STEP: Creating a pod to test consume secrets 06/13/23 03:35:50.257
    Jun 13 03:35:50.275: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f" in namespace "projected-5313" to be "Succeeded or Failed"
    Jun 13 03:35:50.369: INFO: Pod "pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f": Phase="Pending", Reason="", readiness=false. Elapsed: 94.370091ms
    Jun 13 03:35:52.378: INFO: Pod "pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.103019094s
    Jun 13 03:35:54.380: INFO: Pod "pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.104701436s
    STEP: Saw pod success 06/13/23 03:35:54.38
    Jun 13 03:35:54.380: INFO: Pod "pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f" satisfied condition "Succeeded or Failed"
    Jun 13 03:35:54.388: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:35:54.405
    Jun 13 03:35:54.444: INFO: Waiting for pod pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f to disappear
    Jun 13 03:35:54.454: INFO: Pod pod-projected-secrets-46cf4d8e-f307-420d-b0e5-3a2f3ae4440f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 13 03:35:54.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5313" for this suite. 06/13/23 03:35:54.465
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:35:54.514
Jun 13 03:35:54.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename job 06/13/23 03:35:54.516
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:54.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:54.596
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 06/13/23 03:35:54.601
STEP: Ensure pods equal to paralellism count is attached to the job 06/13/23 03:35:54.632
STEP: patching /status 06/13/23 03:35:56.64
STEP: updating /status 06/13/23 03:35:56.656
STEP: get /status 06/13/23 03:35:56.678
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 13 03:35:56.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5025" for this suite. 06/13/23 03:35:56.693
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":281,"skipped":5196,"failed":0}
------------------------------
• [2.193 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:35:54.514
    Jun 13 03:35:54.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename job 06/13/23 03:35:54.516
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:54.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:54.596
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 06/13/23 03:35:54.601
    STEP: Ensure pods equal to paralellism count is attached to the job 06/13/23 03:35:54.632
    STEP: patching /status 06/13/23 03:35:56.64
    STEP: updating /status 06/13/23 03:35:56.656
    STEP: get /status 06/13/23 03:35:56.678
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 13 03:35:56.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5025" for this suite. 06/13/23 03:35:56.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:35:56.709
Jun 13 03:35:56.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:35:56.71
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:56.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:56.75
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 06/13/23 03:35:56.757
Jun 13 03:35:56.776: INFO: Waiting up to 5m0s for pod "downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2" in namespace "downward-api-838" to be "Succeeded or Failed"
Jun 13 03:35:56.784: INFO: Pod "downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.937282ms
Jun 13 03:35:58.793: INFO: Pod "downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016451985s
Jun 13 03:36:00.793: INFO: Pod "downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01667468s
STEP: Saw pod success 06/13/23 03:36:00.793
Jun 13 03:36:00.793: INFO: Pod "downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2" satisfied condition "Succeeded or Failed"
Jun 13 03:36:00.800: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2 container dapi-container: <nil>
STEP: delete the pod 06/13/23 03:36:00.814
Jun 13 03:36:00.845: INFO: Waiting for pod downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2 to disappear
Jun 13 03:36:00.852: INFO: Pod downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 13 03:36:00.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-838" for this suite. 06/13/23 03:36:00.862
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":282,"skipped":5209,"failed":0}
------------------------------
• [4.194 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:35:56.709
    Jun 13 03:35:56.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:35:56.71
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:35:56.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:35:56.75
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 06/13/23 03:35:56.757
    Jun 13 03:35:56.776: INFO: Waiting up to 5m0s for pod "downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2" in namespace "downward-api-838" to be "Succeeded or Failed"
    Jun 13 03:35:56.784: INFO: Pod "downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.937282ms
    Jun 13 03:35:58.793: INFO: Pod "downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016451985s
    Jun 13 03:36:00.793: INFO: Pod "downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01667468s
    STEP: Saw pod success 06/13/23 03:36:00.793
    Jun 13 03:36:00.793: INFO: Pod "downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2" satisfied condition "Succeeded or Failed"
    Jun 13 03:36:00.800: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2 container dapi-container: <nil>
    STEP: delete the pod 06/13/23 03:36:00.814
    Jun 13 03:36:00.845: INFO: Waiting for pod downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2 to disappear
    Jun 13 03:36:00.852: INFO: Pod downward-api-149669d6-aec7-4289-8f4f-5326e7c198f2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 13 03:36:00.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-838" for this suite. 06/13/23 03:36:00.862
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:36:00.903
Jun 13 03:36:00.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename job 06/13/23 03:36:00.904
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:36:01.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:36:01.033
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 06/13/23 03:36:01.039
STEP: Ensuring job reaches completions 06/13/23 03:36:01.056
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 13 03:36:13.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2297" for this suite. 06/13/23 03:36:13.088
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":283,"skipped":5213,"failed":0}
------------------------------
• [SLOW TEST] [12.202 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:36:00.903
    Jun 13 03:36:00.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename job 06/13/23 03:36:00.904
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:36:01.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:36:01.033
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 06/13/23 03:36:01.039
    STEP: Ensuring job reaches completions 06/13/23 03:36:01.056
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 13 03:36:13.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2297" for this suite. 06/13/23 03:36:13.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:36:13.105
Jun 13 03:36:13.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename dns 06/13/23 03:36:13.106
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:36:13.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:36:13.145
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 06/13/23 03:36:13.158
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5061.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local; sleep 1; done
 06/13/23 03:36:13.191
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5061.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local; sleep 1; done
 06/13/23 03:36:13.191
STEP: creating a pod to probe DNS 06/13/23 03:36:13.191
STEP: submitting the pod to kubernetes 06/13/23 03:36:13.192
Jun 13 03:36:13.212: INFO: Waiting up to 15m0s for pod "dns-test-29407806-0e74-41b9-bfff-61108862b131" in namespace "dns-5061" to be "running"
Jun 13 03:36:13.228: INFO: Pod "dns-test-29407806-0e74-41b9-bfff-61108862b131": Phase="Pending", Reason="", readiness=false. Elapsed: 15.616267ms
Jun 13 03:36:15.239: INFO: Pod "dns-test-29407806-0e74-41b9-bfff-61108862b131": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026645879s
Jun 13 03:36:17.238: INFO: Pod "dns-test-29407806-0e74-41b9-bfff-61108862b131": Phase="Running", Reason="", readiness=true. Elapsed: 4.025658938s
Jun 13 03:36:17.238: INFO: Pod "dns-test-29407806-0e74-41b9-bfff-61108862b131" satisfied condition "running"
STEP: retrieving the pod 06/13/23 03:36:17.238
STEP: looking for the results for each expected name from probers 06/13/23 03:36:17.246
Jun 13 03:36:17.271: INFO: DNS probes using dns-test-29407806-0e74-41b9-bfff-61108862b131 succeeded

STEP: deleting the pod 06/13/23 03:36:17.271
STEP: changing the externalName to bar.example.com 06/13/23 03:36:17.307
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5061.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local; sleep 1; done
 06/13/23 03:36:17.342
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5061.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local; sleep 1; done
 06/13/23 03:36:17.342
STEP: creating a second pod to probe DNS 06/13/23 03:36:17.342
STEP: submitting the pod to kubernetes 06/13/23 03:36:17.342
Jun 13 03:36:17.355: INFO: Waiting up to 15m0s for pod "dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3" in namespace "dns-5061" to be "running"
Jun 13 03:36:17.425: INFO: Pod "dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3": Phase="Pending", Reason="", readiness=false. Elapsed: 70.436714ms
Jun 13 03:36:19.434: INFO: Pod "dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079703487s
Jun 13 03:36:21.436: INFO: Pod "dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3": Phase="Running", Reason="", readiness=true. Elapsed: 4.081868479s
Jun 13 03:36:21.437: INFO: Pod "dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3" satisfied condition "running"
STEP: retrieving the pod 06/13/23 03:36:21.437
STEP: looking for the results for each expected name from probers 06/13/23 03:36:21.447
Jun 13 03:36:21.472: INFO: File wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 13 03:36:21.490: INFO: File jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 13 03:36:21.490: INFO: Lookups using dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 failed for: [wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local]

Jun 13 03:36:26.510: INFO: File wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 13 03:36:26.523: INFO: File jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 13 03:36:26.523: INFO: Lookups using dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 failed for: [wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local]

Jun 13 03:36:31.519: INFO: File wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 13 03:36:31.531: INFO: File jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 13 03:36:31.531: INFO: Lookups using dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 failed for: [wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local]

Jun 13 03:36:36.503: INFO: File wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 13 03:36:36.518: INFO: File jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 13 03:36:36.518: INFO: Lookups using dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 failed for: [wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local]

Jun 13 03:36:41.504: INFO: File wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 13 03:36:41.511: INFO: File jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 13 03:36:41.511: INFO: Lookups using dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 failed for: [wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local]

Jun 13 03:36:46.500: INFO: File wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 13 03:36:46.511: INFO: Lookups using dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 failed for: [wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local]

Jun 13 03:36:51.539: INFO: DNS probes using dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 succeeded

STEP: deleting the pod 06/13/23 03:36:51.539
STEP: changing the service to type=ClusterIP 06/13/23 03:36:51.738
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5061.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local; sleep 1; done
 06/13/23 03:36:52.185
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5061.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local; sleep 1; done
 06/13/23 03:36:52.185
STEP: creating a third pod to probe DNS 06/13/23 03:36:52.186
STEP: submitting the pod to kubernetes 06/13/23 03:36:52.194
Jun 13 03:36:52.356: INFO: Waiting up to 15m0s for pod "dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88" in namespace "dns-5061" to be "running"
Jun 13 03:36:52.595: INFO: Pod "dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88": Phase="Pending", Reason="", readiness=false. Elapsed: 238.877031ms
Jun 13 03:36:54.608: INFO: Pod "dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.252213746s
Jun 13 03:36:56.605: INFO: Pod "dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88": Phase="Running", Reason="", readiness=true. Elapsed: 4.249514994s
Jun 13 03:36:56.605: INFO: Pod "dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88" satisfied condition "running"
STEP: retrieving the pod 06/13/23 03:36:56.605
STEP: looking for the results for each expected name from probers 06/13/23 03:36:56.614
Jun 13 03:36:56.640: INFO: File jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88 contains '' instead of '10.101.255.68'
Jun 13 03:36:56.640: INFO: Lookups using dns-5061/dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88 failed for: [jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local]

Jun 13 03:37:01.655: INFO: DNS probes using dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88 succeeded

STEP: deleting the pod 06/13/23 03:37:01.655
STEP: deleting the test externalName service 06/13/23 03:37:01.675
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 13 03:37:01.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5061" for this suite. 06/13/23 03:37:01.756
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":284,"skipped":5227,"failed":0}
------------------------------
• [SLOW TEST] [48.687 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:36:13.105
    Jun 13 03:36:13.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename dns 06/13/23 03:36:13.106
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:36:13.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:36:13.145
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 06/13/23 03:36:13.158
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5061.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local; sleep 1; done
     06/13/23 03:36:13.191
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5061.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local; sleep 1; done
     06/13/23 03:36:13.191
    STEP: creating a pod to probe DNS 06/13/23 03:36:13.191
    STEP: submitting the pod to kubernetes 06/13/23 03:36:13.192
    Jun 13 03:36:13.212: INFO: Waiting up to 15m0s for pod "dns-test-29407806-0e74-41b9-bfff-61108862b131" in namespace "dns-5061" to be "running"
    Jun 13 03:36:13.228: INFO: Pod "dns-test-29407806-0e74-41b9-bfff-61108862b131": Phase="Pending", Reason="", readiness=false. Elapsed: 15.616267ms
    Jun 13 03:36:15.239: INFO: Pod "dns-test-29407806-0e74-41b9-bfff-61108862b131": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026645879s
    Jun 13 03:36:17.238: INFO: Pod "dns-test-29407806-0e74-41b9-bfff-61108862b131": Phase="Running", Reason="", readiness=true. Elapsed: 4.025658938s
    Jun 13 03:36:17.238: INFO: Pod "dns-test-29407806-0e74-41b9-bfff-61108862b131" satisfied condition "running"
    STEP: retrieving the pod 06/13/23 03:36:17.238
    STEP: looking for the results for each expected name from probers 06/13/23 03:36:17.246
    Jun 13 03:36:17.271: INFO: DNS probes using dns-test-29407806-0e74-41b9-bfff-61108862b131 succeeded

    STEP: deleting the pod 06/13/23 03:36:17.271
    STEP: changing the externalName to bar.example.com 06/13/23 03:36:17.307
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5061.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local; sleep 1; done
     06/13/23 03:36:17.342
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5061.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local; sleep 1; done
     06/13/23 03:36:17.342
    STEP: creating a second pod to probe DNS 06/13/23 03:36:17.342
    STEP: submitting the pod to kubernetes 06/13/23 03:36:17.342
    Jun 13 03:36:17.355: INFO: Waiting up to 15m0s for pod "dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3" in namespace "dns-5061" to be "running"
    Jun 13 03:36:17.425: INFO: Pod "dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3": Phase="Pending", Reason="", readiness=false. Elapsed: 70.436714ms
    Jun 13 03:36:19.434: INFO: Pod "dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.079703487s
    Jun 13 03:36:21.436: INFO: Pod "dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3": Phase="Running", Reason="", readiness=true. Elapsed: 4.081868479s
    Jun 13 03:36:21.437: INFO: Pod "dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3" satisfied condition "running"
    STEP: retrieving the pod 06/13/23 03:36:21.437
    STEP: looking for the results for each expected name from probers 06/13/23 03:36:21.447
    Jun 13 03:36:21.472: INFO: File wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 13 03:36:21.490: INFO: File jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 13 03:36:21.490: INFO: Lookups using dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 failed for: [wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local]

    Jun 13 03:36:26.510: INFO: File wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 13 03:36:26.523: INFO: File jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 13 03:36:26.523: INFO: Lookups using dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 failed for: [wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local]

    Jun 13 03:36:31.519: INFO: File wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 13 03:36:31.531: INFO: File jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 13 03:36:31.531: INFO: Lookups using dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 failed for: [wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local]

    Jun 13 03:36:36.503: INFO: File wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 13 03:36:36.518: INFO: File jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 13 03:36:36.518: INFO: Lookups using dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 failed for: [wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local]

    Jun 13 03:36:41.504: INFO: File wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 13 03:36:41.511: INFO: File jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 13 03:36:41.511: INFO: Lookups using dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 failed for: [wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local]

    Jun 13 03:36:46.500: INFO: File wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jun 13 03:36:46.511: INFO: Lookups using dns-5061/dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 failed for: [wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local]

    Jun 13 03:36:51.539: INFO: DNS probes using dns-test-f2a26d74-55bd-427f-8176-e146a1b111b3 succeeded

    STEP: deleting the pod 06/13/23 03:36:51.539
    STEP: changing the service to type=ClusterIP 06/13/23 03:36:51.738
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5061.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5061.svc.cluster.local; sleep 1; done
     06/13/23 03:36:52.185
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5061.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local; sleep 1; done
     06/13/23 03:36:52.185
    STEP: creating a third pod to probe DNS 06/13/23 03:36:52.186
    STEP: submitting the pod to kubernetes 06/13/23 03:36:52.194
    Jun 13 03:36:52.356: INFO: Waiting up to 15m0s for pod "dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88" in namespace "dns-5061" to be "running"
    Jun 13 03:36:52.595: INFO: Pod "dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88": Phase="Pending", Reason="", readiness=false. Elapsed: 238.877031ms
    Jun 13 03:36:54.608: INFO: Pod "dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.252213746s
    Jun 13 03:36:56.605: INFO: Pod "dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88": Phase="Running", Reason="", readiness=true. Elapsed: 4.249514994s
    Jun 13 03:36:56.605: INFO: Pod "dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88" satisfied condition "running"
    STEP: retrieving the pod 06/13/23 03:36:56.605
    STEP: looking for the results for each expected name from probers 06/13/23 03:36:56.614
    Jun 13 03:36:56.640: INFO: File jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local from pod  dns-5061/dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88 contains '' instead of '10.101.255.68'
    Jun 13 03:36:56.640: INFO: Lookups using dns-5061/dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88 failed for: [jessie_udp@dns-test-service-3.dns-5061.svc.cluster.local]

    Jun 13 03:37:01.655: INFO: DNS probes using dns-test-5d5cdb2b-fd63-4735-b485-90e1a145fd88 succeeded

    STEP: deleting the pod 06/13/23 03:37:01.655
    STEP: deleting the test externalName service 06/13/23 03:37:01.675
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 13 03:37:01.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5061" for this suite. 06/13/23 03:37:01.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:37:01.793
Jun 13 03:37:01.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 03:37:01.795
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:37:01.836
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:37:01.843
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 06/13/23 03:37:01.851
Jun 13 03:37:01.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 create -f -'
Jun 13 03:37:03.458: INFO: stderr: ""
Jun 13 03:37:03.458: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/13/23 03:37:03.458
Jun 13 03:37:03.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 13 03:37:03.586: INFO: stderr: ""
Jun 13 03:37:03.586: INFO: stdout: "update-demo-nautilus-bqdrw update-demo-nautilus-ns2b2 "
Jun 13 03:37:03.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods update-demo-nautilus-bqdrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:37:03.707: INFO: stderr: ""
Jun 13 03:37:03.707: INFO: stdout: ""
Jun 13 03:37:03.707: INFO: update-demo-nautilus-bqdrw is created but not running
Jun 13 03:37:08.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 13 03:37:08.824: INFO: stderr: ""
Jun 13 03:37:08.824: INFO: stdout: "update-demo-nautilus-bqdrw update-demo-nautilus-ns2b2 "
Jun 13 03:37:08.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods update-demo-nautilus-bqdrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:37:08.920: INFO: stderr: ""
Jun 13 03:37:08.920: INFO: stdout: "true"
Jun 13 03:37:08.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods update-demo-nautilus-bqdrw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 13 03:37:09.020: INFO: stderr: ""
Jun 13 03:37:09.020: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 13 03:37:09.020: INFO: validating pod update-demo-nautilus-bqdrw
Jun 13 03:37:09.030: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 13 03:37:09.030: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 13 03:37:09.030: INFO: update-demo-nautilus-bqdrw is verified up and running
Jun 13 03:37:09.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods update-demo-nautilus-ns2b2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 13 03:37:09.137: INFO: stderr: ""
Jun 13 03:37:09.137: INFO: stdout: "true"
Jun 13 03:37:09.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods update-demo-nautilus-ns2b2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 13 03:37:09.227: INFO: stderr: ""
Jun 13 03:37:09.227: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 13 03:37:09.227: INFO: validating pod update-demo-nautilus-ns2b2
Jun 13 03:37:09.237: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 13 03:37:09.237: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 13 03:37:09.237: INFO: update-demo-nautilus-ns2b2 is verified up and running
STEP: using delete to clean up resources 06/13/23 03:37:09.237
Jun 13 03:37:09.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 delete --grace-period=0 --force -f -'
Jun 13 03:37:09.340: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 13 03:37:09.340: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 13 03:37:09.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get rc,svc -l name=update-demo --no-headers'
Jun 13 03:37:09.460: INFO: stderr: "No resources found in kubectl-3652 namespace.\n"
Jun 13 03:37:09.460: INFO: stdout: ""
Jun 13 03:37:09.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 13 03:37:09.579: INFO: stderr: ""
Jun 13 03:37:09.579: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 03:37:09.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3652" for this suite. 06/13/23 03:37:09.591
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":285,"skipped":5237,"failed":0}
------------------------------
• [SLOW TEST] [7.814 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:37:01.793
    Jun 13 03:37:01.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 03:37:01.795
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:37:01.836
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:37:01.843
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 06/13/23 03:37:01.851
    Jun 13 03:37:01.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 create -f -'
    Jun 13 03:37:03.458: INFO: stderr: ""
    Jun 13 03:37:03.458: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/13/23 03:37:03.458
    Jun 13 03:37:03.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 13 03:37:03.586: INFO: stderr: ""
    Jun 13 03:37:03.586: INFO: stdout: "update-demo-nautilus-bqdrw update-demo-nautilus-ns2b2 "
    Jun 13 03:37:03.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods update-demo-nautilus-bqdrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:37:03.707: INFO: stderr: ""
    Jun 13 03:37:03.707: INFO: stdout: ""
    Jun 13 03:37:03.707: INFO: update-demo-nautilus-bqdrw is created but not running
    Jun 13 03:37:08.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 13 03:37:08.824: INFO: stderr: ""
    Jun 13 03:37:08.824: INFO: stdout: "update-demo-nautilus-bqdrw update-demo-nautilus-ns2b2 "
    Jun 13 03:37:08.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods update-demo-nautilus-bqdrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:37:08.920: INFO: stderr: ""
    Jun 13 03:37:08.920: INFO: stdout: "true"
    Jun 13 03:37:08.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods update-demo-nautilus-bqdrw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 13 03:37:09.020: INFO: stderr: ""
    Jun 13 03:37:09.020: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 13 03:37:09.020: INFO: validating pod update-demo-nautilus-bqdrw
    Jun 13 03:37:09.030: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 13 03:37:09.030: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 13 03:37:09.030: INFO: update-demo-nautilus-bqdrw is verified up and running
    Jun 13 03:37:09.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods update-demo-nautilus-ns2b2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 13 03:37:09.137: INFO: stderr: ""
    Jun 13 03:37:09.137: INFO: stdout: "true"
    Jun 13 03:37:09.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods update-demo-nautilus-ns2b2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 13 03:37:09.227: INFO: stderr: ""
    Jun 13 03:37:09.227: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 13 03:37:09.227: INFO: validating pod update-demo-nautilus-ns2b2
    Jun 13 03:37:09.237: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 13 03:37:09.237: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 13 03:37:09.237: INFO: update-demo-nautilus-ns2b2 is verified up and running
    STEP: using delete to clean up resources 06/13/23 03:37:09.237
    Jun 13 03:37:09.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 delete --grace-period=0 --force -f -'
    Jun 13 03:37:09.340: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 13 03:37:09.340: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jun 13 03:37:09.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get rc,svc -l name=update-demo --no-headers'
    Jun 13 03:37:09.460: INFO: stderr: "No resources found in kubectl-3652 namespace.\n"
    Jun 13 03:37:09.460: INFO: stdout: ""
    Jun 13 03:37:09.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3652 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun 13 03:37:09.579: INFO: stderr: ""
    Jun 13 03:37:09.579: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 03:37:09.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3652" for this suite. 06/13/23 03:37:09.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:37:09.608
Jun 13 03:37:09.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename deployment 06/13/23 03:37:09.609
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:37:09.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:37:09.65
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jun 13 03:37:09.685: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/13/23 03:37:09.685
Jun 13 03:37:09.686: INFO: Waiting up to 5m0s for pod "test-rollover-controller-mtv2b" in namespace "deployment-5901" to be "running"
Jun 13 03:37:09.695: INFO: Pod "test-rollover-controller-mtv2b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.858861ms
Jun 13 03:37:11.711: INFO: Pod "test-rollover-controller-mtv2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.025305706s
Jun 13 03:37:11.711: INFO: Pod "test-rollover-controller-mtv2b" satisfied condition "running"
Jun 13 03:37:11.711: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 13 03:37:13.732: INFO: Creating deployment "test-rollover-deployment"
Jun 13 03:37:13.752: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 13 03:37:15.778: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 13 03:37:15.799: INFO: Ensure that both replica sets have 1 created replica
Jun 13 03:37:15.818: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 13 03:37:15.843: INFO: Updating deployment test-rollover-deployment
Jun 13 03:37:15.843: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 13 03:37:17.862: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 13 03:37:17.884: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 13 03:37:17.899: INFO: all replica sets need to contain the pod-template-hash label
Jun 13 03:37:17.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:37:19.920: INFO: all replica sets need to contain the pod-template-hash label
Jun 13 03:37:19.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:37:21.931: INFO: all replica sets need to contain the pod-template-hash label
Jun 13 03:37:21.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:37:23.920: INFO: all replica sets need to contain the pod-template-hash label
Jun 13 03:37:23.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:37:25.920: INFO: all replica sets need to contain the pod-template-hash label
Jun 13 03:37:25.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:37:27.928: INFO: 
Jun 13 03:37:27.928: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:37:29.916: INFO: 
Jun 13 03:37:29.916: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 13 03:37:29.943: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-5901  43b0092e-80f5-4d48-8616-9f1846c7bbc5 44316 2 2023-06-13 03:37:13 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-13 03:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:37:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067c9d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-13 03:37:13 +0000 UTC,LastTransitionTime:2023-06-13 03:37:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-06-13 03:37:27 +0000 UTC,LastTransitionTime:2023-06-13 03:37:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 13 03:37:29.951: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-5901  55130cc4-b6da-4528-9f88-1d6c875376ff 44306 2 2023-06-13 03:37:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 43b0092e-80f5-4d48-8616-9f1846c7bbc5 0xc003f4bc37 0xc003f4bc38}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43b0092e-80f5-4d48-8616-9f1846c7bbc5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:37:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f4bcf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 13 03:37:29.951: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 13 03:37:29.951: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5901  23cf0279-a43b-4b6f-b0a6-34d2d6138a3a 44315 2 2023-06-13 03:37:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 43b0092e-80f5-4d48-8616-9f1846c7bbc5 0xc003f4b9e7 0xc003f4b9e8}] [] [{e2e.test Update apps/v1 2023-06-13 03:37:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:37:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43b0092e-80f5-4d48-8616-9f1846c7bbc5\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:37:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f4baa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 13 03:37:29.952: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-5901  04b590d1-579f-432c-8f84-2bb5ef78fc53 44242 2 2023-06-13 03:37:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 43b0092e-80f5-4d48-8616-9f1846c7bbc5 0xc003f4bb17 0xc003f4bb18}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43b0092e-80f5-4d48-8616-9f1846c7bbc5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:37:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f4bbc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 13 03:37:29.965: INFO: Pod "test-rollover-deployment-6d45fd857b-5jcjc" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-5jcjc test-rollover-deployment-6d45fd857b- deployment-5901  5f143a19-e9e9-4839-ad8a-d928b68cac13 44264 0 2023-06-13 03:37:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:b34870a518290626a6ba5d774fb77c057c8627a641d790032939590e9022571a cni.projectcalico.org/podIP:172.16.172.53/32 cni.projectcalico.org/podIPs:172.16.172.53/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 55130cc4-b6da-4528-9f88-1d6c875376ff 0xc0037dc267 0xc0037dc268}] [] [{kube-controller-manager Update v1 2023-06-13 03:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55130cc4-b6da-4528-9f88-1d6c875376ff\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:37:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k4cxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k4cxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:37:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:37:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.53,StartTime:2023-06-13 03:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:37:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://96d659d47d098655ec738005bd76f8cd40d94c65302e635b7f0cb487d5e3da47,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 13 03:37:29.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5901" for this suite. 06/13/23 03:37:29.98
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":286,"skipped":5244,"failed":0}
------------------------------
• [SLOW TEST] [20.391 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:37:09.608
    Jun 13 03:37:09.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename deployment 06/13/23 03:37:09.609
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:37:09.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:37:09.65
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jun 13 03:37:09.685: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/13/23 03:37:09.685
    Jun 13 03:37:09.686: INFO: Waiting up to 5m0s for pod "test-rollover-controller-mtv2b" in namespace "deployment-5901" to be "running"
    Jun 13 03:37:09.695: INFO: Pod "test-rollover-controller-mtv2b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.858861ms
    Jun 13 03:37:11.711: INFO: Pod "test-rollover-controller-mtv2b": Phase="Running", Reason="", readiness=true. Elapsed: 2.025305706s
    Jun 13 03:37:11.711: INFO: Pod "test-rollover-controller-mtv2b" satisfied condition "running"
    Jun 13 03:37:11.711: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jun 13 03:37:13.732: INFO: Creating deployment "test-rollover-deployment"
    Jun 13 03:37:13.752: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jun 13 03:37:15.778: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jun 13 03:37:15.799: INFO: Ensure that both replica sets have 1 created replica
    Jun 13 03:37:15.818: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jun 13 03:37:15.843: INFO: Updating deployment test-rollover-deployment
    Jun 13 03:37:15.843: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jun 13 03:37:17.862: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jun 13 03:37:17.884: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jun 13 03:37:17.899: INFO: all replica sets need to contain the pod-template-hash label
    Jun 13 03:37:17.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:37:19.920: INFO: all replica sets need to contain the pod-template-hash label
    Jun 13 03:37:19.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:37:21.931: INFO: all replica sets need to contain the pod-template-hash label
    Jun 13 03:37:21.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:37:23.920: INFO: all replica sets need to contain the pod-template-hash label
    Jun 13 03:37:23.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:37:25.920: INFO: all replica sets need to contain the pod-template-hash label
    Jun 13 03:37:25.920: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:37:27.928: INFO: 
    Jun 13 03:37:27.928: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 37, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 37, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:37:29.916: INFO: 
    Jun 13 03:37:29.916: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 13 03:37:29.943: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-5901  43b0092e-80f5-4d48-8616-9f1846c7bbc5 44316 2 2023-06-13 03:37:13 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-13 03:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:37:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0067c9d58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-13 03:37:13 +0000 UTC,LastTransitionTime:2023-06-13 03:37:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-06-13 03:37:27 +0000 UTC,LastTransitionTime:2023-06-13 03:37:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 13 03:37:29.951: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-5901  55130cc4-b6da-4528-9f88-1d6c875376ff 44306 2 2023-06-13 03:37:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 43b0092e-80f5-4d48-8616-9f1846c7bbc5 0xc003f4bc37 0xc003f4bc38}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43b0092e-80f5-4d48-8616-9f1846c7bbc5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:37:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f4bcf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 03:37:29.951: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jun 13 03:37:29.951: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-5901  23cf0279-a43b-4b6f-b0a6-34d2d6138a3a 44315 2 2023-06-13 03:37:09 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 43b0092e-80f5-4d48-8616-9f1846c7bbc5 0xc003f4b9e7 0xc003f4b9e8}] [] [{e2e.test Update apps/v1 2023-06-13 03:37:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:37:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43b0092e-80f5-4d48-8616-9f1846c7bbc5\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:37:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f4baa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 03:37:29.952: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-5901  04b590d1-579f-432c-8f84-2bb5ef78fc53 44242 2 2023-06-13 03:37:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 43b0092e-80f5-4d48-8616-9f1846c7bbc5 0xc003f4bb17 0xc003f4bb18}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43b0092e-80f5-4d48-8616-9f1846c7bbc5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:37:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f4bbc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 03:37:29.965: INFO: Pod "test-rollover-deployment-6d45fd857b-5jcjc" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-5jcjc test-rollover-deployment-6d45fd857b- deployment-5901  5f143a19-e9e9-4839-ad8a-d928b68cac13 44264 0 2023-06-13 03:37:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:b34870a518290626a6ba5d774fb77c057c8627a641d790032939590e9022571a cni.projectcalico.org/podIP:172.16.172.53/32 cni.projectcalico.org/podIPs:172.16.172.53/32] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 55130cc4-b6da-4528-9f88-1d6c875376ff 0xc0037dc267 0xc0037dc268}] [] [{kube-controller-manager Update v1 2023-06-13 03:37:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"55130cc4-b6da-4528-9f88-1d6c875376ff\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:37:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:37:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.53\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k4cxt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k4cxt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:37:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:37:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:37:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:37:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.53,StartTime:2023-06-13 03:37:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:37:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://96d659d47d098655ec738005bd76f8cd40d94c65302e635b7f0cb487d5e3da47,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.53,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 13 03:37:29.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5901" for this suite. 06/13/23 03:37:29.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:37:30.001
Jun 13 03:37:30.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 03:37:30.003
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:37:30.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:37:30.071
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 06/13/23 03:37:30.08
Jun 13 03:37:30.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 06/13/23 03:37:53.915
Jun 13 03:37:53.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:37:58.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:38:20.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5223" for this suite. 06/13/23 03:38:20.686
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":287,"skipped":5270,"failed":0}
------------------------------
• [SLOW TEST] [50.696 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:37:30.001
    Jun 13 03:37:30.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 03:37:30.003
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:37:30.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:37:30.071
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 06/13/23 03:37:30.08
    Jun 13 03:37:30.081: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 06/13/23 03:37:53.915
    Jun 13 03:37:53.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:37:58.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:38:20.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5223" for this suite. 06/13/23 03:38:20.686
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:38:20.698
Jun 13 03:38:20.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:38:20.7
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:38:20.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:38:20.733
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 06/13/23 03:38:20.741
Jun 13 03:38:20.766: INFO: Waiting up to 5m0s for pod "downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a" in namespace "downward-api-6453" to be "Succeeded or Failed"
Jun 13 03:38:20.774: INFO: Pod "downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.516016ms
Jun 13 03:38:22.787: INFO: Pod "downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021063968s
Jun 13 03:38:24.786: INFO: Pod "downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020028003s
STEP: Saw pod success 06/13/23 03:38:24.786
Jun 13 03:38:24.786: INFO: Pod "downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a" satisfied condition "Succeeded or Failed"
Jun 13 03:38:24.794: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a container dapi-container: <nil>
STEP: delete the pod 06/13/23 03:38:24.826
Jun 13 03:38:24.860: INFO: Waiting for pod downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a to disappear
Jun 13 03:38:24.871: INFO: Pod downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 13 03:38:24.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6453" for this suite. 06/13/23 03:38:24.884
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":288,"skipped":5270,"failed":0}
------------------------------
• [4.234 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:38:20.698
    Jun 13 03:38:20.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:38:20.7
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:38:20.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:38:20.733
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 06/13/23 03:38:20.741
    Jun 13 03:38:20.766: INFO: Waiting up to 5m0s for pod "downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a" in namespace "downward-api-6453" to be "Succeeded or Failed"
    Jun 13 03:38:20.774: INFO: Pod "downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.516016ms
    Jun 13 03:38:22.787: INFO: Pod "downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021063968s
    Jun 13 03:38:24.786: INFO: Pod "downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020028003s
    STEP: Saw pod success 06/13/23 03:38:24.786
    Jun 13 03:38:24.786: INFO: Pod "downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a" satisfied condition "Succeeded or Failed"
    Jun 13 03:38:24.794: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a container dapi-container: <nil>
    STEP: delete the pod 06/13/23 03:38:24.826
    Jun 13 03:38:24.860: INFO: Waiting for pod downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a to disappear
    Jun 13 03:38:24.871: INFO: Pod downward-api-274b46db-4573-4f0f-b851-5a7ec709d47a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 13 03:38:24.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6453" for this suite. 06/13/23 03:38:24.884
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:38:24.932
Jun 13 03:38:24.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename job 06/13/23 03:38:24.933
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:38:24.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:38:24.971
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 06/13/23 03:38:24.979
STEP: Ensuring active pods == parallelism 06/13/23 03:38:25.002
STEP: Orphaning one of the Job's Pods 06/13/23 03:38:29.024
Jun 13 03:38:29.579: INFO: Successfully updated pod "adopt-release-kwwbp"
STEP: Checking that the Job readopts the Pod 06/13/23 03:38:29.579
Jun 13 03:38:29.580: INFO: Waiting up to 15m0s for pod "adopt-release-kwwbp" in namespace "job-2125" to be "adopted"
Jun 13 03:38:29.610: INFO: Pod "adopt-release-kwwbp": Phase="Running", Reason="", readiness=true. Elapsed: 30.530994ms
Jun 13 03:38:31.621: INFO: Pod "adopt-release-kwwbp": Phase="Running", Reason="", readiness=true. Elapsed: 2.041561417s
Jun 13 03:38:31.621: INFO: Pod "adopt-release-kwwbp" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 06/13/23 03:38:31.621
Jun 13 03:38:32.159: INFO: Successfully updated pod "adopt-release-kwwbp"
STEP: Checking that the Job releases the Pod 06/13/23 03:38:32.159
Jun 13 03:38:32.159: INFO: Waiting up to 15m0s for pod "adopt-release-kwwbp" in namespace "job-2125" to be "released"
Jun 13 03:38:32.170: INFO: Pod "adopt-release-kwwbp": Phase="Running", Reason="", readiness=true. Elapsed: 11.107085ms
Jun 13 03:38:34.183: INFO: Pod "adopt-release-kwwbp": Phase="Running", Reason="", readiness=true. Elapsed: 2.023451049s
Jun 13 03:38:34.183: INFO: Pod "adopt-release-kwwbp" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 13 03:38:34.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2125" for this suite. 06/13/23 03:38:34.262
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":289,"skipped":5271,"failed":0}
------------------------------
• [SLOW TEST] [9.382 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:38:24.932
    Jun 13 03:38:24.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename job 06/13/23 03:38:24.933
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:38:24.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:38:24.971
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 06/13/23 03:38:24.979
    STEP: Ensuring active pods == parallelism 06/13/23 03:38:25.002
    STEP: Orphaning one of the Job's Pods 06/13/23 03:38:29.024
    Jun 13 03:38:29.579: INFO: Successfully updated pod "adopt-release-kwwbp"
    STEP: Checking that the Job readopts the Pod 06/13/23 03:38:29.579
    Jun 13 03:38:29.580: INFO: Waiting up to 15m0s for pod "adopt-release-kwwbp" in namespace "job-2125" to be "adopted"
    Jun 13 03:38:29.610: INFO: Pod "adopt-release-kwwbp": Phase="Running", Reason="", readiness=true. Elapsed: 30.530994ms
    Jun 13 03:38:31.621: INFO: Pod "adopt-release-kwwbp": Phase="Running", Reason="", readiness=true. Elapsed: 2.041561417s
    Jun 13 03:38:31.621: INFO: Pod "adopt-release-kwwbp" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 06/13/23 03:38:31.621
    Jun 13 03:38:32.159: INFO: Successfully updated pod "adopt-release-kwwbp"
    STEP: Checking that the Job releases the Pod 06/13/23 03:38:32.159
    Jun 13 03:38:32.159: INFO: Waiting up to 15m0s for pod "adopt-release-kwwbp" in namespace "job-2125" to be "released"
    Jun 13 03:38:32.170: INFO: Pod "adopt-release-kwwbp": Phase="Running", Reason="", readiness=true. Elapsed: 11.107085ms
    Jun 13 03:38:34.183: INFO: Pod "adopt-release-kwwbp": Phase="Running", Reason="", readiness=true. Elapsed: 2.023451049s
    Jun 13 03:38:34.183: INFO: Pod "adopt-release-kwwbp" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 13 03:38:34.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2125" for this suite. 06/13/23 03:38:34.262
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:38:34.324
Jun 13 03:38:34.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename replicaset 06/13/23 03:38:34.326
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:38:34.539
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:38:34.545
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 06/13/23 03:38:34.561
STEP: Verify that the required pods have come up. 06/13/23 03:38:34.646
Jun 13 03:38:34.655: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 13 03:38:39.665: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/13/23 03:38:39.665
STEP: Getting /status 06/13/23 03:38:39.665
Jun 13 03:38:39.674: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 06/13/23 03:38:39.674
Jun 13 03:38:39.822: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 06/13/23 03:38:39.822
Jun 13 03:38:39.827: INFO: Observed &ReplicaSet event: ADDED
Jun 13 03:38:39.827: INFO: Observed &ReplicaSet event: MODIFIED
Jun 13 03:38:39.827: INFO: Observed &ReplicaSet event: MODIFIED
Jun 13 03:38:39.827: INFO: Observed &ReplicaSet event: MODIFIED
Jun 13 03:38:39.827: INFO: Found replicaset test-rs in namespace replicaset-3226 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 13 03:38:39.827: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 06/13/23 03:38:39.827
Jun 13 03:38:39.828: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 13 03:38:39.842: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 06/13/23 03:38:39.842
Jun 13 03:38:39.846: INFO: Observed &ReplicaSet event: ADDED
Jun 13 03:38:39.846: INFO: Observed &ReplicaSet event: MODIFIED
Jun 13 03:38:39.846: INFO: Observed &ReplicaSet event: MODIFIED
Jun 13 03:38:39.847: INFO: Observed &ReplicaSet event: MODIFIED
Jun 13 03:38:39.847: INFO: Observed replicaset test-rs in namespace replicaset-3226 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 13 03:38:39.847: INFO: Observed &ReplicaSet event: MODIFIED
Jun 13 03:38:39.847: INFO: Found replicaset test-rs in namespace replicaset-3226 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jun 13 03:38:39.847: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 13 03:38:39.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3226" for this suite. 06/13/23 03:38:39.857
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":290,"skipped":5359,"failed":0}
------------------------------
• [SLOW TEST] [5.559 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:38:34.324
    Jun 13 03:38:34.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename replicaset 06/13/23 03:38:34.326
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:38:34.539
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:38:34.545
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 06/13/23 03:38:34.561
    STEP: Verify that the required pods have come up. 06/13/23 03:38:34.646
    Jun 13 03:38:34.655: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 13 03:38:39.665: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/13/23 03:38:39.665
    STEP: Getting /status 06/13/23 03:38:39.665
    Jun 13 03:38:39.674: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 06/13/23 03:38:39.674
    Jun 13 03:38:39.822: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 06/13/23 03:38:39.822
    Jun 13 03:38:39.827: INFO: Observed &ReplicaSet event: ADDED
    Jun 13 03:38:39.827: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 13 03:38:39.827: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 13 03:38:39.827: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 13 03:38:39.827: INFO: Found replicaset test-rs in namespace replicaset-3226 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 13 03:38:39.827: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 06/13/23 03:38:39.827
    Jun 13 03:38:39.828: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun 13 03:38:39.842: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 06/13/23 03:38:39.842
    Jun 13 03:38:39.846: INFO: Observed &ReplicaSet event: ADDED
    Jun 13 03:38:39.846: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 13 03:38:39.846: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 13 03:38:39.847: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 13 03:38:39.847: INFO: Observed replicaset test-rs in namespace replicaset-3226 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 13 03:38:39.847: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 13 03:38:39.847: INFO: Found replicaset test-rs in namespace replicaset-3226 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jun 13 03:38:39.847: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 13 03:38:39.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3226" for this suite. 06/13/23 03:38:39.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:38:39.886
Jun 13 03:38:39.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename container-probe 06/13/23 03:38:39.887
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:38:39.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:38:39.933
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86 in namespace container-probe-2385 06/13/23 03:38:39.94
Jun 13 03:38:39.957: INFO: Waiting up to 5m0s for pod "test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86" in namespace "container-probe-2385" to be "not pending"
Jun 13 03:38:39.972: INFO: Pod "test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86": Phase="Pending", Reason="", readiness=false. Elapsed: 14.452007ms
Jun 13 03:38:41.982: INFO: Pod "test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86": Phase="Running", Reason="", readiness=true. Elapsed: 2.02461589s
Jun 13 03:38:41.982: INFO: Pod "test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86" satisfied condition "not pending"
Jun 13 03:38:41.982: INFO: Started pod test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86 in namespace container-probe-2385
STEP: checking the pod's current state and verifying that restartCount is present 06/13/23 03:38:41.982
Jun 13 03:38:41.990: INFO: Initial restart count of pod test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86 is 0
STEP: deleting the pod 06/13/23 03:42:42.492
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 13 03:42:42.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2385" for this suite. 06/13/23 03:42:42.575
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":291,"skipped":5407,"failed":0}
------------------------------
• [SLOW TEST] [242.733 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:38:39.886
    Jun 13 03:38:39.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename container-probe 06/13/23 03:38:39.887
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:38:39.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:38:39.933
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86 in namespace container-probe-2385 06/13/23 03:38:39.94
    Jun 13 03:38:39.957: INFO: Waiting up to 5m0s for pod "test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86" in namespace "container-probe-2385" to be "not pending"
    Jun 13 03:38:39.972: INFO: Pod "test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86": Phase="Pending", Reason="", readiness=false. Elapsed: 14.452007ms
    Jun 13 03:38:41.982: INFO: Pod "test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86": Phase="Running", Reason="", readiness=true. Elapsed: 2.02461589s
    Jun 13 03:38:41.982: INFO: Pod "test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86" satisfied condition "not pending"
    Jun 13 03:38:41.982: INFO: Started pod test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86 in namespace container-probe-2385
    STEP: checking the pod's current state and verifying that restartCount is present 06/13/23 03:38:41.982
    Jun 13 03:38:41.990: INFO: Initial restart count of pod test-webserver-d000d835-3268-4c5e-b2ed-615d3d410d86 is 0
    STEP: deleting the pod 06/13/23 03:42:42.492
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 13 03:42:42.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2385" for this suite. 06/13/23 03:42:42.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:42:42.62
Jun 13 03:42:42.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename runtimeclass 06/13/23 03:42:42.622
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:42:42.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:42:42.763
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-5902-delete-me 06/13/23 03:42:42.813
STEP: Waiting for the RuntimeClass to disappear 06/13/23 03:42:42.856
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 13 03:42:42.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5902" for this suite. 06/13/23 03:42:42.899
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":292,"skipped":5444,"failed":0}
------------------------------
• [0.313 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:42:42.62
    Jun 13 03:42:42.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename runtimeclass 06/13/23 03:42:42.622
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:42:42.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:42:42.763
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-5902-delete-me 06/13/23 03:42:42.813
    STEP: Waiting for the RuntimeClass to disappear 06/13/23 03:42:42.856
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 13 03:42:42.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5902" for this suite. 06/13/23 03:42:42.899
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:42:42.934
Jun 13 03:42:42.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 03:42:42.935
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:42:42.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:42:42.995
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 13 03:42:43.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-139" for this suite. 06/13/23 03:42:43.205
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":293,"skipped":5446,"failed":0}
------------------------------
• [0.312 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:42:42.934
    Jun 13 03:42:42.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 03:42:42.935
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:42:42.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:42:42.995
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 03:42:43.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-139" for this suite. 06/13/23 03:42:43.205
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:42:43.246
Jun 13 03:42:43.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename disruption 06/13/23 03:42:43.247
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:42:43.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:42:43.296
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 06/13/23 03:42:43.317
STEP: Updating PodDisruptionBudget status 06/13/23 03:42:43.34
STEP: Waiting for all pods to be running 06/13/23 03:42:43.359
Jun 13 03:42:43.371: INFO: running pods: 0 < 1
STEP: locating a running pod 06/13/23 03:42:45.384
STEP: Waiting for the pdb to be processed 06/13/23 03:42:45.443
STEP: Patching PodDisruptionBudget status 06/13/23 03:42:45.469
STEP: Waiting for the pdb to be processed 06/13/23 03:42:45.506
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 13 03:42:45.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8200" for this suite. 06/13/23 03:42:45.551
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":294,"skipped":5449,"failed":0}
------------------------------
• [2.338 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:42:43.246
    Jun 13 03:42:43.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename disruption 06/13/23 03:42:43.247
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:42:43.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:42:43.296
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 06/13/23 03:42:43.317
    STEP: Updating PodDisruptionBudget status 06/13/23 03:42:43.34
    STEP: Waiting for all pods to be running 06/13/23 03:42:43.359
    Jun 13 03:42:43.371: INFO: running pods: 0 < 1
    STEP: locating a running pod 06/13/23 03:42:45.384
    STEP: Waiting for the pdb to be processed 06/13/23 03:42:45.443
    STEP: Patching PodDisruptionBudget status 06/13/23 03:42:45.469
    STEP: Waiting for the pdb to be processed 06/13/23 03:42:45.506
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 13 03:42:45.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8200" for this suite. 06/13/23 03:42:45.551
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:42:45.584
Jun 13 03:42:45.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 03:42:45.586
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:42:45.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:42:45.651
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jun 13 03:42:45.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/13/23 03:42:53.249
Jun 13 03:42:53.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-3334 --namespace=crd-publish-openapi-3334 create -f -'
Jun 13 03:42:54.705: INFO: stderr: ""
Jun 13 03:42:54.705: INFO: stdout: "e2e-test-crd-publish-openapi-7236-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 13 03:42:54.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-3334 --namespace=crd-publish-openapi-3334 delete e2e-test-crd-publish-openapi-7236-crds test-cr'
Jun 13 03:42:54.830: INFO: stderr: ""
Jun 13 03:42:54.830: INFO: stdout: "e2e-test-crd-publish-openapi-7236-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 13 03:42:54.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-3334 --namespace=crd-publish-openapi-3334 apply -f -'
Jun 13 03:42:56.145: INFO: stderr: ""
Jun 13 03:42:56.145: INFO: stdout: "e2e-test-crd-publish-openapi-7236-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 13 03:42:56.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-3334 --namespace=crd-publish-openapi-3334 delete e2e-test-crd-publish-openapi-7236-crds test-cr'
Jun 13 03:42:56.377: INFO: stderr: ""
Jun 13 03:42:56.377: INFO: stdout: "e2e-test-crd-publish-openapi-7236-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 06/13/23 03:42:56.377
Jun 13 03:42:56.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-3334 explain e2e-test-crd-publish-openapi-7236-crds'
Jun 13 03:42:56.785: INFO: stderr: ""
Jun 13 03:42:56.785: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7236-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:43:04.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3334" for this suite. 06/13/23 03:43:04.378
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":295,"skipped":5452,"failed":0}
------------------------------
• [SLOW TEST] [18.816 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:42:45.584
    Jun 13 03:42:45.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 03:42:45.586
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:42:45.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:42:45.651
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jun 13 03:42:45.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/13/23 03:42:53.249
    Jun 13 03:42:53.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-3334 --namespace=crd-publish-openapi-3334 create -f -'
    Jun 13 03:42:54.705: INFO: stderr: ""
    Jun 13 03:42:54.705: INFO: stdout: "e2e-test-crd-publish-openapi-7236-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jun 13 03:42:54.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-3334 --namespace=crd-publish-openapi-3334 delete e2e-test-crd-publish-openapi-7236-crds test-cr'
    Jun 13 03:42:54.830: INFO: stderr: ""
    Jun 13 03:42:54.830: INFO: stdout: "e2e-test-crd-publish-openapi-7236-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jun 13 03:42:54.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-3334 --namespace=crd-publish-openapi-3334 apply -f -'
    Jun 13 03:42:56.145: INFO: stderr: ""
    Jun 13 03:42:56.145: INFO: stdout: "e2e-test-crd-publish-openapi-7236-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jun 13 03:42:56.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-3334 --namespace=crd-publish-openapi-3334 delete e2e-test-crd-publish-openapi-7236-crds test-cr'
    Jun 13 03:42:56.377: INFO: stderr: ""
    Jun 13 03:42:56.377: INFO: stdout: "e2e-test-crd-publish-openapi-7236-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 06/13/23 03:42:56.377
    Jun 13 03:42:56.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-3334 explain e2e-test-crd-publish-openapi-7236-crds'
    Jun 13 03:42:56.785: INFO: stderr: ""
    Jun 13 03:42:56.785: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7236-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:43:04.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3334" for this suite. 06/13/23 03:43:04.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:43:04.401
Jun 13 03:43:04.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:43:04.403
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:43:04.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:43:04.462
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-e8d5a895-e807-4d18-8f86-a7a286777ba4 06/13/23 03:43:04.471
STEP: Creating a pod to test consume configMaps 06/13/23 03:43:04.498
Jun 13 03:43:04.546: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3" in namespace "projected-4733" to be "Succeeded or Failed"
Jun 13 03:43:04.580: INFO: Pod "pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 33.811439ms
Jun 13 03:43:06.592: INFO: Pod "pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04586337s
Jun 13 03:43:08.591: INFO: Pod "pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044718324s
STEP: Saw pod success 06/13/23 03:43:08.591
Jun 13 03:43:08.591: INFO: Pod "pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3" satisfied condition "Succeeded or Failed"
Jun 13 03:43:08.600: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3 container agnhost-container: <nil>
STEP: delete the pod 06/13/23 03:43:08.643
Jun 13 03:43:08.683: INFO: Waiting for pod pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3 to disappear
Jun 13 03:43:08.693: INFO: Pod pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 13 03:43:08.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4733" for this suite. 06/13/23 03:43:08.707
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":296,"skipped":5457,"failed":0}
------------------------------
• [4.333 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:43:04.401
    Jun 13 03:43:04.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:43:04.403
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:43:04.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:43:04.462
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-e8d5a895-e807-4d18-8f86-a7a286777ba4 06/13/23 03:43:04.471
    STEP: Creating a pod to test consume configMaps 06/13/23 03:43:04.498
    Jun 13 03:43:04.546: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3" in namespace "projected-4733" to be "Succeeded or Failed"
    Jun 13 03:43:04.580: INFO: Pod "pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 33.811439ms
    Jun 13 03:43:06.592: INFO: Pod "pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04586337s
    Jun 13 03:43:08.591: INFO: Pod "pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044718324s
    STEP: Saw pod success 06/13/23 03:43:08.591
    Jun 13 03:43:08.591: INFO: Pod "pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3" satisfied condition "Succeeded or Failed"
    Jun 13 03:43:08.600: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3 container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 03:43:08.643
    Jun 13 03:43:08.683: INFO: Waiting for pod pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3 to disappear
    Jun 13 03:43:08.693: INFO: Pod pod-projected-configmaps-495b4441-f827-4623-9937-d2cdbb0f3fd3 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 13 03:43:08.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4733" for this suite. 06/13/23 03:43:08.707
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:43:08.735
Jun 13 03:43:08.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename job 06/13/23 03:43:08.736
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:43:08.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:43:08.816
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 06/13/23 03:43:08.837
STEP: Patching the Job 06/13/23 03:43:08.857
STEP: Watching for Job to be patched 06/13/23 03:43:08.923
Jun 13 03:43:08.928: INFO: Event ADDED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking:]
Jun 13 03:43:08.928: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking:]
Jun 13 03:43:08.928: INFO: Event MODIFIED found for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 06/13/23 03:43:08.928
STEP: Watching for Job to be updated 06/13/23 03:43:08.978
Jun 13 03:43:08.984: INFO: Event MODIFIED found for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 13 03:43:08.984: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 06/13/23 03:43:08.984
Jun 13 03:43:09.016: INFO: Job: e2e-gfrtc as labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc]
STEP: Waiting for job to complete 06/13/23 03:43:09.016
STEP: Delete a job collection with a labelselector 06/13/23 03:43:21.028
STEP: Watching for Job to be deleted 06/13/23 03:43:21.059
Jun 13 03:43:21.069: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 13 03:43:21.069: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 13 03:43:21.069: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 13 03:43:21.069: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 13 03:43:21.069: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 13 03:43:21.069: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 13 03:43:21.072: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 13 03:43:21.072: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 13 03:43:21.072: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 13 03:43:21.072: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 13 03:43:21.072: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 13 03:43:21.072: INFO: Event DELETED found for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 06/13/23 03:43:21.072
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 13 03:43:21.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5602" for this suite. 06/13/23 03:43:21.173
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":297,"skipped":5467,"failed":0}
------------------------------
• [SLOW TEST] [12.464 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:43:08.735
    Jun 13 03:43:08.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename job 06/13/23 03:43:08.736
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:43:08.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:43:08.816
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 06/13/23 03:43:08.837
    STEP: Patching the Job 06/13/23 03:43:08.857
    STEP: Watching for Job to be patched 06/13/23 03:43:08.923
    Jun 13 03:43:08.928: INFO: Event ADDED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jun 13 03:43:08.928: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jun 13 03:43:08.928: INFO: Event MODIFIED found for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 06/13/23 03:43:08.928
    STEP: Watching for Job to be updated 06/13/23 03:43:08.978
    Jun 13 03:43:08.984: INFO: Event MODIFIED found for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 13 03:43:08.984: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 06/13/23 03:43:08.984
    Jun 13 03:43:09.016: INFO: Job: e2e-gfrtc as labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc]
    STEP: Waiting for job to complete 06/13/23 03:43:09.016
    STEP: Delete a job collection with a labelselector 06/13/23 03:43:21.028
    STEP: Watching for Job to be deleted 06/13/23 03:43:21.059
    Jun 13 03:43:21.069: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 13 03:43:21.069: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 13 03:43:21.069: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 13 03:43:21.069: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 13 03:43:21.069: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 13 03:43:21.069: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 13 03:43:21.072: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 13 03:43:21.072: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 13 03:43:21.072: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 13 03:43:21.072: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 13 03:43:21.072: INFO: Event MODIFIED observed for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 13 03:43:21.072: INFO: Event DELETED found for Job e2e-gfrtc in namespace job-5602 with labels: map[e2e-gfrtc:patched e2e-job-label:e2e-gfrtc] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 06/13/23 03:43:21.072
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 13 03:43:21.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5602" for this suite. 06/13/23 03:43:21.173
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:43:21.2
Jun 13 03:43:21.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 03:43:21.202
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:43:21.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:43:21.282
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 06/13/23 03:43:21.3
Jun 13 03:43:21.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:43:28.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:43:55.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7050" for this suite. 06/13/23 03:43:55.731
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":298,"skipped":5485,"failed":0}
------------------------------
• [SLOW TEST] [34.565 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:43:21.2
    Jun 13 03:43:21.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 03:43:21.202
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:43:21.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:43:21.282
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 06/13/23 03:43:21.3
    Jun 13 03:43:21.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:43:28.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:43:55.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7050" for this suite. 06/13/23 03:43:55.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:43:55.768
Jun 13 03:43:55.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename replication-controller 06/13/23 03:43:55.771
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:43:55.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:43:55.896
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 06/13/23 03:43:55.903
STEP: When the matched label of one of its pods change 06/13/23 03:43:55.982
Jun 13 03:43:56.007: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 06/13/23 03:43:56.068
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 13 03:43:57.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4494" for this suite. 06/13/23 03:43:57.118
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":299,"skipped":5530,"failed":0}
------------------------------
• [1.406 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:43:55.768
    Jun 13 03:43:55.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename replication-controller 06/13/23 03:43:55.771
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:43:55.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:43:55.896
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 06/13/23 03:43:55.903
    STEP: When the matched label of one of its pods change 06/13/23 03:43:55.982
    Jun 13 03:43:56.007: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 06/13/23 03:43:56.068
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 13 03:43:57.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4494" for this suite. 06/13/23 03:43:57.118
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:43:57.175
Jun 13 03:43:57.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename gc 06/13/23 03:43:57.176
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:43:57.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:43:57.249
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 06/13/23 03:43:57.279
STEP: delete the rc 06/13/23 03:44:02.525
STEP: wait for the rc to be deleted 06/13/23 03:44:02.588
Jun 13 03:44:03.802: INFO: 80 pods remaining
Jun 13 03:44:03.802: INFO: 80 pods has nil DeletionTimestamp
Jun 13 03:44:03.802: INFO: 
Jun 13 03:44:05.143: INFO: 66 pods remaining
Jun 13 03:44:05.143: INFO: 61 pods has nil DeletionTimestamp
Jun 13 03:44:05.144: INFO: 
Jun 13 03:44:05.684: INFO: 57 pods remaining
Jun 13 03:44:05.684: INFO: 57 pods has nil DeletionTimestamp
Jun 13 03:44:05.684: INFO: 
Jun 13 03:44:06.815: INFO: 42 pods remaining
Jun 13 03:44:06.815: INFO: 42 pods has nil DeletionTimestamp
Jun 13 03:44:06.815: INFO: 
Jun 13 03:44:07.926: INFO: 30 pods remaining
Jun 13 03:44:07.926: INFO: 30 pods has nil DeletionTimestamp
Jun 13 03:44:07.926: INFO: 
Jun 13 03:44:08.795: INFO: 10 pods remaining
Jun 13 03:44:08.795: INFO: 10 pods has nil DeletionTimestamp
Jun 13 03:44:08.795: INFO: 
Jun 13 03:44:09.718: INFO: 0 pods remaining
Jun 13 03:44:09.718: INFO: 0 pods has nil DeletionTimestamp
Jun 13 03:44:09.718: INFO: 
STEP: Gathering metrics 06/13/23 03:44:10.698
Jun 13 03:44:10.933: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" in namespace "kube-system" to be "running and ready"
Jun 13 03:44:10.978: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq": Phase="Running", Reason="", readiness=true. Elapsed: 44.857869ms
Jun 13 03:44:10.978: INFO: The phase of Pod kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq is Running (Ready = true)
Jun 13 03:44:10.978: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" satisfied condition "running and ready"
Jun 13 03:44:11.355: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 13 03:44:11.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1626" for this suite. 06/13/23 03:44:11.41
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":300,"skipped":5530,"failed":0}
------------------------------
• [SLOW TEST] [14.585 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:43:57.175
    Jun 13 03:43:57.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename gc 06/13/23 03:43:57.176
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:43:57.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:43:57.249
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 06/13/23 03:43:57.279
    STEP: delete the rc 06/13/23 03:44:02.525
    STEP: wait for the rc to be deleted 06/13/23 03:44:02.588
    Jun 13 03:44:03.802: INFO: 80 pods remaining
    Jun 13 03:44:03.802: INFO: 80 pods has nil DeletionTimestamp
    Jun 13 03:44:03.802: INFO: 
    Jun 13 03:44:05.143: INFO: 66 pods remaining
    Jun 13 03:44:05.143: INFO: 61 pods has nil DeletionTimestamp
    Jun 13 03:44:05.144: INFO: 
    Jun 13 03:44:05.684: INFO: 57 pods remaining
    Jun 13 03:44:05.684: INFO: 57 pods has nil DeletionTimestamp
    Jun 13 03:44:05.684: INFO: 
    Jun 13 03:44:06.815: INFO: 42 pods remaining
    Jun 13 03:44:06.815: INFO: 42 pods has nil DeletionTimestamp
    Jun 13 03:44:06.815: INFO: 
    Jun 13 03:44:07.926: INFO: 30 pods remaining
    Jun 13 03:44:07.926: INFO: 30 pods has nil DeletionTimestamp
    Jun 13 03:44:07.926: INFO: 
    Jun 13 03:44:08.795: INFO: 10 pods remaining
    Jun 13 03:44:08.795: INFO: 10 pods has nil DeletionTimestamp
    Jun 13 03:44:08.795: INFO: 
    Jun 13 03:44:09.718: INFO: 0 pods remaining
    Jun 13 03:44:09.718: INFO: 0 pods has nil DeletionTimestamp
    Jun 13 03:44:09.718: INFO: 
    STEP: Gathering metrics 06/13/23 03:44:10.698
    Jun 13 03:44:10.933: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" in namespace "kube-system" to be "running and ready"
    Jun 13 03:44:10.978: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq": Phase="Running", Reason="", readiness=true. Elapsed: 44.857869ms
    Jun 13 03:44:10.978: INFO: The phase of Pod kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq is Running (Ready = true)
    Jun 13 03:44:10.978: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" satisfied condition "running and ready"
    Jun 13 03:44:11.355: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 13 03:44:11.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1626" for this suite. 06/13/23 03:44:11.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:44:11.761
Jun 13 03:44:11.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename sched-preemption 06/13/23 03:44:11.763
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:44:12.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:44:12.149
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 13 03:44:12.250: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 13 03:45:12.411: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:45:12.445
Jun 13 03:45:12.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename sched-preemption-path 06/13/23 03:45:12.447
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:12.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:12.562
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 06/13/23 03:45:12.581
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/13/23 03:45:12.581
Jun 13 03:45:12.787: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2856" to be "running"
Jun 13 03:45:12.818: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 30.925701ms
Jun 13 03:45:14.830: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.042776505s
Jun 13 03:45:14.830: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/13/23 03:45:14.84
Jun 13 03:45:14.960: INFO: found a healthy node: sks-test-v1-25-9-workergroup-2q6k2
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jun 13 03:45:23.236: INFO: pods created so far: [1 1 1]
Jun 13 03:45:23.236: INFO: length of pods created so far: 3
Jun 13 03:45:25.259: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jun 13 03:45:32.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-2856" for this suite. 06/13/23 03:45:32.283
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:45:32.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6987" for this suite. 06/13/23 03:45:32.733
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":301,"skipped":5551,"failed":0}
------------------------------
• [SLOW TEST] [81.428 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:44:11.761
    Jun 13 03:44:11.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename sched-preemption 06/13/23 03:44:11.763
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:44:12.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:44:12.149
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jun 13 03:44:12.250: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 13 03:45:12.411: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:45:12.445
    Jun 13 03:45:12.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename sched-preemption-path 06/13/23 03:45:12.447
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:12.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:12.562
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 06/13/23 03:45:12.581
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/13/23 03:45:12.581
    Jun 13 03:45:12.787: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-2856" to be "running"
    Jun 13 03:45:12.818: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 30.925701ms
    Jun 13 03:45:14.830: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.042776505s
    Jun 13 03:45:14.830: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/13/23 03:45:14.84
    Jun 13 03:45:14.960: INFO: found a healthy node: sks-test-v1-25-9-workergroup-2q6k2
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jun 13 03:45:23.236: INFO: pods created so far: [1 1 1]
    Jun 13 03:45:23.236: INFO: length of pods created so far: 3
    Jun 13 03:45:25.259: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jun 13 03:45:32.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-2856" for this suite. 06/13/23 03:45:32.283
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:45:32.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6987" for this suite. 06/13/23 03:45:32.733
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:45:33.192
Jun 13 03:45:33.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 03:45:33.195
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:33.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:33.295
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 06/13/23 03:45:33.302
Jun 13 03:45:33.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 create -f -'
Jun 13 03:45:34.544: INFO: stderr: ""
Jun 13 03:45:34.544: INFO: stdout: "pod/pause created\n"
Jun 13 03:45:34.545: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 13 03:45:34.545: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2856" to be "running and ready"
Jun 13 03:45:34.568: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 23.389329ms
Jun 13 03:45:34.568: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'sks-test-v1-25-9-workergroup-469fm' to be 'Running' but was 'Pending'
Jun 13 03:45:36.625: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080110737s
Jun 13 03:45:36.625: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'sks-test-v1-25-9-workergroup-469fm' to be 'Running' but was 'Pending'
Jun 13 03:45:38.588: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.043442675s
Jun 13 03:45:38.588: INFO: Pod "pause" satisfied condition "running and ready"
Jun 13 03:45:38.588: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 06/13/23 03:45:38.588
Jun 13 03:45:38.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 label pods pause testing-label=testing-label-value'
Jun 13 03:45:38.755: INFO: stderr: ""
Jun 13 03:45:38.755: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 06/13/23 03:45:38.755
Jun 13 03:45:38.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 get pod pause -L testing-label'
Jun 13 03:45:38.933: INFO: stderr: ""
Jun 13 03:45:38.933: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod 06/13/23 03:45:38.934
Jun 13 03:45:38.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 label pods pause testing-label-'
Jun 13 03:45:39.076: INFO: stderr: ""
Jun 13 03:45:39.076: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 06/13/23 03:45:39.076
Jun 13 03:45:39.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 get pod pause -L testing-label'
Jun 13 03:45:39.212: INFO: stderr: ""
Jun 13 03:45:39.212: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 06/13/23 03:45:39.212
Jun 13 03:45:39.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
Jun 13 03:45:39.338: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 13 03:45:39.338: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 13 03:45:39.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 get rc,svc -l name=pause --no-headers'
Jun 13 03:45:39.476: INFO: stderr: "No resources found in kubectl-2856 namespace.\n"
Jun 13 03:45:39.476: INFO: stdout: ""
Jun 13 03:45:39.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 13 03:45:39.575: INFO: stderr: ""
Jun 13 03:45:39.575: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 03:45:39.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2856" for this suite. 06/13/23 03:45:39.587
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":302,"skipped":5583,"failed":0}
------------------------------
• [SLOW TEST] [6.547 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:45:33.192
    Jun 13 03:45:33.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 03:45:33.195
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:33.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:33.295
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 06/13/23 03:45:33.302
    Jun 13 03:45:33.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 create -f -'
    Jun 13 03:45:34.544: INFO: stderr: ""
    Jun 13 03:45:34.544: INFO: stdout: "pod/pause created\n"
    Jun 13 03:45:34.545: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jun 13 03:45:34.545: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2856" to be "running and ready"
    Jun 13 03:45:34.568: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 23.389329ms
    Jun 13 03:45:34.568: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'sks-test-v1-25-9-workergroup-469fm' to be 'Running' but was 'Pending'
    Jun 13 03:45:36.625: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080110737s
    Jun 13 03:45:36.625: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'sks-test-v1-25-9-workergroup-469fm' to be 'Running' but was 'Pending'
    Jun 13 03:45:38.588: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.043442675s
    Jun 13 03:45:38.588: INFO: Pod "pause" satisfied condition "running and ready"
    Jun 13 03:45:38.588: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 06/13/23 03:45:38.588
    Jun 13 03:45:38.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 label pods pause testing-label=testing-label-value'
    Jun 13 03:45:38.755: INFO: stderr: ""
    Jun 13 03:45:38.755: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 06/13/23 03:45:38.755
    Jun 13 03:45:38.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 get pod pause -L testing-label'
    Jun 13 03:45:38.933: INFO: stderr: ""
    Jun 13 03:45:38.933: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 06/13/23 03:45:38.934
    Jun 13 03:45:38.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 label pods pause testing-label-'
    Jun 13 03:45:39.076: INFO: stderr: ""
    Jun 13 03:45:39.076: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 06/13/23 03:45:39.076
    Jun 13 03:45:39.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 get pod pause -L testing-label'
    Jun 13 03:45:39.212: INFO: stderr: ""
    Jun 13 03:45:39.212: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 06/13/23 03:45:39.212
    Jun 13 03:45:39.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 delete --grace-period=0 --force -f -'
    Jun 13 03:45:39.338: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 13 03:45:39.338: INFO: stdout: "pod \"pause\" force deleted\n"
    Jun 13 03:45:39.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 get rc,svc -l name=pause --no-headers'
    Jun 13 03:45:39.476: INFO: stderr: "No resources found in kubectl-2856 namespace.\n"
    Jun 13 03:45:39.476: INFO: stdout: ""
    Jun 13 03:45:39.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-2856 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun 13 03:45:39.575: INFO: stderr: ""
    Jun 13 03:45:39.575: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 03:45:39.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2856" for this suite. 06/13/23 03:45:39.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:45:39.739
Jun 13 03:45:39.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename svcaccounts 06/13/23 03:45:39.742
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:39.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:39.821
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jun 13 03:45:39.898: INFO: created pod pod-service-account-defaultsa
Jun 13 03:45:39.898: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 13 03:45:39.931: INFO: created pod pod-service-account-mountsa
Jun 13 03:45:39.932: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 13 03:45:39.966: INFO: created pod pod-service-account-nomountsa
Jun 13 03:45:39.966: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 13 03:45:39.984: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 13 03:45:39.984: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 13 03:45:40.019: INFO: created pod pod-service-account-mountsa-mountspec
Jun 13 03:45:40.019: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 13 03:45:40.140: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 13 03:45:40.140: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 13 03:45:40.170: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 13 03:45:40.170: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 13 03:45:40.319: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 13 03:45:40.319: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 13 03:45:40.445: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 13 03:45:40.445: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 13 03:45:40.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5137" for this suite. 06/13/23 03:45:40.588
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":303,"skipped":5591,"failed":0}
------------------------------
• [0.894 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:45:39.739
    Jun 13 03:45:39.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename svcaccounts 06/13/23 03:45:39.742
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:39.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:39.821
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jun 13 03:45:39.898: INFO: created pod pod-service-account-defaultsa
    Jun 13 03:45:39.898: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jun 13 03:45:39.931: INFO: created pod pod-service-account-mountsa
    Jun 13 03:45:39.932: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jun 13 03:45:39.966: INFO: created pod pod-service-account-nomountsa
    Jun 13 03:45:39.966: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jun 13 03:45:39.984: INFO: created pod pod-service-account-defaultsa-mountspec
    Jun 13 03:45:39.984: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jun 13 03:45:40.019: INFO: created pod pod-service-account-mountsa-mountspec
    Jun 13 03:45:40.019: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jun 13 03:45:40.140: INFO: created pod pod-service-account-nomountsa-mountspec
    Jun 13 03:45:40.140: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jun 13 03:45:40.170: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jun 13 03:45:40.170: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jun 13 03:45:40.319: INFO: created pod pod-service-account-mountsa-nomountspec
    Jun 13 03:45:40.319: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jun 13 03:45:40.445: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jun 13 03:45:40.445: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 13 03:45:40.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5137" for this suite. 06/13/23 03:45:40.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:45:40.635
Jun 13 03:45:40.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:45:40.637
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:40.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:40.718
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-39c3bb52-f482-4996-aeb0-0574ffd3b2f4 06/13/23 03:45:40.729
STEP: Creating a pod to test consume configMaps 06/13/23 03:45:40.765
Jun 13 03:45:40.828: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90" in namespace "projected-2843" to be "Succeeded or Failed"
Jun 13 03:45:40.844: INFO: Pod "pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90": Phase="Pending", Reason="", readiness=false. Elapsed: 15.456228ms
Jun 13 03:45:42.928: INFO: Pod "pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100041425s
Jun 13 03:45:44.863: INFO: Pod "pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90": Phase="Running", Reason="", readiness=false. Elapsed: 4.034952555s
Jun 13 03:45:46.918: INFO: Pod "pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089514251s
STEP: Saw pod success 06/13/23 03:45:46.918
Jun 13 03:45:46.918: INFO: Pod "pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90" satisfied condition "Succeeded or Failed"
Jun 13 03:45:46.935: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90 container projected-configmap-volume-test: <nil>
STEP: delete the pod 06/13/23 03:45:47.059
Jun 13 03:45:47.140: INFO: Waiting for pod pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90 to disappear
Jun 13 03:45:47.163: INFO: Pod pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 13 03:45:47.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2843" for this suite. 06/13/23 03:45:47.184
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":304,"skipped":5602,"failed":0}
------------------------------
• [SLOW TEST] [6.598 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:45:40.635
    Jun 13 03:45:40.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:45:40.637
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:40.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:40.718
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-39c3bb52-f482-4996-aeb0-0574ffd3b2f4 06/13/23 03:45:40.729
    STEP: Creating a pod to test consume configMaps 06/13/23 03:45:40.765
    Jun 13 03:45:40.828: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90" in namespace "projected-2843" to be "Succeeded or Failed"
    Jun 13 03:45:40.844: INFO: Pod "pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90": Phase="Pending", Reason="", readiness=false. Elapsed: 15.456228ms
    Jun 13 03:45:42.928: INFO: Pod "pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.100041425s
    Jun 13 03:45:44.863: INFO: Pod "pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90": Phase="Running", Reason="", readiness=false. Elapsed: 4.034952555s
    Jun 13 03:45:46.918: INFO: Pod "pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.089514251s
    STEP: Saw pod success 06/13/23 03:45:46.918
    Jun 13 03:45:46.918: INFO: Pod "pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90" satisfied condition "Succeeded or Failed"
    Jun 13 03:45:46.935: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:45:47.059
    Jun 13 03:45:47.140: INFO: Waiting for pod pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90 to disappear
    Jun 13 03:45:47.163: INFO: Pod pod-projected-configmaps-96db9e89-dace-4170-9367-39e98b0b8c90 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 13 03:45:47.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2843" for this suite. 06/13/23 03:45:47.184
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:45:47.234
Jun 13 03:45:47.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubectl 06/13/23 03:45:47.235
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:47.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:47.344
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/13/23 03:45:47.354
Jun 13 03:45:47.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3874 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jun 13 03:45:47.487: INFO: stderr: ""
Jun 13 03:45:47.487: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 06/13/23 03:45:47.487
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jun 13 03:45:47.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3874 delete pods e2e-test-httpd-pod'
Jun 13 03:45:51.242: INFO: stderr: ""
Jun 13 03:45:51.243: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 13 03:45:51.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3874" for this suite. 06/13/23 03:45:51.292
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":305,"skipped":5603,"failed":0}
------------------------------
• [4.168 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:45:47.234
    Jun 13 03:45:47.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubectl 06/13/23 03:45:47.235
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:47.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:47.344
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/13/23 03:45:47.354
    Jun 13 03:45:47.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3874 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jun 13 03:45:47.487: INFO: stderr: ""
    Jun 13 03:45:47.487: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 06/13/23 03:45:47.487
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jun 13 03:45:47.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=kubectl-3874 delete pods e2e-test-httpd-pod'
    Jun 13 03:45:51.242: INFO: stderr: ""
    Jun 13 03:45:51.243: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 13 03:45:51.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3874" for this suite. 06/13/23 03:45:51.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:45:51.403
Jun 13 03:45:51.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename projected 06/13/23 03:45:51.404
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:51.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:51.638
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-77bee96f-a17b-4218-aa86-2c6d9c24e0f6 06/13/23 03:45:51.649
STEP: Creating a pod to test consume secrets 06/13/23 03:45:51.673
Jun 13 03:45:51.715: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108" in namespace "projected-5463" to be "Succeeded or Failed"
Jun 13 03:45:51.727: INFO: Pod "pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108": Phase="Pending", Reason="", readiness=false. Elapsed: 12.037208ms
Jun 13 03:45:53.744: INFO: Pod "pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028959837s
Jun 13 03:45:55.740: INFO: Pod "pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025100736s
Jun 13 03:45:57.779: INFO: Pod "pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064318087s
STEP: Saw pod success 06/13/23 03:45:57.779
Jun 13 03:45:57.780: INFO: Pod "pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108" satisfied condition "Succeeded or Failed"
Jun 13 03:45:57.842: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/13/23 03:45:57.891
Jun 13 03:45:57.951: INFO: Waiting for pod pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108 to disappear
Jun 13 03:45:57.964: INFO: Pod pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 13 03:45:57.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5463" for this suite. 06/13/23 03:45:57.995
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":306,"skipped":5620,"failed":0}
------------------------------
• [SLOW TEST] [6.614 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:45:51.403
    Jun 13 03:45:51.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename projected 06/13/23 03:45:51.404
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:51.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:51.638
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-77bee96f-a17b-4218-aa86-2c6d9c24e0f6 06/13/23 03:45:51.649
    STEP: Creating a pod to test consume secrets 06/13/23 03:45:51.673
    Jun 13 03:45:51.715: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108" in namespace "projected-5463" to be "Succeeded or Failed"
    Jun 13 03:45:51.727: INFO: Pod "pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108": Phase="Pending", Reason="", readiness=false. Elapsed: 12.037208ms
    Jun 13 03:45:53.744: INFO: Pod "pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028959837s
    Jun 13 03:45:55.740: INFO: Pod "pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025100736s
    Jun 13 03:45:57.779: INFO: Pod "pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064318087s
    STEP: Saw pod success 06/13/23 03:45:57.779
    Jun 13 03:45:57.780: INFO: Pod "pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108" satisfied condition "Succeeded or Failed"
    Jun 13 03:45:57.842: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:45:57.891
    Jun 13 03:45:57.951: INFO: Waiting for pod pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108 to disappear
    Jun 13 03:45:57.964: INFO: Pod pod-projected-secrets-85109a20-237b-4784-a2c4-49dcbde90108 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 13 03:45:57.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5463" for this suite. 06/13/23 03:45:57.995
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:45:58.019
Jun 13 03:45:58.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename taint-multiple-pods 06/13/23 03:45:58.022
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:58.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:58.112
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jun 13 03:45:58.131: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 13 03:46:58.266: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jun 13 03:46:58.292: INFO: Starting informer...
STEP: Starting pods... 06/13/23 03:46:58.292
Jun 13 03:46:58.586: INFO: Pod1 is running on sks-test-v1-25-9-workergroup-2q6k2. Tainting Node
Jun 13 03:46:58.865: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5020" to be "running"
Jun 13 03:46:58.879: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.891177ms
Jun 13 03:47:00.896: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.031199609s
Jun 13 03:47:00.896: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jun 13 03:47:00.896: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5020" to be "running"
Jun 13 03:47:00.910: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 13.618239ms
Jun 13 03:47:00.910: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jun 13 03:47:00.910: INFO: Pod2 is running on sks-test-v1-25-9-workergroup-2q6k2. Tainting Node
STEP: Trying to apply a taint on the Node 06/13/23 03:47:00.91
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/13/23 03:47:00.987
STEP: Waiting for Pod1 and Pod2 to be deleted 06/13/23 03:47:01.003
Jun 13 03:47:07.205: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 13 03:47:27.378: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/13/23 03:47:27.419
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:47:27.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5020" for this suite. 06/13/23 03:47:27.458
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":307,"skipped":5648,"failed":0}
------------------------------
• [SLOW TEST] [89.473 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:45:58.019
    Jun 13 03:45:58.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename taint-multiple-pods 06/13/23 03:45:58.022
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:45:58.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:45:58.112
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jun 13 03:45:58.131: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 13 03:46:58.266: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jun 13 03:46:58.292: INFO: Starting informer...
    STEP: Starting pods... 06/13/23 03:46:58.292
    Jun 13 03:46:58.586: INFO: Pod1 is running on sks-test-v1-25-9-workergroup-2q6k2. Tainting Node
    Jun 13 03:46:58.865: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-5020" to be "running"
    Jun 13 03:46:58.879: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.891177ms
    Jun 13 03:47:00.896: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.031199609s
    Jun 13 03:47:00.896: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jun 13 03:47:00.896: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-5020" to be "running"
    Jun 13 03:47:00.910: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 13.618239ms
    Jun 13 03:47:00.910: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jun 13 03:47:00.910: INFO: Pod2 is running on sks-test-v1-25-9-workergroup-2q6k2. Tainting Node
    STEP: Trying to apply a taint on the Node 06/13/23 03:47:00.91
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/13/23 03:47:00.987
    STEP: Waiting for Pod1 and Pod2 to be deleted 06/13/23 03:47:01.003
    Jun 13 03:47:07.205: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jun 13 03:47:27.378: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/13/23 03:47:27.419
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:47:27.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-5020" for this suite. 06/13/23 03:47:27.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:47:27.493
Jun 13 03:47:27.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename podtemplate 06/13/23 03:47:27.495
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:47:27.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:47:27.599
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 06/13/23 03:47:27.61
Jun 13 03:47:27.627: INFO: created test-podtemplate-1
Jun 13 03:47:27.648: INFO: created test-podtemplate-2
Jun 13 03:47:27.670: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 06/13/23 03:47:27.67
STEP: delete collection of pod templates 06/13/23 03:47:27.681
Jun 13 03:47:27.682: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 06/13/23 03:47:27.741
Jun 13 03:47:27.741: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jun 13 03:47:27.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9040" for this suite. 06/13/23 03:47:27.769
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":308,"skipped":5656,"failed":0}
------------------------------
• [0.297 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:47:27.493
    Jun 13 03:47:27.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename podtemplate 06/13/23 03:47:27.495
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:47:27.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:47:27.599
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 06/13/23 03:47:27.61
    Jun 13 03:47:27.627: INFO: created test-podtemplate-1
    Jun 13 03:47:27.648: INFO: created test-podtemplate-2
    Jun 13 03:47:27.670: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 06/13/23 03:47:27.67
    STEP: delete collection of pod templates 06/13/23 03:47:27.681
    Jun 13 03:47:27.682: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 06/13/23 03:47:27.741
    Jun 13 03:47:27.741: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jun 13 03:47:27.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9040" for this suite. 06/13/23 03:47:27.769
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:47:27.791
Jun 13 03:47:27.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pod-network-test 06/13/23 03:47:27.792
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:47:27.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:47:27.868
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-7086 06/13/23 03:47:27.874
STEP: creating a selector 06/13/23 03:47:27.875
STEP: Creating the service pods in kubernetes 06/13/23 03:47:27.875
Jun 13 03:47:27.875: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 13 03:47:28.025: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7086" to be "running and ready"
Jun 13 03:47:28.127: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 102.165522ms
Jun 13 03:47:28.127: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:47:30.141: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.116310863s
Jun 13 03:47:30.141: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:47:32.140: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.115691215s
Jun 13 03:47:32.140: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:47:34.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.112640407s
Jun 13 03:47:34.138: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:47:36.136: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.110950604s
Jun 13 03:47:36.136: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:47:38.140: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.115382158s
Jun 13 03:47:38.140: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:47:40.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.112129272s
Jun 13 03:47:40.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:47:42.139: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.113794275s
Jun 13 03:47:42.139: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:47:44.144: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.118789758s
Jun 13 03:47:44.144: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:47:46.141: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.116281021s
Jun 13 03:47:46.141: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:47:48.144: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.119299726s
Jun 13 03:47:48.144: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:47:50.146: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.120926698s
Jun 13 03:47:50.146: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 13 03:47:50.146: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 13 03:47:50.156: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7086" to be "running and ready"
Jun 13 03:47:50.172: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 15.805322ms
Jun 13 03:47:50.172: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 13 03:47:50.172: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 13 03:47:50.189: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7086" to be "running and ready"
Jun 13 03:47:50.205: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 15.937438ms
Jun 13 03:47:50.205: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 13 03:47:50.205: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/13/23 03:47:50.227
Jun 13 03:47:50.248: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7086" to be "running"
Jun 13 03:47:50.267: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.440253ms
Jun 13 03:47:52.293: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.04535318s
Jun 13 03:47:52.293: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 13 03:47:52.320: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 13 03:47:52.320: INFO: Breadth first check of 172.16.172.10 on host 10.255.64.103...
Jun 13 03:47:52.351: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.77.132:9080/dial?request=hostname&protocol=http&host=172.16.172.10&port=8083&tries=1'] Namespace:pod-network-test-7086 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:47:52.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:47:52.354: INFO: ExecWithOptions: Clientset creation
Jun 13 03:47:52.354: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7086/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.77.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.172.10%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 13 03:47:52.560: INFO: Waiting for responses: map[]
Jun 13 03:47:52.560: INFO: reached 172.16.172.10 after 0/1 tries
Jun 13 03:47:52.560: INFO: Breadth first check of 172.30.77.134 on host 10.255.64.102...
Jun 13 03:47:52.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.77.132:9080/dial?request=hostname&protocol=http&host=172.30.77.134&port=8083&tries=1'] Namespace:pod-network-test-7086 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:47:52.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:47:52.573: INFO: ExecWithOptions: Clientset creation
Jun 13 03:47:52.573: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7086/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.77.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.77.134%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 13 03:47:52.739: INFO: Waiting for responses: map[]
Jun 13 03:47:52.739: INFO: reached 172.30.77.134 after 0/1 tries
Jun 13 03:47:52.739: INFO: Breadth first check of 172.28.156.246 on host 10.255.64.104...
Jun 13 03:47:52.760: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.77.132:9080/dial?request=hostname&protocol=http&host=172.28.156.246&port=8083&tries=1'] Namespace:pod-network-test-7086 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:47:52.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:47:52.762: INFO: ExecWithOptions: Clientset creation
Jun 13 03:47:52.762: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7086/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.77.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.28.156.246%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 13 03:47:52.929: INFO: Waiting for responses: map[]
Jun 13 03:47:52.929: INFO: reached 172.28.156.246 after 0/1 tries
Jun 13 03:47:52.929: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jun 13 03:47:52.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7086" for this suite. 06/13/23 03:47:52.96
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":309,"skipped":5658,"failed":0}
------------------------------
• [SLOW TEST] [25.208 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:47:27.791
    Jun 13 03:47:27.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pod-network-test 06/13/23 03:47:27.792
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:47:27.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:47:27.868
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-7086 06/13/23 03:47:27.874
    STEP: creating a selector 06/13/23 03:47:27.875
    STEP: Creating the service pods in kubernetes 06/13/23 03:47:27.875
    Jun 13 03:47:27.875: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun 13 03:47:28.025: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7086" to be "running and ready"
    Jun 13 03:47:28.127: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 102.165522ms
    Jun 13 03:47:28.127: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:47:30.141: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.116310863s
    Jun 13 03:47:30.141: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:47:32.140: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.115691215s
    Jun 13 03:47:32.140: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:47:34.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.112640407s
    Jun 13 03:47:34.138: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:47:36.136: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.110950604s
    Jun 13 03:47:36.136: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:47:38.140: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.115382158s
    Jun 13 03:47:38.140: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:47:40.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.112129272s
    Jun 13 03:47:40.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:47:42.139: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.113794275s
    Jun 13 03:47:42.139: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:47:44.144: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.118789758s
    Jun 13 03:47:44.144: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:47:46.141: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.116281021s
    Jun 13 03:47:46.141: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:47:48.144: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.119299726s
    Jun 13 03:47:48.144: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:47:50.146: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.120926698s
    Jun 13 03:47:50.146: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 13 03:47:50.146: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 13 03:47:50.156: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7086" to be "running and ready"
    Jun 13 03:47:50.172: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 15.805322ms
    Jun 13 03:47:50.172: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 13 03:47:50.172: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 13 03:47:50.189: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7086" to be "running and ready"
    Jun 13 03:47:50.205: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 15.937438ms
    Jun 13 03:47:50.205: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 13 03:47:50.205: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/13/23 03:47:50.227
    Jun 13 03:47:50.248: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7086" to be "running"
    Jun 13 03:47:50.267: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.440253ms
    Jun 13 03:47:52.293: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.04535318s
    Jun 13 03:47:52.293: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 13 03:47:52.320: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 13 03:47:52.320: INFO: Breadth first check of 172.16.172.10 on host 10.255.64.103...
    Jun 13 03:47:52.351: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.77.132:9080/dial?request=hostname&protocol=http&host=172.16.172.10&port=8083&tries=1'] Namespace:pod-network-test-7086 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:47:52.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:47:52.354: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:47:52.354: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7086/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.77.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.172.10%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 13 03:47:52.560: INFO: Waiting for responses: map[]
    Jun 13 03:47:52.560: INFO: reached 172.16.172.10 after 0/1 tries
    Jun 13 03:47:52.560: INFO: Breadth first check of 172.30.77.134 on host 10.255.64.102...
    Jun 13 03:47:52.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.77.132:9080/dial?request=hostname&protocol=http&host=172.30.77.134&port=8083&tries=1'] Namespace:pod-network-test-7086 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:47:52.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:47:52.573: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:47:52.573: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7086/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.77.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.77.134%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 13 03:47:52.739: INFO: Waiting for responses: map[]
    Jun 13 03:47:52.739: INFO: reached 172.30.77.134 after 0/1 tries
    Jun 13 03:47:52.739: INFO: Breadth first check of 172.28.156.246 on host 10.255.64.104...
    Jun 13 03:47:52.760: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.77.132:9080/dial?request=hostname&protocol=http&host=172.28.156.246&port=8083&tries=1'] Namespace:pod-network-test-7086 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:47:52.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:47:52.762: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:47:52.762: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7086/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.77.132%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.28.156.246%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 13 03:47:52.929: INFO: Waiting for responses: map[]
    Jun 13 03:47:52.929: INFO: reached 172.28.156.246 after 0/1 tries
    Jun 13 03:47:52.929: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jun 13 03:47:52.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-7086" for this suite. 06/13/23 03:47:52.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:47:53.001
Jun 13 03:47:53.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename deployment 06/13/23 03:47:53.003
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:47:53.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:47:53.078
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jun 13 03:47:53.091: INFO: Creating deployment "webserver-deployment"
Jun 13 03:47:53.110: INFO: Waiting for observed generation 1
Jun 13 03:47:55.182: INFO: Waiting for all required pods to come up
Jun 13 03:47:55.213: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 06/13/23 03:47:55.213
Jun 13 03:47:55.216: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-zgg89" in namespace "deployment-4534" to be "running"
Jun 13 03:47:55.216: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2v6vv" in namespace "deployment-4534" to be "running"
Jun 13 03:47:55.216: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-85r67" in namespace "deployment-4534" to be "running"
Jun 13 03:47:55.217: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-8pqm5" in namespace "deployment-4534" to be "running"
Jun 13 03:47:55.217: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mvl2q" in namespace "deployment-4534" to be "running"
Jun 13 03:47:55.217: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-n5mpf" in namespace "deployment-4534" to be "running"
Jun 13 03:47:55.217: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qtwsd" in namespace "deployment-4534" to be "running"
Jun 13 03:47:55.218: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qphd2" in namespace "deployment-4534" to be "running"
Jun 13 03:47:55.218: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-thqxc" in namespace "deployment-4534" to be "running"
Jun 13 03:47:55.218: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-v5g9j" in namespace "deployment-4534" to be "running"
Jun 13 03:47:55.231: INFO: Pod "webserver-deployment-845c8977d9-2v6vv": Phase="Pending", Reason="", readiness=false. Elapsed: 14.405518ms
Jun 13 03:47:55.231: INFO: Pod "webserver-deployment-845c8977d9-thqxc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.153758ms
Jun 13 03:47:55.234: INFO: Pod "webserver-deployment-845c8977d9-mvl2q": Phase="Pending", Reason="", readiness=false. Elapsed: 17.736376ms
Jun 13 03:47:55.235: INFO: Pod "webserver-deployment-845c8977d9-qtwsd": Phase="Pending", Reason="", readiness=false. Elapsed: 17.277867ms
Jun 13 03:47:55.235: INFO: Pod "webserver-deployment-845c8977d9-8pqm5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.11791ms
Jun 13 03:47:55.235: INFO: Pod "webserver-deployment-845c8977d9-v5g9j": Phase="Pending", Reason="", readiness=false. Elapsed: 16.909545ms
Jun 13 03:47:55.235: INFO: Pod "webserver-deployment-845c8977d9-zgg89": Phase="Pending", Reason="", readiness=false. Elapsed: 19.039775ms
Jun 13 03:47:55.235: INFO: Pod "webserver-deployment-845c8977d9-n5mpf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.121129ms
Jun 13 03:47:55.235: INFO: Pod "webserver-deployment-845c8977d9-85r67": Phase="Pending", Reason="", readiness=false. Elapsed: 18.644873ms
Jun 13 03:47:55.236: INFO: Pod "webserver-deployment-845c8977d9-qphd2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.436667ms
Jun 13 03:47:57.241: INFO: Pod "webserver-deployment-845c8977d9-thqxc": Phase="Running", Reason="", readiness=true. Elapsed: 2.023020333s
Jun 13 03:47:57.241: INFO: Pod "webserver-deployment-845c8977d9-thqxc" satisfied condition "running"
Jun 13 03:47:57.244: INFO: Pod "webserver-deployment-845c8977d9-8pqm5": Phase="Running", Reason="", readiness=true. Elapsed: 2.027441825s
Jun 13 03:47:57.244: INFO: Pod "webserver-deployment-845c8977d9-8pqm5" satisfied condition "running"
Jun 13 03:47:57.244: INFO: Pod "webserver-deployment-845c8977d9-2v6vv": Phase="Running", Reason="", readiness=true. Elapsed: 2.028001902s
Jun 13 03:47:57.244: INFO: Pod "webserver-deployment-845c8977d9-2v6vv" satisfied condition "running"
Jun 13 03:47:57.251: INFO: Pod "webserver-deployment-845c8977d9-85r67": Phase="Running", Reason="", readiness=true. Elapsed: 2.034830719s
Jun 13 03:47:57.251: INFO: Pod "webserver-deployment-845c8977d9-85r67" satisfied condition "running"
Jun 13 03:47:57.251: INFO: Pod "webserver-deployment-845c8977d9-mvl2q": Phase="Running", Reason="", readiness=true. Elapsed: 2.034703303s
Jun 13 03:47:57.251: INFO: Pod "webserver-deployment-845c8977d9-mvl2q" satisfied condition "running"
Jun 13 03:47:57.252: INFO: Pod "webserver-deployment-845c8977d9-zgg89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036099593s
Jun 13 03:47:57.252: INFO: Pod "webserver-deployment-845c8977d9-qtwsd": Phase="Running", Reason="", readiness=true. Elapsed: 2.034671858s
Jun 13 03:47:57.252: INFO: Pod "webserver-deployment-845c8977d9-qtwsd" satisfied condition "running"
Jun 13 03:47:57.252: INFO: Pod "webserver-deployment-845c8977d9-v5g9j": Phase="Running", Reason="", readiness=true. Elapsed: 2.034363742s
Jun 13 03:47:57.252: INFO: Pod "webserver-deployment-845c8977d9-v5g9j" satisfied condition "running"
Jun 13 03:47:57.253: INFO: Pod "webserver-deployment-845c8977d9-n5mpf": Phase="Running", Reason="", readiness=true. Elapsed: 2.036309737s
Jun 13 03:47:57.253: INFO: Pod "webserver-deployment-845c8977d9-n5mpf" satisfied condition "running"
Jun 13 03:47:57.253: INFO: Pod "webserver-deployment-845c8977d9-qphd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035709965s
Jun 13 03:47:59.244: INFO: Pod "webserver-deployment-845c8977d9-zgg89": Phase="Running", Reason="", readiness=true. Elapsed: 4.027873342s
Jun 13 03:47:59.244: INFO: Pod "webserver-deployment-845c8977d9-zgg89" satisfied condition "running"
Jun 13 03:47:59.249: INFO: Pod "webserver-deployment-845c8977d9-qphd2": Phase="Running", Reason="", readiness=true. Elapsed: 4.031000773s
Jun 13 03:47:59.249: INFO: Pod "webserver-deployment-845c8977d9-qphd2" satisfied condition "running"
Jun 13 03:47:59.249: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 13 03:47:59.268: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 13 03:47:59.306: INFO: Updating deployment webserver-deployment
Jun 13 03:47:59.306: INFO: Waiting for observed generation 2
Jun 13 03:48:01.358: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 13 03:48:01.391: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 13 03:48:01.400: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 13 03:48:01.460: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 13 03:48:01.460: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 13 03:48:01.483: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 13 03:48:01.528: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 13 03:48:01.528: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 13 03:48:01.576: INFO: Updating deployment webserver-deployment
Jun 13 03:48:01.576: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 13 03:48:01.647: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 13 03:48:03.751: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 13 03:48:03.848: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4534  5881996a-4252-455f-ac85-84d573e239a6 49969 3 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00729e068 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-13 03:48:01 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-06-13 03:48:01 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 13 03:48:03.879: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-4534  61402cb7-1c07-4345-af0f-9a106ea9e6e2 49963 3 2023-06-13 03:47:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 5881996a-4252-455f-ac85-84d573e239a6 0xc0037b11b7 0xc0037b11b8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5881996a-4252-455f-ac85-84d573e239a6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037b1258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 13 03:48:03.879: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 13 03:48:03.879: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-4534  197e6ca9-2c80-49df-8646-3be2f925c9c1 49952 3 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 5881996a-4252-455f-ac85-84d573e239a6 0xc0037b12b7 0xc0037b12b8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5881996a-4252-455f-ac85-84d573e239a6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037b1348 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 13 03:48:03.956: INFO: Pod "webserver-deployment-69b7448995-274x2" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-274x2 webserver-deployment-69b7448995- deployment-4534  5104378c-a249-41b3-91f8-e3baa699fea2 49955 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc0037b17e7 0xc0037b17e8}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5w5vq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5w5vq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.958: INFO: Pod "webserver-deployment-69b7448995-2wn2w" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-2wn2w webserver-deployment-69b7448995- deployment-4534  e197f93c-cd46-45f1-8302-69fba57968cf 50068 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8acf3850209ba7f3c4c47d3e7866e636728d84cd70f704dfdeb3a680dc1f3645 cni.projectcalico.org/podIP:172.30.77.138/32 cni.projectcalico.org/podIPs:172.30.77.138/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc0037b1970 0xc0037b1971}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g5pbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g5pbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.958: INFO: Pod "webserver-deployment-69b7448995-d9pk5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-d9pk5 webserver-deployment-69b7448995- deployment-4534  29528954-88b5-4023-b937-4715e7d1bacc 50016 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc0037b1b67 0xc0037b1b68}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t7m2z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t7m2z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.958: INFO: Pod "webserver-deployment-69b7448995-grdhf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-grdhf webserver-deployment-69b7448995- deployment-4534  a153153f-12fe-4f29-8edb-980d10c34543 49886 0 2023-06-13 03:47:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6c8d8f74610004421dede5427efea8cf1af35571dfe620e82cd972cbde587bd4 cni.projectcalico.org/podIP:172.28.156.251/32 cni.projectcalico.org/podIPs:172.28.156.251/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc0037b1d47 0xc0037b1d48}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mcmnr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mcmnr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:47:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.959: INFO: Pod "webserver-deployment-69b7448995-lgjnj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-lgjnj webserver-deployment-69b7448995- deployment-4534  8aaeea0f-1efb-4db1-9d9f-2df77118d1ae 50036 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc0037b1f57 0xc0037b1f58}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgphw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgphw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.959: INFO: Pod "webserver-deployment-69b7448995-lz5bj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-lz5bj webserver-deployment-69b7448995- deployment-4534  7f64bda1-828e-43cc-9c07-c76fda0b6830 49893 0 2023-06-13 03:47:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a88f53814476dd96aa7f32596376687f2714795248c5634e0b670162b9633c64 cni.projectcalico.org/podIP:172.16.172.59/32 cni.projectcalico.org/podIPs:172.16.172.59/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364a157 0xc00364a158}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btnhw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btnhw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:47:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.959: INFO: Pod "webserver-deployment-69b7448995-t6mpd" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-t6mpd webserver-deployment-69b7448995- deployment-4534  21b98373-5fe3-4785-ab19-eade3e8a28bf 49877 0 2023-06-13 03:47:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8eede85e7905a662a4e73197651fdcda6605bda15bf2203b066fc51d24252794 cni.projectcalico.org/podIP:172.16.172.15/32 cni.projectcalico.org/podIPs:172.16.172.15/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364a377 0xc00364a378}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ws5vm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ws5vm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:47:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.959: INFO: Pod "webserver-deployment-69b7448995-tg8rm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-tg8rm webserver-deployment-69b7448995- deployment-4534  1072db00-0837-4511-a5b1-288c1877087d 50051 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e81f1d78ca8d2e2bbdb8563e954efabc0e33b7e206dcfb29d1b3ab0e020b6648 cni.projectcalico.org/podIP:172.28.156.253/32 cni.projectcalico.org/podIPs:172.28.156.253/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364a577 0xc00364a578}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wdd5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wdd5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.960: INFO: Pod "webserver-deployment-69b7448995-vbjx5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-vbjx5 webserver-deployment-69b7448995- deployment-4534  480de98e-dc98-4691-b7f0-fd0676cec960 50065 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:fcf1b7d035a6bc96e7a6cf898313dbc9d894616a5f4c92168a78ad7025da0e6e cni.projectcalico.org/podIP:172.16.172.61/32 cni.projectcalico.org/podIPs:172.16.172.61/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364a7a7 0xc00364a7a8}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8vmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8vmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.960: INFO: Pod "webserver-deployment-69b7448995-wv7jn" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-wv7jn webserver-deployment-69b7448995- deployment-4534  34caf7e4-ab35-4005-86fd-30f1db71bb6c 49871 0 2023-06-13 03:47:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6ed37156df412df2b357bfcd92b4ae99e9e5c5179691b4a9f7212267a288e2d5 cni.projectcalico.org/podIP:172.28.156.248/32 cni.projectcalico.org/podIPs:172.28.156.248/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364a9a7 0xc00364a9a8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nljwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nljwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:47:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.961: INFO: Pod "webserver-deployment-69b7448995-wzzxr" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-wzzxr webserver-deployment-69b7448995- deployment-4534  9205e200-74cf-4bbe-9d6c-c295134b75e8 50073 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364abd7 0xc00364abd8}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gv659,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gv659,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.961: INFO: Pod "webserver-deployment-69b7448995-xr88t" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-xr88t webserver-deployment-69b7448995- deployment-4534  a5f6b565-6b98-48ca-92f3-6773820c065d 49876 0 2023-06-13 03:47:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5e550a34f2bd2d3c2e197554c68c32da3d9ac220ac231266b37acde54f4103e5 cni.projectcalico.org/podIP:172.30.77.152/32 cni.projectcalico.org/podIPs:172.30.77.152/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364add7 0xc00364add8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5xqzk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5xqzk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:47:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.961: INFO: Pod "webserver-deployment-69b7448995-zpjkk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zpjkk webserver-deployment-69b7448995- deployment-4534  2185f639-ee6a-4a5c-8323-537eb9b9ec3d 50059 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364afd7 0xc00364afd8}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6hdsx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6hdsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.962: INFO: Pod "webserver-deployment-845c8977d9-2v6vv" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-2v6vv webserver-deployment-845c8977d9- deployment-4534  de42fe02-b55f-4005-b7ca-fb6466a899e1 49708 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c5e954d9c52b688c990cf57c88066f89959436420108e724b0fe7056ac3afca7 cni.projectcalico.org/podIP:172.30.77.153/32 cni.projectcalico.org/podIPs:172.30.77.153/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364b1d7 0xc00364b1d8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ppbk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ppbk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.77.153,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ea8ca8962efd8655a163c862a2be89a0a5391e06e4d337514bc615383724575f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.77.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.962: INFO: Pod "webserver-deployment-845c8977d9-85r67" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-85r67 webserver-deployment-845c8977d9- deployment-4534  f3a816e8-5097-491f-b65f-e54dd6b2cac6 49732 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cf5fe8c17b63b16912b30cbfd042720bbe7d59cfbc285774140de13420092cba cni.projectcalico.org/podIP:172.28.156.247/32 cni.projectcalico.org/podIPs:172.28.156.247/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364b3d7 0xc00364b3d8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.156.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wz8zh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wz8zh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.28.156.247,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d677a9cf62372bb033f0d602edd613fd3ea94cf464660b479ea7d065fc2c8a1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.156.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.962: INFO: Pod "webserver-deployment-845c8977d9-8pqm5" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8pqm5 webserver-deployment-845c8977d9- deployment-4534  c5875dec-ec5f-4e7d-a0eb-2a2e19993c35 49743 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c7d1ba9a98920d6c8407a3f8127647536ae321b06ace8c70308880d6801a3d68 cni.projectcalico.org/podIP:172.28.156.244/32 cni.projectcalico.org/podIPs:172.28.156.244/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364b5d7 0xc00364b5d8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.156.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fftlp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fftlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.28.156.244,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://490fcb120ff1469be9b43307ff50f83ea4313312f0aa6c9436da829b6b9d3bd2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.156.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.963: INFO: Pod "webserver-deployment-845c8977d9-8wnz2" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8wnz2 webserver-deployment-845c8977d9- deployment-4534  c3a01b05-0f8c-49fc-8949-30ce7dc94610 50040 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364b7d7 0xc00364b7d8}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5hv4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5hv4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.963: INFO: Pod "webserver-deployment-845c8977d9-98pmh" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-98pmh webserver-deployment-845c8977d9- deployment-4534  8a8c50b7-9555-49f8-981c-2f1196d1882d 50030 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:77f6b4570de59ed51454ac00c8c81ed32dc905293701d505fda0c7c44fa37dd2 cni.projectcalico.org/podIP:172.28.156.252/32 cni.projectcalico.org/podIPs:172.28.156.252/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364b997 0xc00364b998}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-scrfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-scrfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.965: INFO: Pod "webserver-deployment-845c8977d9-999kv" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-999kv webserver-deployment-845c8977d9- deployment-4534  6df0cd76-efdc-4f1b-be11-4d2d3c19ae4c 49975 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364bb77 0xc00364bb78}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jnhf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jnhf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.965: INFO: Pod "webserver-deployment-845c8977d9-bjgn9" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bjgn9 webserver-deployment-845c8977d9- deployment-4534  d398e7e4-978c-4631-bf21-6de76eec3364 50031 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5c3544b9b1c7c71194e30361791d2cf911147c09d5967b5f1755485fb52e5b4f cni.projectcalico.org/podIP:172.16.172.32/32 cni.projectcalico.org/podIPs:172.16.172.32/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364bd57 0xc00364bd58}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdddt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdddt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.965: INFO: Pod "webserver-deployment-845c8977d9-czgg6" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-czgg6 webserver-deployment-845c8977d9- deployment-4534  0e94bf85-c4cf-43a6-9c30-c90b2f6257c7 50056 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:86df894f22d924f624d3f09d04b0e89c9b43c55c97e0f10ff232e5901983b0ae cni.projectcalico.org/podIP:172.30.77.141/32 cni.projectcalico.org/podIPs:172.30.77.141/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364bf57 0xc00364bf58}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8zbz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8zbz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.966: INFO: Pod "webserver-deployment-845c8977d9-d464s" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-d464s webserver-deployment-845c8977d9- deployment-4534  2163fc9d-4ca9-4063-a568-7310c6f788e2 50053 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:66abdab04e79d1a89d304e68fd31bc93098c2e4347bf6d958ad80eb65b47c950 cni.projectcalico.org/podIP:172.16.172.45/32 cni.projectcalico.org/podIPs:172.16.172.45/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2157 0xc003da2158}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-shrbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-shrbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.966: INFO: Pod "webserver-deployment-845c8977d9-hbzhv" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hbzhv webserver-deployment-845c8977d9- deployment-4534  e29d89c8-d978-45bc-892f-f1324cb8e9b3 50011 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:ce9f1ebf941e50a41da478f06c19a8812243d28d29e813eb1d8ec0c9879376ce cni.projectcalico.org/podIP:172.28.156.250/32 cni.projectcalico.org/podIPs:172.28.156.250/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2337 0xc003da2338}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wx5wb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wx5wb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.967: INFO: Pod "webserver-deployment-845c8977d9-j9j52" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-j9j52 webserver-deployment-845c8977d9- deployment-4534  e7b71cd8-41a5-4786-a8cd-a24bf76e21c9 50010 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:53afe6e86ca4eb1870d27f7fbef52a6003efaefccdfc29e224d0f61a51133e7c cni.projectcalico.org/podIP:172.16.172.55/32 cni.projectcalico.org/podIPs:172.16.172.55/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2537 0xc003da2538}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45prz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45prz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.967: INFO: Pod "webserver-deployment-845c8977d9-jsbnd" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jsbnd webserver-deployment-845c8977d9- deployment-4534  f2afb286-acab-419d-89b6-69ff47d0cfd1 49995 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2717 0xc003da2718}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pldmg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pldmg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.967: INFO: Pod "webserver-deployment-845c8977d9-mvl2q" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mvl2q webserver-deployment-845c8977d9- deployment-4534  cd53697d-adfd-441b-b41e-ff68d3018b85 49703 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:57b50122af85d73e5c906ded919979867ac4f13ca9314f0a169e391a1efa6ff7 cni.projectcalico.org/podIP:172.30.77.136/32 cni.projectcalico.org/podIPs:172.30.77.136/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da28f7 0xc003da28f8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5cbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5cbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.77.136,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://686fec2e18746cf38729c571b41e671dabf32436b9d313c1dfe0acffc585317e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.77.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.967: INFO: Pod "webserver-deployment-845c8977d9-n5mpf" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-n5mpf webserver-deployment-845c8977d9- deployment-4534  9466cf2a-0ebd-4659-9ed3-105e4fb33374 49739 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:02b0d48cc9ecd9c9138b51ae0c12664241f279fe62aa72c9e9218dbaefd46dff cni.projectcalico.org/podIP:172.28.156.249/32 cni.projectcalico.org/podIPs:172.28.156.249/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2af7 0xc003da2af8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.156.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j55z5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j55z5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.28.156.249,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4a391c975de7dd606015331a773e9a55962c7aeba856ed44fbc30faadf81704c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.156.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.968: INFO: Pod "webserver-deployment-845c8977d9-qtwsd" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qtwsd webserver-deployment-845c8977d9- deployment-4534  80b16e8f-d1bc-4343-a938-9d6de2d37406 49735 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7ef7be300e2ec51929a169ac4dd58da62836d81058b8b1085b0917a29d260724 cni.projectcalico.org/podIP:172.30.77.135/32 cni.projectcalico.org/podIPs:172.30.77.135/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2d47 0xc003da2d48}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jphl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jphl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.77.135,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c1735d07de96d6b0e246ad88c958f90558cf958c28f1f07013695bd74426c837,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.77.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.968: INFO: Pod "webserver-deployment-845c8977d9-rf99v" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rf99v webserver-deployment-845c8977d9- deployment-4534  63c4e825-dec4-4ede-bee2-a739bb45d984 49948 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2f47 0xc003da2f48}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drs94,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drs94,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.968: INFO: Pod "webserver-deployment-845c8977d9-ttjr2" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-ttjr2 webserver-deployment-845c8977d9- deployment-4534  fab79cfe-8ca2-41b7-9a6c-d5c3e8cae765 50028 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:329a516faa1c4cba5b63a54c980c3f0f0337f85c07a88ab8662c54ad4a38633a cni.projectcalico.org/podIP:172.30.77.148/32 cni.projectcalico.org/podIPs:172.30.77.148/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da30c0 0xc003da30c1}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7x9g9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7x9g9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.969: INFO: Pod "webserver-deployment-845c8977d9-v5g9j" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-v5g9j webserver-deployment-845c8977d9- deployment-4534  87391116-fd43-4285-907b-fc3f1e797c97 49700 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9b50c216f06d6643bfd09198f160f09ac31f2abf41d4272e991ab6f4fab63525 cni.projectcalico.org/podIP:172.16.172.20/32 cni.projectcalico.org/podIPs:172.16.172.20/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da32b7 0xc003da32b8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zl5g5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zl5g5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.20,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://45aecd97006bd0f9eea740a077ff23eefa953004fb060ff2fed2739de845ee72,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.969: INFO: Pod "webserver-deployment-845c8977d9-vvr4r" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-vvr4r webserver-deployment-845c8977d9- deployment-4534  79852924-abc4-46a6-9767-369f2b6c2047 49979 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da34c7 0xc003da34c8}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmkzj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmkzj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 13 03:48:03.969: INFO: Pod "webserver-deployment-845c8977d9-zgg89" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-zgg89 webserver-deployment-845c8977d9- deployment-4534  aecf639d-114c-4115-8c5b-fef290839321 49753 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9643994a871058b197b0721ef4ed58e70c8962fa9ea9c7a2ff697b091ffac1d3 cni.projectcalico.org/podIP:172.16.172.50/32 cni.projectcalico.org/podIPs:172.16.172.50/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da36b7 0xc003da36b8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p527h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p527h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.50,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://128ed21e3e049e665075f6384eb5064e327b5a3e1291ece59f00b54a65eff6b3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 13 03:48:03.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4534" for this suite. 06/13/23 03:48:04.011
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":310,"skipped":5705,"failed":0}
------------------------------
• [SLOW TEST] [11.037 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:47:53.001
    Jun 13 03:47:53.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename deployment 06/13/23 03:47:53.003
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:47:53.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:47:53.078
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jun 13 03:47:53.091: INFO: Creating deployment "webserver-deployment"
    Jun 13 03:47:53.110: INFO: Waiting for observed generation 1
    Jun 13 03:47:55.182: INFO: Waiting for all required pods to come up
    Jun 13 03:47:55.213: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 06/13/23 03:47:55.213
    Jun 13 03:47:55.216: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-zgg89" in namespace "deployment-4534" to be "running"
    Jun 13 03:47:55.216: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2v6vv" in namespace "deployment-4534" to be "running"
    Jun 13 03:47:55.216: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-85r67" in namespace "deployment-4534" to be "running"
    Jun 13 03:47:55.217: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-8pqm5" in namespace "deployment-4534" to be "running"
    Jun 13 03:47:55.217: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mvl2q" in namespace "deployment-4534" to be "running"
    Jun 13 03:47:55.217: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-n5mpf" in namespace "deployment-4534" to be "running"
    Jun 13 03:47:55.217: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qtwsd" in namespace "deployment-4534" to be "running"
    Jun 13 03:47:55.218: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qphd2" in namespace "deployment-4534" to be "running"
    Jun 13 03:47:55.218: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-thqxc" in namespace "deployment-4534" to be "running"
    Jun 13 03:47:55.218: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-v5g9j" in namespace "deployment-4534" to be "running"
    Jun 13 03:47:55.231: INFO: Pod "webserver-deployment-845c8977d9-2v6vv": Phase="Pending", Reason="", readiness=false. Elapsed: 14.405518ms
    Jun 13 03:47:55.231: INFO: Pod "webserver-deployment-845c8977d9-thqxc": Phase="Pending", Reason="", readiness=false. Elapsed: 13.153758ms
    Jun 13 03:47:55.234: INFO: Pod "webserver-deployment-845c8977d9-mvl2q": Phase="Pending", Reason="", readiness=false. Elapsed: 17.736376ms
    Jun 13 03:47:55.235: INFO: Pod "webserver-deployment-845c8977d9-qtwsd": Phase="Pending", Reason="", readiness=false. Elapsed: 17.277867ms
    Jun 13 03:47:55.235: INFO: Pod "webserver-deployment-845c8977d9-8pqm5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.11791ms
    Jun 13 03:47:55.235: INFO: Pod "webserver-deployment-845c8977d9-v5g9j": Phase="Pending", Reason="", readiness=false. Elapsed: 16.909545ms
    Jun 13 03:47:55.235: INFO: Pod "webserver-deployment-845c8977d9-zgg89": Phase="Pending", Reason="", readiness=false. Elapsed: 19.039775ms
    Jun 13 03:47:55.235: INFO: Pod "webserver-deployment-845c8977d9-n5mpf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.121129ms
    Jun 13 03:47:55.235: INFO: Pod "webserver-deployment-845c8977d9-85r67": Phase="Pending", Reason="", readiness=false. Elapsed: 18.644873ms
    Jun 13 03:47:55.236: INFO: Pod "webserver-deployment-845c8977d9-qphd2": Phase="Pending", Reason="", readiness=false. Elapsed: 18.436667ms
    Jun 13 03:47:57.241: INFO: Pod "webserver-deployment-845c8977d9-thqxc": Phase="Running", Reason="", readiness=true. Elapsed: 2.023020333s
    Jun 13 03:47:57.241: INFO: Pod "webserver-deployment-845c8977d9-thqxc" satisfied condition "running"
    Jun 13 03:47:57.244: INFO: Pod "webserver-deployment-845c8977d9-8pqm5": Phase="Running", Reason="", readiness=true. Elapsed: 2.027441825s
    Jun 13 03:47:57.244: INFO: Pod "webserver-deployment-845c8977d9-8pqm5" satisfied condition "running"
    Jun 13 03:47:57.244: INFO: Pod "webserver-deployment-845c8977d9-2v6vv": Phase="Running", Reason="", readiness=true. Elapsed: 2.028001902s
    Jun 13 03:47:57.244: INFO: Pod "webserver-deployment-845c8977d9-2v6vv" satisfied condition "running"
    Jun 13 03:47:57.251: INFO: Pod "webserver-deployment-845c8977d9-85r67": Phase="Running", Reason="", readiness=true. Elapsed: 2.034830719s
    Jun 13 03:47:57.251: INFO: Pod "webserver-deployment-845c8977d9-85r67" satisfied condition "running"
    Jun 13 03:47:57.251: INFO: Pod "webserver-deployment-845c8977d9-mvl2q": Phase="Running", Reason="", readiness=true. Elapsed: 2.034703303s
    Jun 13 03:47:57.251: INFO: Pod "webserver-deployment-845c8977d9-mvl2q" satisfied condition "running"
    Jun 13 03:47:57.252: INFO: Pod "webserver-deployment-845c8977d9-zgg89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036099593s
    Jun 13 03:47:57.252: INFO: Pod "webserver-deployment-845c8977d9-qtwsd": Phase="Running", Reason="", readiness=true. Elapsed: 2.034671858s
    Jun 13 03:47:57.252: INFO: Pod "webserver-deployment-845c8977d9-qtwsd" satisfied condition "running"
    Jun 13 03:47:57.252: INFO: Pod "webserver-deployment-845c8977d9-v5g9j": Phase="Running", Reason="", readiness=true. Elapsed: 2.034363742s
    Jun 13 03:47:57.252: INFO: Pod "webserver-deployment-845c8977d9-v5g9j" satisfied condition "running"
    Jun 13 03:47:57.253: INFO: Pod "webserver-deployment-845c8977d9-n5mpf": Phase="Running", Reason="", readiness=true. Elapsed: 2.036309737s
    Jun 13 03:47:57.253: INFO: Pod "webserver-deployment-845c8977d9-n5mpf" satisfied condition "running"
    Jun 13 03:47:57.253: INFO: Pod "webserver-deployment-845c8977d9-qphd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035709965s
    Jun 13 03:47:59.244: INFO: Pod "webserver-deployment-845c8977d9-zgg89": Phase="Running", Reason="", readiness=true. Elapsed: 4.027873342s
    Jun 13 03:47:59.244: INFO: Pod "webserver-deployment-845c8977d9-zgg89" satisfied condition "running"
    Jun 13 03:47:59.249: INFO: Pod "webserver-deployment-845c8977d9-qphd2": Phase="Running", Reason="", readiness=true. Elapsed: 4.031000773s
    Jun 13 03:47:59.249: INFO: Pod "webserver-deployment-845c8977d9-qphd2" satisfied condition "running"
    Jun 13 03:47:59.249: INFO: Waiting for deployment "webserver-deployment" to complete
    Jun 13 03:47:59.268: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jun 13 03:47:59.306: INFO: Updating deployment webserver-deployment
    Jun 13 03:47:59.306: INFO: Waiting for observed generation 2
    Jun 13 03:48:01.358: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jun 13 03:48:01.391: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jun 13 03:48:01.400: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jun 13 03:48:01.460: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jun 13 03:48:01.460: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jun 13 03:48:01.483: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jun 13 03:48:01.528: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jun 13 03:48:01.528: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jun 13 03:48:01.576: INFO: Updating deployment webserver-deployment
    Jun 13 03:48:01.576: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jun 13 03:48:01.647: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jun 13 03:48:03.751: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 13 03:48:03.848: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-4534  5881996a-4252-455f-ac85-84d573e239a6 49969 3 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00729e068 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-13 03:48:01 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-06-13 03:48:01 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jun 13 03:48:03.879: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-4534  61402cb7-1c07-4345-af0f-9a106ea9e6e2 49963 3 2023-06-13 03:47:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 5881996a-4252-455f-ac85-84d573e239a6 0xc0037b11b7 0xc0037b11b8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5881996a-4252-455f-ac85-84d573e239a6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037b1258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 03:48:03.879: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jun 13 03:48:03.879: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-4534  197e6ca9-2c80-49df-8646-3be2f925c9c1 49952 3 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 5881996a-4252-455f-ac85-84d573e239a6 0xc0037b12b7 0xc0037b12b8}] [] [{kube-controller-manager Update apps/v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5881996a-4252-455f-ac85-84d573e239a6\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037b1348 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jun 13 03:48:03.956: INFO: Pod "webserver-deployment-69b7448995-274x2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-274x2 webserver-deployment-69b7448995- deployment-4534  5104378c-a249-41b3-91f8-e3baa699fea2 49955 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc0037b17e7 0xc0037b17e8}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5w5vq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5w5vq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.958: INFO: Pod "webserver-deployment-69b7448995-2wn2w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-2wn2w webserver-deployment-69b7448995- deployment-4534  e197f93c-cd46-45f1-8302-69fba57968cf 50068 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8acf3850209ba7f3c4c47d3e7866e636728d84cd70f704dfdeb3a680dc1f3645 cni.projectcalico.org/podIP:172.30.77.138/32 cni.projectcalico.org/podIPs:172.30.77.138/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc0037b1970 0xc0037b1971}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g5pbv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g5pbv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.958: INFO: Pod "webserver-deployment-69b7448995-d9pk5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-d9pk5 webserver-deployment-69b7448995- deployment-4534  29528954-88b5-4023-b937-4715e7d1bacc 50016 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc0037b1b67 0xc0037b1b68}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t7m2z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t7m2z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.958: INFO: Pod "webserver-deployment-69b7448995-grdhf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-grdhf webserver-deployment-69b7448995- deployment-4534  a153153f-12fe-4f29-8edb-980d10c34543 49886 0 2023-06-13 03:47:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6c8d8f74610004421dede5427efea8cf1af35571dfe620e82cd972cbde587bd4 cni.projectcalico.org/podIP:172.28.156.251/32 cni.projectcalico.org/podIPs:172.28.156.251/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc0037b1d47 0xc0037b1d48}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mcmnr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mcmnr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:47:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.959: INFO: Pod "webserver-deployment-69b7448995-lgjnj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-lgjnj webserver-deployment-69b7448995- deployment-4534  8aaeea0f-1efb-4db1-9d9f-2df77118d1ae 50036 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc0037b1f57 0xc0037b1f58}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zgphw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zgphw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.959: INFO: Pod "webserver-deployment-69b7448995-lz5bj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-lz5bj webserver-deployment-69b7448995- deployment-4534  7f64bda1-828e-43cc-9c07-c76fda0b6830 49893 0 2023-06-13 03:47:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:a88f53814476dd96aa7f32596376687f2714795248c5634e0b670162b9633c64 cni.projectcalico.org/podIP:172.16.172.59/32 cni.projectcalico.org/podIPs:172.16.172.59/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364a157 0xc00364a158}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-btnhw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-btnhw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:47:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.959: INFO: Pod "webserver-deployment-69b7448995-t6mpd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-t6mpd webserver-deployment-69b7448995- deployment-4534  21b98373-5fe3-4785-ab19-eade3e8a28bf 49877 0 2023-06-13 03:47:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:8eede85e7905a662a4e73197651fdcda6605bda15bf2203b066fc51d24252794 cni.projectcalico.org/podIP:172.16.172.15/32 cni.projectcalico.org/podIPs:172.16.172.15/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364a377 0xc00364a378}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ws5vm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ws5vm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:47:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.959: INFO: Pod "webserver-deployment-69b7448995-tg8rm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-tg8rm webserver-deployment-69b7448995- deployment-4534  1072db00-0837-4511-a5b1-288c1877087d 50051 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e81f1d78ca8d2e2bbdb8563e954efabc0e33b7e206dcfb29d1b3ab0e020b6648 cni.projectcalico.org/podIP:172.28.156.253/32 cni.projectcalico.org/podIPs:172.28.156.253/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364a577 0xc00364a578}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wdd5f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wdd5f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.960: INFO: Pod "webserver-deployment-69b7448995-vbjx5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-vbjx5 webserver-deployment-69b7448995- deployment-4534  480de98e-dc98-4691-b7f0-fd0676cec960 50065 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:fcf1b7d035a6bc96e7a6cf898313dbc9d894616a5f4c92168a78ad7025da0e6e cni.projectcalico.org/podIP:172.16.172.61/32 cni.projectcalico.org/podIPs:172.16.172.61/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364a7a7 0xc00364a7a8}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d8vmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d8vmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.960: INFO: Pod "webserver-deployment-69b7448995-wv7jn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-wv7jn webserver-deployment-69b7448995- deployment-4534  34caf7e4-ab35-4005-86fd-30f1db71bb6c 49871 0 2023-06-13 03:47:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:6ed37156df412df2b357bfcd92b4ae99e9e5c5179691b4a9f7212267a288e2d5 cni.projectcalico.org/podIP:172.28.156.248/32 cni.projectcalico.org/podIPs:172.28.156.248/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364a9a7 0xc00364a9a8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nljwh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nljwh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:47:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.961: INFO: Pod "webserver-deployment-69b7448995-wzzxr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-wzzxr webserver-deployment-69b7448995- deployment-4534  9205e200-74cf-4bbe-9d6c-c295134b75e8 50073 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364abd7 0xc00364abd8}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gv659,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gv659,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.961: INFO: Pod "webserver-deployment-69b7448995-xr88t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-xr88t webserver-deployment-69b7448995- deployment-4534  a5f6b565-6b98-48ca-92f3-6773820c065d 49876 0 2023-06-13 03:47:59 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:5e550a34f2bd2d3c2e197554c68c32da3d9ac220ac231266b37acde54f4103e5 cni.projectcalico.org/podIP:172.30.77.152/32 cni.projectcalico.org/podIPs:172.30.77.152/32] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364add7 0xc00364add8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:47:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5xqzk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5xqzk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:47:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.961: INFO: Pod "webserver-deployment-69b7448995-zpjkk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zpjkk webserver-deployment-69b7448995- deployment-4534  2185f639-ee6a-4a5c-8323-537eb9b9ec3d 50059 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 61402cb7-1c07-4345-af0f-9a106ea9e6e2 0xc00364afd7 0xc00364afd8}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61402cb7-1c07-4345-af0f-9a106ea9e6e2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6hdsx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6hdsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.962: INFO: Pod "webserver-deployment-845c8977d9-2v6vv" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-2v6vv webserver-deployment-845c8977d9- deployment-4534  de42fe02-b55f-4005-b7ca-fb6466a899e1 49708 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c5e954d9c52b688c990cf57c88066f89959436420108e724b0fe7056ac3afca7 cni.projectcalico.org/podIP:172.30.77.153/32 cni.projectcalico.org/podIPs:172.30.77.153/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364b1d7 0xc00364b1d8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ppbk8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ppbk8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.77.153,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:54 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://ea8ca8962efd8655a163c862a2be89a0a5391e06e4d337514bc615383724575f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.77.153,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.962: INFO: Pod "webserver-deployment-845c8977d9-85r67" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-85r67 webserver-deployment-845c8977d9- deployment-4534  f3a816e8-5097-491f-b65f-e54dd6b2cac6 49732 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:cf5fe8c17b63b16912b30cbfd042720bbe7d59cfbc285774140de13420092cba cni.projectcalico.org/podIP:172.28.156.247/32 cni.projectcalico.org/podIPs:172.28.156.247/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364b3d7 0xc00364b3d8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.156.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wz8zh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wz8zh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.28.156.247,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://d677a9cf62372bb033f0d602edd613fd3ea94cf464660b479ea7d065fc2c8a1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.156.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.962: INFO: Pod "webserver-deployment-845c8977d9-8pqm5" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8pqm5 webserver-deployment-845c8977d9- deployment-4534  c5875dec-ec5f-4e7d-a0eb-2a2e19993c35 49743 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:c7d1ba9a98920d6c8407a3f8127647536ae321b06ace8c70308880d6801a3d68 cni.projectcalico.org/podIP:172.28.156.244/32 cni.projectcalico.org/podIPs:172.28.156.244/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364b5d7 0xc00364b5d8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.156.244\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fftlp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fftlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.28.156.244,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://490fcb120ff1469be9b43307ff50f83ea4313312f0aa6c9436da829b6b9d3bd2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.156.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.963: INFO: Pod "webserver-deployment-845c8977d9-8wnz2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8wnz2 webserver-deployment-845c8977d9- deployment-4534  c3a01b05-0f8c-49fc-8949-30ce7dc94610 50040 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364b7d7 0xc00364b7d8}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5hv4d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5hv4d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.963: INFO: Pod "webserver-deployment-845c8977d9-98pmh" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-98pmh webserver-deployment-845c8977d9- deployment-4534  8a8c50b7-9555-49f8-981c-2f1196d1882d 50030 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:77f6b4570de59ed51454ac00c8c81ed32dc905293701d505fda0c7c44fa37dd2 cni.projectcalico.org/podIP:172.28.156.252/32 cni.projectcalico.org/podIPs:172.28.156.252/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364b997 0xc00364b998}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-scrfp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-scrfp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.965: INFO: Pod "webserver-deployment-845c8977d9-999kv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-999kv webserver-deployment-845c8977d9- deployment-4534  6df0cd76-efdc-4f1b-be11-4d2d3c19ae4c 49975 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364bb77 0xc00364bb78}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jnhf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jnhf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.965: INFO: Pod "webserver-deployment-845c8977d9-bjgn9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bjgn9 webserver-deployment-845c8977d9- deployment-4534  d398e7e4-978c-4631-bf21-6de76eec3364 50031 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5c3544b9b1c7c71194e30361791d2cf911147c09d5967b5f1755485fb52e5b4f cni.projectcalico.org/podIP:172.16.172.32/32 cni.projectcalico.org/podIPs:172.16.172.32/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364bd57 0xc00364bd58}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdddt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdddt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.965: INFO: Pod "webserver-deployment-845c8977d9-czgg6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-czgg6 webserver-deployment-845c8977d9- deployment-4534  0e94bf85-c4cf-43a6-9c30-c90b2f6257c7 50056 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:86df894f22d924f624d3f09d04b0e89c9b43c55c97e0f10ff232e5901983b0ae cni.projectcalico.org/podIP:172.30.77.141/32 cni.projectcalico.org/podIPs:172.30.77.141/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc00364bf57 0xc00364bf58}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8zbz7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8zbz7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.966: INFO: Pod "webserver-deployment-845c8977d9-d464s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-d464s webserver-deployment-845c8977d9- deployment-4534  2163fc9d-4ca9-4063-a568-7310c6f788e2 50053 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:66abdab04e79d1a89d304e68fd31bc93098c2e4347bf6d958ad80eb65b47c950 cni.projectcalico.org/podIP:172.16.172.45/32 cni.projectcalico.org/podIPs:172.16.172.45/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2157 0xc003da2158}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:48:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-shrbs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-shrbs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.966: INFO: Pod "webserver-deployment-845c8977d9-hbzhv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hbzhv webserver-deployment-845c8977d9- deployment-4534  e29d89c8-d978-45bc-892f-f1324cb8e9b3 50011 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:ce9f1ebf941e50a41da478f06c19a8812243d28d29e813eb1d8ec0c9879376ce cni.projectcalico.org/podIP:172.28.156.250/32 cni.projectcalico.org/podIPs:172.28.156.250/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2337 0xc003da2338}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wx5wb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wx5wb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.967: INFO: Pod "webserver-deployment-845c8977d9-j9j52" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-j9j52 webserver-deployment-845c8977d9- deployment-4534  e7b71cd8-41a5-4786-a8cd-a24bf76e21c9 50010 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:53afe6e86ca4eb1870d27f7fbef52a6003efaefccdfc29e224d0f61a51133e7c cni.projectcalico.org/podIP:172.16.172.55/32 cni.projectcalico.org/podIPs:172.16.172.55/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2537 0xc003da2538}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-45prz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-45prz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.967: INFO: Pod "webserver-deployment-845c8977d9-jsbnd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jsbnd webserver-deployment-845c8977d9- deployment-4534  f2afb286-acab-419d-89b6-69ff47d0cfd1 49995 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2717 0xc003da2718}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pldmg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pldmg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.967: INFO: Pod "webserver-deployment-845c8977d9-mvl2q" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mvl2q webserver-deployment-845c8977d9- deployment-4534  cd53697d-adfd-441b-b41e-ff68d3018b85 49703 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:57b50122af85d73e5c906ded919979867ac4f13ca9314f0a169e391a1efa6ff7 cni.projectcalico.org/podIP:172.30.77.136/32 cni.projectcalico.org/podIPs:172.30.77.136/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da28f7 0xc003da28f8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l5cbh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l5cbh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.77.136,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://686fec2e18746cf38729c571b41e671dabf32436b9d313c1dfe0acffc585317e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.77.136,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.967: INFO: Pod "webserver-deployment-845c8977d9-n5mpf" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-n5mpf webserver-deployment-845c8977d9- deployment-4534  9466cf2a-0ebd-4659-9ed3-105e4fb33374 49739 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:02b0d48cc9ecd9c9138b51ae0c12664241f279fe62aa72c9e9218dbaefd46dff cni.projectcalico.org/podIP:172.28.156.249/32 cni.projectcalico.org/podIPs:172.28.156.249/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2af7 0xc003da2af8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.28.156.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j55z5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j55z5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:172.28.156.249,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4a391c975de7dd606015331a773e9a55962c7aeba856ed44fbc30faadf81704c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.28.156.249,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.968: INFO: Pod "webserver-deployment-845c8977d9-qtwsd" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qtwsd webserver-deployment-845c8977d9- deployment-4534  80b16e8f-d1bc-4343-a938-9d6de2d37406 49735 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:7ef7be300e2ec51929a169ac4dd58da62836d81058b8b1085b0917a29d260724 cni.projectcalico.org/podIP:172.30.77.135/32 cni.projectcalico.org/podIPs:172.30.77.135/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2d47 0xc003da2d48}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.77.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jphl2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jphl2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:172.30.77.135,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c1735d07de96d6b0e246ad88c958f90558cf958c28f1f07013695bd74426c837,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.77.135,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.968: INFO: Pod "webserver-deployment-845c8977d9-rf99v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rf99v webserver-deployment-845c8977d9- deployment-4534  63c4e825-dec4-4ede-bee2-a739bb45d984 49948 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da2f47 0xc003da2f48}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-drs94,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-drs94,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.968: INFO: Pod "webserver-deployment-845c8977d9-ttjr2" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-ttjr2 webserver-deployment-845c8977d9- deployment-4534  fab79cfe-8ca2-41b7-9a6c-d5c3e8cae765 50028 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:329a516faa1c4cba5b63a54c980c3f0f0337f85c07a88ab8662c54ad4a38633a cni.projectcalico.org/podIP:172.30.77.148/32 cni.projectcalico.org/podIPs:172.30.77.148/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da30c0 0xc003da30c1}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-13 03:48:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7x9g9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7x9g9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-469fm,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.102,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.969: INFO: Pod "webserver-deployment-845c8977d9-v5g9j" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-v5g9j webserver-deployment-845c8977d9- deployment-4534  87391116-fd43-4285-907b-fc3f1e797c97 49700 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9b50c216f06d6643bfd09198f160f09ac31f2abf41d4272e991ab6f4fab63525 cni.projectcalico.org/podIP:172.16.172.20/32 cni.projectcalico.org/podIPs:172.16.172.20/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da32b7 0xc003da32b8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zl5g5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zl5g5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.20,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://45aecd97006bd0f9eea740a077ff23eefa953004fb060ff2fed2739de845ee72,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.969: INFO: Pod "webserver-deployment-845c8977d9-vvr4r" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-vvr4r webserver-deployment-845c8977d9- deployment-4534  79852924-abc4-46a6-9767-369f2b6c2047 49979 0 2023-06-13 03:48:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da34c7 0xc003da34c8}] [] [{kube-controller-manager Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-13 03:48:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zmkzj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zmkzj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-l5gcd,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:48:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.104,PodIP:,StartTime:2023-06-13 03:48:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 13 03:48:03.969: INFO: Pod "webserver-deployment-845c8977d9-zgg89" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-zgg89 webserver-deployment-845c8977d9- deployment-4534  aecf639d-114c-4115-8c5b-fef290839321 49753 0 2023-06-13 03:47:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9643994a871058b197b0721ef4ed58e70c8962fa9ea9c7a2ff697b091ffac1d3 cni.projectcalico.org/podIP:172.16.172.50/32 cni.projectcalico.org/podIPs:172.16.172.50/32] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 197e6ca9-2c80-49df-8646-3be2f925c9c1 0xc003da36b7 0xc003da36b8}] [] [{kube-controller-manager Update v1 2023-06-13 03:47:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"197e6ca9-2c80-49df-8646-3be2f925c9c1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-13 03:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-13 03:47:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.172.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p527h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p527h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:sks-test-v1-25-9-workergroup-2q6k2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-13 03:47:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.255.64.103,PodIP:172.16.172.50,StartTime:2023-06-13 03:47:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-13 03:47:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://128ed21e3e049e665075f6384eb5064e327b5a3e1291ece59f00b54a65eff6b3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.172.50,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 13 03:48:03.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4534" for this suite. 06/13/23 03:48:04.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:48:04.045
Jun 13 03:48:04.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pods 06/13/23 03:48:04.046
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:04.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:04.154
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jun 13 03:48:04.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: creating the pod 06/13/23 03:48:04.182
STEP: submitting the pod to kubernetes 06/13/23 03:48:04.182
Jun 13 03:48:04.219: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536" in namespace "pods-5206" to be "running and ready"
Jun 13 03:48:04.254: INFO: Pod "pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536": Phase="Pending", Reason="", readiness=false. Elapsed: 35.338602ms
Jun 13 03:48:04.254: INFO: The phase of Pod pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:48:06.346: INFO: Pod "pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536": Phase="Pending", Reason="", readiness=false. Elapsed: 2.127701613s
Jun 13 03:48:06.347: INFO: The phase of Pod pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:48:08.298: INFO: Pod "pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536": Phase="Running", Reason="", readiness=true. Elapsed: 4.079820981s
Jun 13 03:48:08.299: INFO: The phase of Pod pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536 is Running (Ready = true)
Jun 13 03:48:08.299: INFO: Pod "pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 13 03:48:08.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5206" for this suite. 06/13/23 03:48:08.387
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":311,"skipped":5710,"failed":0}
------------------------------
• [4.373 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:48:04.045
    Jun 13 03:48:04.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pods 06/13/23 03:48:04.046
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:04.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:04.154
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jun 13 03:48:04.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: creating the pod 06/13/23 03:48:04.182
    STEP: submitting the pod to kubernetes 06/13/23 03:48:04.182
    Jun 13 03:48:04.219: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536" in namespace "pods-5206" to be "running and ready"
    Jun 13 03:48:04.254: INFO: Pod "pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536": Phase="Pending", Reason="", readiness=false. Elapsed: 35.338602ms
    Jun 13 03:48:04.254: INFO: The phase of Pod pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:48:06.346: INFO: Pod "pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536": Phase="Pending", Reason="", readiness=false. Elapsed: 2.127701613s
    Jun 13 03:48:06.347: INFO: The phase of Pod pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:48:08.298: INFO: Pod "pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536": Phase="Running", Reason="", readiness=true. Elapsed: 4.079820981s
    Jun 13 03:48:08.299: INFO: The phase of Pod pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536 is Running (Ready = true)
    Jun 13 03:48:08.299: INFO: Pod "pod-logs-websocket-1f24e8fc-56ac-4b90-9a50-399fb73d0536" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 13 03:48:08.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5206" for this suite. 06/13/23 03:48:08.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:48:08.419
Jun 13 03:48:08.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 03:48:08.421
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:08.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:08.496
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-3ed780bc-5f2f-4e72-9ab2-2e28966c87b1 06/13/23 03:48:08.507
STEP: Creating a pod to test consume secrets 06/13/23 03:48:08.521
Jun 13 03:48:08.545: INFO: Waiting up to 5m0s for pod "pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd" in namespace "secrets-5080" to be "Succeeded or Failed"
Jun 13 03:48:08.567: INFO: Pod "pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.138125ms
Jun 13 03:48:10.584: INFO: Pod "pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038796371s
Jun 13 03:48:12.590: INFO: Pod "pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04529937s
Jun 13 03:48:14.621: INFO: Pod "pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075766974s
STEP: Saw pod success 06/13/23 03:48:14.621
Jun 13 03:48:14.621: INFO: Pod "pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd" satisfied condition "Succeeded or Failed"
Jun 13 03:48:14.684: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd container secret-volume-test: <nil>
STEP: delete the pod 06/13/23 03:48:14.839
Jun 13 03:48:14.903: INFO: Waiting for pod pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd to disappear
Jun 13 03:48:14.991: INFO: Pod pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 13 03:48:14.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5080" for this suite. 06/13/23 03:48:15.023
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":312,"skipped":5717,"failed":0}
------------------------------
• [SLOW TEST] [6.628 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:48:08.419
    Jun 13 03:48:08.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 03:48:08.421
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:08.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:08.496
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-3ed780bc-5f2f-4e72-9ab2-2e28966c87b1 06/13/23 03:48:08.507
    STEP: Creating a pod to test consume secrets 06/13/23 03:48:08.521
    Jun 13 03:48:08.545: INFO: Waiting up to 5m0s for pod "pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd" in namespace "secrets-5080" to be "Succeeded or Failed"
    Jun 13 03:48:08.567: INFO: Pod "pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd": Phase="Pending", Reason="", readiness=false. Elapsed: 22.138125ms
    Jun 13 03:48:10.584: INFO: Pod "pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038796371s
    Jun 13 03:48:12.590: INFO: Pod "pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04529937s
    Jun 13 03:48:14.621: INFO: Pod "pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.075766974s
    STEP: Saw pod success 06/13/23 03:48:14.621
    Jun 13 03:48:14.621: INFO: Pod "pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd" satisfied condition "Succeeded or Failed"
    Jun 13 03:48:14.684: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd container secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:48:14.839
    Jun 13 03:48:14.903: INFO: Waiting for pod pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd to disappear
    Jun 13 03:48:14.991: INFO: Pod pod-secrets-db4353c5-a77b-4c09-91e4-e5a618bef3dd no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 03:48:14.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5080" for this suite. 06/13/23 03:48:15.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:48:15.051
Jun 13 03:48:15.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 03:48:15.053
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:15.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:15.143
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 03:48:15.233
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:48:15.984
STEP: Deploying the webhook pod 06/13/23 03:48:16.049
STEP: Wait for the deployment to be ready 06/13/23 03:48:16.121
Jun 13 03:48:16.223: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 13 03:48:18.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 48, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 48, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 48, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 48, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/13/23 03:48:20.351
STEP: Verifying the service has paired with the endpoint 06/13/23 03:48:20.744
Jun 13 03:48:21.744: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 06/13/23 03:48:21.757
STEP: Registering slow webhook via the AdmissionRegistration API 06/13/23 03:48:21.758
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 06/13/23 03:48:21.812
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 06/13/23 03:48:22.856
STEP: Registering slow webhook via the AdmissionRegistration API 06/13/23 03:48:22.856
STEP: Having no error when timeout is longer than webhook latency 06/13/23 03:48:24.003
STEP: Registering slow webhook via the AdmissionRegistration API 06/13/23 03:48:24.003
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 06/13/23 03:48:29.137
STEP: Registering slow webhook via the AdmissionRegistration API 06/13/23 03:48:29.137
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:48:34.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8530" for this suite. 06/13/23 03:48:34.313
STEP: Destroying namespace "webhook-8530-markers" for this suite. 06/13/23 03:48:34.349
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":313,"skipped":5767,"failed":0}
------------------------------
• [SLOW TEST] [19.545 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:48:15.051
    Jun 13 03:48:15.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 03:48:15.053
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:15.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:15.143
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 03:48:15.233
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:48:15.984
    STEP: Deploying the webhook pod 06/13/23 03:48:16.049
    STEP: Wait for the deployment to be ready 06/13/23 03:48:16.121
    Jun 13 03:48:16.223: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 13 03:48:18.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 48, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 48, 16, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 48, 16, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 48, 16, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/13/23 03:48:20.351
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:48:20.744
    Jun 13 03:48:21.744: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 06/13/23 03:48:21.757
    STEP: Registering slow webhook via the AdmissionRegistration API 06/13/23 03:48:21.758
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 06/13/23 03:48:21.812
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 06/13/23 03:48:22.856
    STEP: Registering slow webhook via the AdmissionRegistration API 06/13/23 03:48:22.856
    STEP: Having no error when timeout is longer than webhook latency 06/13/23 03:48:24.003
    STEP: Registering slow webhook via the AdmissionRegistration API 06/13/23 03:48:24.003
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 06/13/23 03:48:29.137
    STEP: Registering slow webhook via the AdmissionRegistration API 06/13/23 03:48:29.137
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:48:34.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8530" for this suite. 06/13/23 03:48:34.313
    STEP: Destroying namespace "webhook-8530-markers" for this suite. 06/13/23 03:48:34.349
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:48:34.598
Jun 13 03:48:34.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 03:48:34.599
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:34.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:34.674
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 06/13/23 03:48:34.69
Jun 13 03:48:34.719: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-db616313-057e-437b-9e19-eeebbbd5a867" in namespace "emptydir-9233" to be "running"
Jun 13 03:48:34.730: INFO: Pod "pod-sharedvolume-db616313-057e-437b-9e19-eeebbbd5a867": Phase="Pending", Reason="", readiness=false. Elapsed: 11.768936ms
Jun 13 03:48:36.740: INFO: Pod "pod-sharedvolume-db616313-057e-437b-9e19-eeebbbd5a867": Phase="Running", Reason="", readiness=false. Elapsed: 2.020962948s
Jun 13 03:48:36.740: INFO: Pod "pod-sharedvolume-db616313-057e-437b-9e19-eeebbbd5a867" satisfied condition "running"
STEP: Reading file content from the nginx-container 06/13/23 03:48:36.74
Jun 13 03:48:36.740: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9233 PodName:pod-sharedvolume-db616313-057e-437b-9e19-eeebbbd5a867 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:48:36.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:48:36.741: INFO: ExecWithOptions: Clientset creation
Jun 13 03:48:36.741: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-9233/pods/pod-sharedvolume-db616313-057e-437b-9e19-eeebbbd5a867/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jun 13 03:48:36.880: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 03:48:36.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9233" for this suite. 06/13/23 03:48:36.898
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":314,"skipped":5782,"failed":0}
------------------------------
• [2.372 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:48:34.598
    Jun 13 03:48:34.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 03:48:34.599
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:34.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:34.674
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 06/13/23 03:48:34.69
    Jun 13 03:48:34.719: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-db616313-057e-437b-9e19-eeebbbd5a867" in namespace "emptydir-9233" to be "running"
    Jun 13 03:48:34.730: INFO: Pod "pod-sharedvolume-db616313-057e-437b-9e19-eeebbbd5a867": Phase="Pending", Reason="", readiness=false. Elapsed: 11.768936ms
    Jun 13 03:48:36.740: INFO: Pod "pod-sharedvolume-db616313-057e-437b-9e19-eeebbbd5a867": Phase="Running", Reason="", readiness=false. Elapsed: 2.020962948s
    Jun 13 03:48:36.740: INFO: Pod "pod-sharedvolume-db616313-057e-437b-9e19-eeebbbd5a867" satisfied condition "running"
    STEP: Reading file content from the nginx-container 06/13/23 03:48:36.74
    Jun 13 03:48:36.740: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9233 PodName:pod-sharedvolume-db616313-057e-437b-9e19-eeebbbd5a867 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:48:36.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:48:36.741: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:48:36.741: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-9233/pods/pod-sharedvolume-db616313-057e-437b-9e19-eeebbbd5a867/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jun 13 03:48:36.880: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 03:48:36.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9233" for this suite. 06/13/23 03:48:36.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:48:36.971
Jun 13 03:48:36.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename proxy 06/13/23 03:48:36.973
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:37.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:37.287
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 06/13/23 03:48:37.539
STEP: creating replication controller proxy-service-t4gbv in namespace proxy-9724 06/13/23 03:48:37.54
I0613 03:48:37.571481      18 runners.go:193] Created replication controller with name: proxy-service-t4gbv, namespace: proxy-9724, replica count: 1
I0613 03:48:38.623159      18 runners.go:193] proxy-service-t4gbv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0613 03:48:39.624347      18 runners.go:193] proxy-service-t4gbv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 13 03:48:39.632: INFO: setup took 2.336977342s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 06/13/23 03:48:39.632
Jun 13 03:48:39.657: INFO: (0) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 24.710242ms)
Jun 13 03:48:39.657: INFO: (0) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 24.286598ms)
Jun 13 03:48:39.657: INFO: (0) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 25.187807ms)
Jun 13 03:48:39.657: INFO: (0) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 23.586267ms)
Jun 13 03:48:39.657: INFO: (0) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 24.993554ms)
Jun 13 03:48:39.659: INFO: (0) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 25.121137ms)
Jun 13 03:48:39.664: INFO: (0) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 30.37803ms)
Jun 13 03:48:39.668: INFO: (0) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 33.739298ms)
Jun 13 03:48:39.668: INFO: (0) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 35.1221ms)
Jun 13 03:48:39.676: INFO: (0) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 42.102096ms)
Jun 13 03:48:39.682: INFO: (0) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 48.597122ms)
Jun 13 03:48:39.682: INFO: (0) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 48.2255ms)
Jun 13 03:48:39.682: INFO: (0) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 48.499463ms)
Jun 13 03:48:39.682: INFO: (0) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 48.365567ms)
Jun 13 03:48:39.689: INFO: (0) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 56.164222ms)
Jun 13 03:48:39.689: INFO: (0) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 54.917801ms)
Jun 13 03:48:39.702: INFO: (1) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 12.891972ms)
Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 36.824761ms)
Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 37.057088ms)
Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 37.533458ms)
Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 36.778504ms)
Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 37.042341ms)
Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 37.022645ms)
Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 36.982252ms)
Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 36.852094ms)
Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 37.624927ms)
Jun 13 03:48:39.729: INFO: (1) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 38.622491ms)
Jun 13 03:48:39.733: INFO: (1) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 42.912635ms)
Jun 13 03:48:39.733: INFO: (1) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 43.065997ms)
Jun 13 03:48:39.733: INFO: (1) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 43.240282ms)
Jun 13 03:48:39.733: INFO: (1) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 43.187395ms)
Jun 13 03:48:39.733: INFO: (1) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 43.569376ms)
Jun 13 03:48:39.763: INFO: (2) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 28.028506ms)
Jun 13 03:48:39.772: INFO: (2) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 37.021782ms)
Jun 13 03:48:39.776: INFO: (2) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 42.081676ms)
Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 42.314201ms)
Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 41.844423ms)
Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 42.877136ms)
Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 41.683956ms)
Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 42.058604ms)
Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 43.321742ms)
Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 42.31669ms)
Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 41.776436ms)
Jun 13 03:48:39.779: INFO: (2) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 45.283472ms)
Jun 13 03:48:39.779: INFO: (2) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 44.522892ms)
Jun 13 03:48:39.779: INFO: (2) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 45.297128ms)
Jun 13 03:48:39.780: INFO: (2) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 46.488413ms)
Jun 13 03:48:39.780: INFO: (2) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 46.731278ms)
Jun 13 03:48:39.795: INFO: (3) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 14.877221ms)
Jun 13 03:48:39.803: INFO: (3) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 22.217929ms)
Jun 13 03:48:39.804: INFO: (3) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 22.882356ms)
Jun 13 03:48:39.804: INFO: (3) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 23.122933ms)
Jun 13 03:48:39.813: INFO: (3) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 31.607936ms)
Jun 13 03:48:39.813: INFO: (3) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 32.04191ms)
Jun 13 03:48:39.813: INFO: (3) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 31.750254ms)
Jun 13 03:48:39.813: INFO: (3) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 31.588739ms)
Jun 13 03:48:39.813: INFO: (3) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 31.684309ms)
Jun 13 03:48:39.813: INFO: (3) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 31.658891ms)
Jun 13 03:48:39.819: INFO: (3) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 37.654543ms)
Jun 13 03:48:39.822: INFO: (3) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 41.060109ms)
Jun 13 03:48:39.822: INFO: (3) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 40.985564ms)
Jun 13 03:48:39.823: INFO: (3) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 41.101137ms)
Jun 13 03:48:39.823: INFO: (3) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 41.480028ms)
Jun 13 03:48:39.823: INFO: (3) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 41.143316ms)
Jun 13 03:48:39.835: INFO: (4) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 12.858554ms)
Jun 13 03:48:39.855: INFO: (4) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 30.685521ms)
Jun 13 03:48:39.855: INFO: (4) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 30.8424ms)
Jun 13 03:48:39.855: INFO: (4) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 30.331876ms)
Jun 13 03:48:39.855: INFO: (4) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 30.38773ms)
Jun 13 03:48:39.856: INFO: (4) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 30.636227ms)
Jun 13 03:48:39.856: INFO: (4) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 31.686158ms)
Jun 13 03:48:39.858: INFO: (4) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 34.024665ms)
Jun 13 03:48:39.859: INFO: (4) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 34.994318ms)
Jun 13 03:48:39.859: INFO: (4) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 35.271073ms)
Jun 13 03:48:39.903: INFO: (4) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 79.023006ms)
Jun 13 03:48:39.903: INFO: (4) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 78.814098ms)
Jun 13 03:48:39.903: INFO: (4) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 78.414823ms)
Jun 13 03:48:39.904: INFO: (4) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 78.63576ms)
Jun 13 03:48:39.904: INFO: (4) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 78.807122ms)
Jun 13 03:48:39.904: INFO: (4) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 79.468807ms)
Jun 13 03:48:39.916: INFO: (5) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 11.035298ms)
Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 12.733778ms)
Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 12.706443ms)
Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 13.211932ms)
Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 12.662825ms)
Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 13.638074ms)
Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 12.579524ms)
Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 13.475779ms)
Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 13.236304ms)
Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 13.016262ms)
Jun 13 03:48:39.920: INFO: (5) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 14.980119ms)
Jun 13 03:48:39.934: INFO: (5) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 28.432902ms)
Jun 13 03:48:39.934: INFO: (5) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 28.83836ms)
Jun 13 03:48:39.935: INFO: (5) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 29.857103ms)
Jun 13 03:48:39.935: INFO: (5) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 30.220933ms)
Jun 13 03:48:39.935: INFO: (5) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 29.653915ms)
Jun 13 03:48:39.986: INFO: (6) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 50.828075ms)
Jun 13 03:48:39.986: INFO: (6) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 51.212505ms)
Jun 13 03:48:39.986: INFO: (6) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 51.105287ms)
Jun 13 03:48:39.986: INFO: (6) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 51.395004ms)
Jun 13 03:48:39.986: INFO: (6) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 51.410135ms)
Jun 13 03:48:39.987: INFO: (6) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 51.298036ms)
Jun 13 03:48:39.987: INFO: (6) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 51.763333ms)
Jun 13 03:48:39.987: INFO: (6) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 51.165891ms)
Jun 13 03:48:39.987: INFO: (6) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 51.452175ms)
Jun 13 03:48:39.994: INFO: (6) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 58.676336ms)
Jun 13 03:48:39.995: INFO: (6) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 59.491189ms)
Jun 13 03:48:39.995: INFO: (6) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 59.675316ms)
Jun 13 03:48:39.997: INFO: (6) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 61.647915ms)
Jun 13 03:48:39.997: INFO: (6) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 61.342156ms)
Jun 13 03:48:40.009: INFO: (6) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 74.104291ms)
Jun 13 03:48:40.009: INFO: (6) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 73.677138ms)
Jun 13 03:48:40.034: INFO: (7) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 22.732141ms)
Jun 13 03:48:40.034: INFO: (7) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 22.788099ms)
Jun 13 03:48:40.038: INFO: (7) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 28.376074ms)
Jun 13 03:48:40.038: INFO: (7) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 27.319991ms)
Jun 13 03:48:40.038: INFO: (7) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 28.895676ms)
Jun 13 03:48:40.038: INFO: (7) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 29.038829ms)
Jun 13 03:48:40.038: INFO: (7) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 28.872817ms)
Jun 13 03:48:40.038: INFO: (7) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 28.807348ms)
Jun 13 03:48:40.045: INFO: (7) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 34.717144ms)
Jun 13 03:48:40.045: INFO: (7) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 36.235701ms)
Jun 13 03:48:40.049: INFO: (7) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 38.674918ms)
Jun 13 03:48:40.059: INFO: (7) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 48.734076ms)
Jun 13 03:48:40.062: INFO: (7) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 51.92498ms)
Jun 13 03:48:40.062: INFO: (7) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 51.233566ms)
Jun 13 03:48:40.062: INFO: (7) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 51.671309ms)
Jun 13 03:48:40.062: INFO: (7) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 51.780323ms)
Jun 13 03:48:40.102: INFO: (8) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 39.800934ms)
Jun 13 03:48:40.106: INFO: (8) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 43.828491ms)
Jun 13 03:48:40.107: INFO: (8) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 44.355608ms)
Jun 13 03:48:40.107: INFO: (8) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 44.252184ms)
Jun 13 03:48:40.107: INFO: (8) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 44.094901ms)
Jun 13 03:48:40.107: INFO: (8) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 44.55391ms)
Jun 13 03:48:40.108: INFO: (8) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 45.923847ms)
Jun 13 03:48:40.108: INFO: (8) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 45.231072ms)
Jun 13 03:48:40.115: INFO: (8) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 52.546478ms)
Jun 13 03:48:40.115: INFO: (8) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 52.743336ms)
Jun 13 03:48:40.117: INFO: (8) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 55.707208ms)
Jun 13 03:48:40.118: INFO: (8) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 55.513421ms)
Jun 13 03:48:40.117: INFO: (8) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 54.489973ms)
Jun 13 03:48:40.119: INFO: (8) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 55.773424ms)
Jun 13 03:48:40.119: INFO: (8) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 55.7081ms)
Jun 13 03:48:40.137: INFO: (8) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 74.923166ms)
Jun 13 03:48:40.155: INFO: (9) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 17.724864ms)
Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 31.810418ms)
Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 32.41628ms)
Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 32.340811ms)
Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 32.71628ms)
Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 32.97606ms)
Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 32.89012ms)
Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 33.310194ms)
Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 31.918775ms)
Jun 13 03:48:40.188: INFO: (9) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 49.111419ms)
Jun 13 03:48:40.195: INFO: (9) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 55.767084ms)
Jun 13 03:48:40.195: INFO: (9) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 56.545067ms)
Jun 13 03:48:40.196: INFO: (9) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 57.402059ms)
Jun 13 03:48:40.196: INFO: (9) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 56.915922ms)
Jun 13 03:48:40.200: INFO: (9) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 60.533197ms)
Jun 13 03:48:40.200: INFO: (9) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 60.50129ms)
Jun 13 03:48:40.220: INFO: (10) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 19.730462ms)
Jun 13 03:48:40.225: INFO: (10) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 23.882203ms)
Jun 13 03:48:40.225: INFO: (10) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 24.731754ms)
Jun 13 03:48:40.228: INFO: (10) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 28.351795ms)
Jun 13 03:48:40.239: INFO: (10) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 38.449051ms)
Jun 13 03:48:40.240: INFO: (10) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 39.353085ms)
Jun 13 03:48:40.240: INFO: (10) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 39.560827ms)
Jun 13 03:48:40.240: INFO: (10) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 39.756849ms)
Jun 13 03:48:40.240: INFO: (10) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 38.913709ms)
Jun 13 03:48:40.242: INFO: (10) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 42.599983ms)
Jun 13 03:48:40.242: INFO: (10) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 41.958404ms)
Jun 13 03:48:40.246: INFO: (10) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 44.915965ms)
Jun 13 03:48:40.260: INFO: (10) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 58.819245ms)
Jun 13 03:48:40.262: INFO: (10) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 60.628501ms)
Jun 13 03:48:40.262: INFO: (10) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 61.619161ms)
Jun 13 03:48:40.262: INFO: (10) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 60.766529ms)
Jun 13 03:48:40.304: INFO: (11) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 40.486938ms)
Jun 13 03:48:40.304: INFO: (11) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 40.353573ms)
Jun 13 03:48:40.304: INFO: (11) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 40.999787ms)
Jun 13 03:48:40.304: INFO: (11) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 40.511279ms)
Jun 13 03:48:40.304: INFO: (11) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 41.253833ms)
Jun 13 03:48:40.305: INFO: (11) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 41.672147ms)
Jun 13 03:48:40.305: INFO: (11) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 41.834266ms)
Jun 13 03:48:40.305: INFO: (11) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 42.448462ms)
Jun 13 03:48:40.305: INFO: (11) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 43.4008ms)
Jun 13 03:48:40.306: INFO: (11) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 41.982532ms)
Jun 13 03:48:40.306: INFO: (11) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 42.532749ms)
Jun 13 03:48:40.306: INFO: (11) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 43.009085ms)
Jun 13 03:48:40.306: INFO: (11) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 43.291486ms)
Jun 13 03:48:40.306: INFO: (11) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 43.214736ms)
Jun 13 03:48:40.309: INFO: (11) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 46.048014ms)
Jun 13 03:48:40.333: INFO: (11) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 68.944379ms)
Jun 13 03:48:40.359: INFO: (12) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 26.536728ms)
Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 60.295929ms)
Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 59.594682ms)
Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 60.50579ms)
Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 59.757172ms)
Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 59.726083ms)
Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 60.299735ms)
Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 59.872749ms)
Jun 13 03:48:40.396: INFO: (12) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 62.767023ms)
Jun 13 03:48:40.398: INFO: (12) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 64.52351ms)
Jun 13 03:48:40.398: INFO: (12) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 64.955462ms)
Jun 13 03:48:40.406: INFO: (12) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 72.709254ms)
Jun 13 03:48:40.406: INFO: (12) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 72.585461ms)
Jun 13 03:48:40.406: INFO: (12) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 72.485753ms)
Jun 13 03:48:40.406: INFO: (12) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 72.694367ms)
Jun 13 03:48:40.406: INFO: (12) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 72.580735ms)
Jun 13 03:48:40.430: INFO: (13) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 23.136ms)
Jun 13 03:48:40.433: INFO: (13) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 25.805121ms)
Jun 13 03:48:40.433: INFO: (13) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 26.407718ms)
Jun 13 03:48:40.433: INFO: (13) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 26.294032ms)
Jun 13 03:48:40.433: INFO: (13) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 25.811317ms)
Jun 13 03:48:40.443: INFO: (13) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 36.368552ms)
Jun 13 03:48:40.444: INFO: (13) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 36.216576ms)
Jun 13 03:48:40.444: INFO: (13) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 36.72865ms)
Jun 13 03:48:40.444: INFO: (13) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 36.405215ms)
Jun 13 03:48:40.444: INFO: (13) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 36.918951ms)
Jun 13 03:48:40.444: INFO: (13) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 37.476803ms)
Jun 13 03:48:40.445: INFO: (13) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 37.172217ms)
Jun 13 03:48:40.445: INFO: (13) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 37.466599ms)
Jun 13 03:48:40.445: INFO: (13) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 37.213328ms)
Jun 13 03:48:40.446: INFO: (13) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 38.239689ms)
Jun 13 03:48:40.448: INFO: (13) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 40.777377ms)
Jun 13 03:48:40.508: INFO: (14) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 59.58072ms)
Jun 13 03:48:40.508: INFO: (14) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 59.550577ms)
Jun 13 03:48:40.508: INFO: (14) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 58.908718ms)
Jun 13 03:48:40.508: INFO: (14) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 59.484618ms)
Jun 13 03:48:40.508: INFO: (14) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 60.042356ms)
Jun 13 03:48:40.508: INFO: (14) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 59.610898ms)
Jun 13 03:48:40.510: INFO: (14) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 60.879365ms)
Jun 13 03:48:40.510: INFO: (14) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 61.075088ms)
Jun 13 03:48:40.512: INFO: (14) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 63.189272ms)
Jun 13 03:48:40.513: INFO: (14) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 64.514961ms)
Jun 13 03:48:40.519: INFO: (14) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 70.742487ms)
Jun 13 03:48:40.528: INFO: (14) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 79.609335ms)
Jun 13 03:48:40.528: INFO: (14) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 79.510308ms)
Jun 13 03:48:40.528: INFO: (14) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 80.100444ms)
Jun 13 03:48:40.529: INFO: (14) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 79.764395ms)
Jun 13 03:48:40.529: INFO: (14) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 80.0946ms)
Jun 13 03:48:40.553: INFO: (15) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 24.118792ms)
Jun 13 03:48:40.588: INFO: (15) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 58.155481ms)
Jun 13 03:48:40.588: INFO: (15) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 58.189129ms)
Jun 13 03:48:40.588: INFO: (15) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 58.343963ms)
Jun 13 03:48:40.588: INFO: (15) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 58.18638ms)
Jun 13 03:48:40.591: INFO: (15) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 61.317258ms)
Jun 13 03:48:40.597: INFO: (15) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 67.210053ms)
Jun 13 03:48:40.597: INFO: (15) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 66.977949ms)
Jun 13 03:48:40.597: INFO: (15) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 67.237239ms)
Jun 13 03:48:40.597: INFO: (15) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 67.717901ms)
Jun 13 03:48:40.597: INFO: (15) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 67.464342ms)
Jun 13 03:48:40.610: INFO: (15) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 80.445147ms)
Jun 13 03:48:40.610: INFO: (15) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 80.973115ms)
Jun 13 03:48:40.611: INFO: (15) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 80.852218ms)
Jun 13 03:48:40.611: INFO: (15) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 81.144988ms)
Jun 13 03:48:40.611: INFO: (15) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 81.060249ms)
Jun 13 03:48:40.657: INFO: (16) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 46.597121ms)
Jun 13 03:48:40.664: INFO: (16) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 53.335172ms)
Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 53.341888ms)
Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 53.305552ms)
Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 53.655703ms)
Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 54.030618ms)
Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 53.436559ms)
Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 53.487899ms)
Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 54.119641ms)
Jun 13 03:48:40.666: INFO: (16) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 54.624175ms)
Jun 13 03:48:40.675: INFO: (16) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 64.052325ms)
Jun 13 03:48:40.678: INFO: (16) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 67.197263ms)
Jun 13 03:48:40.678: INFO: (16) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 67.33514ms)
Jun 13 03:48:40.678: INFO: (16) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 67.140418ms)
Jun 13 03:48:40.678: INFO: (16) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 67.218993ms)
Jun 13 03:48:40.678: INFO: (16) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 67.491046ms)
Jun 13 03:48:40.714: INFO: (17) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 35.399133ms)
Jun 13 03:48:40.716: INFO: (17) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 35.23224ms)
Jun 13 03:48:40.716: INFO: (17) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 35.629937ms)
Jun 13 03:48:40.716: INFO: (17) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 35.281782ms)
Jun 13 03:48:40.716: INFO: (17) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 37.365328ms)
Jun 13 03:48:40.716: INFO: (17) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 37.164032ms)
Jun 13 03:48:40.721: INFO: (17) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 42.484232ms)
Jun 13 03:48:40.721: INFO: (17) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 42.620982ms)
Jun 13 03:48:40.721: INFO: (17) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 40.894236ms)
Jun 13 03:48:40.721: INFO: (17) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 41.648891ms)
Jun 13 03:48:40.731: INFO: (17) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 50.270033ms)
Jun 13 03:48:40.740: INFO: (17) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 60.212457ms)
Jun 13 03:48:40.741: INFO: (17) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 60.391614ms)
Jun 13 03:48:40.741: INFO: (17) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 61.568611ms)
Jun 13 03:48:40.741: INFO: (17) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 60.410984ms)
Jun 13 03:48:40.741: INFO: (17) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 60.55139ms)
Jun 13 03:48:40.768: INFO: (18) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 25.728079ms)
Jun 13 03:48:40.769: INFO: (18) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 26.190093ms)
Jun 13 03:48:40.771: INFO: (18) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 28.707517ms)
Jun 13 03:48:40.771: INFO: (18) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 29.013979ms)
Jun 13 03:48:40.771: INFO: (18) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 27.842084ms)
Jun 13 03:48:40.771: INFO: (18) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 27.827306ms)
Jun 13 03:48:40.772: INFO: (18) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 29.51335ms)
Jun 13 03:48:40.772: INFO: (18) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 29.66628ms)
Jun 13 03:48:40.790: INFO: (18) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 47.78562ms)
Jun 13 03:48:40.791: INFO: (18) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 48.226611ms)
Jun 13 03:48:40.791: INFO: (18) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 47.614679ms)
Jun 13 03:48:40.795: INFO: (18) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 53.695815ms)
Jun 13 03:48:40.795: INFO: (18) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 52.235193ms)
Jun 13 03:48:40.795: INFO: (18) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 52.463831ms)
Jun 13 03:48:40.796: INFO: (18) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 53.159526ms)
Jun 13 03:48:40.801: INFO: (18) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 58.134769ms)
Jun 13 03:48:40.812: INFO: (19) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 10.651067ms)
Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 20.478811ms)
Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 19.916458ms)
Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 20.530632ms)
Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 20.854615ms)
Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 20.406414ms)
Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 20.383494ms)
Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 20.074191ms)
Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 20.600174ms)
Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 20.028408ms)
Jun 13 03:48:40.834: INFO: (19) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 31.921748ms)
Jun 13 03:48:40.836: INFO: (19) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 34.649965ms)
Jun 13 03:48:40.836: INFO: (19) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 34.263463ms)
Jun 13 03:48:40.836: INFO: (19) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 34.676444ms)
Jun 13 03:48:40.841: INFO: (19) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 39.536379ms)
Jun 13 03:48:40.841: INFO: (19) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 39.7671ms)
STEP: deleting ReplicationController proxy-service-t4gbv in namespace proxy-9724, will wait for the garbage collector to delete the pods 06/13/23 03:48:40.841
Jun 13 03:48:40.934: INFO: Deleting ReplicationController proxy-service-t4gbv took: 27.418882ms
Jun 13 03:48:41.036: INFO: Terminating ReplicationController proxy-service-t4gbv pods took: 101.255273ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jun 13 03:48:43.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9724" for this suite. 06/13/23 03:48:43.456
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":315,"skipped":5796,"failed":0}
------------------------------
• [SLOW TEST] [6.545 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:48:36.971
    Jun 13 03:48:36.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename proxy 06/13/23 03:48:36.973
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:37.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:37.287
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 06/13/23 03:48:37.539
    STEP: creating replication controller proxy-service-t4gbv in namespace proxy-9724 06/13/23 03:48:37.54
    I0613 03:48:37.571481      18 runners.go:193] Created replication controller with name: proxy-service-t4gbv, namespace: proxy-9724, replica count: 1
    I0613 03:48:38.623159      18 runners.go:193] proxy-service-t4gbv Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0613 03:48:39.624347      18 runners.go:193] proxy-service-t4gbv Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 13 03:48:39.632: INFO: setup took 2.336977342s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 06/13/23 03:48:39.632
    Jun 13 03:48:39.657: INFO: (0) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 24.710242ms)
    Jun 13 03:48:39.657: INFO: (0) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 24.286598ms)
    Jun 13 03:48:39.657: INFO: (0) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 25.187807ms)
    Jun 13 03:48:39.657: INFO: (0) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 23.586267ms)
    Jun 13 03:48:39.657: INFO: (0) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 24.993554ms)
    Jun 13 03:48:39.659: INFO: (0) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 25.121137ms)
    Jun 13 03:48:39.664: INFO: (0) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 30.37803ms)
    Jun 13 03:48:39.668: INFO: (0) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 33.739298ms)
    Jun 13 03:48:39.668: INFO: (0) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 35.1221ms)
    Jun 13 03:48:39.676: INFO: (0) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 42.102096ms)
    Jun 13 03:48:39.682: INFO: (0) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 48.597122ms)
    Jun 13 03:48:39.682: INFO: (0) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 48.2255ms)
    Jun 13 03:48:39.682: INFO: (0) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 48.499463ms)
    Jun 13 03:48:39.682: INFO: (0) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 48.365567ms)
    Jun 13 03:48:39.689: INFO: (0) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 56.164222ms)
    Jun 13 03:48:39.689: INFO: (0) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 54.917801ms)
    Jun 13 03:48:39.702: INFO: (1) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 12.891972ms)
    Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 36.824761ms)
    Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 37.057088ms)
    Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 37.533458ms)
    Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 36.778504ms)
    Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 37.042341ms)
    Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 37.022645ms)
    Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 36.982252ms)
    Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 36.852094ms)
    Jun 13 03:48:39.727: INFO: (1) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 37.624927ms)
    Jun 13 03:48:39.729: INFO: (1) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 38.622491ms)
    Jun 13 03:48:39.733: INFO: (1) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 42.912635ms)
    Jun 13 03:48:39.733: INFO: (1) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 43.065997ms)
    Jun 13 03:48:39.733: INFO: (1) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 43.240282ms)
    Jun 13 03:48:39.733: INFO: (1) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 43.187395ms)
    Jun 13 03:48:39.733: INFO: (1) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 43.569376ms)
    Jun 13 03:48:39.763: INFO: (2) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 28.028506ms)
    Jun 13 03:48:39.772: INFO: (2) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 37.021782ms)
    Jun 13 03:48:39.776: INFO: (2) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 42.081676ms)
    Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 42.314201ms)
    Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 41.844423ms)
    Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 42.877136ms)
    Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 41.683956ms)
    Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 42.058604ms)
    Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 43.321742ms)
    Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 42.31669ms)
    Jun 13 03:48:39.777: INFO: (2) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 41.776436ms)
    Jun 13 03:48:39.779: INFO: (2) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 45.283472ms)
    Jun 13 03:48:39.779: INFO: (2) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 44.522892ms)
    Jun 13 03:48:39.779: INFO: (2) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 45.297128ms)
    Jun 13 03:48:39.780: INFO: (2) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 46.488413ms)
    Jun 13 03:48:39.780: INFO: (2) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 46.731278ms)
    Jun 13 03:48:39.795: INFO: (3) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 14.877221ms)
    Jun 13 03:48:39.803: INFO: (3) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 22.217929ms)
    Jun 13 03:48:39.804: INFO: (3) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 22.882356ms)
    Jun 13 03:48:39.804: INFO: (3) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 23.122933ms)
    Jun 13 03:48:39.813: INFO: (3) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 31.607936ms)
    Jun 13 03:48:39.813: INFO: (3) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 32.04191ms)
    Jun 13 03:48:39.813: INFO: (3) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 31.750254ms)
    Jun 13 03:48:39.813: INFO: (3) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 31.588739ms)
    Jun 13 03:48:39.813: INFO: (3) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 31.684309ms)
    Jun 13 03:48:39.813: INFO: (3) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 31.658891ms)
    Jun 13 03:48:39.819: INFO: (3) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 37.654543ms)
    Jun 13 03:48:39.822: INFO: (3) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 41.060109ms)
    Jun 13 03:48:39.822: INFO: (3) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 40.985564ms)
    Jun 13 03:48:39.823: INFO: (3) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 41.101137ms)
    Jun 13 03:48:39.823: INFO: (3) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 41.480028ms)
    Jun 13 03:48:39.823: INFO: (3) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 41.143316ms)
    Jun 13 03:48:39.835: INFO: (4) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 12.858554ms)
    Jun 13 03:48:39.855: INFO: (4) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 30.685521ms)
    Jun 13 03:48:39.855: INFO: (4) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 30.8424ms)
    Jun 13 03:48:39.855: INFO: (4) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 30.331876ms)
    Jun 13 03:48:39.855: INFO: (4) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 30.38773ms)
    Jun 13 03:48:39.856: INFO: (4) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 30.636227ms)
    Jun 13 03:48:39.856: INFO: (4) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 31.686158ms)
    Jun 13 03:48:39.858: INFO: (4) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 34.024665ms)
    Jun 13 03:48:39.859: INFO: (4) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 34.994318ms)
    Jun 13 03:48:39.859: INFO: (4) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 35.271073ms)
    Jun 13 03:48:39.903: INFO: (4) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 79.023006ms)
    Jun 13 03:48:39.903: INFO: (4) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 78.814098ms)
    Jun 13 03:48:39.903: INFO: (4) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 78.414823ms)
    Jun 13 03:48:39.904: INFO: (4) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 78.63576ms)
    Jun 13 03:48:39.904: INFO: (4) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 78.807122ms)
    Jun 13 03:48:39.904: INFO: (4) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 79.468807ms)
    Jun 13 03:48:39.916: INFO: (5) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 11.035298ms)
    Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 12.733778ms)
    Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 12.706443ms)
    Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 13.211932ms)
    Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 12.662825ms)
    Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 13.638074ms)
    Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 12.579524ms)
    Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 13.475779ms)
    Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 13.236304ms)
    Jun 13 03:48:39.918: INFO: (5) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 13.016262ms)
    Jun 13 03:48:39.920: INFO: (5) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 14.980119ms)
    Jun 13 03:48:39.934: INFO: (5) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 28.432902ms)
    Jun 13 03:48:39.934: INFO: (5) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 28.83836ms)
    Jun 13 03:48:39.935: INFO: (5) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 29.857103ms)
    Jun 13 03:48:39.935: INFO: (5) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 30.220933ms)
    Jun 13 03:48:39.935: INFO: (5) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 29.653915ms)
    Jun 13 03:48:39.986: INFO: (6) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 50.828075ms)
    Jun 13 03:48:39.986: INFO: (6) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 51.212505ms)
    Jun 13 03:48:39.986: INFO: (6) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 51.105287ms)
    Jun 13 03:48:39.986: INFO: (6) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 51.395004ms)
    Jun 13 03:48:39.986: INFO: (6) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 51.410135ms)
    Jun 13 03:48:39.987: INFO: (6) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 51.298036ms)
    Jun 13 03:48:39.987: INFO: (6) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 51.763333ms)
    Jun 13 03:48:39.987: INFO: (6) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 51.165891ms)
    Jun 13 03:48:39.987: INFO: (6) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 51.452175ms)
    Jun 13 03:48:39.994: INFO: (6) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 58.676336ms)
    Jun 13 03:48:39.995: INFO: (6) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 59.491189ms)
    Jun 13 03:48:39.995: INFO: (6) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 59.675316ms)
    Jun 13 03:48:39.997: INFO: (6) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 61.647915ms)
    Jun 13 03:48:39.997: INFO: (6) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 61.342156ms)
    Jun 13 03:48:40.009: INFO: (6) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 74.104291ms)
    Jun 13 03:48:40.009: INFO: (6) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 73.677138ms)
    Jun 13 03:48:40.034: INFO: (7) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 22.732141ms)
    Jun 13 03:48:40.034: INFO: (7) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 22.788099ms)
    Jun 13 03:48:40.038: INFO: (7) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 28.376074ms)
    Jun 13 03:48:40.038: INFO: (7) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 27.319991ms)
    Jun 13 03:48:40.038: INFO: (7) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 28.895676ms)
    Jun 13 03:48:40.038: INFO: (7) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 29.038829ms)
    Jun 13 03:48:40.038: INFO: (7) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 28.872817ms)
    Jun 13 03:48:40.038: INFO: (7) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 28.807348ms)
    Jun 13 03:48:40.045: INFO: (7) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 34.717144ms)
    Jun 13 03:48:40.045: INFO: (7) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 36.235701ms)
    Jun 13 03:48:40.049: INFO: (7) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 38.674918ms)
    Jun 13 03:48:40.059: INFO: (7) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 48.734076ms)
    Jun 13 03:48:40.062: INFO: (7) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 51.92498ms)
    Jun 13 03:48:40.062: INFO: (7) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 51.233566ms)
    Jun 13 03:48:40.062: INFO: (7) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 51.671309ms)
    Jun 13 03:48:40.062: INFO: (7) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 51.780323ms)
    Jun 13 03:48:40.102: INFO: (8) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 39.800934ms)
    Jun 13 03:48:40.106: INFO: (8) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 43.828491ms)
    Jun 13 03:48:40.107: INFO: (8) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 44.355608ms)
    Jun 13 03:48:40.107: INFO: (8) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 44.252184ms)
    Jun 13 03:48:40.107: INFO: (8) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 44.094901ms)
    Jun 13 03:48:40.107: INFO: (8) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 44.55391ms)
    Jun 13 03:48:40.108: INFO: (8) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 45.923847ms)
    Jun 13 03:48:40.108: INFO: (8) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 45.231072ms)
    Jun 13 03:48:40.115: INFO: (8) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 52.546478ms)
    Jun 13 03:48:40.115: INFO: (8) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 52.743336ms)
    Jun 13 03:48:40.117: INFO: (8) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 55.707208ms)
    Jun 13 03:48:40.118: INFO: (8) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 55.513421ms)
    Jun 13 03:48:40.117: INFO: (8) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 54.489973ms)
    Jun 13 03:48:40.119: INFO: (8) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 55.773424ms)
    Jun 13 03:48:40.119: INFO: (8) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 55.7081ms)
    Jun 13 03:48:40.137: INFO: (8) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 74.923166ms)
    Jun 13 03:48:40.155: INFO: (9) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 17.724864ms)
    Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 31.810418ms)
    Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 32.41628ms)
    Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 32.340811ms)
    Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 32.71628ms)
    Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 32.97606ms)
    Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 32.89012ms)
    Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 33.310194ms)
    Jun 13 03:48:40.171: INFO: (9) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 31.918775ms)
    Jun 13 03:48:40.188: INFO: (9) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 49.111419ms)
    Jun 13 03:48:40.195: INFO: (9) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 55.767084ms)
    Jun 13 03:48:40.195: INFO: (9) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 56.545067ms)
    Jun 13 03:48:40.196: INFO: (9) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 57.402059ms)
    Jun 13 03:48:40.196: INFO: (9) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 56.915922ms)
    Jun 13 03:48:40.200: INFO: (9) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 60.533197ms)
    Jun 13 03:48:40.200: INFO: (9) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 60.50129ms)
    Jun 13 03:48:40.220: INFO: (10) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 19.730462ms)
    Jun 13 03:48:40.225: INFO: (10) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 23.882203ms)
    Jun 13 03:48:40.225: INFO: (10) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 24.731754ms)
    Jun 13 03:48:40.228: INFO: (10) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 28.351795ms)
    Jun 13 03:48:40.239: INFO: (10) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 38.449051ms)
    Jun 13 03:48:40.240: INFO: (10) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 39.353085ms)
    Jun 13 03:48:40.240: INFO: (10) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 39.560827ms)
    Jun 13 03:48:40.240: INFO: (10) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 39.756849ms)
    Jun 13 03:48:40.240: INFO: (10) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 38.913709ms)
    Jun 13 03:48:40.242: INFO: (10) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 42.599983ms)
    Jun 13 03:48:40.242: INFO: (10) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 41.958404ms)
    Jun 13 03:48:40.246: INFO: (10) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 44.915965ms)
    Jun 13 03:48:40.260: INFO: (10) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 58.819245ms)
    Jun 13 03:48:40.262: INFO: (10) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 60.628501ms)
    Jun 13 03:48:40.262: INFO: (10) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 61.619161ms)
    Jun 13 03:48:40.262: INFO: (10) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 60.766529ms)
    Jun 13 03:48:40.304: INFO: (11) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 40.486938ms)
    Jun 13 03:48:40.304: INFO: (11) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 40.353573ms)
    Jun 13 03:48:40.304: INFO: (11) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 40.999787ms)
    Jun 13 03:48:40.304: INFO: (11) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 40.511279ms)
    Jun 13 03:48:40.304: INFO: (11) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 41.253833ms)
    Jun 13 03:48:40.305: INFO: (11) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 41.672147ms)
    Jun 13 03:48:40.305: INFO: (11) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 41.834266ms)
    Jun 13 03:48:40.305: INFO: (11) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 42.448462ms)
    Jun 13 03:48:40.305: INFO: (11) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 43.4008ms)
    Jun 13 03:48:40.306: INFO: (11) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 41.982532ms)
    Jun 13 03:48:40.306: INFO: (11) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 42.532749ms)
    Jun 13 03:48:40.306: INFO: (11) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 43.009085ms)
    Jun 13 03:48:40.306: INFO: (11) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 43.291486ms)
    Jun 13 03:48:40.306: INFO: (11) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 43.214736ms)
    Jun 13 03:48:40.309: INFO: (11) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 46.048014ms)
    Jun 13 03:48:40.333: INFO: (11) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 68.944379ms)
    Jun 13 03:48:40.359: INFO: (12) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 26.536728ms)
    Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 60.295929ms)
    Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 59.594682ms)
    Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 60.50579ms)
    Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 59.757172ms)
    Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 59.726083ms)
    Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 60.299735ms)
    Jun 13 03:48:40.394: INFO: (12) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 59.872749ms)
    Jun 13 03:48:40.396: INFO: (12) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 62.767023ms)
    Jun 13 03:48:40.398: INFO: (12) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 64.52351ms)
    Jun 13 03:48:40.398: INFO: (12) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 64.955462ms)
    Jun 13 03:48:40.406: INFO: (12) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 72.709254ms)
    Jun 13 03:48:40.406: INFO: (12) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 72.585461ms)
    Jun 13 03:48:40.406: INFO: (12) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 72.485753ms)
    Jun 13 03:48:40.406: INFO: (12) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 72.694367ms)
    Jun 13 03:48:40.406: INFO: (12) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 72.580735ms)
    Jun 13 03:48:40.430: INFO: (13) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 23.136ms)
    Jun 13 03:48:40.433: INFO: (13) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 25.805121ms)
    Jun 13 03:48:40.433: INFO: (13) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 26.407718ms)
    Jun 13 03:48:40.433: INFO: (13) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 26.294032ms)
    Jun 13 03:48:40.433: INFO: (13) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 25.811317ms)
    Jun 13 03:48:40.443: INFO: (13) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 36.368552ms)
    Jun 13 03:48:40.444: INFO: (13) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 36.216576ms)
    Jun 13 03:48:40.444: INFO: (13) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 36.72865ms)
    Jun 13 03:48:40.444: INFO: (13) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 36.405215ms)
    Jun 13 03:48:40.444: INFO: (13) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 36.918951ms)
    Jun 13 03:48:40.444: INFO: (13) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 37.476803ms)
    Jun 13 03:48:40.445: INFO: (13) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 37.172217ms)
    Jun 13 03:48:40.445: INFO: (13) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 37.466599ms)
    Jun 13 03:48:40.445: INFO: (13) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 37.213328ms)
    Jun 13 03:48:40.446: INFO: (13) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 38.239689ms)
    Jun 13 03:48:40.448: INFO: (13) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 40.777377ms)
    Jun 13 03:48:40.508: INFO: (14) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 59.58072ms)
    Jun 13 03:48:40.508: INFO: (14) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 59.550577ms)
    Jun 13 03:48:40.508: INFO: (14) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 58.908718ms)
    Jun 13 03:48:40.508: INFO: (14) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 59.484618ms)
    Jun 13 03:48:40.508: INFO: (14) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 60.042356ms)
    Jun 13 03:48:40.508: INFO: (14) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 59.610898ms)
    Jun 13 03:48:40.510: INFO: (14) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 60.879365ms)
    Jun 13 03:48:40.510: INFO: (14) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 61.075088ms)
    Jun 13 03:48:40.512: INFO: (14) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 63.189272ms)
    Jun 13 03:48:40.513: INFO: (14) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 64.514961ms)
    Jun 13 03:48:40.519: INFO: (14) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 70.742487ms)
    Jun 13 03:48:40.528: INFO: (14) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 79.609335ms)
    Jun 13 03:48:40.528: INFO: (14) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 79.510308ms)
    Jun 13 03:48:40.528: INFO: (14) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 80.100444ms)
    Jun 13 03:48:40.529: INFO: (14) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 79.764395ms)
    Jun 13 03:48:40.529: INFO: (14) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 80.0946ms)
    Jun 13 03:48:40.553: INFO: (15) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 24.118792ms)
    Jun 13 03:48:40.588: INFO: (15) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 58.155481ms)
    Jun 13 03:48:40.588: INFO: (15) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 58.189129ms)
    Jun 13 03:48:40.588: INFO: (15) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 58.343963ms)
    Jun 13 03:48:40.588: INFO: (15) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 58.18638ms)
    Jun 13 03:48:40.591: INFO: (15) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 61.317258ms)
    Jun 13 03:48:40.597: INFO: (15) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 67.210053ms)
    Jun 13 03:48:40.597: INFO: (15) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 66.977949ms)
    Jun 13 03:48:40.597: INFO: (15) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 67.237239ms)
    Jun 13 03:48:40.597: INFO: (15) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 67.717901ms)
    Jun 13 03:48:40.597: INFO: (15) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 67.464342ms)
    Jun 13 03:48:40.610: INFO: (15) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 80.445147ms)
    Jun 13 03:48:40.610: INFO: (15) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 80.973115ms)
    Jun 13 03:48:40.611: INFO: (15) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 80.852218ms)
    Jun 13 03:48:40.611: INFO: (15) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 81.144988ms)
    Jun 13 03:48:40.611: INFO: (15) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 81.060249ms)
    Jun 13 03:48:40.657: INFO: (16) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 46.597121ms)
    Jun 13 03:48:40.664: INFO: (16) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 53.335172ms)
    Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 53.341888ms)
    Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 53.305552ms)
    Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 53.655703ms)
    Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 54.030618ms)
    Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 53.436559ms)
    Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 53.487899ms)
    Jun 13 03:48:40.665: INFO: (16) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 54.119641ms)
    Jun 13 03:48:40.666: INFO: (16) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 54.624175ms)
    Jun 13 03:48:40.675: INFO: (16) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 64.052325ms)
    Jun 13 03:48:40.678: INFO: (16) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 67.197263ms)
    Jun 13 03:48:40.678: INFO: (16) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 67.33514ms)
    Jun 13 03:48:40.678: INFO: (16) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 67.140418ms)
    Jun 13 03:48:40.678: INFO: (16) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 67.218993ms)
    Jun 13 03:48:40.678: INFO: (16) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 67.491046ms)
    Jun 13 03:48:40.714: INFO: (17) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 35.399133ms)
    Jun 13 03:48:40.716: INFO: (17) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 35.23224ms)
    Jun 13 03:48:40.716: INFO: (17) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 35.629937ms)
    Jun 13 03:48:40.716: INFO: (17) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 35.281782ms)
    Jun 13 03:48:40.716: INFO: (17) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 37.365328ms)
    Jun 13 03:48:40.716: INFO: (17) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 37.164032ms)
    Jun 13 03:48:40.721: INFO: (17) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 42.484232ms)
    Jun 13 03:48:40.721: INFO: (17) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 42.620982ms)
    Jun 13 03:48:40.721: INFO: (17) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 40.894236ms)
    Jun 13 03:48:40.721: INFO: (17) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 41.648891ms)
    Jun 13 03:48:40.731: INFO: (17) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 50.270033ms)
    Jun 13 03:48:40.740: INFO: (17) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 60.212457ms)
    Jun 13 03:48:40.741: INFO: (17) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 60.391614ms)
    Jun 13 03:48:40.741: INFO: (17) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 61.568611ms)
    Jun 13 03:48:40.741: INFO: (17) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 60.410984ms)
    Jun 13 03:48:40.741: INFO: (17) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 60.55139ms)
    Jun 13 03:48:40.768: INFO: (18) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 25.728079ms)
    Jun 13 03:48:40.769: INFO: (18) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 26.190093ms)
    Jun 13 03:48:40.771: INFO: (18) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 28.707517ms)
    Jun 13 03:48:40.771: INFO: (18) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 29.013979ms)
    Jun 13 03:48:40.771: INFO: (18) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 27.842084ms)
    Jun 13 03:48:40.771: INFO: (18) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 27.827306ms)
    Jun 13 03:48:40.772: INFO: (18) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 29.51335ms)
    Jun 13 03:48:40.772: INFO: (18) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 29.66628ms)
    Jun 13 03:48:40.790: INFO: (18) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 47.78562ms)
    Jun 13 03:48:40.791: INFO: (18) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 48.226611ms)
    Jun 13 03:48:40.791: INFO: (18) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 47.614679ms)
    Jun 13 03:48:40.795: INFO: (18) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 53.695815ms)
    Jun 13 03:48:40.795: INFO: (18) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 52.235193ms)
    Jun 13 03:48:40.795: INFO: (18) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 52.463831ms)
    Jun 13 03:48:40.796: INFO: (18) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 53.159526ms)
    Jun 13 03:48:40.801: INFO: (18) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 58.134769ms)
    Jun 13 03:48:40.812: INFO: (19) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:462/proxy/: tls qux (200; 10.651067ms)
    Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 20.478811ms)
    Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">test<... (200; 19.916458ms)
    Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 20.530632ms)
    Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9:160/proxy/: foo (200; 20.854615ms)
    Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/proxy-service-t4gbv-m5zm9/proxy/rewriteme">test</a> (200; 20.406414ms)
    Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:1080/proxy/rewriteme">... (200; 20.383494ms)
    Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/: <a href="/api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:443/proxy/tlsrewritem... (200; 20.074191ms)
    Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/https:proxy-service-t4gbv-m5zm9:460/proxy/: tls baz (200; 20.600174ms)
    Jun 13 03:48:40.822: INFO: (19) /api/v1/namespaces/proxy-9724/pods/http:proxy-service-t4gbv-m5zm9:162/proxy/: bar (200; 20.028408ms)
    Jun 13 03:48:40.834: INFO: (19) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname2/proxy/: tls qux (200; 31.921748ms)
    Jun 13 03:48:40.836: INFO: (19) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname2/proxy/: bar (200; 34.649965ms)
    Jun 13 03:48:40.836: INFO: (19) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname1/proxy/: foo (200; 34.263463ms)
    Jun 13 03:48:40.836: INFO: (19) /api/v1/namespaces/proxy-9724/services/proxy-service-t4gbv:portname2/proxy/: bar (200; 34.676444ms)
    Jun 13 03:48:40.841: INFO: (19) /api/v1/namespaces/proxy-9724/services/https:proxy-service-t4gbv:tlsportname1/proxy/: tls baz (200; 39.536379ms)
    Jun 13 03:48:40.841: INFO: (19) /api/v1/namespaces/proxy-9724/services/http:proxy-service-t4gbv:portname1/proxy/: foo (200; 39.7671ms)
    STEP: deleting ReplicationController proxy-service-t4gbv in namespace proxy-9724, will wait for the garbage collector to delete the pods 06/13/23 03:48:40.841
    Jun 13 03:48:40.934: INFO: Deleting ReplicationController proxy-service-t4gbv took: 27.418882ms
    Jun 13 03:48:41.036: INFO: Terminating ReplicationController proxy-service-t4gbv pods took: 101.255273ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jun 13 03:48:43.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-9724" for this suite. 06/13/23 03:48:43.456
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:48:43.518
Jun 13 03:48:43.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:48:43.52
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:43.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:43.608
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 06/13/23 03:48:43.618
Jun 13 03:48:43.645: INFO: Waiting up to 5m0s for pod "annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66" in namespace "downward-api-4469" to be "running and ready"
Jun 13 03:48:43.677: INFO: Pod "annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66": Phase="Pending", Reason="", readiness=false. Elapsed: 31.575894ms
Jun 13 03:48:43.677: INFO: The phase of Pod annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:48:45.749: INFO: Pod "annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66": Phase="Running", Reason="", readiness=true. Elapsed: 2.103969618s
Jun 13 03:48:45.749: INFO: The phase of Pod annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66 is Running (Ready = true)
Jun 13 03:48:45.749: INFO: Pod "annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66" satisfied condition "running and ready"
Jun 13 03:48:46.374: INFO: Successfully updated pod "annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 13 03:48:48.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4469" for this suite. 06/13/23 03:48:48.463
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":316,"skipped":5800,"failed":0}
------------------------------
• [4.976 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:48:43.518
    Jun 13 03:48:43.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:48:43.52
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:43.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:43.608
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 06/13/23 03:48:43.618
    Jun 13 03:48:43.645: INFO: Waiting up to 5m0s for pod "annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66" in namespace "downward-api-4469" to be "running and ready"
    Jun 13 03:48:43.677: INFO: Pod "annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66": Phase="Pending", Reason="", readiness=false. Elapsed: 31.575894ms
    Jun 13 03:48:43.677: INFO: The phase of Pod annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:48:45.749: INFO: Pod "annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66": Phase="Running", Reason="", readiness=true. Elapsed: 2.103969618s
    Jun 13 03:48:45.749: INFO: The phase of Pod annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66 is Running (Ready = true)
    Jun 13 03:48:45.749: INFO: Pod "annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66" satisfied condition "running and ready"
    Jun 13 03:48:46.374: INFO: Successfully updated pod "annotationupdate31cdbf31-75b0-4424-a31b-6c6ea6fd6d66"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 13 03:48:48.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4469" for this suite. 06/13/23 03:48:48.463
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:48:48.498
Jun 13 03:48:48.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename secrets 06/13/23 03:48:48.499
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:48.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:48.554
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-cc3c6211-bb8b-410a-bb2f-abfa1909ca26 06/13/23 03:48:48.563
STEP: Creating a pod to test consume secrets 06/13/23 03:48:48.589
Jun 13 03:48:48.618: INFO: Waiting up to 5m0s for pod "pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77" in namespace "secrets-8577" to be "Succeeded or Failed"
Jun 13 03:48:48.641: INFO: Pod "pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77": Phase="Pending", Reason="", readiness=false. Elapsed: 22.178886ms
Jun 13 03:48:50.699: INFO: Pod "pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080615143s
Jun 13 03:48:52.657: INFO: Pod "pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038903342s
STEP: Saw pod success 06/13/23 03:48:52.657
Jun 13 03:48:52.657: INFO: Pod "pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77" satisfied condition "Succeeded or Failed"
Jun 13 03:48:52.671: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77 container secret-volume-test: <nil>
STEP: delete the pod 06/13/23 03:48:52.7
Jun 13 03:48:52.729: INFO: Waiting for pod pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77 to disappear
Jun 13 03:48:52.740: INFO: Pod pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 13 03:48:52.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8577" for this suite. 06/13/23 03:48:52.754
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":317,"skipped":5870,"failed":0}
------------------------------
• [4.332 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:48:48.498
    Jun 13 03:48:48.498: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename secrets 06/13/23 03:48:48.499
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:48.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:48.554
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-cc3c6211-bb8b-410a-bb2f-abfa1909ca26 06/13/23 03:48:48.563
    STEP: Creating a pod to test consume secrets 06/13/23 03:48:48.589
    Jun 13 03:48:48.618: INFO: Waiting up to 5m0s for pod "pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77" in namespace "secrets-8577" to be "Succeeded or Failed"
    Jun 13 03:48:48.641: INFO: Pod "pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77": Phase="Pending", Reason="", readiness=false. Elapsed: 22.178886ms
    Jun 13 03:48:50.699: INFO: Pod "pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.080615143s
    Jun 13 03:48:52.657: INFO: Pod "pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.038903342s
    STEP: Saw pod success 06/13/23 03:48:52.657
    Jun 13 03:48:52.657: INFO: Pod "pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77" satisfied condition "Succeeded or Failed"
    Jun 13 03:48:52.671: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77 container secret-volume-test: <nil>
    STEP: delete the pod 06/13/23 03:48:52.7
    Jun 13 03:48:52.729: INFO: Waiting for pod pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77 to disappear
    Jun 13 03:48:52.740: INFO: Pod pod-secrets-4c5585a9-6106-4e41-8429-1a31cfe96b77 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 13 03:48:52.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8577" for this suite. 06/13/23 03:48:52.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:48:52.832
Jun 13 03:48:52.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename server-version 06/13/23 03:48:52.834
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:52.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:52.953
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 06/13/23 03:48:52.96
STEP: Confirm major version 06/13/23 03:48:52.963
Jun 13 03:48:52.963: INFO: Major version: 1
STEP: Confirm minor version 06/13/23 03:48:52.963
Jun 13 03:48:52.964: INFO: cleanMinorVersion: 25
Jun 13 03:48:52.964: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jun 13 03:48:52.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8223" for this suite. 06/13/23 03:48:52.984
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":318,"skipped":5912,"failed":0}
------------------------------
• [0.173 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:48:52.832
    Jun 13 03:48:52.833: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename server-version 06/13/23 03:48:52.834
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:52.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:52.953
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 06/13/23 03:48:52.96
    STEP: Confirm major version 06/13/23 03:48:52.963
    Jun 13 03:48:52.963: INFO: Major version: 1
    STEP: Confirm minor version 06/13/23 03:48:52.963
    Jun 13 03:48:52.964: INFO: cleanMinorVersion: 25
    Jun 13 03:48:52.964: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jun 13 03:48:52.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-8223" for this suite. 06/13/23 03:48:52.984
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:48:53.005
Jun 13 03:48:53.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:48:53.007
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:53.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:53.087
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 06/13/23 03:48:53.115
Jun 13 03:48:53.163: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294" in namespace "downward-api-7192" to be "Succeeded or Failed"
Jun 13 03:48:53.181: INFO: Pod "downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294": Phase="Pending", Reason="", readiness=false. Elapsed: 17.959231ms
Jun 13 03:48:55.190: INFO: Pod "downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027778824s
Jun 13 03:48:57.193: INFO: Pod "downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030152363s
Jun 13 03:48:59.194: INFO: Pod "downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030942754s
STEP: Saw pod success 06/13/23 03:48:59.194
Jun 13 03:48:59.194: INFO: Pod "downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294" satisfied condition "Succeeded or Failed"
Jun 13 03:48:59.202: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294 container client-container: <nil>
STEP: delete the pod 06/13/23 03:48:59.227
Jun 13 03:48:59.255: INFO: Waiting for pod downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294 to disappear
Jun 13 03:48:59.263: INFO: Pod downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 13 03:48:59.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7192" for this suite. 06/13/23 03:48:59.278
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":319,"skipped":5914,"failed":0}
------------------------------
• [SLOW TEST] [6.289 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:48:53.005
    Jun 13 03:48:53.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:48:53.007
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:53.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:53.087
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 06/13/23 03:48:53.115
    Jun 13 03:48:53.163: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294" in namespace "downward-api-7192" to be "Succeeded or Failed"
    Jun 13 03:48:53.181: INFO: Pod "downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294": Phase="Pending", Reason="", readiness=false. Elapsed: 17.959231ms
    Jun 13 03:48:55.190: INFO: Pod "downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027778824s
    Jun 13 03:48:57.193: INFO: Pod "downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030152363s
    Jun 13 03:48:59.194: INFO: Pod "downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030942754s
    STEP: Saw pod success 06/13/23 03:48:59.194
    Jun 13 03:48:59.194: INFO: Pod "downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294" satisfied condition "Succeeded or Failed"
    Jun 13 03:48:59.202: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294 container client-container: <nil>
    STEP: delete the pod 06/13/23 03:48:59.227
    Jun 13 03:48:59.255: INFO: Waiting for pod downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294 to disappear
    Jun 13 03:48:59.263: INFO: Pod downwardapi-volume-e8a55602-f37f-4d41-913d-3d814037f294 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 13 03:48:59.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7192" for this suite. 06/13/23 03:48:59.278
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:48:59.295
Jun 13 03:48:59.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 03:48:59.297
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:59.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:59.34
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 06/13/23 03:48:59.348
Jun 13 03:48:59.376: INFO: Waiting up to 5m0s for pod "pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c" in namespace "emptydir-3970" to be "Succeeded or Failed"
Jun 13 03:48:59.388: INFO: Pod "pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.755123ms
Jun 13 03:49:01.397: INFO: Pod "pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020528872s
Jun 13 03:49:03.419: INFO: Pod "pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042736316s
STEP: Saw pod success 06/13/23 03:49:03.419
Jun 13 03:49:03.420: INFO: Pod "pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c" satisfied condition "Succeeded or Failed"
Jun 13 03:49:03.445: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c container test-container: <nil>
STEP: delete the pod 06/13/23 03:49:03.463
Jun 13 03:49:03.502: INFO: Waiting for pod pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c to disappear
Jun 13 03:49:03.528: INFO: Pod pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 03:49:03.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3970" for this suite. 06/13/23 03:49:03.557
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":320,"skipped":5915,"failed":0}
------------------------------
• [4.298 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:48:59.295
    Jun 13 03:48:59.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 03:48:59.297
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:48:59.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:48:59.34
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 06/13/23 03:48:59.348
    Jun 13 03:48:59.376: INFO: Waiting up to 5m0s for pod "pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c" in namespace "emptydir-3970" to be "Succeeded or Failed"
    Jun 13 03:48:59.388: INFO: Pod "pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 11.755123ms
    Jun 13 03:49:01.397: INFO: Pod "pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020528872s
    Jun 13 03:49:03.419: INFO: Pod "pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042736316s
    STEP: Saw pod success 06/13/23 03:49:03.419
    Jun 13 03:49:03.420: INFO: Pod "pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c" satisfied condition "Succeeded or Failed"
    Jun 13 03:49:03.445: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c container test-container: <nil>
    STEP: delete the pod 06/13/23 03:49:03.463
    Jun 13 03:49:03.502: INFO: Waiting for pod pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c to disappear
    Jun 13 03:49:03.528: INFO: Pod pod-cb8d9ed8-1732-4bb0-a23e-996b94119f6c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 03:49:03.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3970" for this suite. 06/13/23 03:49:03.557
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:49:03.593
Jun 13 03:49:03.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename runtimeclass 06/13/23 03:49:03.595
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:49:03.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:49:03.672
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jun 13 03:49:03.733: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5074 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 13 03:49:03.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5074" for this suite. 06/13/23 03:49:03.803
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":321,"skipped":5917,"failed":0}
------------------------------
• [0.235 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:49:03.593
    Jun 13 03:49:03.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename runtimeclass 06/13/23 03:49:03.595
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:49:03.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:49:03.672
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jun 13 03:49:03.733: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5074 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 13 03:49:03.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5074" for this suite. 06/13/23 03:49:03.803
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:49:03.829
Jun 13 03:49:03.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename watch 06/13/23 03:49:03.83
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:49:03.868
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:49:03.883
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 06/13/23 03:49:03.915
STEP: creating a new configmap 06/13/23 03:49:03.929
STEP: modifying the configmap once 06/13/23 03:49:03.945
STEP: changing the label value of the configmap 06/13/23 03:49:03.978
STEP: Expecting to observe a delete notification for the watched object 06/13/23 03:49:04.016
Jun 13 03:49:04.017: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7373  77b05820-6368-4ab2-bddf-a3871525cfad 51062 0 2023-06-13 03:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-13 03:49:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 03:49:04.017: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7373  77b05820-6368-4ab2-bddf-a3871525cfad 51064 0 2023-06-13 03:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-13 03:49:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 03:49:04.017: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7373  77b05820-6368-4ab2-bddf-a3871525cfad 51067 0 2023-06-13 03:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-13 03:49:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 06/13/23 03:49:04.017
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 06/13/23 03:49:04.056
STEP: changing the label value of the configmap back 06/13/23 03:49:14.056
STEP: modifying the configmap a third time 06/13/23 03:49:14.084
STEP: deleting the configmap 06/13/23 03:49:14.133
STEP: Expecting to observe an add notification for the watched object when the label value was restored 06/13/23 03:49:14.159
Jun 13 03:49:14.159: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7373  77b05820-6368-4ab2-bddf-a3871525cfad 51132 0 2023-06-13 03:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-13 03:49:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 03:49:14.160: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7373  77b05820-6368-4ab2-bddf-a3871525cfad 51133 0 2023-06-13 03:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-13 03:49:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 13 03:49:14.160: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7373  77b05820-6368-4ab2-bddf-a3871525cfad 51134 0 2023-06-13 03:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-13 03:49:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 13 03:49:14.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7373" for this suite. 06/13/23 03:49:14.185
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":322,"skipped":5920,"failed":0}
------------------------------
• [SLOW TEST] [10.399 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:49:03.829
    Jun 13 03:49:03.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename watch 06/13/23 03:49:03.83
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:49:03.868
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:49:03.883
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 06/13/23 03:49:03.915
    STEP: creating a new configmap 06/13/23 03:49:03.929
    STEP: modifying the configmap once 06/13/23 03:49:03.945
    STEP: changing the label value of the configmap 06/13/23 03:49:03.978
    STEP: Expecting to observe a delete notification for the watched object 06/13/23 03:49:04.016
    Jun 13 03:49:04.017: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7373  77b05820-6368-4ab2-bddf-a3871525cfad 51062 0 2023-06-13 03:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-13 03:49:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 03:49:04.017: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7373  77b05820-6368-4ab2-bddf-a3871525cfad 51064 0 2023-06-13 03:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-13 03:49:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 03:49:04.017: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7373  77b05820-6368-4ab2-bddf-a3871525cfad 51067 0 2023-06-13 03:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-13 03:49:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 06/13/23 03:49:04.017
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 06/13/23 03:49:04.056
    STEP: changing the label value of the configmap back 06/13/23 03:49:14.056
    STEP: modifying the configmap a third time 06/13/23 03:49:14.084
    STEP: deleting the configmap 06/13/23 03:49:14.133
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 06/13/23 03:49:14.159
    Jun 13 03:49:14.159: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7373  77b05820-6368-4ab2-bddf-a3871525cfad 51132 0 2023-06-13 03:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-13 03:49:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 03:49:14.160: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7373  77b05820-6368-4ab2-bddf-a3871525cfad 51133 0 2023-06-13 03:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-13 03:49:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 13 03:49:14.160: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7373  77b05820-6368-4ab2-bddf-a3871525cfad 51134 0 2023-06-13 03:49:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-13 03:49:14 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 13 03:49:14.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7373" for this suite. 06/13/23 03:49:14.185
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:49:14.228
Jun 13 03:49:14.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename proxy 06/13/23 03:49:14.23
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:49:14.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:49:14.288
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jun 13 03:49:14.300: INFO: Creating pod...
Jun 13 03:49:14.336: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5633" to be "running"
Jun 13 03:49:14.357: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 20.458083ms
Jun 13 03:49:16.389: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052347417s
Jun 13 03:49:18.431: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.094551697s
Jun 13 03:49:18.431: INFO: Pod "agnhost" satisfied condition "running"
Jun 13 03:49:18.431: INFO: Creating service...
Jun 13 03:49:18.577: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=DELETE
Jun 13 03:49:18.603: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 13 03:49:18.603: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=OPTIONS
Jun 13 03:49:18.643: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 13 03:49:18.643: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=PATCH
Jun 13 03:49:18.661: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 13 03:49:18.661: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=POST
Jun 13 03:49:18.734: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 13 03:49:18.734: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=PUT
Jun 13 03:49:18.801: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 13 03:49:18.802: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=DELETE
Jun 13 03:49:18.838: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 13 03:49:18.838: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jun 13 03:49:18.859: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 13 03:49:18.859: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=PATCH
Jun 13 03:49:18.891: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 13 03:49:18.892: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=POST
Jun 13 03:49:18.914: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 13 03:49:18.914: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=PUT
Jun 13 03:49:18.961: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 13 03:49:18.961: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=GET
Jun 13 03:49:18.973: INFO: http.Client request:GET StatusCode:301
Jun 13 03:49:18.973: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=GET
Jun 13 03:49:18.994: INFO: http.Client request:GET StatusCode:301
Jun 13 03:49:18.994: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=HEAD
Jun 13 03:49:19.012: INFO: http.Client request:HEAD StatusCode:301
Jun 13 03:49:19.012: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=HEAD
Jun 13 03:49:19.028: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jun 13 03:49:19.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5633" for this suite. 06/13/23 03:49:19.054
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":323,"skipped":5922,"failed":0}
------------------------------
• [4.848 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:49:14.228
    Jun 13 03:49:14.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename proxy 06/13/23 03:49:14.23
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:49:14.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:49:14.288
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jun 13 03:49:14.300: INFO: Creating pod...
    Jun 13 03:49:14.336: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5633" to be "running"
    Jun 13 03:49:14.357: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 20.458083ms
    Jun 13 03:49:16.389: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052347417s
    Jun 13 03:49:18.431: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 4.094551697s
    Jun 13 03:49:18.431: INFO: Pod "agnhost" satisfied condition "running"
    Jun 13 03:49:18.431: INFO: Creating service...
    Jun 13 03:49:18.577: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=DELETE
    Jun 13 03:49:18.603: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 13 03:49:18.603: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=OPTIONS
    Jun 13 03:49:18.643: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 13 03:49:18.643: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=PATCH
    Jun 13 03:49:18.661: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 13 03:49:18.661: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=POST
    Jun 13 03:49:18.734: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 13 03:49:18.734: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=PUT
    Jun 13 03:49:18.801: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun 13 03:49:18.802: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=DELETE
    Jun 13 03:49:18.838: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 13 03:49:18.838: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jun 13 03:49:18.859: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 13 03:49:18.859: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=PATCH
    Jun 13 03:49:18.891: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 13 03:49:18.892: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=POST
    Jun 13 03:49:18.914: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 13 03:49:18.914: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=PUT
    Jun 13 03:49:18.961: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun 13 03:49:18.961: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=GET
    Jun 13 03:49:18.973: INFO: http.Client request:GET StatusCode:301
    Jun 13 03:49:18.973: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=GET
    Jun 13 03:49:18.994: INFO: http.Client request:GET StatusCode:301
    Jun 13 03:49:18.994: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/pods/agnhost/proxy?method=HEAD
    Jun 13 03:49:19.012: INFO: http.Client request:HEAD StatusCode:301
    Jun 13 03:49:19.012: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-5633/services/e2e-proxy-test-service/proxy?method=HEAD
    Jun 13 03:49:19.028: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jun 13 03:49:19.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-5633" for this suite. 06/13/23 03:49:19.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:49:19.079
Jun 13 03:49:19.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 03:49:19.082
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:49:19.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:49:19.139
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 03:49:19.195
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:49:19.739
STEP: Deploying the webhook pod 06/13/23 03:49:19.763
STEP: Wait for the deployment to be ready 06/13/23 03:49:19.828
Jun 13 03:49:19.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 49, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 49, 19, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 49, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 49, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/13/23 03:49:21.923
STEP: Verifying the service has paired with the endpoint 06/13/23 03:49:22.008
Jun 13 03:49:23.008: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jun 13 03:49:23.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3833-crds.webhook.example.com via the AdmissionRegistration API 06/13/23 03:49:23.539
STEP: Creating a custom resource that should be mutated by the webhook 06/13/23 03:49:23.587
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:49:26.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-120" for this suite. 06/13/23 03:49:26.303
STEP: Destroying namespace "webhook-120-markers" for this suite. 06/13/23 03:49:26.327
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":324,"skipped":5961,"failed":0}
------------------------------
• [SLOW TEST] [7.499 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:49:19.079
    Jun 13 03:49:19.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 03:49:19.082
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:49:19.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:49:19.139
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 03:49:19.195
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:49:19.739
    STEP: Deploying the webhook pod 06/13/23 03:49:19.763
    STEP: Wait for the deployment to be ready 06/13/23 03:49:19.828
    Jun 13 03:49:19.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 49, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 49, 19, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 49, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 49, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/13/23 03:49:21.923
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:49:22.008
    Jun 13 03:49:23.008: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jun 13 03:49:23.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3833-crds.webhook.example.com via the AdmissionRegistration API 06/13/23 03:49:23.539
    STEP: Creating a custom resource that should be mutated by the webhook 06/13/23 03:49:23.587
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:49:26.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-120" for this suite. 06/13/23 03:49:26.303
    STEP: Destroying namespace "webhook-120-markers" for this suite. 06/13/23 03:49:26.327
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:49:26.579
Jun 13 03:49:26.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename events 06/13/23 03:49:26.581
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:49:26.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:49:26.654
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 06/13/23 03:49:26.667
STEP: get a list of Events with a label in the current namespace 06/13/23 03:49:26.756
STEP: delete a list of events 06/13/23 03:49:26.771
Jun 13 03:49:26.772: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 06/13/23 03:49:26.867
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jun 13 03:49:26.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6813" for this suite. 06/13/23 03:49:26.908
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":325,"skipped":5980,"failed":0}
------------------------------
• [0.358 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:49:26.579
    Jun 13 03:49:26.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename events 06/13/23 03:49:26.581
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:49:26.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:49:26.654
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 06/13/23 03:49:26.667
    STEP: get a list of Events with a label in the current namespace 06/13/23 03:49:26.756
    STEP: delete a list of events 06/13/23 03:49:26.771
    Jun 13 03:49:26.772: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 06/13/23 03:49:26.867
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jun 13 03:49:26.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6813" for this suite. 06/13/23 03:49:26.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:49:26.937
Jun 13 03:49:26.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename cronjob 06/13/23 03:49:26.938
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:49:27.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:49:27.043
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 06/13/23 03:49:27.055
STEP: Ensuring a job is scheduled 06/13/23 03:49:27.089
STEP: Ensuring exactly one is scheduled 06/13/23 03:50:01.112
STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/13/23 03:50:01.127
STEP: Ensuring the job is replaced with a new one 06/13/23 03:50:01.144
STEP: Removing cronjob 06/13/23 03:51:01.163
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 13 03:51:01.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-4821" for this suite. 06/13/23 03:51:01.203
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":326,"skipped":5987,"failed":0}
------------------------------
• [SLOW TEST] [94.285 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:49:26.937
    Jun 13 03:49:26.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename cronjob 06/13/23 03:49:26.938
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:49:27.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:49:27.043
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 06/13/23 03:49:27.055
    STEP: Ensuring a job is scheduled 06/13/23 03:49:27.089
    STEP: Ensuring exactly one is scheduled 06/13/23 03:50:01.112
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/13/23 03:50:01.127
    STEP: Ensuring the job is replaced with a new one 06/13/23 03:50:01.144
    STEP: Removing cronjob 06/13/23 03:51:01.163
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 13 03:51:01.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-4821" for this suite. 06/13/23 03:51:01.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:51:01.224
Jun 13 03:51:01.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename kubelet-test 06/13/23 03:51:01.226
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:01.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:01.294
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 13 03:51:01.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6597" for this suite. 06/13/23 03:51:01.406
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":327,"skipped":5993,"failed":0}
------------------------------
• [0.206 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:51:01.224
    Jun 13 03:51:01.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename kubelet-test 06/13/23 03:51:01.226
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:01.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:01.294
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 13 03:51:01.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6597" for this suite. 06/13/23 03:51:01.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:51:01.432
Jun 13 03:51:01.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename ephemeral-containers-test 06/13/23 03:51:01.434
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:01.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:01.498
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 06/13/23 03:51:01.507
Jun 13 03:51:01.532: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9469" to be "running and ready"
Jun 13 03:51:01.551: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.054841ms
Jun 13 03:51:01.552: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:51:03.568: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036026764s
Jun 13 03:51:03.568: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:51:05.570: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.038430712s
Jun 13 03:51:05.570: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jun 13 03:51:05.570: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 06/13/23 03:51:05.593
Jun 13 03:51:05.633: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9469" to be "container debugger running"
Jun 13 03:51:05.674: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 40.885848ms
Jun 13 03:51:07.685: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.052205217s
Jun 13 03:51:07.685: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 06/13/23 03:51:07.685
Jun 13 03:51:07.685: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9469 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:51:07.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:51:07.686: INFO: ExecWithOptions: Clientset creation
Jun 13 03:51:07.686: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-9469/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jun 13 03:51:07.846: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 13 03:51:07.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-9469" for this suite. 06/13/23 03:51:07.914
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":328,"skipped":6001,"failed":0}
------------------------------
• [SLOW TEST] [6.513 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:51:01.432
    Jun 13 03:51:01.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename ephemeral-containers-test 06/13/23 03:51:01.434
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:01.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:01.498
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 06/13/23 03:51:01.507
    Jun 13 03:51:01.532: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9469" to be "running and ready"
    Jun 13 03:51:01.551: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 19.054841ms
    Jun 13 03:51:01.552: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:51:03.568: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036026764s
    Jun 13 03:51:03.568: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:51:05.570: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.038430712s
    Jun 13 03:51:05.570: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jun 13 03:51:05.570: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 06/13/23 03:51:05.593
    Jun 13 03:51:05.633: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-9469" to be "container debugger running"
    Jun 13 03:51:05.674: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 40.885848ms
    Jun 13 03:51:07.685: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.052205217s
    Jun 13 03:51:07.685: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 06/13/23 03:51:07.685
    Jun 13 03:51:07.685: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-9469 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:51:07.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:51:07.686: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:51:07.686: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-9469/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jun 13 03:51:07.846: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 13 03:51:07.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-9469" for this suite. 06/13/23 03:51:07.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:51:07.949
Jun 13 03:51:07.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename gc 06/13/23 03:51:07.95
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:08.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:08.036
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 06/13/23 03:51:08.053
STEP: Wait for the Deployment to create new ReplicaSet 06/13/23 03:51:08.098
STEP: delete the deployment 06/13/23 03:51:08.298
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 06/13/23 03:51:08.342
STEP: Gathering metrics 06/13/23 03:51:09.211
Jun 13 03:51:09.282: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" in namespace "kube-system" to be "running and ready"
Jun 13 03:51:09.294: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq": Phase="Running", Reason="", readiness=true. Elapsed: 11.603789ms
Jun 13 03:51:09.294: INFO: The phase of Pod kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq is Running (Ready = true)
Jun 13 03:51:09.294: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" satisfied condition "running and ready"
Jun 13 03:51:09.510: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 13 03:51:09.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7624" for this suite. 06/13/23 03:51:09.644
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":329,"skipped":6056,"failed":0}
------------------------------
• [1.915 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:51:07.949
    Jun 13 03:51:07.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename gc 06/13/23 03:51:07.95
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:08.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:08.036
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 06/13/23 03:51:08.053
    STEP: Wait for the Deployment to create new ReplicaSet 06/13/23 03:51:08.098
    STEP: delete the deployment 06/13/23 03:51:08.298
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 06/13/23 03:51:08.342
    STEP: Gathering metrics 06/13/23 03:51:09.211
    Jun 13 03:51:09.282: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" in namespace "kube-system" to be "running and ready"
    Jun 13 03:51:09.294: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq": Phase="Running", Reason="", readiness=true. Elapsed: 11.603789ms
    Jun 13 03:51:09.294: INFO: The phase of Pod kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq is Running (Ready = true)
    Jun 13 03:51:09.294: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" satisfied condition "running and ready"
    Jun 13 03:51:09.510: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 13 03:51:09.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7624" for this suite. 06/13/23 03:51:09.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:51:09.866
Jun 13 03:51:09.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename sched-pred 06/13/23 03:51:09.867
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:09.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:09.97
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 13 03:51:09.988: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 13 03:51:10.032: INFO: Waiting for terminating namespaces to be deleted...
Jun 13 03:51:10.055: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-2q6k2 before test
Jun 13 03:51:10.086: INFO: calico-node-xb8kf from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.086: INFO: 	Container calico-node ready: true, restart count 0
Jun 13 03:51:10.086: INFO: calico-typha-79dfdd7d65-w95lk from calico-system started at 2023-06-13 02:04:34 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.086: INFO: 	Container calico-typha ready: true, restart count 0
Jun 13 03:51:10.086: INFO: csi-node-driver-ftxkk from calico-system started at 2023-06-13 03:47:27 +0000 UTC (2 container statuses recorded)
Jun 13 03:51:10.086: INFO: 	Container calico-csi ready: true, restart count 0
Jun 13 03:51:10.086: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun 13 03:51:10.086: INFO: replace-28110471-mpqnc from cronjob-4821 started at 2023-06-13 03:51:00 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.086: INFO: 	Container c ready: true, restart count 0
Jun 13 03:51:10.086: INFO: simpletest.deployment-594f986645-q989b from gc-7624 started at 2023-06-13 03:51:08 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.086: INFO: 	Container nginx ready: false, restart count 0
Jun 13 03:51:10.086: INFO: kube-proxy-g2qcx from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.086: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 13 03:51:10.086: INFO: bin-false03bf8fd3-be89-42dc-8e51-122e53956d6f from kubelet-test-6597 started at 2023-06-13 03:51:01 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.086: INFO: 	Container bin-false03bf8fd3-be89-42dc-8e51-122e53956d6f ready: false, restart count 0
Jun 13 03:51:10.086: INFO: smtx-elf-csi-driver-node-plugin-gqbkk from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
Jun 13 03:51:10.086: INFO: 	Container csi-driver ready: true, restart count 0
Jun 13 03:51:10.086: INFO: 	Container driver-registrar ready: true, restart count 0
Jun 13 03:51:10.086: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 13 03:51:10.086: INFO: sonobuoy from sonobuoy started at 2023-06-13 02:14:11 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.086: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 13 03:51:10.086: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-f9dsl from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:51:10.086: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 03:51:10.086: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 13 03:51:10.086: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-469fm before test
Jun 13 03:51:10.115: INFO: calico-node-lhmmz from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.115: INFO: 	Container calico-node ready: true, restart count 0
Jun 13 03:51:10.115: INFO: csi-node-driver-jrfzt from calico-system started at 2023-06-13 02:04:55 +0000 UTC (2 container statuses recorded)
Jun 13 03:51:10.115: INFO: 	Container calico-csi ready: true, restart count 0
Jun 13 03:51:10.115: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun 13 03:51:10.115: INFO: replace-28110470-dxzmq from cronjob-4821 started at 2023-06-13 03:50:00 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.115: INFO: 	Container c ready: true, restart count 0
Jun 13 03:51:10.115: INFO: simpletest.deployment-594f986645-kv25r from gc-7624 started at 2023-06-13 03:51:08 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.115: INFO: 	Container nginx ready: false, restart count 0
Jun 13 03:51:10.115: INFO: kube-proxy-fn2dq from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.116: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 13 03:51:10.116: INFO: snapshot-controller-76c6888c5-9m5td from kube-system started at 2023-06-13 02:04:55 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.116: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 13 03:51:10.116: INFO: smtx-elf-csi-driver-node-plugin-h8k7m from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
Jun 13 03:51:10.116: INFO: 	Container csi-driver ready: true, restart count 0
Jun 13 03:51:10.116: INFO: 	Container driver-registrar ready: true, restart count 0
Jun 13 03:51:10.116: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 13 03:51:10.116: INFO: tigera-operator-6f49bd984-r7pkz from sks-system started at 2023-06-13 02:04:29 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.116: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 13 03:51:10.116: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-hcvsr from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:51:10.116: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 03:51:10.116: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 13 03:51:10.116: INFO: 
Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-l5gcd before test
Jun 13 03:51:10.167: INFO: calico-node-fptww from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.167: INFO: 	Container calico-node ready: true, restart count 0
Jun 13 03:51:10.167: INFO: calico-typha-79dfdd7d65-nrkt5 from calico-system started at 2023-06-13 02:04:43 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.167: INFO: 	Container calico-typha ready: true, restart count 0
Jun 13 03:51:10.167: INFO: csi-node-driver-66bp4 from calico-system started at 2023-06-13 02:05:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:51:10.167: INFO: 	Container calico-csi ready: true, restart count 0
Jun 13 03:51:10.167: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Jun 13 03:51:10.167: INFO: ephemeral-containers-target-pod from ephemeral-containers-test-9469 started at 2023-06-13 03:51:01 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.167: INFO: 	Container test-container-1 ready: true, restart count 0
Jun 13 03:51:10.167: INFO: kube-proxy-669th from kube-system started at 2023-06-13 02:03:17 +0000 UTC (1 container statuses recorded)
Jun 13 03:51:10.167: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 13 03:51:10.167: INFO: smtx-elf-csi-driver-node-plugin-dtt2k from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
Jun 13 03:51:10.167: INFO: 	Container csi-driver ready: true, restart count 0
Jun 13 03:51:10.167: INFO: 	Container driver-registrar ready: true, restart count 0
Jun 13 03:51:10.167: INFO: 	Container liveness-probe ready: true, restart count 0
Jun 13 03:51:10.167: INFO: sonobuoy-e2e-job-246c2b6dcb314696 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:51:10.167: INFO: 	Container e2e ready: true, restart count 0
Jun 13 03:51:10.167: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 03:51:10.167: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-bvbp2 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
Jun 13 03:51:10.167: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 13 03:51:10.167: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 06/13/23 03:51:10.168
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17681b79c5396cf3], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 06/13/23 03:51:10.284
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:51:11.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3860" for this suite. 06/13/23 03:51:11.304
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":330,"skipped":6077,"failed":0}
------------------------------
• [1.457 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:51:09.866
    Jun 13 03:51:09.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename sched-pred 06/13/23 03:51:09.867
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:09.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:09.97
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jun 13 03:51:09.988: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 13 03:51:10.032: INFO: Waiting for terminating namespaces to be deleted...
    Jun 13 03:51:10.055: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-2q6k2 before test
    Jun 13 03:51:10.086: INFO: calico-node-xb8kf from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.086: INFO: 	Container calico-node ready: true, restart count 0
    Jun 13 03:51:10.086: INFO: calico-typha-79dfdd7d65-w95lk from calico-system started at 2023-06-13 02:04:34 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.086: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 13 03:51:10.086: INFO: csi-node-driver-ftxkk from calico-system started at 2023-06-13 03:47:27 +0000 UTC (2 container statuses recorded)
    Jun 13 03:51:10.086: INFO: 	Container calico-csi ready: true, restart count 0
    Jun 13 03:51:10.086: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun 13 03:51:10.086: INFO: replace-28110471-mpqnc from cronjob-4821 started at 2023-06-13 03:51:00 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.086: INFO: 	Container c ready: true, restart count 0
    Jun 13 03:51:10.086: INFO: simpletest.deployment-594f986645-q989b from gc-7624 started at 2023-06-13 03:51:08 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.086: INFO: 	Container nginx ready: false, restart count 0
    Jun 13 03:51:10.086: INFO: kube-proxy-g2qcx from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.086: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 13 03:51:10.086: INFO: bin-false03bf8fd3-be89-42dc-8e51-122e53956d6f from kubelet-test-6597 started at 2023-06-13 03:51:01 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.086: INFO: 	Container bin-false03bf8fd3-be89-42dc-8e51-122e53956d6f ready: false, restart count 0
    Jun 13 03:51:10.086: INFO: smtx-elf-csi-driver-node-plugin-gqbkk from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
    Jun 13 03:51:10.086: INFO: 	Container csi-driver ready: true, restart count 0
    Jun 13 03:51:10.086: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun 13 03:51:10.086: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 13 03:51:10.086: INFO: sonobuoy from sonobuoy started at 2023-06-13 02:14:11 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.086: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 13 03:51:10.086: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-f9dsl from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:51:10.086: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 03:51:10.086: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 13 03:51:10.086: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-469fm before test
    Jun 13 03:51:10.115: INFO: calico-node-lhmmz from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.115: INFO: 	Container calico-node ready: true, restart count 0
    Jun 13 03:51:10.115: INFO: csi-node-driver-jrfzt from calico-system started at 2023-06-13 02:04:55 +0000 UTC (2 container statuses recorded)
    Jun 13 03:51:10.115: INFO: 	Container calico-csi ready: true, restart count 0
    Jun 13 03:51:10.115: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun 13 03:51:10.115: INFO: replace-28110470-dxzmq from cronjob-4821 started at 2023-06-13 03:50:00 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.115: INFO: 	Container c ready: true, restart count 0
    Jun 13 03:51:10.115: INFO: simpletest.deployment-594f986645-kv25r from gc-7624 started at 2023-06-13 03:51:08 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.115: INFO: 	Container nginx ready: false, restart count 0
    Jun 13 03:51:10.115: INFO: kube-proxy-fn2dq from kube-system started at 2023-06-13 02:03:10 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.116: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 13 03:51:10.116: INFO: snapshot-controller-76c6888c5-9m5td from kube-system started at 2023-06-13 02:04:55 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.116: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun 13 03:51:10.116: INFO: smtx-elf-csi-driver-node-plugin-h8k7m from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
    Jun 13 03:51:10.116: INFO: 	Container csi-driver ready: true, restart count 0
    Jun 13 03:51:10.116: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun 13 03:51:10.116: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 13 03:51:10.116: INFO: tigera-operator-6f49bd984-r7pkz from sks-system started at 2023-06-13 02:04:29 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.116: INFO: 	Container tigera-operator ready: true, restart count 0
    Jun 13 03:51:10.116: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-hcvsr from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:51:10.116: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 03:51:10.116: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 13 03:51:10.116: INFO: 
    Logging pods the apiserver thinks is on node sks-test-v1-25-9-workergroup-l5gcd before test
    Jun 13 03:51:10.167: INFO: calico-node-fptww from calico-system started at 2023-06-13 02:04:35 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.167: INFO: 	Container calico-node ready: true, restart count 0
    Jun 13 03:51:10.167: INFO: calico-typha-79dfdd7d65-nrkt5 from calico-system started at 2023-06-13 02:04:43 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.167: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 13 03:51:10.167: INFO: csi-node-driver-66bp4 from calico-system started at 2023-06-13 02:05:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:51:10.167: INFO: 	Container calico-csi ready: true, restart count 0
    Jun 13 03:51:10.167: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Jun 13 03:51:10.167: INFO: ephemeral-containers-target-pod from ephemeral-containers-test-9469 started at 2023-06-13 03:51:01 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.167: INFO: 	Container test-container-1 ready: true, restart count 0
    Jun 13 03:51:10.167: INFO: kube-proxy-669th from kube-system started at 2023-06-13 02:03:17 +0000 UTC (1 container statuses recorded)
    Jun 13 03:51:10.167: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 13 03:51:10.167: INFO: smtx-elf-csi-driver-node-plugin-dtt2k from sks-system started at 2023-06-13 02:05:23 +0000 UTC (3 container statuses recorded)
    Jun 13 03:51:10.167: INFO: 	Container csi-driver ready: true, restart count 0
    Jun 13 03:51:10.167: INFO: 	Container driver-registrar ready: true, restart count 0
    Jun 13 03:51:10.167: INFO: 	Container liveness-probe ready: true, restart count 0
    Jun 13 03:51:10.167: INFO: sonobuoy-e2e-job-246c2b6dcb314696 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:51:10.167: INFO: 	Container e2e ready: true, restart count 0
    Jun 13 03:51:10.167: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 03:51:10.167: INFO: sonobuoy-systemd-logs-daemon-set-db8fa52093014e43-bvbp2 from sonobuoy started at 2023-06-13 02:14:20 +0000 UTC (2 container statuses recorded)
    Jun 13 03:51:10.167: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 13 03:51:10.167: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 06/13/23 03:51:10.168
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17681b79c5396cf3], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.] 06/13/23 03:51:10.284
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:51:11.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3860" for this suite. 06/13/23 03:51:11.304
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:51:11.327
Jun 13 03:51:11.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename resourcequota 06/13/23 03:51:11.329
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:11.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:11.391
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 06/13/23 03:51:28.42
STEP: Creating a ResourceQuota 06/13/23 03:51:33.43
STEP: Ensuring resource quota status is calculated 06/13/23 03:51:33.497
STEP: Creating a ConfigMap 06/13/23 03:51:35.515
STEP: Ensuring resource quota status captures configMap creation 06/13/23 03:51:35.581
STEP: Deleting a ConfigMap 06/13/23 03:51:37.595
STEP: Ensuring resource quota status released usage 06/13/23 03:51:37.618
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 13 03:51:39.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6680" for this suite. 06/13/23 03:51:39.639
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":331,"skipped":6121,"failed":0}
------------------------------
• [SLOW TEST] [28.349 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:51:11.327
    Jun 13 03:51:11.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename resourcequota 06/13/23 03:51:11.329
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:11.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:11.391
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 06/13/23 03:51:28.42
    STEP: Creating a ResourceQuota 06/13/23 03:51:33.43
    STEP: Ensuring resource quota status is calculated 06/13/23 03:51:33.497
    STEP: Creating a ConfigMap 06/13/23 03:51:35.515
    STEP: Ensuring resource quota status captures configMap creation 06/13/23 03:51:35.581
    STEP: Deleting a ConfigMap 06/13/23 03:51:37.595
    STEP: Ensuring resource quota status released usage 06/13/23 03:51:37.618
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 13 03:51:39.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6680" for this suite. 06/13/23 03:51:39.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:51:39.677
Jun 13 03:51:39.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 03:51:39.678
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:39.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:39.8
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 06/13/23 03:51:39.813
Jun 13 03:51:39.978: INFO: Waiting up to 5m0s for pod "pod-dba50679-b240-443b-a878-b878c8bbed89" in namespace "emptydir-3132" to be "Succeeded or Failed"
Jun 13 03:51:40.046: INFO: Pod "pod-dba50679-b240-443b-a878-b878c8bbed89": Phase="Pending", Reason="", readiness=false. Elapsed: 67.685784ms
Jun 13 03:51:42.060: INFO: Pod "pod-dba50679-b240-443b-a878-b878c8bbed89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081102806s
Jun 13 03:51:44.066: INFO: Pod "pod-dba50679-b240-443b-a878-b878c8bbed89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087660428s
Jun 13 03:51:46.056: INFO: Pod "pod-dba50679-b240-443b-a878-b878c8bbed89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077315841s
STEP: Saw pod success 06/13/23 03:51:46.056
Jun 13 03:51:46.056: INFO: Pod "pod-dba50679-b240-443b-a878-b878c8bbed89" satisfied condition "Succeeded or Failed"
Jun 13 03:51:46.068: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-dba50679-b240-443b-a878-b878c8bbed89 container test-container: <nil>
STEP: delete the pod 06/13/23 03:51:46.103
Jun 13 03:51:46.138: INFO: Waiting for pod pod-dba50679-b240-443b-a878-b878c8bbed89 to disappear
Jun 13 03:51:46.167: INFO: Pod pod-dba50679-b240-443b-a878-b878c8bbed89 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 03:51:46.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3132" for this suite. 06/13/23 03:51:46.18
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":332,"skipped":6134,"failed":0}
------------------------------
• [SLOW TEST] [6.538 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:51:39.677
    Jun 13 03:51:39.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 03:51:39.678
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:39.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:39.8
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 06/13/23 03:51:39.813
    Jun 13 03:51:39.978: INFO: Waiting up to 5m0s for pod "pod-dba50679-b240-443b-a878-b878c8bbed89" in namespace "emptydir-3132" to be "Succeeded or Failed"
    Jun 13 03:51:40.046: INFO: Pod "pod-dba50679-b240-443b-a878-b878c8bbed89": Phase="Pending", Reason="", readiness=false. Elapsed: 67.685784ms
    Jun 13 03:51:42.060: INFO: Pod "pod-dba50679-b240-443b-a878-b878c8bbed89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081102806s
    Jun 13 03:51:44.066: INFO: Pod "pod-dba50679-b240-443b-a878-b878c8bbed89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.087660428s
    Jun 13 03:51:46.056: INFO: Pod "pod-dba50679-b240-443b-a878-b878c8bbed89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.077315841s
    STEP: Saw pod success 06/13/23 03:51:46.056
    Jun 13 03:51:46.056: INFO: Pod "pod-dba50679-b240-443b-a878-b878c8bbed89" satisfied condition "Succeeded or Failed"
    Jun 13 03:51:46.068: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-dba50679-b240-443b-a878-b878c8bbed89 container test-container: <nil>
    STEP: delete the pod 06/13/23 03:51:46.103
    Jun 13 03:51:46.138: INFO: Waiting for pod pod-dba50679-b240-443b-a878-b878c8bbed89 to disappear
    Jun 13 03:51:46.167: INFO: Pod pod-dba50679-b240-443b-a878-b878c8bbed89 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 03:51:46.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3132" for this suite. 06/13/23 03:51:46.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:51:46.216
Jun 13 03:51:46.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 03:51:46.218
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:46.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:46.293
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 06/13/23 03:51:46.306
Jun 13 03:51:46.345: INFO: Waiting up to 5m0s for pod "pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea" in namespace "emptydir-4224" to be "Succeeded or Failed"
Jun 13 03:51:46.373: INFO: Pod "pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea": Phase="Pending", Reason="", readiness=false. Elapsed: 27.83766ms
Jun 13 03:51:48.418: INFO: Pod "pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072490634s
Jun 13 03:51:50.422: INFO: Pod "pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077006986s
Jun 13 03:51:52.385: INFO: Pod "pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039815365s
STEP: Saw pod success 06/13/23 03:51:52.385
Jun 13 03:51:52.385: INFO: Pod "pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea" satisfied condition "Succeeded or Failed"
Jun 13 03:51:52.402: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea container test-container: <nil>
STEP: delete the pod 06/13/23 03:51:52.426
Jun 13 03:51:52.482: INFO: Waiting for pod pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea to disappear
Jun 13 03:51:52.491: INFO: Pod pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 03:51:52.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4224" for this suite. 06/13/23 03:51:52.53
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":333,"skipped":6165,"failed":0}
------------------------------
• [SLOW TEST] [6.334 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:51:46.216
    Jun 13 03:51:46.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 03:51:46.218
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:46.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:46.293
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 06/13/23 03:51:46.306
    Jun 13 03:51:46.345: INFO: Waiting up to 5m0s for pod "pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea" in namespace "emptydir-4224" to be "Succeeded or Failed"
    Jun 13 03:51:46.373: INFO: Pod "pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea": Phase="Pending", Reason="", readiness=false. Elapsed: 27.83766ms
    Jun 13 03:51:48.418: INFO: Pod "pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072490634s
    Jun 13 03:51:50.422: INFO: Pod "pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.077006986s
    Jun 13 03:51:52.385: INFO: Pod "pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039815365s
    STEP: Saw pod success 06/13/23 03:51:52.385
    Jun 13 03:51:52.385: INFO: Pod "pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea" satisfied condition "Succeeded or Failed"
    Jun 13 03:51:52.402: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea container test-container: <nil>
    STEP: delete the pod 06/13/23 03:51:52.426
    Jun 13 03:51:52.482: INFO: Waiting for pod pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea to disappear
    Jun 13 03:51:52.491: INFO: Pod pod-511bcbb3-eeba-4b70-8803-f152d02fe8ea no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 03:51:52.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4224" for this suite. 06/13/23 03:51:52.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:51:52.551
Jun 13 03:51:52.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 03:51:52.553
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:52.645
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:52.661
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 06/13/23 03:51:52.682
Jun 13 03:51:52.730: INFO: Waiting up to 5m0s for pod "pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c" in namespace "emptydir-7053" to be "Succeeded or Failed"
Jun 13 03:51:52.748: INFO: Pod "pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.205047ms
Jun 13 03:51:54.760: INFO: Pod "pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029661864s
Jun 13 03:51:56.767: INFO: Pod "pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036232608s
STEP: Saw pod success 06/13/23 03:51:56.767
Jun 13 03:51:56.767: INFO: Pod "pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c" satisfied condition "Succeeded or Failed"
Jun 13 03:51:56.779: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c container test-container: <nil>
STEP: delete the pod 06/13/23 03:51:56.8
Jun 13 03:51:56.844: INFO: Waiting for pod pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c to disappear
Jun 13 03:51:56.862: INFO: Pod pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 03:51:56.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7053" for this suite. 06/13/23 03:51:56.882
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":334,"skipped":6186,"failed":0}
------------------------------
• [4.386 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:51:52.551
    Jun 13 03:51:52.551: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 03:51:52.553
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:52.645
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:52.661
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 06/13/23 03:51:52.682
    Jun 13 03:51:52.730: INFO: Waiting up to 5m0s for pod "pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c" in namespace "emptydir-7053" to be "Succeeded or Failed"
    Jun 13 03:51:52.748: INFO: Pod "pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c": Phase="Pending", Reason="", readiness=false. Elapsed: 17.205047ms
    Jun 13 03:51:54.760: INFO: Pod "pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029661864s
    Jun 13 03:51:56.767: INFO: Pod "pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036232608s
    STEP: Saw pod success 06/13/23 03:51:56.767
    Jun 13 03:51:56.767: INFO: Pod "pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c" satisfied condition "Succeeded or Failed"
    Jun 13 03:51:56.779: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c container test-container: <nil>
    STEP: delete the pod 06/13/23 03:51:56.8
    Jun 13 03:51:56.844: INFO: Waiting for pod pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c to disappear
    Jun 13 03:51:56.862: INFO: Pod pod-35cf5077-a527-4e5f-8bdf-3aa9419c297c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 03:51:56.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7053" for this suite. 06/13/23 03:51:56.882
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:51:56.937
Jun 13 03:51:56.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 03:51:56.939
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:56.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:56.994
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-b38e9f80-c34d-4356-b06d-cbd9d74ee7ab 06/13/23 03:51:57.019
STEP: Creating a pod to test consume configMaps 06/13/23 03:51:57.031
Jun 13 03:51:57.056: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60" in namespace "configmap-4662" to be "Succeeded or Failed"
Jun 13 03:51:57.178: INFO: Pod "pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60": Phase="Pending", Reason="", readiness=false. Elapsed: 122.274826ms
Jun 13 03:51:59.189: INFO: Pod "pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132916788s
Jun 13 03:52:01.201: INFO: Pod "pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.144906624s
Jun 13 03:52:03.196: INFO: Pod "pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.140045551s
STEP: Saw pod success 06/13/23 03:52:03.196
Jun 13 03:52:03.196: INFO: Pod "pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60" satisfied condition "Succeeded or Failed"
Jun 13 03:52:03.208: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60 container agnhost-container: <nil>
STEP: delete the pod 06/13/23 03:52:03.232
Jun 13 03:52:03.330: INFO: Waiting for pod pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60 to disappear
Jun 13 03:52:03.344: INFO: Pod pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 03:52:03.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4662" for this suite. 06/13/23 03:52:03.381
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":335,"skipped":6187,"failed":0}
------------------------------
• [SLOW TEST] [6.481 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:51:56.937
    Jun 13 03:51:56.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 03:51:56.939
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:51:56.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:51:56.994
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-b38e9f80-c34d-4356-b06d-cbd9d74ee7ab 06/13/23 03:51:57.019
    STEP: Creating a pod to test consume configMaps 06/13/23 03:51:57.031
    Jun 13 03:51:57.056: INFO: Waiting up to 5m0s for pod "pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60" in namespace "configmap-4662" to be "Succeeded or Failed"
    Jun 13 03:51:57.178: INFO: Pod "pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60": Phase="Pending", Reason="", readiness=false. Elapsed: 122.274826ms
    Jun 13 03:51:59.189: INFO: Pod "pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.132916788s
    Jun 13 03:52:01.201: INFO: Pod "pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.144906624s
    Jun 13 03:52:03.196: INFO: Pod "pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.140045551s
    STEP: Saw pod success 06/13/23 03:52:03.196
    Jun 13 03:52:03.196: INFO: Pod "pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60" satisfied condition "Succeeded or Failed"
    Jun 13 03:52:03.208: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60 container agnhost-container: <nil>
    STEP: delete the pod 06/13/23 03:52:03.232
    Jun 13 03:52:03.330: INFO: Waiting for pod pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60 to disappear
    Jun 13 03:52:03.344: INFO: Pod pod-configmaps-bdff1936-4af6-4f02-9352-bf3c94405a60 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 03:52:03.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4662" for this suite. 06/13/23 03:52:03.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:52:03.42
Jun 13 03:52:03.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:52:03.423
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:52:03.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:52:03.528
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-4599 06/13/23 03:52:03.537
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4599 to expose endpoints map[] 06/13/23 03:52:03.631
Jun 13 03:52:03.673: INFO: successfully validated that service multi-endpoint-test in namespace services-4599 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4599 06/13/23 03:52:03.673
Jun 13 03:52:03.722: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4599" to be "running and ready"
Jun 13 03:52:03.758: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 35.960494ms
Jun 13 03:52:03.758: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:52:05.776: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053391824s
Jun 13 03:52:05.776: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:52:07.794: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.072186378s
Jun 13 03:52:07.795: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun 13 03:52:07.795: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4599 to expose endpoints map[pod1:[100]] 06/13/23 03:52:07.817
Jun 13 03:52:07.879: INFO: successfully validated that service multi-endpoint-test in namespace services-4599 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4599 06/13/23 03:52:07.879
Jun 13 03:52:07.909: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4599" to be "running and ready"
Jun 13 03:52:07.933: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.114905ms
Jun 13 03:52:07.933: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:52:09.943: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034347607s
Jun 13 03:52:09.943: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:52:11.995: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.086320985s
Jun 13 03:52:11.995: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun 13 03:52:11.995: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4599 to expose endpoints map[pod1:[100] pod2:[101]] 06/13/23 03:52:12.016
Jun 13 03:52:12.232: INFO: successfully validated that service multi-endpoint-test in namespace services-4599 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 06/13/23 03:52:12.233
Jun 13 03:52:12.233: INFO: Creating new exec pod
Jun 13 03:52:12.249: INFO: Waiting up to 5m0s for pod "execpod2fxwr" in namespace "services-4599" to be "running"
Jun 13 03:52:12.272: INFO: Pod "execpod2fxwr": Phase="Pending", Reason="", readiness=false. Elapsed: 22.4162ms
Jun 13 03:52:14.304: INFO: Pod "execpod2fxwr": Phase="Running", Reason="", readiness=true. Elapsed: 2.055239385s
Jun 13 03:52:14.304: INFO: Pod "execpod2fxwr" satisfied condition "running"
Jun 13 03:52:15.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4599 exec execpod2fxwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jun 13 03:52:15.666: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jun 13 03:52:15.666: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 03:52:15.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4599 exec execpod2fxwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.122.246 80'
Jun 13 03:52:15.920: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.122.246 80\nConnection to 10.97.122.246 80 port [tcp/http] succeeded!\n"
Jun 13 03:52:15.920: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 03:52:15.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4599 exec execpod2fxwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jun 13 03:52:16.233: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jun 13 03:52:16.233: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 13 03:52:16.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4599 exec execpod2fxwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.122.246 81'
Jun 13 03:52:16.505: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.122.246 81\nConnection to 10.97.122.246 81 port [tcp/*] succeeded!\n"
Jun 13 03:52:16.505: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4599 06/13/23 03:52:16.505
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4599 to expose endpoints map[pod2:[101]] 06/13/23 03:52:16.547
Jun 13 03:52:16.604: INFO: successfully validated that service multi-endpoint-test in namespace services-4599 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4599 06/13/23 03:52:16.604
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4599 to expose endpoints map[] 06/13/23 03:52:16.689
Jun 13 03:52:16.719: INFO: successfully validated that service multi-endpoint-test in namespace services-4599 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:52:16.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4599" for this suite. 06/13/23 03:52:16.931
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":336,"skipped":6205,"failed":0}
------------------------------
• [SLOW TEST] [13.626 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:52:03.42
    Jun 13 03:52:03.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:52:03.423
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:52:03.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:52:03.528
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-4599 06/13/23 03:52:03.537
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4599 to expose endpoints map[] 06/13/23 03:52:03.631
    Jun 13 03:52:03.673: INFO: successfully validated that service multi-endpoint-test in namespace services-4599 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4599 06/13/23 03:52:03.673
    Jun 13 03:52:03.722: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4599" to be "running and ready"
    Jun 13 03:52:03.758: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 35.960494ms
    Jun 13 03:52:03.758: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:52:05.776: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053391824s
    Jun 13 03:52:05.776: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:52:07.794: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.072186378s
    Jun 13 03:52:07.795: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun 13 03:52:07.795: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4599 to expose endpoints map[pod1:[100]] 06/13/23 03:52:07.817
    Jun 13 03:52:07.879: INFO: successfully validated that service multi-endpoint-test in namespace services-4599 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-4599 06/13/23 03:52:07.879
    Jun 13 03:52:07.909: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4599" to be "running and ready"
    Jun 13 03:52:07.933: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 24.114905ms
    Jun 13 03:52:07.933: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:52:09.943: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034347607s
    Jun 13 03:52:09.943: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:52:11.995: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.086320985s
    Jun 13 03:52:11.995: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun 13 03:52:11.995: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4599 to expose endpoints map[pod1:[100] pod2:[101]] 06/13/23 03:52:12.016
    Jun 13 03:52:12.232: INFO: successfully validated that service multi-endpoint-test in namespace services-4599 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 06/13/23 03:52:12.233
    Jun 13 03:52:12.233: INFO: Creating new exec pod
    Jun 13 03:52:12.249: INFO: Waiting up to 5m0s for pod "execpod2fxwr" in namespace "services-4599" to be "running"
    Jun 13 03:52:12.272: INFO: Pod "execpod2fxwr": Phase="Pending", Reason="", readiness=false. Elapsed: 22.4162ms
    Jun 13 03:52:14.304: INFO: Pod "execpod2fxwr": Phase="Running", Reason="", readiness=true. Elapsed: 2.055239385s
    Jun 13 03:52:14.304: INFO: Pod "execpod2fxwr" satisfied condition "running"
    Jun 13 03:52:15.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4599 exec execpod2fxwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jun 13 03:52:15.666: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jun 13 03:52:15.666: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 03:52:15.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4599 exec execpod2fxwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.122.246 80'
    Jun 13 03:52:15.920: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.122.246 80\nConnection to 10.97.122.246 80 port [tcp/http] succeeded!\n"
    Jun 13 03:52:15.920: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 03:52:15.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4599 exec execpod2fxwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jun 13 03:52:16.233: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jun 13 03:52:16.233: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 13 03:52:16.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-4599 exec execpod2fxwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.122.246 81'
    Jun 13 03:52:16.505: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.122.246 81\nConnection to 10.97.122.246 81 port [tcp/*] succeeded!\n"
    Jun 13 03:52:16.505: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-4599 06/13/23 03:52:16.505
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4599 to expose endpoints map[pod2:[101]] 06/13/23 03:52:16.547
    Jun 13 03:52:16.604: INFO: successfully validated that service multi-endpoint-test in namespace services-4599 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-4599 06/13/23 03:52:16.604
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4599 to expose endpoints map[] 06/13/23 03:52:16.689
    Jun 13 03:52:16.719: INFO: successfully validated that service multi-endpoint-test in namespace services-4599 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:52:16.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4599" for this suite. 06/13/23 03:52:16.931
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:52:17.047
Jun 13 03:52:17.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename namespaces 06/13/23 03:52:17.051
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:52:17.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:52:17.11
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 06/13/23 03:52:17.117
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:52:17.176
STEP: Creating a service in the namespace 06/13/23 03:52:17.189
STEP: Deleting the namespace 06/13/23 03:52:17.455
STEP: Waiting for the namespace to be removed. 06/13/23 03:52:17.489
STEP: Recreating the namespace 06/13/23 03:52:23.498
STEP: Verifying there is no service in the namespace 06/13/23 03:52:23.573
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:52:23.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3081" for this suite. 06/13/23 03:52:23.626
STEP: Destroying namespace "nsdeletetest-1506" for this suite. 06/13/23 03:52:23.68
Jun 13 03:52:23.698: INFO: Namespace nsdeletetest-1506 was already deleted
STEP: Destroying namespace "nsdeletetest-2529" for this suite. 06/13/23 03:52:23.698
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":337,"skipped":6235,"failed":0}
------------------------------
• [SLOW TEST] [6.682 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:52:17.047
    Jun 13 03:52:17.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename namespaces 06/13/23 03:52:17.051
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:52:17.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:52:17.11
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 06/13/23 03:52:17.117
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:52:17.176
    STEP: Creating a service in the namespace 06/13/23 03:52:17.189
    STEP: Deleting the namespace 06/13/23 03:52:17.455
    STEP: Waiting for the namespace to be removed. 06/13/23 03:52:17.489
    STEP: Recreating the namespace 06/13/23 03:52:23.498
    STEP: Verifying there is no service in the namespace 06/13/23 03:52:23.573
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:52:23.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-3081" for this suite. 06/13/23 03:52:23.626
    STEP: Destroying namespace "nsdeletetest-1506" for this suite. 06/13/23 03:52:23.68
    Jun 13 03:52:23.698: INFO: Namespace nsdeletetest-1506 was already deleted
    STEP: Destroying namespace "nsdeletetest-2529" for this suite. 06/13/23 03:52:23.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:52:23.73
Jun 13 03:52:23.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:52:23.731
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:52:23.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:52:23.805
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 06/13/23 03:52:23.815
Jun 13 03:52:23.860: INFO: Waiting up to 5m0s for pod "downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77" in namespace "downward-api-130" to be "Succeeded or Failed"
Jun 13 03:52:23.904: INFO: Pod "downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77": Phase="Pending", Reason="", readiness=false. Elapsed: 43.467864ms
Jun 13 03:52:25.924: INFO: Pod "downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77": Phase="Running", Reason="", readiness=true. Elapsed: 2.063902925s
Jun 13 03:52:27.925: INFO: Pod "downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77": Phase="Running", Reason="", readiness=false. Elapsed: 4.064446459s
Jun 13 03:52:29.919: INFO: Pod "downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058581804s
STEP: Saw pod success 06/13/23 03:52:29.919
Jun 13 03:52:29.919: INFO: Pod "downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77" satisfied condition "Succeeded or Failed"
Jun 13 03:52:29.927: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77 container dapi-container: <nil>
STEP: delete the pod 06/13/23 03:52:29.974
Jun 13 03:52:30.038: INFO: Waiting for pod downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77 to disappear
Jun 13 03:52:30.052: INFO: Pod downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 13 03:52:30.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-130" for this suite. 06/13/23 03:52:30.073
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":338,"skipped":6243,"failed":0}
------------------------------
• [SLOW TEST] [6.387 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:52:23.73
    Jun 13 03:52:23.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:52:23.731
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:52:23.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:52:23.805
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 06/13/23 03:52:23.815
    Jun 13 03:52:23.860: INFO: Waiting up to 5m0s for pod "downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77" in namespace "downward-api-130" to be "Succeeded or Failed"
    Jun 13 03:52:23.904: INFO: Pod "downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77": Phase="Pending", Reason="", readiness=false. Elapsed: 43.467864ms
    Jun 13 03:52:25.924: INFO: Pod "downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77": Phase="Running", Reason="", readiness=true. Elapsed: 2.063902925s
    Jun 13 03:52:27.925: INFO: Pod "downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77": Phase="Running", Reason="", readiness=false. Elapsed: 4.064446459s
    Jun 13 03:52:29.919: INFO: Pod "downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.058581804s
    STEP: Saw pod success 06/13/23 03:52:29.919
    Jun 13 03:52:29.919: INFO: Pod "downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77" satisfied condition "Succeeded or Failed"
    Jun 13 03:52:29.927: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77 container dapi-container: <nil>
    STEP: delete the pod 06/13/23 03:52:29.974
    Jun 13 03:52:30.038: INFO: Waiting for pod downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77 to disappear
    Jun 13 03:52:30.052: INFO: Pod downward-api-aa3068f4-870b-4636-ab69-976d7dddcc77 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 13 03:52:30.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-130" for this suite. 06/13/23 03:52:30.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:52:30.119
Jun 13 03:52:30.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename var-expansion 06/13/23 03:52:30.121
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:52:30.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:52:30.202
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 06/13/23 03:52:30.213
Jun 13 03:52:30.249: INFO: Waiting up to 5m0s for pod "var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7" in namespace "var-expansion-4349" to be "Succeeded or Failed"
Jun 13 03:52:30.282: INFO: Pod "var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.96302ms
Jun 13 03:52:32.302: INFO: Pod "var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053374341s
Jun 13 03:52:34.344: INFO: Pod "var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095722601s
Jun 13 03:52:36.296: INFO: Pod "var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047498179s
STEP: Saw pod success 06/13/23 03:52:36.296
Jun 13 03:52:36.296: INFO: Pod "var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7" satisfied condition "Succeeded or Failed"
Jun 13 03:52:36.306: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7 container dapi-container: <nil>
STEP: delete the pod 06/13/23 03:52:36.33
Jun 13 03:52:36.380: INFO: Waiting for pod var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7 to disappear
Jun 13 03:52:36.398: INFO: Pod var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 13 03:52:36.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4349" for this suite. 06/13/23 03:52:36.43
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":339,"skipped":6256,"failed":0}
------------------------------
• [SLOW TEST] [6.333 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:52:30.119
    Jun 13 03:52:30.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename var-expansion 06/13/23 03:52:30.121
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:52:30.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:52:30.202
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 06/13/23 03:52:30.213
    Jun 13 03:52:30.249: INFO: Waiting up to 5m0s for pod "var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7" in namespace "var-expansion-4349" to be "Succeeded or Failed"
    Jun 13 03:52:30.282: INFO: Pod "var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7": Phase="Pending", Reason="", readiness=false. Elapsed: 32.96302ms
    Jun 13 03:52:32.302: INFO: Pod "var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053374341s
    Jun 13 03:52:34.344: INFO: Pod "var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095722601s
    Jun 13 03:52:36.296: INFO: Pod "var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047498179s
    STEP: Saw pod success 06/13/23 03:52:36.296
    Jun 13 03:52:36.296: INFO: Pod "var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7" satisfied condition "Succeeded or Failed"
    Jun 13 03:52:36.306: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7 container dapi-container: <nil>
    STEP: delete the pod 06/13/23 03:52:36.33
    Jun 13 03:52:36.380: INFO: Waiting for pod var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7 to disappear
    Jun 13 03:52:36.398: INFO: Pod var-expansion-6fc6480c-119c-465d-b63a-2c55f0113be7 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 13 03:52:36.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4349" for this suite. 06/13/23 03:52:36.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:52:36.455
Jun 13 03:52:36.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename taint-single-pod 06/13/23 03:52:36.457
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:52:36.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:52:36.517
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jun 13 03:52:36.526: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 13 03:53:36.657: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jun 13 03:53:36.685: INFO: Starting informer...
STEP: Starting pod... 06/13/23 03:53:36.685
Jun 13 03:53:36.968: INFO: Pod is running on sks-test-v1-25-9-workergroup-2q6k2. Tainting Node
STEP: Trying to apply a taint on the Node 06/13/23 03:53:36.968
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/13/23 03:53:37.071
STEP: Waiting short time to make sure Pod is queued for deletion 06/13/23 03:53:37.134
Jun 13 03:53:37.135: INFO: Pod wasn't evicted. Proceeding
Jun 13 03:53:37.135: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/13/23 03:53:37.237
STEP: Waiting some time to make sure that toleration time passed. 06/13/23 03:53:37.274
Jun 13 03:54:52.275: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jun 13 03:54:52.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-9855" for this suite. 06/13/23 03:54:52.297
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":340,"skipped":6278,"failed":0}
------------------------------
• [SLOW TEST] [135.864 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:52:36.455
    Jun 13 03:52:36.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename taint-single-pod 06/13/23 03:52:36.457
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:52:36.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:52:36.517
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jun 13 03:52:36.526: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 13 03:53:36.657: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jun 13 03:53:36.685: INFO: Starting informer...
    STEP: Starting pod... 06/13/23 03:53:36.685
    Jun 13 03:53:36.968: INFO: Pod is running on sks-test-v1-25-9-workergroup-2q6k2. Tainting Node
    STEP: Trying to apply a taint on the Node 06/13/23 03:53:36.968
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/13/23 03:53:37.071
    STEP: Waiting short time to make sure Pod is queued for deletion 06/13/23 03:53:37.134
    Jun 13 03:53:37.135: INFO: Pod wasn't evicted. Proceeding
    Jun 13 03:53:37.135: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/13/23 03:53:37.237
    STEP: Waiting some time to make sure that toleration time passed. 06/13/23 03:53:37.274
    Jun 13 03:54:52.275: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jun 13 03:54:52.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-9855" for this suite. 06/13/23 03:54:52.297
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:54:52.32
Jun 13 03:54:52.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 03:54:52.324
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:54:52.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:54:52.392
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 06/13/23 03:54:52.404
Jun 13 03:54:52.427: INFO: Waiting up to 5m0s for pod "pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b" in namespace "emptydir-1932" to be "Succeeded or Failed"
Jun 13 03:54:52.443: INFO: Pod "pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.993543ms
Jun 13 03:54:54.457: INFO: Pod "pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029781545s
Jun 13 03:54:56.453: INFO: Pod "pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026128064s
STEP: Saw pod success 06/13/23 03:54:56.453
Jun 13 03:54:56.453: INFO: Pod "pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b" satisfied condition "Succeeded or Failed"
Jun 13 03:54:56.483: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b container test-container: <nil>
STEP: delete the pod 06/13/23 03:54:56.521
Jun 13 03:54:56.574: INFO: Waiting for pod pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b to disappear
Jun 13 03:54:56.586: INFO: Pod pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 03:54:56.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1932" for this suite. 06/13/23 03:54:56.611
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":341,"skipped":6278,"failed":0}
------------------------------
• [4.311 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:54:52.32
    Jun 13 03:54:52.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 03:54:52.324
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:54:52.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:54:52.392
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 06/13/23 03:54:52.404
    Jun 13 03:54:52.427: INFO: Waiting up to 5m0s for pod "pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b" in namespace "emptydir-1932" to be "Succeeded or Failed"
    Jun 13 03:54:52.443: INFO: Pod "pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.993543ms
    Jun 13 03:54:54.457: INFO: Pod "pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029781545s
    Jun 13 03:54:56.453: INFO: Pod "pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026128064s
    STEP: Saw pod success 06/13/23 03:54:56.453
    Jun 13 03:54:56.453: INFO: Pod "pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b" satisfied condition "Succeeded or Failed"
    Jun 13 03:54:56.483: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b container test-container: <nil>
    STEP: delete the pod 06/13/23 03:54:56.521
    Jun 13 03:54:56.574: INFO: Waiting for pod pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b to disappear
    Jun 13 03:54:56.586: INFO: Pod pod-943fb0aa-ae11-4afd-bae9-98dc75bef72b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 03:54:56.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1932" for this suite. 06/13/23 03:54:56.611
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:54:56.633
Jun 13 03:54:56.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename configmap 06/13/23 03:54:56.635
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:54:56.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:54:56.685
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-3201/configmap-test-cbcc179f-7460-46ba-a198-6eaab09cca41 06/13/23 03:54:56.705
STEP: Creating a pod to test consume configMaps 06/13/23 03:54:56.727
Jun 13 03:54:56.752: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf" in namespace "configmap-3201" to be "Succeeded or Failed"
Jun 13 03:54:56.771: INFO: Pod "pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.948387ms
Jun 13 03:54:58.790: INFO: Pod "pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03811584s
Jun 13 03:55:00.800: INFO: Pod "pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048484689s
Jun 13 03:55:02.781: INFO: Pod "pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029420641s
STEP: Saw pod success 06/13/23 03:55:02.781
Jun 13 03:55:02.782: INFO: Pod "pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf" satisfied condition "Succeeded or Failed"
Jun 13 03:55:02.798: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf container env-test: <nil>
STEP: delete the pod 06/13/23 03:55:02.821
Jun 13 03:55:03.043: INFO: Waiting for pod pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf to disappear
Jun 13 03:55:03.056: INFO: Pod pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jun 13 03:55:03.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3201" for this suite. 06/13/23 03:55:03.081
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":342,"skipped":6298,"failed":0}
------------------------------
• [SLOW TEST] [6.475 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:54:56.633
    Jun 13 03:54:56.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename configmap 06/13/23 03:54:56.635
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:54:56.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:54:56.685
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-3201/configmap-test-cbcc179f-7460-46ba-a198-6eaab09cca41 06/13/23 03:54:56.705
    STEP: Creating a pod to test consume configMaps 06/13/23 03:54:56.727
    Jun 13 03:54:56.752: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf" in namespace "configmap-3201" to be "Succeeded or Failed"
    Jun 13 03:54:56.771: INFO: Pod "pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf": Phase="Pending", Reason="", readiness=false. Elapsed: 18.948387ms
    Jun 13 03:54:58.790: INFO: Pod "pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03811584s
    Jun 13 03:55:00.800: INFO: Pod "pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048484689s
    Jun 13 03:55:02.781: INFO: Pod "pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029420641s
    STEP: Saw pod success 06/13/23 03:55:02.781
    Jun 13 03:55:02.782: INFO: Pod "pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf" satisfied condition "Succeeded or Failed"
    Jun 13 03:55:02.798: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf container env-test: <nil>
    STEP: delete the pod 06/13/23 03:55:02.821
    Jun 13 03:55:03.043: INFO: Waiting for pod pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf to disappear
    Jun 13 03:55:03.056: INFO: Pod pod-configmaps-7ed00f77-f332-4d84-a4d9-f72c9c2071bf no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 13 03:55:03.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3201" for this suite. 06/13/23 03:55:03.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:55:03.113
Jun 13 03:55:03.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename pod-network-test 06/13/23 03:55:03.114
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:55:03.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:55:03.186
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-505 06/13/23 03:55:03.207
STEP: creating a selector 06/13/23 03:55:03.207
STEP: Creating the service pods in kubernetes 06/13/23 03:55:03.207
Jun 13 03:55:03.208: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 13 03:55:03.461: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-505" to be "running and ready"
Jun 13 03:55:03.491: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.209428ms
Jun 13 03:55:03.491: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:55:05.506: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044819356s
Jun 13 03:55:05.506: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 13 03:55:07.520: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.058973223s
Jun 13 03:55:07.520: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:55:09.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.039377076s
Jun 13 03:55:09.501: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:55:11.504: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.042720071s
Jun 13 03:55:11.504: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:55:13.510: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.049113459s
Jun 13 03:55:13.510: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:55:15.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.038779251s
Jun 13 03:55:15.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:55:17.505: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.043745307s
Jun 13 03:55:17.505: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:55:19.503: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.042039832s
Jun 13 03:55:19.503: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:55:21.503: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.042278769s
Jun 13 03:55:21.503: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:55:23.506: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.045208684s
Jun 13 03:55:23.506: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 13 03:55:25.529: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.068019801s
Jun 13 03:55:25.529: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 13 03:55:25.529: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 13 03:55:25.548: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-505" to be "running and ready"
Jun 13 03:55:25.557: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.39351ms
Jun 13 03:55:25.557: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 13 03:55:25.557: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 13 03:55:25.571: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-505" to be "running and ready"
Jun 13 03:55:25.586: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 15.32293ms
Jun 13 03:55:25.586: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 13 03:55:25.586: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/13/23 03:55:25.606
Jun 13 03:55:25.665: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-505" to be "running"
Jun 13 03:55:25.684: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.755957ms
Jun 13 03:55:27.701: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.035739659s
Jun 13 03:55:27.701: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 13 03:55:27.719: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-505" to be "running"
Jun 13 03:55:27.736: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 16.345918ms
Jun 13 03:55:27.736: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jun 13 03:55:27.747: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 13 03:55:27.747: INFO: Going to poll 172.16.172.58 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 13 03:55:27.761: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.172.58:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:55:27.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:55:27.762: INFO: ExecWithOptions: Clientset creation
Jun 13 03:55:27.762: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.172.58%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 13 03:55:27.950: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 13 03:55:27.950: INFO: Going to poll 172.30.77.173 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 13 03:55:27.965: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.77.173:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:55:27.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:55:27.966: INFO: ExecWithOptions: Clientset creation
Jun 13 03:55:27.966: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.77.173%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 13 03:55:28.084: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 13 03:55:28.084: INFO: Going to poll 172.28.156.208 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 13 03:55:28.133: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.28.156.208:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 13 03:55:28.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
Jun 13 03:55:28.134: INFO: ExecWithOptions: Clientset creation
Jun 13 03:55:28.134: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.28.156.208%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 13 03:55:28.283: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jun 13 03:55:28.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-505" for this suite. 06/13/23 03:55:28.296
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":343,"skipped":6379,"failed":0}
------------------------------
• [SLOW TEST] [25.203 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:55:03.113
    Jun 13 03:55:03.113: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename pod-network-test 06/13/23 03:55:03.114
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:55:03.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:55:03.186
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-505 06/13/23 03:55:03.207
    STEP: creating a selector 06/13/23 03:55:03.207
    STEP: Creating the service pods in kubernetes 06/13/23 03:55:03.207
    Jun 13 03:55:03.208: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun 13 03:55:03.461: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-505" to be "running and ready"
    Jun 13 03:55:03.491: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.209428ms
    Jun 13 03:55:03.491: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:55:05.506: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044819356s
    Jun 13 03:55:05.506: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 13 03:55:07.520: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.058973223s
    Jun 13 03:55:07.520: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:55:09.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.039377076s
    Jun 13 03:55:09.501: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:55:11.504: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.042720071s
    Jun 13 03:55:11.504: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:55:13.510: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.049113459s
    Jun 13 03:55:13.510: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:55:15.500: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.038779251s
    Jun 13 03:55:15.500: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:55:17.505: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.043745307s
    Jun 13 03:55:17.505: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:55:19.503: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.042039832s
    Jun 13 03:55:19.503: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:55:21.503: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.042278769s
    Jun 13 03:55:21.503: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:55:23.506: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.045208684s
    Jun 13 03:55:23.506: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 13 03:55:25.529: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.068019801s
    Jun 13 03:55:25.529: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 13 03:55:25.529: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 13 03:55:25.548: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-505" to be "running and ready"
    Jun 13 03:55:25.557: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.39351ms
    Jun 13 03:55:25.557: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 13 03:55:25.557: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 13 03:55:25.571: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-505" to be "running and ready"
    Jun 13 03:55:25.586: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 15.32293ms
    Jun 13 03:55:25.586: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 13 03:55:25.586: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/13/23 03:55:25.606
    Jun 13 03:55:25.665: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-505" to be "running"
    Jun 13 03:55:25.684: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 18.755957ms
    Jun 13 03:55:27.701: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.035739659s
    Jun 13 03:55:27.701: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 13 03:55:27.719: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-505" to be "running"
    Jun 13 03:55:27.736: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 16.345918ms
    Jun 13 03:55:27.736: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jun 13 03:55:27.747: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 13 03:55:27.747: INFO: Going to poll 172.16.172.58 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun 13 03:55:27.761: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.172.58:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:55:27.761: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:55:27.762: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:55:27.762: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.172.58%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 13 03:55:27.950: INFO: Found all 1 expected endpoints: [netserver-0]
    Jun 13 03:55:27.950: INFO: Going to poll 172.30.77.173 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun 13 03:55:27.965: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.77.173:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:55:27.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:55:27.966: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:55:27.966: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.77.173%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 13 03:55:28.084: INFO: Found all 1 expected endpoints: [netserver-1]
    Jun 13 03:55:28.084: INFO: Going to poll 172.28.156.208 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun 13 03:55:28.133: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.28.156.208:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 13 03:55:28.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    Jun 13 03:55:28.134: INFO: ExecWithOptions: Clientset creation
    Jun 13 03:55:28.134: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.28.156.208%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 13 03:55:28.283: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jun 13 03:55:28.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-505" for this suite. 06/13/23 03:55:28.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:55:28.319
Jun 13 03:55:28.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename gc 06/13/23 03:55:28.321
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:55:28.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:55:28.385
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 06/13/23 03:55:28.395
STEP: Wait for the Deployment to create new ReplicaSet 06/13/23 03:55:28.417
STEP: delete the deployment 06/13/23 03:55:28.442
STEP: wait for all rs to be garbage collected 06/13/23 03:55:28.546
STEP: expected 0 pods, got 2 pods 06/13/23 03:55:28.571
STEP: expected 0 rs, got 1 rs 06/13/23 03:55:28.68
STEP: Gathering metrics 06/13/23 03:55:29.217
Jun 13 03:55:29.267: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" in namespace "kube-system" to be "running and ready"
Jun 13 03:55:29.276: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq": Phase="Running", Reason="", readiness=true. Elapsed: 8.820948ms
Jun 13 03:55:29.276: INFO: The phase of Pod kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq is Running (Ready = true)
Jun 13 03:55:29.276: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" satisfied condition "running and ready"
Jun 13 03:55:29.537: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 13 03:55:29.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-821" for this suite. 06/13/23 03:55:29.572
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":344,"skipped":6397,"failed":0}
------------------------------
• [1.291 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:55:28.319
    Jun 13 03:55:28.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename gc 06/13/23 03:55:28.321
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:55:28.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:55:28.385
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 06/13/23 03:55:28.395
    STEP: Wait for the Deployment to create new ReplicaSet 06/13/23 03:55:28.417
    STEP: delete the deployment 06/13/23 03:55:28.442
    STEP: wait for all rs to be garbage collected 06/13/23 03:55:28.546
    STEP: expected 0 pods, got 2 pods 06/13/23 03:55:28.571
    STEP: expected 0 rs, got 1 rs 06/13/23 03:55:28.68
    STEP: Gathering metrics 06/13/23 03:55:29.217
    Jun 13 03:55:29.267: INFO: Waiting up to 5m0s for pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" in namespace "kube-system" to be "running and ready"
    Jun 13 03:55:29.276: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq": Phase="Running", Reason="", readiness=true. Elapsed: 8.820948ms
    Jun 13 03:55:29.276: INFO: The phase of Pod kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq is Running (Ready = true)
    Jun 13 03:55:29.276: INFO: Pod "kube-controller-manager-sks-test-v1-25-9-controlplane-rmxsq" satisfied condition "running and ready"
    Jun 13 03:55:29.537: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 13 03:55:29.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-821" for this suite. 06/13/23 03:55:29.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:55:29.613
Jun 13 03:55:29.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:55:29.615
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:55:29.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:55:29.674
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 06/13/23 03:55:29.687
Jun 13 03:55:29.734: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a" in namespace "downward-api-5048" to be "Succeeded or Failed"
Jun 13 03:55:29.768: INFO: Pod "downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a": Phase="Pending", Reason="", readiness=false. Elapsed: 34.494423ms
Jun 13 03:55:31.786: INFO: Pod "downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052429068s
Jun 13 03:55:33.868: INFO: Pod "downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.134433489s
STEP: Saw pod success 06/13/23 03:55:33.868
Jun 13 03:55:33.869: INFO: Pod "downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a" satisfied condition "Succeeded or Failed"
Jun 13 03:55:33.887: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-l5gcd pod downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a container client-container: <nil>
STEP: delete the pod 06/13/23 03:55:33.943
Jun 13 03:55:34.179: INFO: Waiting for pod downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a to disappear
Jun 13 03:55:34.225: INFO: Pod downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 13 03:55:34.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5048" for this suite. 06/13/23 03:55:34.259
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":345,"skipped":6443,"failed":0}
------------------------------
• [4.683 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:55:29.613
    Jun 13 03:55:29.614: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:55:29.615
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:55:29.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:55:29.674
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 06/13/23 03:55:29.687
    Jun 13 03:55:29.734: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a" in namespace "downward-api-5048" to be "Succeeded or Failed"
    Jun 13 03:55:29.768: INFO: Pod "downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a": Phase="Pending", Reason="", readiness=false. Elapsed: 34.494423ms
    Jun 13 03:55:31.786: INFO: Pod "downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052429068s
    Jun 13 03:55:33.868: INFO: Pod "downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.134433489s
    STEP: Saw pod success 06/13/23 03:55:33.868
    Jun 13 03:55:33.869: INFO: Pod "downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a" satisfied condition "Succeeded or Failed"
    Jun 13 03:55:33.887: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-l5gcd pod downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a container client-container: <nil>
    STEP: delete the pod 06/13/23 03:55:33.943
    Jun 13 03:55:34.179: INFO: Waiting for pod downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a to disappear
    Jun 13 03:55:34.225: INFO: Pod downwardapi-volume-58a4452c-1e93-455f-bcaf-afc4ad30e76a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 13 03:55:34.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5048" for this suite. 06/13/23 03:55:34.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:55:34.298
Jun 13 03:55:34.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:55:34.301
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:55:34.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:55:34.459
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9926 06/13/23 03:55:34.468
STEP: changing the ExternalName service to type=ClusterIP 06/13/23 03:55:34.491
STEP: creating replication controller externalname-service in namespace services-9926 06/13/23 03:55:34.811
I0613 03:55:34.878544      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9926, replica count: 2
I0613 03:55:37.930160      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 13 03:55:37.930: INFO: Creating new exec pod
Jun 13 03:55:37.965: INFO: Waiting up to 5m0s for pod "execpodsgrwr" in namespace "services-9926" to be "running"
Jun 13 03:55:37.980: INFO: Pod "execpodsgrwr": Phase="Pending", Reason="", readiness=false. Elapsed: 14.315698ms
Jun 13 03:55:39.997: INFO: Pod "execpodsgrwr": Phase="Running", Reason="", readiness=true. Elapsed: 2.031317678s
Jun 13 03:55:39.997: INFO: Pod "execpodsgrwr" satisfied condition "running"
Jun 13 03:55:40.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 13 03:55:41.213: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 13 03:55:41.213: INFO: stdout: ""
Jun 13 03:55:42.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 13 03:55:42.464: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 13 03:55:42.464: INFO: stdout: ""
Jun 13 03:55:43.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 13 03:55:43.445: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 13 03:55:43.445: INFO: stdout: "externalname-service-xrxw6"
Jun 13 03:55:43.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.180.31 80'
Jun 13 03:55:43.725: INFO: stderr: "+ nc -v -t -w 2 10.99.180.31 80\n+ echo hostName\nConnection to 10.99.180.31 80 port [tcp/http] succeeded!\n"
Jun 13 03:55:43.725: INFO: stdout: ""
Jun 13 03:55:44.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.180.31 80'
Jun 13 03:55:44.954: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.180.31 80\nConnection to 10.99.180.31 80 port [tcp/http] succeeded!\n"
Jun 13 03:55:44.954: INFO: stdout: ""
Jun 13 03:55:45.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.180.31 80'
Jun 13 03:55:45.928: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.180.31 80\nConnection to 10.99.180.31 80 port [tcp/http] succeeded!\n"
Jun 13 03:55:45.928: INFO: stdout: ""
Jun 13 03:55:46.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.180.31 80'
Jun 13 03:55:46.948: INFO: stderr: "+ nc -v -t -w 2 10.99.180.31 80\n+ echo hostName\nConnection to 10.99.180.31 80 port [tcp/http] succeeded!\n"
Jun 13 03:55:46.948: INFO: stdout: ""
Jun 13 03:55:47.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.180.31 80'
Jun 13 03:55:47.955: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.180.31 80\nConnection to 10.99.180.31 80 port [tcp/http] succeeded!\n"
Jun 13 03:55:47.955: INFO: stdout: ""
Jun 13 03:55:48.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.180.31 80'
Jun 13 03:55:48.965: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.180.31 80\nConnection to 10.99.180.31 80 port [tcp/http] succeeded!\n"
Jun 13 03:55:48.965: INFO: stdout: "externalname-service-fl55b"
Jun 13 03:55:48.965: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:55:49.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9926" for this suite. 06/13/23 03:55:49.136
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":346,"skipped":6459,"failed":0}
------------------------------
• [SLOW TEST] [14.881 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:55:34.298
    Jun 13 03:55:34.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:55:34.301
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:55:34.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:55:34.459
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9926 06/13/23 03:55:34.468
    STEP: changing the ExternalName service to type=ClusterIP 06/13/23 03:55:34.491
    STEP: creating replication controller externalname-service in namespace services-9926 06/13/23 03:55:34.811
    I0613 03:55:34.878544      18 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9926, replica count: 2
    I0613 03:55:37.930160      18 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 13 03:55:37.930: INFO: Creating new exec pod
    Jun 13 03:55:37.965: INFO: Waiting up to 5m0s for pod "execpodsgrwr" in namespace "services-9926" to be "running"
    Jun 13 03:55:37.980: INFO: Pod "execpodsgrwr": Phase="Pending", Reason="", readiness=false. Elapsed: 14.315698ms
    Jun 13 03:55:39.997: INFO: Pod "execpodsgrwr": Phase="Running", Reason="", readiness=true. Elapsed: 2.031317678s
    Jun 13 03:55:39.997: INFO: Pod "execpodsgrwr" satisfied condition "running"
    Jun 13 03:55:40.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 13 03:55:41.213: INFO: stderr: "+ + nc -v -t -w 2 externalname-service 80\necho hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 13 03:55:41.213: INFO: stdout: ""
    Jun 13 03:55:42.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 13 03:55:42.464: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 13 03:55:42.464: INFO: stdout: ""
    Jun 13 03:55:43.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 13 03:55:43.445: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 13 03:55:43.445: INFO: stdout: "externalname-service-xrxw6"
    Jun 13 03:55:43.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.180.31 80'
    Jun 13 03:55:43.725: INFO: stderr: "+ nc -v -t -w 2 10.99.180.31 80\n+ echo hostName\nConnection to 10.99.180.31 80 port [tcp/http] succeeded!\n"
    Jun 13 03:55:43.725: INFO: stdout: ""
    Jun 13 03:55:44.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.180.31 80'
    Jun 13 03:55:44.954: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.180.31 80\nConnection to 10.99.180.31 80 port [tcp/http] succeeded!\n"
    Jun 13 03:55:44.954: INFO: stdout: ""
    Jun 13 03:55:45.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.180.31 80'
    Jun 13 03:55:45.928: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.180.31 80\nConnection to 10.99.180.31 80 port [tcp/http] succeeded!\n"
    Jun 13 03:55:45.928: INFO: stdout: ""
    Jun 13 03:55:46.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.180.31 80'
    Jun 13 03:55:46.948: INFO: stderr: "+ nc -v -t -w 2 10.99.180.31 80\n+ echo hostName\nConnection to 10.99.180.31 80 port [tcp/http] succeeded!\n"
    Jun 13 03:55:46.948: INFO: stdout: ""
    Jun 13 03:55:47.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.180.31 80'
    Jun 13 03:55:47.955: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.180.31 80\nConnection to 10.99.180.31 80 port [tcp/http] succeeded!\n"
    Jun 13 03:55:47.955: INFO: stdout: ""
    Jun 13 03:55:48.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-9926 exec execpodsgrwr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.180.31 80'
    Jun 13 03:55:48.965: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.180.31 80\nConnection to 10.99.180.31 80 port [tcp/http] succeeded!\n"
    Jun 13 03:55:48.965: INFO: stdout: "externalname-service-fl55b"
    Jun 13 03:55:48.965: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:55:49.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9926" for this suite. 06/13/23 03:55:49.136
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:55:49.179
Jun 13 03:55:49.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename limitrange 06/13/23 03:55:49.183
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:55:49.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:55:49.252
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 06/13/23 03:55:49.263
STEP: Setting up watch 06/13/23 03:55:49.263
STEP: Submitting a LimitRange 06/13/23 03:55:49.372
STEP: Verifying LimitRange creation was observed 06/13/23 03:55:49.397
STEP: Fetching the LimitRange to ensure it has proper values 06/13/23 03:55:49.397
Jun 13 03:55:49.414: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 13 03:55:49.414: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 06/13/23 03:55:49.414
STEP: Ensuring Pod has resource requirements applied from LimitRange 06/13/23 03:55:49.438
Jun 13 03:55:49.630: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 13 03:55:49.630: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 06/13/23 03:55:49.63
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 06/13/23 03:55:49.679
Jun 13 03:55:49.719: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun 13 03:55:49.719: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 06/13/23 03:55:49.719
STEP: Failing to create a Pod with more than max resources 06/13/23 03:55:49.741
STEP: Updating a LimitRange 06/13/23 03:55:49.768
STEP: Verifying LimitRange updating is effective 06/13/23 03:55:49.817
STEP: Creating a Pod with less than former min resources 06/13/23 03:55:51.829
STEP: Failing to create a Pod with more than max resources 06/13/23 03:55:51.854
STEP: Deleting a LimitRange 06/13/23 03:55:51.865
STEP: Verifying the LimitRange was deleted 06/13/23 03:55:51.904
Jun 13 03:55:56.919: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 06/13/23 03:55:56.919
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jun 13 03:55:56.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-4965" for this suite. 06/13/23 03:55:56.967
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":347,"skipped":6469,"failed":0}
------------------------------
• [SLOW TEST] [7.865 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:55:49.179
    Jun 13 03:55:49.180: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename limitrange 06/13/23 03:55:49.183
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:55:49.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:55:49.252
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 06/13/23 03:55:49.263
    STEP: Setting up watch 06/13/23 03:55:49.263
    STEP: Submitting a LimitRange 06/13/23 03:55:49.372
    STEP: Verifying LimitRange creation was observed 06/13/23 03:55:49.397
    STEP: Fetching the LimitRange to ensure it has proper values 06/13/23 03:55:49.397
    Jun 13 03:55:49.414: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jun 13 03:55:49.414: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 06/13/23 03:55:49.414
    STEP: Ensuring Pod has resource requirements applied from LimitRange 06/13/23 03:55:49.438
    Jun 13 03:55:49.630: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jun 13 03:55:49.630: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 06/13/23 03:55:49.63
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 06/13/23 03:55:49.679
    Jun 13 03:55:49.719: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jun 13 03:55:49.719: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 06/13/23 03:55:49.719
    STEP: Failing to create a Pod with more than max resources 06/13/23 03:55:49.741
    STEP: Updating a LimitRange 06/13/23 03:55:49.768
    STEP: Verifying LimitRange updating is effective 06/13/23 03:55:49.817
    STEP: Creating a Pod with less than former min resources 06/13/23 03:55:51.829
    STEP: Failing to create a Pod with more than max resources 06/13/23 03:55:51.854
    STEP: Deleting a LimitRange 06/13/23 03:55:51.865
    STEP: Verifying the LimitRange was deleted 06/13/23 03:55:51.904
    Jun 13 03:55:56.919: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 06/13/23 03:55:56.919
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jun 13 03:55:56.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-4965" for this suite. 06/13/23 03:55:56.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:55:57.046
Jun 13 03:55:57.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename aggregator 06/13/23 03:55:57.049
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:55:57.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:55:57.473
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jun 13 03:55:57.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 06/13/23 03:55:57.497
Jun 13 03:55:58.240: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 13 03:56:00.710: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:02.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:04.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:06.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:08.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:10.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:12.718: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:14.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:16.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:18.730: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:20.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:22.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:24.774: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:26.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:28.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:30.756: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:32.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:34.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:36.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:38.776: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:40.751: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:42.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:44.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:46.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:48.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:50.889: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 13 03:56:52.998: INFO: Waited 192.818147ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 06/13/23 03:56:53.282
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 06/13/23 03:56:53.297
STEP: List APIServices 06/13/23 03:56:53.417
Jun 13 03:56:53.524: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jun 13 03:56:55.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8612" for this suite. 06/13/23 03:56:55.297
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":348,"skipped":6484,"failed":0}
------------------------------
• [SLOW TEST] [58.301 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:55:57.046
    Jun 13 03:55:57.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename aggregator 06/13/23 03:55:57.049
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:55:57.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:55:57.473
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jun 13 03:55:57.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 06/13/23 03:55:57.497
    Jun 13 03:55:58.240: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jun 13 03:56:00.710: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:02.855: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:04.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:06.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:08.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:10.760: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:12.718: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:14.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:16.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:18.730: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:20.731: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:22.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:24.774: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:26.753: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:28.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:30.756: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:32.779: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:34.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:36.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:38.776: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:40.751: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:42.719: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:44.755: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:46.721: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:48.735: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:50.889: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 55, 58, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 13 03:56:52.998: INFO: Waited 192.818147ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 06/13/23 03:56:53.282
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 06/13/23 03:56:53.297
    STEP: List APIServices 06/13/23 03:56:53.417
    Jun 13 03:56:53.524: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jun 13 03:56:55.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-8612" for this suite. 06/13/23 03:56:55.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:56:55.347
Jun 13 03:56:55.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename var-expansion 06/13/23 03:56:55.349
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:56:55.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:56:55.599
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jun 13 03:56:55.735: INFO: Waiting up to 2m0s for pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0" in namespace "var-expansion-5729" to be "container 0 failed with reason CreateContainerConfigError"
Jun 13 03:56:55.773: INFO: Pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0": Phase="Pending", Reason="", readiness=false. Elapsed: 37.890378ms
Jun 13 03:56:57.795: INFO: Pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060456307s
Jun 13 03:56:59.828: INFO: Pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0932749s
Jun 13 03:56:59.828: INFO: Pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jun 13 03:56:59.828: INFO: Deleting pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0" in namespace "var-expansion-5729"
Jun 13 03:56:59.911: INFO: Wait up to 5m0s for pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 13 03:57:03.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5729" for this suite. 06/13/23 03:57:03.984
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":349,"skipped":6493,"failed":0}
------------------------------
• [SLOW TEST] [8.692 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:56:55.347
    Jun 13 03:56:55.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename var-expansion 06/13/23 03:56:55.349
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:56:55.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:56:55.599
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jun 13 03:56:55.735: INFO: Waiting up to 2m0s for pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0" in namespace "var-expansion-5729" to be "container 0 failed with reason CreateContainerConfigError"
    Jun 13 03:56:55.773: INFO: Pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0": Phase="Pending", Reason="", readiness=false. Elapsed: 37.890378ms
    Jun 13 03:56:57.795: INFO: Pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060456307s
    Jun 13 03:56:59.828: INFO: Pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0932749s
    Jun 13 03:56:59.828: INFO: Pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jun 13 03:56:59.828: INFO: Deleting pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0" in namespace "var-expansion-5729"
    Jun 13 03:56:59.911: INFO: Wait up to 5m0s for pod "var-expansion-cf612553-cd66-4a89-95fe-77b3e5dd45f0" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 13 03:57:03.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5729" for this suite. 06/13/23 03:57:03.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:57:04.043
Jun 13 03:57:04.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename downward-api 06/13/23 03:57:04.045
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:57:04.181
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:57:04.198
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 06/13/23 03:57:04.214
Jun 13 03:57:04.290: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a" in namespace "downward-api-4453" to be "Succeeded or Failed"
Jun 13 03:57:04.368: INFO: Pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 78.70004ms
Jun 13 03:57:06.382: INFO: Pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092369685s
Jun 13 03:57:08.391: INFO: Pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.101286206s
Jun 13 03:57:10.454: INFO: Pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a": Phase="Running", Reason="", readiness=false. Elapsed: 6.164514453s
Jun 13 03:57:12.386: INFO: Pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.096489726s
STEP: Saw pod success 06/13/23 03:57:12.386
Jun 13 03:57:12.387: INFO: Pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a" satisfied condition "Succeeded or Failed"
Jun 13 03:57:12.415: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a container client-container: <nil>
STEP: delete the pod 06/13/23 03:57:12.469
Jun 13 03:57:12.867: INFO: Waiting for pod downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a to disappear
Jun 13 03:57:12.876: INFO: Pod downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 13 03:57:12.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4453" for this suite. 06/13/23 03:57:12.89
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":350,"skipped":6533,"failed":0}
------------------------------
• [SLOW TEST] [9.072 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:57:04.043
    Jun 13 03:57:04.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename downward-api 06/13/23 03:57:04.045
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:57:04.181
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:57:04.198
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 06/13/23 03:57:04.214
    Jun 13 03:57:04.290: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a" in namespace "downward-api-4453" to be "Succeeded or Failed"
    Jun 13 03:57:04.368: INFO: Pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 78.70004ms
    Jun 13 03:57:06.382: INFO: Pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092369685s
    Jun 13 03:57:08.391: INFO: Pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a": Phase="Running", Reason="", readiness=false. Elapsed: 4.101286206s
    Jun 13 03:57:10.454: INFO: Pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a": Phase="Running", Reason="", readiness=false. Elapsed: 6.164514453s
    Jun 13 03:57:12.386: INFO: Pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.096489726s
    STEP: Saw pod success 06/13/23 03:57:12.386
    Jun 13 03:57:12.387: INFO: Pod "downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a" satisfied condition "Succeeded or Failed"
    Jun 13 03:57:12.415: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-469fm pod downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a container client-container: <nil>
    STEP: delete the pod 06/13/23 03:57:12.469
    Jun 13 03:57:12.867: INFO: Waiting for pod downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a to disappear
    Jun 13 03:57:12.876: INFO: Pod downwardapi-volume-d438d327-146c-4871-b3a6-5a75afae7b8a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 13 03:57:12.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4453" for this suite. 06/13/23 03:57:12.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:57:13.116
Jun 13 03:57:13.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 03:57:13.118
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:57:13.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:57:13.417
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 03:57:13.638
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:57:14.218
STEP: Deploying the webhook pod 06/13/23 03:57:14.312
STEP: Wait for the deployment to be ready 06/13/23 03:57:14.391
Jun 13 03:57:14.462: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 13 03:57:16.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 57, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 57, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 57, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 57, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/13/23 03:57:18.619
STEP: Verifying the service has paired with the endpoint 06/13/23 03:57:18.789
Jun 13 03:57:19.789: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 06/13/23 03:57:19.859
STEP: create a configmap that should be updated by the webhook 06/13/23 03:57:19.965
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:57:20.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4747" for this suite. 06/13/23 03:57:20.144
STEP: Destroying namespace "webhook-4747-markers" for this suite. 06/13/23 03:57:20.245
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":351,"skipped":6555,"failed":0}
------------------------------
• [SLOW TEST] [8.202 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:57:13.116
    Jun 13 03:57:13.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 03:57:13.118
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:57:13.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:57:13.417
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 03:57:13.638
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:57:14.218
    STEP: Deploying the webhook pod 06/13/23 03:57:14.312
    STEP: Wait for the deployment to be ready 06/13/23 03:57:14.391
    Jun 13 03:57:14.462: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 13 03:57:16.601: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 57, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 57, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 57, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 57, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/13/23 03:57:18.619
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:57:18.789
    Jun 13 03:57:19.789: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 06/13/23 03:57:19.859
    STEP: create a configmap that should be updated by the webhook 06/13/23 03:57:19.965
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:57:20.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4747" for this suite. 06/13/23 03:57:20.144
    STEP: Destroying namespace "webhook-4747-markers" for this suite. 06/13/23 03:57:20.245
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:57:21.319
Jun 13 03:57:21.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename svc-latency 06/13/23 03:57:21.321
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:57:21.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:57:21.747
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jun 13 03:57:21.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7882 06/13/23 03:57:21.782
I0613 03:57:21.871558      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7882, replica count: 1
I0613 03:57:22.922918      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0613 03:57:23.923162      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0613 03:57:24.923479      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0613 03:57:25.923750      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 13 03:57:26.153: INFO: Created: latency-svc-zxspz
Jun 13 03:57:26.308: INFO: Got endpoints: latency-svc-zxspz [283.787238ms]
Jun 13 03:57:26.578: INFO: Created: latency-svc-fd825
Jun 13 03:57:26.671: INFO: Got endpoints: latency-svc-fd825 [361.470526ms]
Jun 13 03:57:26.808: INFO: Created: latency-svc-49dn7
Jun 13 03:57:26.871: INFO: Got endpoints: latency-svc-49dn7 [561.497194ms]
Jun 13 03:57:27.011: INFO: Created: latency-svc-7ht9x
Jun 13 03:57:27.087: INFO: Got endpoints: latency-svc-7ht9x [778.377317ms]
Jun 13 03:57:27.393: INFO: Created: latency-svc-ssgrd
Jun 13 03:57:27.643: INFO: Got endpoints: latency-svc-ssgrd [1.333749349s]
Jun 13 03:57:27.719: INFO: Created: latency-svc-pzvpn
Jun 13 03:57:27.851: INFO: Got endpoints: latency-svc-pzvpn [1.54126576s]
Jun 13 03:57:27.968: INFO: Created: latency-svc-sctwv
Jun 13 03:57:28.087: INFO: Got endpoints: latency-svc-sctwv [1.777053615s]
Jun 13 03:57:28.430: INFO: Created: latency-svc-mln5p
Jun 13 03:57:28.677: INFO: Got endpoints: latency-svc-mln5p [2.366469657s]
Jun 13 03:57:28.694: INFO: Created: latency-svc-tp9xr
Jun 13 03:57:28.850: INFO: Got endpoints: latency-svc-tp9xr [2.539978751s]
Jun 13 03:57:28.899: INFO: Created: latency-svc-l2bvj
Jun 13 03:57:29.087: INFO: Got endpoints: latency-svc-l2bvj [2.776507018s]
Jun 13 03:57:29.143: INFO: Created: latency-svc-bd4bc
Jun 13 03:57:29.162: INFO: Got endpoints: latency-svc-bd4bc [2.851912057s]
Jun 13 03:57:29.297: INFO: Created: latency-svc-2m6nm
Jun 13 03:57:29.415: INFO: Got endpoints: latency-svc-2m6nm [3.104023459s]
Jun 13 03:57:29.445: INFO: Created: latency-svc-pvpwb
Jun 13 03:57:29.540: INFO: Got endpoints: latency-svc-pvpwb [3.229262021s]
Jun 13 03:57:29.595: INFO: Created: latency-svc-hrqxv
Jun 13 03:57:29.680: INFO: Got endpoints: latency-svc-hrqxv [3.369229359s]
Jun 13 03:57:29.685: INFO: Created: latency-svc-4sgwm
Jun 13 03:57:29.721: INFO: Got endpoints: latency-svc-4sgwm [3.410664124s]
Jun 13 03:57:29.789: INFO: Created: latency-svc-ghs4v
Jun 13 03:57:29.891: INFO: Got endpoints: latency-svc-ghs4v [3.581187077s]
Jun 13 03:57:29.895: INFO: Created: latency-svc-8mtwx
Jun 13 03:57:29.975: INFO: Got endpoints: latency-svc-8mtwx [3.303324031s]
Jun 13 03:57:30.043: INFO: Created: latency-svc-nkh4q
Jun 13 03:57:30.135: INFO: Got endpoints: latency-svc-nkh4q [3.263851426s]
Jun 13 03:57:30.157: INFO: Created: latency-svc-jm5mk
Jun 13 03:57:30.190: INFO: Got endpoints: latency-svc-jm5mk [3.103409218s]
Jun 13 03:57:30.394: INFO: Created: latency-svc-m462m
Jun 13 03:57:30.459: INFO: Got endpoints: latency-svc-m462m [2.816046889s]
Jun 13 03:57:30.677: INFO: Created: latency-svc-kkk6s
Jun 13 03:57:30.924: INFO: Got endpoints: latency-svc-kkk6s [3.072963306s]
Jun 13 03:57:30.998: INFO: Created: latency-svc-hg4nr
Jun 13 03:57:31.112: INFO: Got endpoints: latency-svc-hg4nr [3.025273304s]
Jun 13 03:57:31.323: INFO: Created: latency-svc-l9zps
Jun 13 03:57:31.550: INFO: Got endpoints: latency-svc-l9zps [2.873575201s]
Jun 13 03:57:31.579: INFO: Created: latency-svc-5vx98
Jun 13 03:57:31.788: INFO: Got endpoints: latency-svc-5vx98 [2.937945123s]
Jun 13 03:57:31.856: INFO: Created: latency-svc-qgjm4
Jun 13 03:57:32.031: INFO: Got endpoints: latency-svc-qgjm4 [2.94432813s]
Jun 13 03:57:32.060: INFO: Created: latency-svc-r264p
Jun 13 03:57:32.117: INFO: Got endpoints: latency-svc-r264p [2.954971801s]
Jun 13 03:57:32.220: INFO: Created: latency-svc-9j5hj
Jun 13 03:57:32.281: INFO: Got endpoints: latency-svc-9j5hj [2.865854587s]
Jun 13 03:57:32.401: INFO: Created: latency-svc-d29l9
Jun 13 03:57:32.513: INFO: Got endpoints: latency-svc-d29l9 [2.973045108s]
Jun 13 03:57:32.541: INFO: Created: latency-svc-jxddr
Jun 13 03:57:32.891: INFO: Got endpoints: latency-svc-jxddr [3.210853474s]
Jun 13 03:57:32.891: INFO: Created: latency-svc-ncchn
Jun 13 03:57:33.151: INFO: Got endpoints: latency-svc-ncchn [3.429722995s]
Jun 13 03:57:33.295: INFO: Created: latency-svc-pzhjw
Jun 13 03:57:33.781: INFO: Got endpoints: latency-svc-pzhjw [3.890271775s]
Jun 13 03:57:34.014: INFO: Created: latency-svc-fqcnz
Jun 13 03:57:34.026: INFO: Got endpoints: latency-svc-fqcnz [4.051557207s]
Jun 13 03:57:34.203: INFO: Created: latency-svc-vfxqj
Jun 13 03:57:34.329: INFO: Got endpoints: latency-svc-vfxqj [4.193445492s]
Jun 13 03:57:34.491: INFO: Created: latency-svc-hps58
Jun 13 03:57:34.595: INFO: Got endpoints: latency-svc-hps58 [4.404698355s]
Jun 13 03:57:34.654: INFO: Created: latency-svc-fq94z
Jun 13 03:57:34.684: INFO: Got endpoints: latency-svc-fq94z [4.225285926s]
Jun 13 03:57:34.842: INFO: Created: latency-svc-tqvbw
Jun 13 03:57:34.879: INFO: Got endpoints: latency-svc-tqvbw [3.954166934s]
Jun 13 03:57:34.898: INFO: Created: latency-svc-5q84m
Jun 13 03:57:34.931: INFO: Got endpoints: latency-svc-5q84m [3.818457434s]
Jun 13 03:57:35.071: INFO: Created: latency-svc-fjgv9
Jun 13 03:57:35.195: INFO: Got endpoints: latency-svc-fjgv9 [3.644813238s]
Jun 13 03:57:35.271: INFO: Created: latency-svc-gfc9b
Jun 13 03:57:35.366: INFO: Got endpoints: latency-svc-gfc9b [3.57727844s]
Jun 13 03:57:35.514: INFO: Created: latency-svc-v5sj4
Jun 13 03:57:35.731: INFO: Got endpoints: latency-svc-v5sj4 [3.699308994s]
Jun 13 03:57:35.799: INFO: Created: latency-svc-jxhkb
Jun 13 03:57:35.899: INFO: Got endpoints: latency-svc-jxhkb [3.781258711s]
Jun 13 03:57:36.109: INFO: Created: latency-svc-7f6gx
Jun 13 03:57:36.230: INFO: Got endpoints: latency-svc-7f6gx [3.948974653s]
Jun 13 03:57:36.251: INFO: Created: latency-svc-p2ndp
Jun 13 03:57:36.331: INFO: Got endpoints: latency-svc-p2ndp [3.817720045s]
Jun 13 03:57:36.632: INFO: Created: latency-svc-fxwrj
Jun 13 03:57:36.634: INFO: Got endpoints: latency-svc-fxwrj [3.742831802s]
Jun 13 03:57:36.690: INFO: Created: latency-svc-6kplq
Jun 13 03:57:36.852: INFO: Got endpoints: latency-svc-6kplq [3.700456488s]
Jun 13 03:57:36.936: INFO: Created: latency-svc-c7qzr
Jun 13 03:57:37.020: INFO: Got endpoints: latency-svc-c7qzr [3.238608578s]
Jun 13 03:57:37.055: INFO: Created: latency-svc-p88gk
Jun 13 03:57:37.213: INFO: Got endpoints: latency-svc-p88gk [3.18691971s]
Jun 13 03:57:37.297: INFO: Created: latency-svc-w2kfm
Jun 13 03:57:37.316: INFO: Got endpoints: latency-svc-w2kfm [2.987578598s]
Jun 13 03:57:37.371: INFO: Created: latency-svc-8q5tc
Jun 13 03:57:37.372: INFO: Got endpoints: latency-svc-8q5tc [2.776552416s]
Jun 13 03:57:37.580: INFO: Created: latency-svc-sq4xg
Jun 13 03:57:37.737: INFO: Got endpoints: latency-svc-sq4xg [3.053161731s]
Jun 13 03:57:37.853: INFO: Created: latency-svc-cwn5d
Jun 13 03:57:38.138: INFO: Got endpoints: latency-svc-cwn5d [3.259501201s]
Jun 13 03:57:38.139: INFO: Created: latency-svc-rvxb9
Jun 13 03:57:38.291: INFO: Created: latency-svc-h7qfj
Jun 13 03:57:38.428: INFO: Got endpoints: latency-svc-rvxb9 [3.496726619s]
Jun 13 03:57:38.573: INFO: Got endpoints: latency-svc-h7qfj [3.378232984s]
Jun 13 03:57:38.704: INFO: Created: latency-svc-549nv
Jun 13 03:57:38.870: INFO: Got endpoints: latency-svc-549nv [3.504852291s]
Jun 13 03:57:38.912: INFO: Created: latency-svc-mkd28
Jun 13 03:57:39.034: INFO: Created: latency-svc-rhtcm
Jun 13 03:57:39.067: INFO: Got endpoints: latency-svc-mkd28 [3.336020098s]
Jun 13 03:57:39.119: INFO: Got endpoints: latency-svc-rhtcm [3.220551109s]
Jun 13 03:57:39.251: INFO: Created: latency-svc-t7t4p
Jun 13 03:57:39.275: INFO: Got endpoints: latency-svc-t7t4p [3.045555392s]
Jun 13 03:57:39.523: INFO: Created: latency-svc-mncjq
Jun 13 03:57:39.544: INFO: Got endpoints: latency-svc-mncjq [3.213011984s]
Jun 13 03:57:39.677: INFO: Created: latency-svc-w8xr4
Jun 13 03:57:39.766: INFO: Got endpoints: latency-svc-w8xr4 [3.132591771s]
Jun 13 03:57:39.862: INFO: Created: latency-svc-dvs2f
Jun 13 03:57:39.978: INFO: Got endpoints: latency-svc-dvs2f [3.126016356s]
Jun 13 03:57:40.033: INFO: Created: latency-svc-gjphw
Jun 13 03:57:40.200: INFO: Got endpoints: latency-svc-gjphw [3.180208591s]
Jun 13 03:57:40.243: INFO: Created: latency-svc-bvj66
Jun 13 03:57:40.354: INFO: Got endpoints: latency-svc-bvj66 [3.140412425s]
Jun 13 03:57:40.422: INFO: Created: latency-svc-bj8t4
Jun 13 03:57:40.554: INFO: Got endpoints: latency-svc-bj8t4 [3.237913317s]
Jun 13 03:57:40.586: INFO: Created: latency-svc-k92hd
Jun 13 03:57:40.753: INFO: Got endpoints: latency-svc-k92hd [3.381420789s]
Jun 13 03:57:40.873: INFO: Created: latency-svc-74j7m
Jun 13 03:57:41.032: INFO: Got endpoints: latency-svc-74j7m [3.294644701s]
Jun 13 03:57:41.034: INFO: Created: latency-svc-ntpch
Jun 13 03:57:41.171: INFO: Got endpoints: latency-svc-ntpch [3.032071552s]
Jun 13 03:57:41.299: INFO: Created: latency-svc-wvh9d
Jun 13 03:57:41.497: INFO: Got endpoints: latency-svc-wvh9d [3.069273485s]
Jun 13 03:57:41.576: INFO: Created: latency-svc-5c42s
Jun 13 03:57:41.781: INFO: Got endpoints: latency-svc-5c42s [3.207850682s]
Jun 13 03:57:41.851: INFO: Created: latency-svc-rv6x8
Jun 13 03:57:42.040: INFO: Got endpoints: latency-svc-rv6x8 [3.16923782s]
Jun 13 03:57:42.140: INFO: Created: latency-svc-pdfgv
Jun 13 03:57:42.226: INFO: Got endpoints: latency-svc-pdfgv [3.159288709s]
Jun 13 03:57:42.234: INFO: Created: latency-svc-hzwxv
Jun 13 03:57:42.290: INFO: Got endpoints: latency-svc-hzwxv [3.170157021s]
Jun 13 03:57:42.364: INFO: Created: latency-svc-cv5xb
Jun 13 03:57:42.386: INFO: Got endpoints: latency-svc-cv5xb [3.11094263s]
Jun 13 03:57:42.492: INFO: Created: latency-svc-qg7gg
Jun 13 03:57:42.577: INFO: Got endpoints: latency-svc-qg7gg [3.032436896s]
Jun 13 03:57:42.636: INFO: Created: latency-svc-nhkkd
Jun 13 03:57:42.770: INFO: Got endpoints: latency-svc-nhkkd [3.003717244s]
Jun 13 03:57:42.801: INFO: Created: latency-svc-85ljr
Jun 13 03:57:42.908: INFO: Created: latency-svc-phsc8
Jun 13 03:57:42.995: INFO: Got endpoints: latency-svc-phsc8 [2.794593261s]
Jun 13 03:57:42.995: INFO: Got endpoints: latency-svc-85ljr [3.016934083s]
Jun 13 03:57:43.024: INFO: Created: latency-svc-pcdbd
Jun 13 03:57:43.132: INFO: Got endpoints: latency-svc-pcdbd [2.777914036s]
Jun 13 03:57:43.132: INFO: Created: latency-svc-kh7xk
Jun 13 03:57:43.174: INFO: Got endpoints: latency-svc-kh7xk [2.619555191s]
Jun 13 03:57:43.192: INFO: Created: latency-svc-c75tv
Jun 13 03:57:43.269: INFO: Got endpoints: latency-svc-c75tv [2.515430434s]
Jun 13 03:57:43.331: INFO: Created: latency-svc-p48q6
Jun 13 03:57:43.440: INFO: Got endpoints: latency-svc-p48q6 [2.40783227s]
Jun 13 03:57:43.464: INFO: Created: latency-svc-2mjl7
Jun 13 03:57:43.692: INFO: Created: latency-svc-cvs4p
Jun 13 03:57:43.764: INFO: Got endpoints: latency-svc-2mjl7 [2.59364163s]
Jun 13 03:57:43.777: INFO: Got endpoints: latency-svc-cvs4p [2.279486478s]
Jun 13 03:57:43.912: INFO: Created: latency-svc-2xnpt
Jun 13 03:57:44.023: INFO: Got endpoints: latency-svc-2xnpt [2.241164306s]
Jun 13 03:57:44.072: INFO: Created: latency-svc-2pg2x
Jun 13 03:57:44.118: INFO: Got endpoints: latency-svc-2pg2x [2.078222079s]
Jun 13 03:57:44.197: INFO: Created: latency-svc-qvqps
Jun 13 03:57:44.287: INFO: Got endpoints: latency-svc-qvqps [2.061015367s]
Jun 13 03:57:44.334: INFO: Created: latency-svc-l9xnt
Jun 13 03:57:44.371: INFO: Got endpoints: latency-svc-l9xnt [2.081572792s]
Jun 13 03:57:44.470: INFO: Created: latency-svc-h8fhj
Jun 13 03:57:44.538: INFO: Got endpoints: latency-svc-h8fhj [2.151845168s]
Jun 13 03:57:44.620: INFO: Created: latency-svc-645pl
Jun 13 03:57:44.680: INFO: Got endpoints: latency-svc-645pl [2.103064508s]
Jun 13 03:57:44.690: INFO: Created: latency-svc-qhrk9
Jun 13 03:57:44.720: INFO: Got endpoints: latency-svc-qhrk9 [1.950095373s]
Jun 13 03:57:44.785: INFO: Created: latency-svc-dpt9c
Jun 13 03:57:44.866: INFO: Got endpoints: latency-svc-dpt9c [1.871623165s]
Jun 13 03:57:44.968: INFO: Created: latency-svc-w8pxg
Jun 13 03:57:45.011: INFO: Got endpoints: latency-svc-w8pxg [2.016426779s]
Jun 13 03:57:45.059: INFO: Created: latency-svc-slnqq
Jun 13 03:57:45.107: INFO: Got endpoints: latency-svc-slnqq [1.97543297s]
Jun 13 03:57:45.256: INFO: Created: latency-svc-24qlp
Jun 13 03:57:45.343: INFO: Got endpoints: latency-svc-24qlp [2.168873245s]
Jun 13 03:57:45.381: INFO: Created: latency-svc-dnplx
Jun 13 03:57:45.547: INFO: Created: latency-svc-t8s79
Jun 13 03:57:45.548: INFO: Got endpoints: latency-svc-dnplx [2.278834826s]
Jun 13 03:57:45.590: INFO: Got endpoints: latency-svc-t8s79 [2.149723917s]
Jun 13 03:57:45.806: INFO: Created: latency-svc-7frrq
Jun 13 03:57:45.909: INFO: Got endpoints: latency-svc-7frrq [2.145130985s]
Jun 13 03:57:45.915: INFO: Created: latency-svc-dg6t7
Jun 13 03:57:45.968: INFO: Got endpoints: latency-svc-dg6t7 [2.191744532s]
Jun 13 03:57:46.054: INFO: Created: latency-svc-p6wz4
Jun 13 03:57:46.143: INFO: Got endpoints: latency-svc-p6wz4 [2.120161281s]
Jun 13 03:57:46.185: INFO: Created: latency-svc-7kgnj
Jun 13 03:57:46.261: INFO: Got endpoints: latency-svc-7kgnj [2.142519669s]
Jun 13 03:57:46.276: INFO: Created: latency-svc-tjlgx
Jun 13 03:57:46.328: INFO: Got endpoints: latency-svc-tjlgx [2.040627663s]
Jun 13 03:57:46.357: INFO: Created: latency-svc-8lg9q
Jun 13 03:57:46.470: INFO: Got endpoints: latency-svc-8lg9q [2.098628806s]
Jun 13 03:57:46.494: INFO: Created: latency-svc-dgcpr
Jun 13 03:57:46.732: INFO: Got endpoints: latency-svc-dgcpr [2.193967694s]
Jun 13 03:57:46.786: INFO: Created: latency-svc-qhf92
Jun 13 03:57:46.797: INFO: Got endpoints: latency-svc-qhf92 [2.117597657s]
Jun 13 03:57:46.894: INFO: Created: latency-svc-lkrrm
Jun 13 03:57:47.059: INFO: Got endpoints: latency-svc-lkrrm [2.338244824s]
Jun 13 03:57:47.085: INFO: Created: latency-svc-55wnm
Jun 13 03:57:47.166: INFO: Got endpoints: latency-svc-55wnm [2.29977135s]
Jun 13 03:57:47.216: INFO: Created: latency-svc-csqdg
Jun 13 03:57:47.422: INFO: Got endpoints: latency-svc-csqdg [2.410531669s]
Jun 13 03:57:47.447: INFO: Created: latency-svc-xxjd4
Jun 13 03:57:47.493: INFO: Got endpoints: latency-svc-xxjd4 [2.385842745s]
Jun 13 03:57:47.689: INFO: Created: latency-svc-z9fmq
Jun 13 03:57:47.794: INFO: Got endpoints: latency-svc-z9fmq [2.451035329s]
Jun 13 03:57:47.954: INFO: Created: latency-svc-d6qfk
Jun 13 03:57:48.072: INFO: Got endpoints: latency-svc-d6qfk [2.5236138s]
Jun 13 03:57:48.155: INFO: Created: latency-svc-bcqwk
Jun 13 03:57:48.231: INFO: Got endpoints: latency-svc-bcqwk [2.641237567s]
Jun 13 03:57:48.489: INFO: Created: latency-svc-cnbtx
Jun 13 03:57:48.610: INFO: Got endpoints: latency-svc-cnbtx [2.700725746s]
Jun 13 03:57:48.859: INFO: Created: latency-svc-mgm57
Jun 13 03:57:48.983: INFO: Got endpoints: latency-svc-mgm57 [3.014641837s]
Jun 13 03:57:49.014: INFO: Created: latency-svc-xxtxk
Jun 13 03:57:49.123: INFO: Got endpoints: latency-svc-xxtxk [2.980441407s]
Jun 13 03:57:49.176: INFO: Created: latency-svc-ffrjq
Jun 13 03:57:49.176: INFO: Got endpoints: latency-svc-ffrjq [2.915144806s]
Jun 13 03:57:49.337: INFO: Created: latency-svc-j7cjx
Jun 13 03:57:49.348: INFO: Got endpoints: latency-svc-j7cjx [3.020453598s]
Jun 13 03:57:49.376: INFO: Created: latency-svc-n4klv
Jun 13 03:57:49.500: INFO: Got endpoints: latency-svc-n4klv [3.029641969s]
Jun 13 03:57:49.543: INFO: Created: latency-svc-9v9qz
Jun 13 03:57:49.579: INFO: Got endpoints: latency-svc-9v9qz [2.847021418s]
Jun 13 03:57:49.666: INFO: Created: latency-svc-rthkr
Jun 13 03:57:49.809: INFO: Got endpoints: latency-svc-rthkr [3.011513519s]
Jun 13 03:57:49.833: INFO: Created: latency-svc-lbjnh
Jun 13 03:57:49.934: INFO: Created: latency-svc-pf7z7
Jun 13 03:57:50.005: INFO: Got endpoints: latency-svc-lbjnh [2.946847237s]
Jun 13 03:57:50.012: INFO: Got endpoints: latency-svc-pf7z7 [2.845156759s]
Jun 13 03:57:50.055: INFO: Created: latency-svc-s5h77
Jun 13 03:57:50.097: INFO: Got endpoints: latency-svc-s5h77 [2.675262655s]
Jun 13 03:57:50.204: INFO: Created: latency-svc-xkmtl
Jun 13 03:57:50.258: INFO: Got endpoints: latency-svc-xkmtl [2.764633894s]
Jun 13 03:57:50.319: INFO: Created: latency-svc-4s7ff
Jun 13 03:57:50.442: INFO: Got endpoints: latency-svc-4s7ff [2.648137641s]
Jun 13 03:57:50.475: INFO: Created: latency-svc-zcdhn
Jun 13 03:57:50.582: INFO: Created: latency-svc-fxbvw
Jun 13 03:57:50.622: INFO: Got endpoints: latency-svc-zcdhn [2.550457235s]
Jun 13 03:57:50.657: INFO: Got endpoints: latency-svc-fxbvw [2.425420244s]
Jun 13 03:57:50.850: INFO: Created: latency-svc-9l8wz
Jun 13 03:57:50.981: INFO: Got endpoints: latency-svc-9l8wz [2.370227119s]
Jun 13 03:57:51.031: INFO: Created: latency-svc-tqcj9
Jun 13 03:57:51.134: INFO: Got endpoints: latency-svc-tqcj9 [2.15057728s]
Jun 13 03:57:51.134: INFO: Created: latency-svc-5knxs
Jun 13 03:57:51.174: INFO: Got endpoints: latency-svc-5knxs [2.050578797s]
Jun 13 03:57:51.336: INFO: Created: latency-svc-njhg9
Jun 13 03:57:51.512: INFO: Got endpoints: latency-svc-njhg9 [2.336574194s]
Jun 13 03:57:51.875: INFO: Created: latency-svc-fg58z
Jun 13 03:57:51.944: INFO: Created: latency-svc-gzwnc
Jun 13 03:57:52.251: INFO: Created: latency-svc-vs4ml
Jun 13 03:57:52.251: INFO: Got endpoints: latency-svc-gzwnc [2.751871033s]
Jun 13 03:57:52.252: INFO: Got endpoints: latency-svc-fg58z [2.903492741s]
Jun 13 03:57:52.429: INFO: Got endpoints: latency-svc-vs4ml [2.849398313s]
Jun 13 03:57:52.486: INFO: Created: latency-svc-5s8dx
Jun 13 03:57:52.609: INFO: Got endpoints: latency-svc-5s8dx [2.799980197s]
Jun 13 03:57:52.646: INFO: Created: latency-svc-sbpgx
Jun 13 03:57:52.726: INFO: Got endpoints: latency-svc-sbpgx [2.720035856s]
Jun 13 03:57:52.808: INFO: Created: latency-svc-2w9zx
Jun 13 03:57:52.905: INFO: Got endpoints: latency-svc-2w9zx [2.893107017s]
Jun 13 03:57:53.007: INFO: Created: latency-svc-5zk8b
Jun 13 03:57:53.045: INFO: Got endpoints: latency-svc-5zk8b [2.947468888s]
Jun 13 03:57:53.060: INFO: Created: latency-svc-j6sv4
Jun 13 03:57:53.149: INFO: Got endpoints: latency-svc-j6sv4 [2.891416427s]
Jun 13 03:57:53.177: INFO: Created: latency-svc-f6ffc
Jun 13 03:57:53.193: INFO: Got endpoints: latency-svc-f6ffc [2.750319205s]
Jun 13 03:57:53.248: INFO: Created: latency-svc-9jl2r
Jun 13 03:57:53.299: INFO: Got endpoints: latency-svc-9jl2r [2.676445272s]
Jun 13 03:57:53.348: INFO: Created: latency-svc-2c8xq
Jun 13 03:57:53.485: INFO: Got endpoints: latency-svc-2c8xq [2.828526313s]
Jun 13 03:57:53.501: INFO: Created: latency-svc-qp2mz
Jun 13 03:57:53.547: INFO: Got endpoints: latency-svc-qp2mz [2.566652509s]
Jun 13 03:57:53.614: INFO: Created: latency-svc-92hr9
Jun 13 03:57:53.893: INFO: Got endpoints: latency-svc-92hr9 [2.759251859s]
Jun 13 03:57:53.893: INFO: Created: latency-svc-bjf2v
Jun 13 03:57:53.991: INFO: Got endpoints: latency-svc-bjf2v [2.817136655s]
Jun 13 03:57:54.044: INFO: Created: latency-svc-dg4xj
Jun 13 03:57:54.167: INFO: Got endpoints: latency-svc-dg4xj [2.654681476s]
Jun 13 03:57:54.167: INFO: Created: latency-svc-gcbht
Jun 13 03:57:54.195: INFO: Got endpoints: latency-svc-gcbht [1.943945643s]
Jun 13 03:57:54.410: INFO: Created: latency-svc-gc9mr
Jun 13 03:57:54.710: INFO: Got endpoints: latency-svc-gc9mr [2.458510892s]
Jun 13 03:57:54.909: INFO: Created: latency-svc-xkz2j
Jun 13 03:57:55.073: INFO: Got endpoints: latency-svc-xkz2j [2.644406954s]
Jun 13 03:57:55.092: INFO: Created: latency-svc-zvsxd
Jun 13 03:57:55.149: INFO: Got endpoints: latency-svc-zvsxd [2.539750823s]
Jun 13 03:57:55.163: INFO: Created: latency-svc-dwk74
Jun 13 03:57:55.189: INFO: Got endpoints: latency-svc-dwk74 [2.463449678s]
Jun 13 03:57:55.413: INFO: Created: latency-svc-2r4sd
Jun 13 03:57:55.433: INFO: Got endpoints: latency-svc-2r4sd [2.5280834s]
Jun 13 03:57:55.578: INFO: Created: latency-svc-nb4sc
Jun 13 03:57:55.579: INFO: Got endpoints: latency-svc-nb4sc [2.533486877s]
Jun 13 03:57:55.809: INFO: Created: latency-svc-s2q4s
Jun 13 03:57:55.889: INFO: Got endpoints: latency-svc-s2q4s [2.739237733s]
Jun 13 03:57:55.903: INFO: Created: latency-svc-ndwrk
Jun 13 03:57:55.988: INFO: Got endpoints: latency-svc-ndwrk [2.79572906s]
Jun 13 03:57:56.113: INFO: Created: latency-svc-lt428
Jun 13 03:57:56.113: INFO: Got endpoints: latency-svc-lt428 [2.814391284s]
Jun 13 03:57:56.256: INFO: Created: latency-svc-n8jjd
Jun 13 03:57:56.275: INFO: Got endpoints: latency-svc-n8jjd [2.789732977s]
Jun 13 03:57:56.400: INFO: Created: latency-svc-qz8pf
Jun 13 03:57:56.400: INFO: Got endpoints: latency-svc-qz8pf [2.852399468s]
Jun 13 03:57:56.485: INFO: Created: latency-svc-66tmt
Jun 13 03:57:56.578: INFO: Got endpoints: latency-svc-66tmt [2.684543948s]
Jun 13 03:57:56.604: INFO: Created: latency-svc-8l6rg
Jun 13 03:57:56.819: INFO: Got endpoints: latency-svc-8l6rg [2.827233539s]
Jun 13 03:57:56.901: INFO: Created: latency-svc-5q5md
Jun 13 03:57:56.969: INFO: Got endpoints: latency-svc-5q5md [2.802133635s]
Jun 13 03:57:57.073: INFO: Created: latency-svc-7d5hw
Jun 13 03:57:57.276: INFO: Got endpoints: latency-svc-7d5hw [3.080259993s]
Jun 13 03:57:57.307: INFO: Created: latency-svc-xdkc4
Jun 13 03:57:57.321: INFO: Got endpoints: latency-svc-xdkc4 [2.610351276s]
Jun 13 03:57:57.390: INFO: Created: latency-svc-kd8b5
Jun 13 03:57:57.417: INFO: Got endpoints: latency-svc-kd8b5 [2.344037902s]
Jun 13 03:57:57.465: INFO: Created: latency-svc-rh8lc
Jun 13 03:57:57.565: INFO: Got endpoints: latency-svc-rh8lc [2.416113739s]
Jun 13 03:57:57.678: INFO: Created: latency-svc-n27dj
Jun 13 03:57:57.710: INFO: Got endpoints: latency-svc-n27dj [2.521025929s]
Jun 13 03:57:57.724: INFO: Created: latency-svc-dlw2l
Jun 13 03:57:57.791: INFO: Got endpoints: latency-svc-dlw2l [2.3578992s]
Jun 13 03:57:57.858: INFO: Created: latency-svc-cnmx9
Jun 13 03:57:58.216: INFO: Got endpoints: latency-svc-cnmx9 [2.637212094s]
Jun 13 03:57:58.509: INFO: Created: latency-svc-jrqg2
Jun 13 03:57:58.659: INFO: Got endpoints: latency-svc-jrqg2 [2.770072185s]
Jun 13 03:57:58.809: INFO: Created: latency-svc-cl7sg
Jun 13 03:57:59.126: INFO: Got endpoints: latency-svc-cl7sg [3.137309118s]
Jun 13 03:57:59.176: INFO: Created: latency-svc-dv6vb
Jun 13 03:57:59.216: INFO: Got endpoints: latency-svc-dv6vb [3.102582126s]
Jun 13 03:57:59.255: INFO: Created: latency-svc-bvtg9
Jun 13 03:57:59.385: INFO: Got endpoints: latency-svc-bvtg9 [3.109646481s]
Jun 13 03:57:59.385: INFO: Created: latency-svc-htnqg
Jun 13 03:57:59.471: INFO: Got endpoints: latency-svc-htnqg [3.071079112s]
Jun 13 03:57:59.501: INFO: Created: latency-svc-6j6nl
Jun 13 03:57:59.501: INFO: Got endpoints: latency-svc-6j6nl [2.923367375s]
Jun 13 03:57:59.635: INFO: Created: latency-svc-9tqmp
Jun 13 03:57:59.653: INFO: Got endpoints: latency-svc-9tqmp [2.834375313s]
Jun 13 03:57:59.691: INFO: Created: latency-svc-5dfl6
Jun 13 03:57:59.711: INFO: Got endpoints: latency-svc-5dfl6 [2.741516963s]
Jun 13 03:57:59.847: INFO: Created: latency-svc-kt2kl
Jun 13 03:57:59.893: INFO: Got endpoints: latency-svc-kt2kl [2.617023621s]
Jun 13 03:57:59.949: INFO: Created: latency-svc-tdcz4
Jun 13 03:57:59.994: INFO: Got endpoints: latency-svc-tdcz4 [2.673599793s]
Jun 13 03:58:00.103: INFO: Created: latency-svc-8qnxf
Jun 13 03:58:00.217: INFO: Got endpoints: latency-svc-8qnxf [2.799367136s]
Jun 13 03:58:00.289: INFO: Created: latency-svc-4mhj7
Jun 13 03:58:00.289: INFO: Got endpoints: latency-svc-4mhj7 [2.723603486s]
Jun 13 03:58:00.381: INFO: Created: latency-svc-j7pkg
Jun 13 03:58:00.439: INFO: Got endpoints: latency-svc-j7pkg [2.728645669s]
Jun 13 03:58:00.537: INFO: Created: latency-svc-wpz6l
Jun 13 03:58:00.649: INFO: Got endpoints: latency-svc-wpz6l [2.857774423s]
Jun 13 03:58:00.660: INFO: Created: latency-svc-hmdhr
Jun 13 03:58:00.703: INFO: Got endpoints: latency-svc-hmdhr [2.486999575s]
Jun 13 03:58:00.931: INFO: Created: latency-svc-2g82v
Jun 13 03:58:00.957: INFO: Got endpoints: latency-svc-2g82v [2.298652093s]
Jun 13 03:58:01.087: INFO: Created: latency-svc-8l877
Jun 13 03:58:01.133: INFO: Got endpoints: latency-svc-8l877 [2.007670902s]
Jun 13 03:58:01.235: INFO: Created: latency-svc-64lls
Jun 13 03:58:01.371: INFO: Got endpoints: latency-svc-64lls [2.154811579s]
Jun 13 03:58:01.442: INFO: Created: latency-svc-4rwmf
Jun 13 03:58:01.594: INFO: Got endpoints: latency-svc-4rwmf [2.209254414s]
Jun 13 03:58:01.666: INFO: Created: latency-svc-jdk2f
Jun 13 03:58:01.939: INFO: Created: latency-svc-gbxp6
Jun 13 03:58:01.940: INFO: Got endpoints: latency-svc-jdk2f [2.468759157s]
Jun 13 03:58:02.112: INFO: Got endpoints: latency-svc-gbxp6 [2.610356016s]
Jun 13 03:58:02.191: INFO: Created: latency-svc-xg6tk
Jun 13 03:58:02.493: INFO: Got endpoints: latency-svc-xg6tk [2.840443459s]
Jun 13 03:58:02.496: INFO: Created: latency-svc-gbqjg
Jun 13 03:58:02.679: INFO: Got endpoints: latency-svc-gbqjg [2.968046542s]
Jun 13 03:58:02.750: INFO: Created: latency-svc-v668g
Jun 13 03:58:02.973: INFO: Got endpoints: latency-svc-v668g [3.079750377s]
Jun 13 03:58:02.978: INFO: Created: latency-svc-s5w86
Jun 13 03:58:03.029: INFO: Got endpoints: latency-svc-s5w86 [3.035012751s]
Jun 13 03:58:03.087: INFO: Created: latency-svc-w7bbf
Jun 13 03:58:03.200: INFO: Got endpoints: latency-svc-w7bbf [2.983102434s]
Jun 13 03:58:03.272: INFO: Created: latency-svc-494rv
Jun 13 03:58:03.407: INFO: Got endpoints: latency-svc-494rv [3.118062217s]
Jun 13 03:58:03.431: INFO: Created: latency-svc-96vrp
Jun 13 03:58:03.468: INFO: Got endpoints: latency-svc-96vrp [3.029145119s]
Jun 13 03:58:03.547: INFO: Created: latency-svc-dgv2f
Jun 13 03:58:03.575: INFO: Got endpoints: latency-svc-dgv2f [2.926226849s]
Jun 13 03:58:03.670: INFO: Created: latency-svc-2h8q4
Jun 13 03:58:03.688: INFO: Got endpoints: latency-svc-2h8q4 [2.984979311s]
Jun 13 03:58:03.948: INFO: Created: latency-svc-zgg82
Jun 13 03:58:04.106: INFO: Got endpoints: latency-svc-zgg82 [3.148213905s]
Jun 13 03:58:04.152: INFO: Created: latency-svc-xqhtv
Jun 13 03:58:04.264: INFO: Got endpoints: latency-svc-xqhtv [3.130927025s]
Jun 13 03:58:04.425: INFO: Created: latency-svc-dqr64
Jun 13 03:58:04.565: INFO: Got endpoints: latency-svc-dqr64 [3.194433514s]
Jun 13 03:58:04.583: INFO: Created: latency-svc-mqhwc
Jun 13 03:58:04.692: INFO: Got endpoints: latency-svc-mqhwc [3.09744118s]
Jun 13 03:58:04.711: INFO: Created: latency-svc-2xk2r
Jun 13 03:58:04.749: INFO: Got endpoints: latency-svc-2xk2r [2.808989465s]
Jun 13 03:58:04.749: INFO: Latencies: [361.470526ms 561.497194ms 778.377317ms 1.333749349s 1.54126576s 1.777053615s 1.871623165s 1.943945643s 1.950095373s 1.97543297s 2.007670902s 2.016426779s 2.040627663s 2.050578797s 2.061015367s 2.078222079s 2.081572792s 2.098628806s 2.103064508s 2.117597657s 2.120161281s 2.142519669s 2.145130985s 2.149723917s 2.15057728s 2.151845168s 2.154811579s 2.168873245s 2.191744532s 2.193967694s 2.209254414s 2.241164306s 2.278834826s 2.279486478s 2.298652093s 2.29977135s 2.336574194s 2.338244824s 2.344037902s 2.3578992s 2.366469657s 2.370227119s 2.385842745s 2.40783227s 2.410531669s 2.416113739s 2.425420244s 2.451035329s 2.458510892s 2.463449678s 2.468759157s 2.486999575s 2.515430434s 2.521025929s 2.5236138s 2.5280834s 2.533486877s 2.539750823s 2.539978751s 2.550457235s 2.566652509s 2.59364163s 2.610351276s 2.610356016s 2.617023621s 2.619555191s 2.637212094s 2.641237567s 2.644406954s 2.648137641s 2.654681476s 2.673599793s 2.675262655s 2.676445272s 2.684543948s 2.700725746s 2.720035856s 2.723603486s 2.728645669s 2.739237733s 2.741516963s 2.750319205s 2.751871033s 2.759251859s 2.764633894s 2.770072185s 2.776507018s 2.776552416s 2.777914036s 2.789732977s 2.794593261s 2.79572906s 2.799367136s 2.799980197s 2.802133635s 2.808989465s 2.814391284s 2.816046889s 2.817136655s 2.827233539s 2.828526313s 2.834375313s 2.840443459s 2.845156759s 2.847021418s 2.849398313s 2.851912057s 2.852399468s 2.857774423s 2.865854587s 2.873575201s 2.891416427s 2.893107017s 2.903492741s 2.915144806s 2.923367375s 2.926226849s 2.937945123s 2.94432813s 2.946847237s 2.947468888s 2.954971801s 2.968046542s 2.973045108s 2.980441407s 2.983102434s 2.984979311s 2.987578598s 3.003717244s 3.011513519s 3.014641837s 3.016934083s 3.020453598s 3.025273304s 3.029145119s 3.029641969s 3.032071552s 3.032436896s 3.035012751s 3.045555392s 3.053161731s 3.069273485s 3.071079112s 3.072963306s 3.079750377s 3.080259993s 3.09744118s 3.102582126s 3.103409218s 3.104023459s 3.109646481s 3.11094263s 3.118062217s 3.126016356s 3.130927025s 3.132591771s 3.137309118s 3.140412425s 3.148213905s 3.159288709s 3.16923782s 3.170157021s 3.180208591s 3.18691971s 3.194433514s 3.207850682s 3.210853474s 3.213011984s 3.220551109s 3.229262021s 3.237913317s 3.238608578s 3.259501201s 3.263851426s 3.294644701s 3.303324031s 3.336020098s 3.369229359s 3.378232984s 3.381420789s 3.410664124s 3.429722995s 3.496726619s 3.504852291s 3.57727844s 3.581187077s 3.644813238s 3.699308994s 3.700456488s 3.742831802s 3.781258711s 3.817720045s 3.818457434s 3.890271775s 3.948974653s 3.954166934s 4.051557207s 4.193445492s 4.225285926s 4.404698355s]
Jun 13 03:58:04.749: INFO: 50 %ile: 2.828526313s
Jun 13 03:58:04.749: INFO: 90 %ile: 3.410664124s
Jun 13 03:58:04.749: INFO: 99 %ile: 4.225285926s
Jun 13 03:58:04.749: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jun 13 03:58:04.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7882" for this suite. 06/13/23 03:58:04.842
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":352,"skipped":6559,"failed":0}
------------------------------
• [SLOW TEST] [43.585 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:57:21.319
    Jun 13 03:57:21.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename svc-latency 06/13/23 03:57:21.321
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:57:21.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:57:21.747
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jun 13 03:57:21.780: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7882 06/13/23 03:57:21.782
    I0613 03:57:21.871558      18 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7882, replica count: 1
    I0613 03:57:22.922918      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0613 03:57:23.923162      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0613 03:57:24.923479      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0613 03:57:25.923750      18 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 13 03:57:26.153: INFO: Created: latency-svc-zxspz
    Jun 13 03:57:26.308: INFO: Got endpoints: latency-svc-zxspz [283.787238ms]
    Jun 13 03:57:26.578: INFO: Created: latency-svc-fd825
    Jun 13 03:57:26.671: INFO: Got endpoints: latency-svc-fd825 [361.470526ms]
    Jun 13 03:57:26.808: INFO: Created: latency-svc-49dn7
    Jun 13 03:57:26.871: INFO: Got endpoints: latency-svc-49dn7 [561.497194ms]
    Jun 13 03:57:27.011: INFO: Created: latency-svc-7ht9x
    Jun 13 03:57:27.087: INFO: Got endpoints: latency-svc-7ht9x [778.377317ms]
    Jun 13 03:57:27.393: INFO: Created: latency-svc-ssgrd
    Jun 13 03:57:27.643: INFO: Got endpoints: latency-svc-ssgrd [1.333749349s]
    Jun 13 03:57:27.719: INFO: Created: latency-svc-pzvpn
    Jun 13 03:57:27.851: INFO: Got endpoints: latency-svc-pzvpn [1.54126576s]
    Jun 13 03:57:27.968: INFO: Created: latency-svc-sctwv
    Jun 13 03:57:28.087: INFO: Got endpoints: latency-svc-sctwv [1.777053615s]
    Jun 13 03:57:28.430: INFO: Created: latency-svc-mln5p
    Jun 13 03:57:28.677: INFO: Got endpoints: latency-svc-mln5p [2.366469657s]
    Jun 13 03:57:28.694: INFO: Created: latency-svc-tp9xr
    Jun 13 03:57:28.850: INFO: Got endpoints: latency-svc-tp9xr [2.539978751s]
    Jun 13 03:57:28.899: INFO: Created: latency-svc-l2bvj
    Jun 13 03:57:29.087: INFO: Got endpoints: latency-svc-l2bvj [2.776507018s]
    Jun 13 03:57:29.143: INFO: Created: latency-svc-bd4bc
    Jun 13 03:57:29.162: INFO: Got endpoints: latency-svc-bd4bc [2.851912057s]
    Jun 13 03:57:29.297: INFO: Created: latency-svc-2m6nm
    Jun 13 03:57:29.415: INFO: Got endpoints: latency-svc-2m6nm [3.104023459s]
    Jun 13 03:57:29.445: INFO: Created: latency-svc-pvpwb
    Jun 13 03:57:29.540: INFO: Got endpoints: latency-svc-pvpwb [3.229262021s]
    Jun 13 03:57:29.595: INFO: Created: latency-svc-hrqxv
    Jun 13 03:57:29.680: INFO: Got endpoints: latency-svc-hrqxv [3.369229359s]
    Jun 13 03:57:29.685: INFO: Created: latency-svc-4sgwm
    Jun 13 03:57:29.721: INFO: Got endpoints: latency-svc-4sgwm [3.410664124s]
    Jun 13 03:57:29.789: INFO: Created: latency-svc-ghs4v
    Jun 13 03:57:29.891: INFO: Got endpoints: latency-svc-ghs4v [3.581187077s]
    Jun 13 03:57:29.895: INFO: Created: latency-svc-8mtwx
    Jun 13 03:57:29.975: INFO: Got endpoints: latency-svc-8mtwx [3.303324031s]
    Jun 13 03:57:30.043: INFO: Created: latency-svc-nkh4q
    Jun 13 03:57:30.135: INFO: Got endpoints: latency-svc-nkh4q [3.263851426s]
    Jun 13 03:57:30.157: INFO: Created: latency-svc-jm5mk
    Jun 13 03:57:30.190: INFO: Got endpoints: latency-svc-jm5mk [3.103409218s]
    Jun 13 03:57:30.394: INFO: Created: latency-svc-m462m
    Jun 13 03:57:30.459: INFO: Got endpoints: latency-svc-m462m [2.816046889s]
    Jun 13 03:57:30.677: INFO: Created: latency-svc-kkk6s
    Jun 13 03:57:30.924: INFO: Got endpoints: latency-svc-kkk6s [3.072963306s]
    Jun 13 03:57:30.998: INFO: Created: latency-svc-hg4nr
    Jun 13 03:57:31.112: INFO: Got endpoints: latency-svc-hg4nr [3.025273304s]
    Jun 13 03:57:31.323: INFO: Created: latency-svc-l9zps
    Jun 13 03:57:31.550: INFO: Got endpoints: latency-svc-l9zps [2.873575201s]
    Jun 13 03:57:31.579: INFO: Created: latency-svc-5vx98
    Jun 13 03:57:31.788: INFO: Got endpoints: latency-svc-5vx98 [2.937945123s]
    Jun 13 03:57:31.856: INFO: Created: latency-svc-qgjm4
    Jun 13 03:57:32.031: INFO: Got endpoints: latency-svc-qgjm4 [2.94432813s]
    Jun 13 03:57:32.060: INFO: Created: latency-svc-r264p
    Jun 13 03:57:32.117: INFO: Got endpoints: latency-svc-r264p [2.954971801s]
    Jun 13 03:57:32.220: INFO: Created: latency-svc-9j5hj
    Jun 13 03:57:32.281: INFO: Got endpoints: latency-svc-9j5hj [2.865854587s]
    Jun 13 03:57:32.401: INFO: Created: latency-svc-d29l9
    Jun 13 03:57:32.513: INFO: Got endpoints: latency-svc-d29l9 [2.973045108s]
    Jun 13 03:57:32.541: INFO: Created: latency-svc-jxddr
    Jun 13 03:57:32.891: INFO: Got endpoints: latency-svc-jxddr [3.210853474s]
    Jun 13 03:57:32.891: INFO: Created: latency-svc-ncchn
    Jun 13 03:57:33.151: INFO: Got endpoints: latency-svc-ncchn [3.429722995s]
    Jun 13 03:57:33.295: INFO: Created: latency-svc-pzhjw
    Jun 13 03:57:33.781: INFO: Got endpoints: latency-svc-pzhjw [3.890271775s]
    Jun 13 03:57:34.014: INFO: Created: latency-svc-fqcnz
    Jun 13 03:57:34.026: INFO: Got endpoints: latency-svc-fqcnz [4.051557207s]
    Jun 13 03:57:34.203: INFO: Created: latency-svc-vfxqj
    Jun 13 03:57:34.329: INFO: Got endpoints: latency-svc-vfxqj [4.193445492s]
    Jun 13 03:57:34.491: INFO: Created: latency-svc-hps58
    Jun 13 03:57:34.595: INFO: Got endpoints: latency-svc-hps58 [4.404698355s]
    Jun 13 03:57:34.654: INFO: Created: latency-svc-fq94z
    Jun 13 03:57:34.684: INFO: Got endpoints: latency-svc-fq94z [4.225285926s]
    Jun 13 03:57:34.842: INFO: Created: latency-svc-tqvbw
    Jun 13 03:57:34.879: INFO: Got endpoints: latency-svc-tqvbw [3.954166934s]
    Jun 13 03:57:34.898: INFO: Created: latency-svc-5q84m
    Jun 13 03:57:34.931: INFO: Got endpoints: latency-svc-5q84m [3.818457434s]
    Jun 13 03:57:35.071: INFO: Created: latency-svc-fjgv9
    Jun 13 03:57:35.195: INFO: Got endpoints: latency-svc-fjgv9 [3.644813238s]
    Jun 13 03:57:35.271: INFO: Created: latency-svc-gfc9b
    Jun 13 03:57:35.366: INFO: Got endpoints: latency-svc-gfc9b [3.57727844s]
    Jun 13 03:57:35.514: INFO: Created: latency-svc-v5sj4
    Jun 13 03:57:35.731: INFO: Got endpoints: latency-svc-v5sj4 [3.699308994s]
    Jun 13 03:57:35.799: INFO: Created: latency-svc-jxhkb
    Jun 13 03:57:35.899: INFO: Got endpoints: latency-svc-jxhkb [3.781258711s]
    Jun 13 03:57:36.109: INFO: Created: latency-svc-7f6gx
    Jun 13 03:57:36.230: INFO: Got endpoints: latency-svc-7f6gx [3.948974653s]
    Jun 13 03:57:36.251: INFO: Created: latency-svc-p2ndp
    Jun 13 03:57:36.331: INFO: Got endpoints: latency-svc-p2ndp [3.817720045s]
    Jun 13 03:57:36.632: INFO: Created: latency-svc-fxwrj
    Jun 13 03:57:36.634: INFO: Got endpoints: latency-svc-fxwrj [3.742831802s]
    Jun 13 03:57:36.690: INFO: Created: latency-svc-6kplq
    Jun 13 03:57:36.852: INFO: Got endpoints: latency-svc-6kplq [3.700456488s]
    Jun 13 03:57:36.936: INFO: Created: latency-svc-c7qzr
    Jun 13 03:57:37.020: INFO: Got endpoints: latency-svc-c7qzr [3.238608578s]
    Jun 13 03:57:37.055: INFO: Created: latency-svc-p88gk
    Jun 13 03:57:37.213: INFO: Got endpoints: latency-svc-p88gk [3.18691971s]
    Jun 13 03:57:37.297: INFO: Created: latency-svc-w2kfm
    Jun 13 03:57:37.316: INFO: Got endpoints: latency-svc-w2kfm [2.987578598s]
    Jun 13 03:57:37.371: INFO: Created: latency-svc-8q5tc
    Jun 13 03:57:37.372: INFO: Got endpoints: latency-svc-8q5tc [2.776552416s]
    Jun 13 03:57:37.580: INFO: Created: latency-svc-sq4xg
    Jun 13 03:57:37.737: INFO: Got endpoints: latency-svc-sq4xg [3.053161731s]
    Jun 13 03:57:37.853: INFO: Created: latency-svc-cwn5d
    Jun 13 03:57:38.138: INFO: Got endpoints: latency-svc-cwn5d [3.259501201s]
    Jun 13 03:57:38.139: INFO: Created: latency-svc-rvxb9
    Jun 13 03:57:38.291: INFO: Created: latency-svc-h7qfj
    Jun 13 03:57:38.428: INFO: Got endpoints: latency-svc-rvxb9 [3.496726619s]
    Jun 13 03:57:38.573: INFO: Got endpoints: latency-svc-h7qfj [3.378232984s]
    Jun 13 03:57:38.704: INFO: Created: latency-svc-549nv
    Jun 13 03:57:38.870: INFO: Got endpoints: latency-svc-549nv [3.504852291s]
    Jun 13 03:57:38.912: INFO: Created: latency-svc-mkd28
    Jun 13 03:57:39.034: INFO: Created: latency-svc-rhtcm
    Jun 13 03:57:39.067: INFO: Got endpoints: latency-svc-mkd28 [3.336020098s]
    Jun 13 03:57:39.119: INFO: Got endpoints: latency-svc-rhtcm [3.220551109s]
    Jun 13 03:57:39.251: INFO: Created: latency-svc-t7t4p
    Jun 13 03:57:39.275: INFO: Got endpoints: latency-svc-t7t4p [3.045555392s]
    Jun 13 03:57:39.523: INFO: Created: latency-svc-mncjq
    Jun 13 03:57:39.544: INFO: Got endpoints: latency-svc-mncjq [3.213011984s]
    Jun 13 03:57:39.677: INFO: Created: latency-svc-w8xr4
    Jun 13 03:57:39.766: INFO: Got endpoints: latency-svc-w8xr4 [3.132591771s]
    Jun 13 03:57:39.862: INFO: Created: latency-svc-dvs2f
    Jun 13 03:57:39.978: INFO: Got endpoints: latency-svc-dvs2f [3.126016356s]
    Jun 13 03:57:40.033: INFO: Created: latency-svc-gjphw
    Jun 13 03:57:40.200: INFO: Got endpoints: latency-svc-gjphw [3.180208591s]
    Jun 13 03:57:40.243: INFO: Created: latency-svc-bvj66
    Jun 13 03:57:40.354: INFO: Got endpoints: latency-svc-bvj66 [3.140412425s]
    Jun 13 03:57:40.422: INFO: Created: latency-svc-bj8t4
    Jun 13 03:57:40.554: INFO: Got endpoints: latency-svc-bj8t4 [3.237913317s]
    Jun 13 03:57:40.586: INFO: Created: latency-svc-k92hd
    Jun 13 03:57:40.753: INFO: Got endpoints: latency-svc-k92hd [3.381420789s]
    Jun 13 03:57:40.873: INFO: Created: latency-svc-74j7m
    Jun 13 03:57:41.032: INFO: Got endpoints: latency-svc-74j7m [3.294644701s]
    Jun 13 03:57:41.034: INFO: Created: latency-svc-ntpch
    Jun 13 03:57:41.171: INFO: Got endpoints: latency-svc-ntpch [3.032071552s]
    Jun 13 03:57:41.299: INFO: Created: latency-svc-wvh9d
    Jun 13 03:57:41.497: INFO: Got endpoints: latency-svc-wvh9d [3.069273485s]
    Jun 13 03:57:41.576: INFO: Created: latency-svc-5c42s
    Jun 13 03:57:41.781: INFO: Got endpoints: latency-svc-5c42s [3.207850682s]
    Jun 13 03:57:41.851: INFO: Created: latency-svc-rv6x8
    Jun 13 03:57:42.040: INFO: Got endpoints: latency-svc-rv6x8 [3.16923782s]
    Jun 13 03:57:42.140: INFO: Created: latency-svc-pdfgv
    Jun 13 03:57:42.226: INFO: Got endpoints: latency-svc-pdfgv [3.159288709s]
    Jun 13 03:57:42.234: INFO: Created: latency-svc-hzwxv
    Jun 13 03:57:42.290: INFO: Got endpoints: latency-svc-hzwxv [3.170157021s]
    Jun 13 03:57:42.364: INFO: Created: latency-svc-cv5xb
    Jun 13 03:57:42.386: INFO: Got endpoints: latency-svc-cv5xb [3.11094263s]
    Jun 13 03:57:42.492: INFO: Created: latency-svc-qg7gg
    Jun 13 03:57:42.577: INFO: Got endpoints: latency-svc-qg7gg [3.032436896s]
    Jun 13 03:57:42.636: INFO: Created: latency-svc-nhkkd
    Jun 13 03:57:42.770: INFO: Got endpoints: latency-svc-nhkkd [3.003717244s]
    Jun 13 03:57:42.801: INFO: Created: latency-svc-85ljr
    Jun 13 03:57:42.908: INFO: Created: latency-svc-phsc8
    Jun 13 03:57:42.995: INFO: Got endpoints: latency-svc-phsc8 [2.794593261s]
    Jun 13 03:57:42.995: INFO: Got endpoints: latency-svc-85ljr [3.016934083s]
    Jun 13 03:57:43.024: INFO: Created: latency-svc-pcdbd
    Jun 13 03:57:43.132: INFO: Got endpoints: latency-svc-pcdbd [2.777914036s]
    Jun 13 03:57:43.132: INFO: Created: latency-svc-kh7xk
    Jun 13 03:57:43.174: INFO: Got endpoints: latency-svc-kh7xk [2.619555191s]
    Jun 13 03:57:43.192: INFO: Created: latency-svc-c75tv
    Jun 13 03:57:43.269: INFO: Got endpoints: latency-svc-c75tv [2.515430434s]
    Jun 13 03:57:43.331: INFO: Created: latency-svc-p48q6
    Jun 13 03:57:43.440: INFO: Got endpoints: latency-svc-p48q6 [2.40783227s]
    Jun 13 03:57:43.464: INFO: Created: latency-svc-2mjl7
    Jun 13 03:57:43.692: INFO: Created: latency-svc-cvs4p
    Jun 13 03:57:43.764: INFO: Got endpoints: latency-svc-2mjl7 [2.59364163s]
    Jun 13 03:57:43.777: INFO: Got endpoints: latency-svc-cvs4p [2.279486478s]
    Jun 13 03:57:43.912: INFO: Created: latency-svc-2xnpt
    Jun 13 03:57:44.023: INFO: Got endpoints: latency-svc-2xnpt [2.241164306s]
    Jun 13 03:57:44.072: INFO: Created: latency-svc-2pg2x
    Jun 13 03:57:44.118: INFO: Got endpoints: latency-svc-2pg2x [2.078222079s]
    Jun 13 03:57:44.197: INFO: Created: latency-svc-qvqps
    Jun 13 03:57:44.287: INFO: Got endpoints: latency-svc-qvqps [2.061015367s]
    Jun 13 03:57:44.334: INFO: Created: latency-svc-l9xnt
    Jun 13 03:57:44.371: INFO: Got endpoints: latency-svc-l9xnt [2.081572792s]
    Jun 13 03:57:44.470: INFO: Created: latency-svc-h8fhj
    Jun 13 03:57:44.538: INFO: Got endpoints: latency-svc-h8fhj [2.151845168s]
    Jun 13 03:57:44.620: INFO: Created: latency-svc-645pl
    Jun 13 03:57:44.680: INFO: Got endpoints: latency-svc-645pl [2.103064508s]
    Jun 13 03:57:44.690: INFO: Created: latency-svc-qhrk9
    Jun 13 03:57:44.720: INFO: Got endpoints: latency-svc-qhrk9 [1.950095373s]
    Jun 13 03:57:44.785: INFO: Created: latency-svc-dpt9c
    Jun 13 03:57:44.866: INFO: Got endpoints: latency-svc-dpt9c [1.871623165s]
    Jun 13 03:57:44.968: INFO: Created: latency-svc-w8pxg
    Jun 13 03:57:45.011: INFO: Got endpoints: latency-svc-w8pxg [2.016426779s]
    Jun 13 03:57:45.059: INFO: Created: latency-svc-slnqq
    Jun 13 03:57:45.107: INFO: Got endpoints: latency-svc-slnqq [1.97543297s]
    Jun 13 03:57:45.256: INFO: Created: latency-svc-24qlp
    Jun 13 03:57:45.343: INFO: Got endpoints: latency-svc-24qlp [2.168873245s]
    Jun 13 03:57:45.381: INFO: Created: latency-svc-dnplx
    Jun 13 03:57:45.547: INFO: Created: latency-svc-t8s79
    Jun 13 03:57:45.548: INFO: Got endpoints: latency-svc-dnplx [2.278834826s]
    Jun 13 03:57:45.590: INFO: Got endpoints: latency-svc-t8s79 [2.149723917s]
    Jun 13 03:57:45.806: INFO: Created: latency-svc-7frrq
    Jun 13 03:57:45.909: INFO: Got endpoints: latency-svc-7frrq [2.145130985s]
    Jun 13 03:57:45.915: INFO: Created: latency-svc-dg6t7
    Jun 13 03:57:45.968: INFO: Got endpoints: latency-svc-dg6t7 [2.191744532s]
    Jun 13 03:57:46.054: INFO: Created: latency-svc-p6wz4
    Jun 13 03:57:46.143: INFO: Got endpoints: latency-svc-p6wz4 [2.120161281s]
    Jun 13 03:57:46.185: INFO: Created: latency-svc-7kgnj
    Jun 13 03:57:46.261: INFO: Got endpoints: latency-svc-7kgnj [2.142519669s]
    Jun 13 03:57:46.276: INFO: Created: latency-svc-tjlgx
    Jun 13 03:57:46.328: INFO: Got endpoints: latency-svc-tjlgx [2.040627663s]
    Jun 13 03:57:46.357: INFO: Created: latency-svc-8lg9q
    Jun 13 03:57:46.470: INFO: Got endpoints: latency-svc-8lg9q [2.098628806s]
    Jun 13 03:57:46.494: INFO: Created: latency-svc-dgcpr
    Jun 13 03:57:46.732: INFO: Got endpoints: latency-svc-dgcpr [2.193967694s]
    Jun 13 03:57:46.786: INFO: Created: latency-svc-qhf92
    Jun 13 03:57:46.797: INFO: Got endpoints: latency-svc-qhf92 [2.117597657s]
    Jun 13 03:57:46.894: INFO: Created: latency-svc-lkrrm
    Jun 13 03:57:47.059: INFO: Got endpoints: latency-svc-lkrrm [2.338244824s]
    Jun 13 03:57:47.085: INFO: Created: latency-svc-55wnm
    Jun 13 03:57:47.166: INFO: Got endpoints: latency-svc-55wnm [2.29977135s]
    Jun 13 03:57:47.216: INFO: Created: latency-svc-csqdg
    Jun 13 03:57:47.422: INFO: Got endpoints: latency-svc-csqdg [2.410531669s]
    Jun 13 03:57:47.447: INFO: Created: latency-svc-xxjd4
    Jun 13 03:57:47.493: INFO: Got endpoints: latency-svc-xxjd4 [2.385842745s]
    Jun 13 03:57:47.689: INFO: Created: latency-svc-z9fmq
    Jun 13 03:57:47.794: INFO: Got endpoints: latency-svc-z9fmq [2.451035329s]
    Jun 13 03:57:47.954: INFO: Created: latency-svc-d6qfk
    Jun 13 03:57:48.072: INFO: Got endpoints: latency-svc-d6qfk [2.5236138s]
    Jun 13 03:57:48.155: INFO: Created: latency-svc-bcqwk
    Jun 13 03:57:48.231: INFO: Got endpoints: latency-svc-bcqwk [2.641237567s]
    Jun 13 03:57:48.489: INFO: Created: latency-svc-cnbtx
    Jun 13 03:57:48.610: INFO: Got endpoints: latency-svc-cnbtx [2.700725746s]
    Jun 13 03:57:48.859: INFO: Created: latency-svc-mgm57
    Jun 13 03:57:48.983: INFO: Got endpoints: latency-svc-mgm57 [3.014641837s]
    Jun 13 03:57:49.014: INFO: Created: latency-svc-xxtxk
    Jun 13 03:57:49.123: INFO: Got endpoints: latency-svc-xxtxk [2.980441407s]
    Jun 13 03:57:49.176: INFO: Created: latency-svc-ffrjq
    Jun 13 03:57:49.176: INFO: Got endpoints: latency-svc-ffrjq [2.915144806s]
    Jun 13 03:57:49.337: INFO: Created: latency-svc-j7cjx
    Jun 13 03:57:49.348: INFO: Got endpoints: latency-svc-j7cjx [3.020453598s]
    Jun 13 03:57:49.376: INFO: Created: latency-svc-n4klv
    Jun 13 03:57:49.500: INFO: Got endpoints: latency-svc-n4klv [3.029641969s]
    Jun 13 03:57:49.543: INFO: Created: latency-svc-9v9qz
    Jun 13 03:57:49.579: INFO: Got endpoints: latency-svc-9v9qz [2.847021418s]
    Jun 13 03:57:49.666: INFO: Created: latency-svc-rthkr
    Jun 13 03:57:49.809: INFO: Got endpoints: latency-svc-rthkr [3.011513519s]
    Jun 13 03:57:49.833: INFO: Created: latency-svc-lbjnh
    Jun 13 03:57:49.934: INFO: Created: latency-svc-pf7z7
    Jun 13 03:57:50.005: INFO: Got endpoints: latency-svc-lbjnh [2.946847237s]
    Jun 13 03:57:50.012: INFO: Got endpoints: latency-svc-pf7z7 [2.845156759s]
    Jun 13 03:57:50.055: INFO: Created: latency-svc-s5h77
    Jun 13 03:57:50.097: INFO: Got endpoints: latency-svc-s5h77 [2.675262655s]
    Jun 13 03:57:50.204: INFO: Created: latency-svc-xkmtl
    Jun 13 03:57:50.258: INFO: Got endpoints: latency-svc-xkmtl [2.764633894s]
    Jun 13 03:57:50.319: INFO: Created: latency-svc-4s7ff
    Jun 13 03:57:50.442: INFO: Got endpoints: latency-svc-4s7ff [2.648137641s]
    Jun 13 03:57:50.475: INFO: Created: latency-svc-zcdhn
    Jun 13 03:57:50.582: INFO: Created: latency-svc-fxbvw
    Jun 13 03:57:50.622: INFO: Got endpoints: latency-svc-zcdhn [2.550457235s]
    Jun 13 03:57:50.657: INFO: Got endpoints: latency-svc-fxbvw [2.425420244s]
    Jun 13 03:57:50.850: INFO: Created: latency-svc-9l8wz
    Jun 13 03:57:50.981: INFO: Got endpoints: latency-svc-9l8wz [2.370227119s]
    Jun 13 03:57:51.031: INFO: Created: latency-svc-tqcj9
    Jun 13 03:57:51.134: INFO: Got endpoints: latency-svc-tqcj9 [2.15057728s]
    Jun 13 03:57:51.134: INFO: Created: latency-svc-5knxs
    Jun 13 03:57:51.174: INFO: Got endpoints: latency-svc-5knxs [2.050578797s]
    Jun 13 03:57:51.336: INFO: Created: latency-svc-njhg9
    Jun 13 03:57:51.512: INFO: Got endpoints: latency-svc-njhg9 [2.336574194s]
    Jun 13 03:57:51.875: INFO: Created: latency-svc-fg58z
    Jun 13 03:57:51.944: INFO: Created: latency-svc-gzwnc
    Jun 13 03:57:52.251: INFO: Created: latency-svc-vs4ml
    Jun 13 03:57:52.251: INFO: Got endpoints: latency-svc-gzwnc [2.751871033s]
    Jun 13 03:57:52.252: INFO: Got endpoints: latency-svc-fg58z [2.903492741s]
    Jun 13 03:57:52.429: INFO: Got endpoints: latency-svc-vs4ml [2.849398313s]
    Jun 13 03:57:52.486: INFO: Created: latency-svc-5s8dx
    Jun 13 03:57:52.609: INFO: Got endpoints: latency-svc-5s8dx [2.799980197s]
    Jun 13 03:57:52.646: INFO: Created: latency-svc-sbpgx
    Jun 13 03:57:52.726: INFO: Got endpoints: latency-svc-sbpgx [2.720035856s]
    Jun 13 03:57:52.808: INFO: Created: latency-svc-2w9zx
    Jun 13 03:57:52.905: INFO: Got endpoints: latency-svc-2w9zx [2.893107017s]
    Jun 13 03:57:53.007: INFO: Created: latency-svc-5zk8b
    Jun 13 03:57:53.045: INFO: Got endpoints: latency-svc-5zk8b [2.947468888s]
    Jun 13 03:57:53.060: INFO: Created: latency-svc-j6sv4
    Jun 13 03:57:53.149: INFO: Got endpoints: latency-svc-j6sv4 [2.891416427s]
    Jun 13 03:57:53.177: INFO: Created: latency-svc-f6ffc
    Jun 13 03:57:53.193: INFO: Got endpoints: latency-svc-f6ffc [2.750319205s]
    Jun 13 03:57:53.248: INFO: Created: latency-svc-9jl2r
    Jun 13 03:57:53.299: INFO: Got endpoints: latency-svc-9jl2r [2.676445272s]
    Jun 13 03:57:53.348: INFO: Created: latency-svc-2c8xq
    Jun 13 03:57:53.485: INFO: Got endpoints: latency-svc-2c8xq [2.828526313s]
    Jun 13 03:57:53.501: INFO: Created: latency-svc-qp2mz
    Jun 13 03:57:53.547: INFO: Got endpoints: latency-svc-qp2mz [2.566652509s]
    Jun 13 03:57:53.614: INFO: Created: latency-svc-92hr9
    Jun 13 03:57:53.893: INFO: Got endpoints: latency-svc-92hr9 [2.759251859s]
    Jun 13 03:57:53.893: INFO: Created: latency-svc-bjf2v
    Jun 13 03:57:53.991: INFO: Got endpoints: latency-svc-bjf2v [2.817136655s]
    Jun 13 03:57:54.044: INFO: Created: latency-svc-dg4xj
    Jun 13 03:57:54.167: INFO: Got endpoints: latency-svc-dg4xj [2.654681476s]
    Jun 13 03:57:54.167: INFO: Created: latency-svc-gcbht
    Jun 13 03:57:54.195: INFO: Got endpoints: latency-svc-gcbht [1.943945643s]
    Jun 13 03:57:54.410: INFO: Created: latency-svc-gc9mr
    Jun 13 03:57:54.710: INFO: Got endpoints: latency-svc-gc9mr [2.458510892s]
    Jun 13 03:57:54.909: INFO: Created: latency-svc-xkz2j
    Jun 13 03:57:55.073: INFO: Got endpoints: latency-svc-xkz2j [2.644406954s]
    Jun 13 03:57:55.092: INFO: Created: latency-svc-zvsxd
    Jun 13 03:57:55.149: INFO: Got endpoints: latency-svc-zvsxd [2.539750823s]
    Jun 13 03:57:55.163: INFO: Created: latency-svc-dwk74
    Jun 13 03:57:55.189: INFO: Got endpoints: latency-svc-dwk74 [2.463449678s]
    Jun 13 03:57:55.413: INFO: Created: latency-svc-2r4sd
    Jun 13 03:57:55.433: INFO: Got endpoints: latency-svc-2r4sd [2.5280834s]
    Jun 13 03:57:55.578: INFO: Created: latency-svc-nb4sc
    Jun 13 03:57:55.579: INFO: Got endpoints: latency-svc-nb4sc [2.533486877s]
    Jun 13 03:57:55.809: INFO: Created: latency-svc-s2q4s
    Jun 13 03:57:55.889: INFO: Got endpoints: latency-svc-s2q4s [2.739237733s]
    Jun 13 03:57:55.903: INFO: Created: latency-svc-ndwrk
    Jun 13 03:57:55.988: INFO: Got endpoints: latency-svc-ndwrk [2.79572906s]
    Jun 13 03:57:56.113: INFO: Created: latency-svc-lt428
    Jun 13 03:57:56.113: INFO: Got endpoints: latency-svc-lt428 [2.814391284s]
    Jun 13 03:57:56.256: INFO: Created: latency-svc-n8jjd
    Jun 13 03:57:56.275: INFO: Got endpoints: latency-svc-n8jjd [2.789732977s]
    Jun 13 03:57:56.400: INFO: Created: latency-svc-qz8pf
    Jun 13 03:57:56.400: INFO: Got endpoints: latency-svc-qz8pf [2.852399468s]
    Jun 13 03:57:56.485: INFO: Created: latency-svc-66tmt
    Jun 13 03:57:56.578: INFO: Got endpoints: latency-svc-66tmt [2.684543948s]
    Jun 13 03:57:56.604: INFO: Created: latency-svc-8l6rg
    Jun 13 03:57:56.819: INFO: Got endpoints: latency-svc-8l6rg [2.827233539s]
    Jun 13 03:57:56.901: INFO: Created: latency-svc-5q5md
    Jun 13 03:57:56.969: INFO: Got endpoints: latency-svc-5q5md [2.802133635s]
    Jun 13 03:57:57.073: INFO: Created: latency-svc-7d5hw
    Jun 13 03:57:57.276: INFO: Got endpoints: latency-svc-7d5hw [3.080259993s]
    Jun 13 03:57:57.307: INFO: Created: latency-svc-xdkc4
    Jun 13 03:57:57.321: INFO: Got endpoints: latency-svc-xdkc4 [2.610351276s]
    Jun 13 03:57:57.390: INFO: Created: latency-svc-kd8b5
    Jun 13 03:57:57.417: INFO: Got endpoints: latency-svc-kd8b5 [2.344037902s]
    Jun 13 03:57:57.465: INFO: Created: latency-svc-rh8lc
    Jun 13 03:57:57.565: INFO: Got endpoints: latency-svc-rh8lc [2.416113739s]
    Jun 13 03:57:57.678: INFO: Created: latency-svc-n27dj
    Jun 13 03:57:57.710: INFO: Got endpoints: latency-svc-n27dj [2.521025929s]
    Jun 13 03:57:57.724: INFO: Created: latency-svc-dlw2l
    Jun 13 03:57:57.791: INFO: Got endpoints: latency-svc-dlw2l [2.3578992s]
    Jun 13 03:57:57.858: INFO: Created: latency-svc-cnmx9
    Jun 13 03:57:58.216: INFO: Got endpoints: latency-svc-cnmx9 [2.637212094s]
    Jun 13 03:57:58.509: INFO: Created: latency-svc-jrqg2
    Jun 13 03:57:58.659: INFO: Got endpoints: latency-svc-jrqg2 [2.770072185s]
    Jun 13 03:57:58.809: INFO: Created: latency-svc-cl7sg
    Jun 13 03:57:59.126: INFO: Got endpoints: latency-svc-cl7sg [3.137309118s]
    Jun 13 03:57:59.176: INFO: Created: latency-svc-dv6vb
    Jun 13 03:57:59.216: INFO: Got endpoints: latency-svc-dv6vb [3.102582126s]
    Jun 13 03:57:59.255: INFO: Created: latency-svc-bvtg9
    Jun 13 03:57:59.385: INFO: Got endpoints: latency-svc-bvtg9 [3.109646481s]
    Jun 13 03:57:59.385: INFO: Created: latency-svc-htnqg
    Jun 13 03:57:59.471: INFO: Got endpoints: latency-svc-htnqg [3.071079112s]
    Jun 13 03:57:59.501: INFO: Created: latency-svc-6j6nl
    Jun 13 03:57:59.501: INFO: Got endpoints: latency-svc-6j6nl [2.923367375s]
    Jun 13 03:57:59.635: INFO: Created: latency-svc-9tqmp
    Jun 13 03:57:59.653: INFO: Got endpoints: latency-svc-9tqmp [2.834375313s]
    Jun 13 03:57:59.691: INFO: Created: latency-svc-5dfl6
    Jun 13 03:57:59.711: INFO: Got endpoints: latency-svc-5dfl6 [2.741516963s]
    Jun 13 03:57:59.847: INFO: Created: latency-svc-kt2kl
    Jun 13 03:57:59.893: INFO: Got endpoints: latency-svc-kt2kl [2.617023621s]
    Jun 13 03:57:59.949: INFO: Created: latency-svc-tdcz4
    Jun 13 03:57:59.994: INFO: Got endpoints: latency-svc-tdcz4 [2.673599793s]
    Jun 13 03:58:00.103: INFO: Created: latency-svc-8qnxf
    Jun 13 03:58:00.217: INFO: Got endpoints: latency-svc-8qnxf [2.799367136s]
    Jun 13 03:58:00.289: INFO: Created: latency-svc-4mhj7
    Jun 13 03:58:00.289: INFO: Got endpoints: latency-svc-4mhj7 [2.723603486s]
    Jun 13 03:58:00.381: INFO: Created: latency-svc-j7pkg
    Jun 13 03:58:00.439: INFO: Got endpoints: latency-svc-j7pkg [2.728645669s]
    Jun 13 03:58:00.537: INFO: Created: latency-svc-wpz6l
    Jun 13 03:58:00.649: INFO: Got endpoints: latency-svc-wpz6l [2.857774423s]
    Jun 13 03:58:00.660: INFO: Created: latency-svc-hmdhr
    Jun 13 03:58:00.703: INFO: Got endpoints: latency-svc-hmdhr [2.486999575s]
    Jun 13 03:58:00.931: INFO: Created: latency-svc-2g82v
    Jun 13 03:58:00.957: INFO: Got endpoints: latency-svc-2g82v [2.298652093s]
    Jun 13 03:58:01.087: INFO: Created: latency-svc-8l877
    Jun 13 03:58:01.133: INFO: Got endpoints: latency-svc-8l877 [2.007670902s]
    Jun 13 03:58:01.235: INFO: Created: latency-svc-64lls
    Jun 13 03:58:01.371: INFO: Got endpoints: latency-svc-64lls [2.154811579s]
    Jun 13 03:58:01.442: INFO: Created: latency-svc-4rwmf
    Jun 13 03:58:01.594: INFO: Got endpoints: latency-svc-4rwmf [2.209254414s]
    Jun 13 03:58:01.666: INFO: Created: latency-svc-jdk2f
    Jun 13 03:58:01.939: INFO: Created: latency-svc-gbxp6
    Jun 13 03:58:01.940: INFO: Got endpoints: latency-svc-jdk2f [2.468759157s]
    Jun 13 03:58:02.112: INFO: Got endpoints: latency-svc-gbxp6 [2.610356016s]
    Jun 13 03:58:02.191: INFO: Created: latency-svc-xg6tk
    Jun 13 03:58:02.493: INFO: Got endpoints: latency-svc-xg6tk [2.840443459s]
    Jun 13 03:58:02.496: INFO: Created: latency-svc-gbqjg
    Jun 13 03:58:02.679: INFO: Got endpoints: latency-svc-gbqjg [2.968046542s]
    Jun 13 03:58:02.750: INFO: Created: latency-svc-v668g
    Jun 13 03:58:02.973: INFO: Got endpoints: latency-svc-v668g [3.079750377s]
    Jun 13 03:58:02.978: INFO: Created: latency-svc-s5w86
    Jun 13 03:58:03.029: INFO: Got endpoints: latency-svc-s5w86 [3.035012751s]
    Jun 13 03:58:03.087: INFO: Created: latency-svc-w7bbf
    Jun 13 03:58:03.200: INFO: Got endpoints: latency-svc-w7bbf [2.983102434s]
    Jun 13 03:58:03.272: INFO: Created: latency-svc-494rv
    Jun 13 03:58:03.407: INFO: Got endpoints: latency-svc-494rv [3.118062217s]
    Jun 13 03:58:03.431: INFO: Created: latency-svc-96vrp
    Jun 13 03:58:03.468: INFO: Got endpoints: latency-svc-96vrp [3.029145119s]
    Jun 13 03:58:03.547: INFO: Created: latency-svc-dgv2f
    Jun 13 03:58:03.575: INFO: Got endpoints: latency-svc-dgv2f [2.926226849s]
    Jun 13 03:58:03.670: INFO: Created: latency-svc-2h8q4
    Jun 13 03:58:03.688: INFO: Got endpoints: latency-svc-2h8q4 [2.984979311s]
    Jun 13 03:58:03.948: INFO: Created: latency-svc-zgg82
    Jun 13 03:58:04.106: INFO: Got endpoints: latency-svc-zgg82 [3.148213905s]
    Jun 13 03:58:04.152: INFO: Created: latency-svc-xqhtv
    Jun 13 03:58:04.264: INFO: Got endpoints: latency-svc-xqhtv [3.130927025s]
    Jun 13 03:58:04.425: INFO: Created: latency-svc-dqr64
    Jun 13 03:58:04.565: INFO: Got endpoints: latency-svc-dqr64 [3.194433514s]
    Jun 13 03:58:04.583: INFO: Created: latency-svc-mqhwc
    Jun 13 03:58:04.692: INFO: Got endpoints: latency-svc-mqhwc [3.09744118s]
    Jun 13 03:58:04.711: INFO: Created: latency-svc-2xk2r
    Jun 13 03:58:04.749: INFO: Got endpoints: latency-svc-2xk2r [2.808989465s]
    Jun 13 03:58:04.749: INFO: Latencies: [361.470526ms 561.497194ms 778.377317ms 1.333749349s 1.54126576s 1.777053615s 1.871623165s 1.943945643s 1.950095373s 1.97543297s 2.007670902s 2.016426779s 2.040627663s 2.050578797s 2.061015367s 2.078222079s 2.081572792s 2.098628806s 2.103064508s 2.117597657s 2.120161281s 2.142519669s 2.145130985s 2.149723917s 2.15057728s 2.151845168s 2.154811579s 2.168873245s 2.191744532s 2.193967694s 2.209254414s 2.241164306s 2.278834826s 2.279486478s 2.298652093s 2.29977135s 2.336574194s 2.338244824s 2.344037902s 2.3578992s 2.366469657s 2.370227119s 2.385842745s 2.40783227s 2.410531669s 2.416113739s 2.425420244s 2.451035329s 2.458510892s 2.463449678s 2.468759157s 2.486999575s 2.515430434s 2.521025929s 2.5236138s 2.5280834s 2.533486877s 2.539750823s 2.539978751s 2.550457235s 2.566652509s 2.59364163s 2.610351276s 2.610356016s 2.617023621s 2.619555191s 2.637212094s 2.641237567s 2.644406954s 2.648137641s 2.654681476s 2.673599793s 2.675262655s 2.676445272s 2.684543948s 2.700725746s 2.720035856s 2.723603486s 2.728645669s 2.739237733s 2.741516963s 2.750319205s 2.751871033s 2.759251859s 2.764633894s 2.770072185s 2.776507018s 2.776552416s 2.777914036s 2.789732977s 2.794593261s 2.79572906s 2.799367136s 2.799980197s 2.802133635s 2.808989465s 2.814391284s 2.816046889s 2.817136655s 2.827233539s 2.828526313s 2.834375313s 2.840443459s 2.845156759s 2.847021418s 2.849398313s 2.851912057s 2.852399468s 2.857774423s 2.865854587s 2.873575201s 2.891416427s 2.893107017s 2.903492741s 2.915144806s 2.923367375s 2.926226849s 2.937945123s 2.94432813s 2.946847237s 2.947468888s 2.954971801s 2.968046542s 2.973045108s 2.980441407s 2.983102434s 2.984979311s 2.987578598s 3.003717244s 3.011513519s 3.014641837s 3.016934083s 3.020453598s 3.025273304s 3.029145119s 3.029641969s 3.032071552s 3.032436896s 3.035012751s 3.045555392s 3.053161731s 3.069273485s 3.071079112s 3.072963306s 3.079750377s 3.080259993s 3.09744118s 3.102582126s 3.103409218s 3.104023459s 3.109646481s 3.11094263s 3.118062217s 3.126016356s 3.130927025s 3.132591771s 3.137309118s 3.140412425s 3.148213905s 3.159288709s 3.16923782s 3.170157021s 3.180208591s 3.18691971s 3.194433514s 3.207850682s 3.210853474s 3.213011984s 3.220551109s 3.229262021s 3.237913317s 3.238608578s 3.259501201s 3.263851426s 3.294644701s 3.303324031s 3.336020098s 3.369229359s 3.378232984s 3.381420789s 3.410664124s 3.429722995s 3.496726619s 3.504852291s 3.57727844s 3.581187077s 3.644813238s 3.699308994s 3.700456488s 3.742831802s 3.781258711s 3.817720045s 3.818457434s 3.890271775s 3.948974653s 3.954166934s 4.051557207s 4.193445492s 4.225285926s 4.404698355s]
    Jun 13 03:58:04.749: INFO: 50 %ile: 2.828526313s
    Jun 13 03:58:04.749: INFO: 90 %ile: 3.410664124s
    Jun 13 03:58:04.749: INFO: 99 %ile: 4.225285926s
    Jun 13 03:58:04.749: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jun 13 03:58:04.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-7882" for this suite. 06/13/23 03:58:04.842
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:58:04.906
Jun 13 03:58:04.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename init-container 06/13/23 03:58:04.91
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:58:04.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:58:04.986
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 06/13/23 03:58:05.031
Jun 13 03:58:05.031: INFO: PodSpec: initContainers in spec.initContainers
Jun 13 03:58:50.172: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-64546b10-bd3c-463f-bc82-ce657decec01", GenerateName:"", Namespace:"init-container-5733", SelfLink:"", UID:"0b1f073a-264c-4313-bb31-af62c0ebbe4f", ResourceVersion:"56433", Generation:0, CreationTimestamp:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"31442833"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"a38bb2f38175169c05b01c03089224f728c7d1bef1735aca41a816118ec146b7", "cni.projectcalico.org/podIP":"172.30.77.178/32", "cni.projectcalico.org/podIPs":"172.30.77.178/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003794150), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 13, 3, 58, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003794198), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 13, 3, 58, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037941c8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-lrclk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc008796960), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lrclk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lrclk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lrclk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc007241ba8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"sks-test-v1-25-9-workergroup-469fm", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000c4ae00), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007241c70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007241c90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc007241c98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc007241c9c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003f969f0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.255.64.102", PodIP:"172.30.77.178", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.77.178"}}, StartTime:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc003794210), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c4aee0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://0d30a484a39bc4a533e91aba45cde4b5e5979ee61f85e61d3565038ba50ae389", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0087969e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0087969c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc007241d14)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 13 03:58:50.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5733" for this suite. 06/13/23 03:58:50.195
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":353,"skipped":6576,"failed":0}
------------------------------
• [SLOW TEST] [45.314 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:58:04.906
    Jun 13 03:58:04.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename init-container 06/13/23 03:58:04.91
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:58:04.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:58:04.986
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 06/13/23 03:58:05.031
    Jun 13 03:58:05.031: INFO: PodSpec: initContainers in spec.initContainers
    Jun 13 03:58:50.172: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-64546b10-bd3c-463f-bc82-ce657decec01", GenerateName:"", Namespace:"init-container-5733", SelfLink:"", UID:"0b1f073a-264c-4313-bb31-af62c0ebbe4f", ResourceVersion:"56433", Generation:0, CreationTimestamp:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"31442833"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"a38bb2f38175169c05b01c03089224f728c7d1bef1735aca41a816118ec146b7", "cni.projectcalico.org/podIP":"172.30.77.178/32", "cni.projectcalico.org/podIPs":"172.30.77.178/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003794150), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 13, 3, 58, 6, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc003794198), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 13, 3, 58, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0037941c8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-lrclk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc008796960), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lrclk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lrclk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-lrclk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc007241ba8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"sks-test-v1-25-9-workergroup-469fm", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000c4ae00), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007241c70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc007241c90)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc007241c98), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc007241c9c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc003f969f0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.255.64.102", PodIP:"172.30.77.178", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.77.178"}}, StartTime:time.Date(2023, time.June, 13, 3, 58, 5, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc003794210), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c4aee0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://0d30a484a39bc4a533e91aba45cde4b5e5979ee61f85e61d3565038ba50ae389", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0087969e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0087969c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc007241d14)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 13 03:58:50.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5733" for this suite. 06/13/23 03:58:50.195
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:58:50.221
Jun 13 03:58:50.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 03:58:50.223
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:58:50.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:58:50.282
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jun 13 03:58:50.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/13/23 03:58:57.789
Jun 13 03:58:57.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-8545 --namespace=crd-publish-openapi-8545 create -f -'
Jun 13 03:58:59.018: INFO: stderr: ""
Jun 13 03:58:59.018: INFO: stdout: "e2e-test-crd-publish-openapi-9483-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 13 03:58:59.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-8545 --namespace=crd-publish-openapi-8545 delete e2e-test-crd-publish-openapi-9483-crds test-cr'
Jun 13 03:58:59.151: INFO: stderr: ""
Jun 13 03:58:59.151: INFO: stdout: "e2e-test-crd-publish-openapi-9483-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 13 03:58:59.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-8545 --namespace=crd-publish-openapi-8545 apply -f -'
Jun 13 03:58:59.741: INFO: stderr: ""
Jun 13 03:58:59.741: INFO: stdout: "e2e-test-crd-publish-openapi-9483-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 13 03:58:59.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-8545 --namespace=crd-publish-openapi-8545 delete e2e-test-crd-publish-openapi-9483-crds test-cr'
Jun 13 03:59:00.035: INFO: stderr: ""
Jun 13 03:59:00.035: INFO: stdout: "e2e-test-crd-publish-openapi-9483-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 06/13/23 03:59:00.035
Jun 13 03:59:00.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-8545 explain e2e-test-crd-publish-openapi-9483-crds'
Jun 13 03:59:01.085: INFO: stderr: ""
Jun 13 03:59:01.085: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9483-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:59:07.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8545" for this suite. 06/13/23 03:59:07.518
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":354,"skipped":6578,"failed":0}
------------------------------
• [SLOW TEST] [17.313 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:58:50.221
    Jun 13 03:58:50.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename crd-publish-openapi 06/13/23 03:58:50.223
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:58:50.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:58:50.282
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jun 13 03:58:50.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/13/23 03:58:57.789
    Jun 13 03:58:57.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-8545 --namespace=crd-publish-openapi-8545 create -f -'
    Jun 13 03:58:59.018: INFO: stderr: ""
    Jun 13 03:58:59.018: INFO: stdout: "e2e-test-crd-publish-openapi-9483-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jun 13 03:58:59.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-8545 --namespace=crd-publish-openapi-8545 delete e2e-test-crd-publish-openapi-9483-crds test-cr'
    Jun 13 03:58:59.151: INFO: stderr: ""
    Jun 13 03:58:59.151: INFO: stdout: "e2e-test-crd-publish-openapi-9483-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jun 13 03:58:59.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-8545 --namespace=crd-publish-openapi-8545 apply -f -'
    Jun 13 03:58:59.741: INFO: stderr: ""
    Jun 13 03:58:59.741: INFO: stdout: "e2e-test-crd-publish-openapi-9483-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jun 13 03:58:59.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-8545 --namespace=crd-publish-openapi-8545 delete e2e-test-crd-publish-openapi-9483-crds test-cr'
    Jun 13 03:59:00.035: INFO: stderr: ""
    Jun 13 03:59:00.035: INFO: stdout: "e2e-test-crd-publish-openapi-9483-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 06/13/23 03:59:00.035
    Jun 13 03:59:00.035: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=crd-publish-openapi-8545 explain e2e-test-crd-publish-openapi-9483-crds'
    Jun 13 03:59:01.085: INFO: stderr: ""
    Jun 13 03:59:01.085: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9483-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:59:07.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8545" for this suite. 06/13/23 03:59:07.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:59:07.536
Jun 13 03:59:07.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename webhook 06/13/23 03:59:07.538
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:59:07.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:59:07.599
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/13/23 03:59:07.638
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:59:08.126
STEP: Deploying the webhook pod 06/13/23 03:59:08.146
STEP: Wait for the deployment to be ready 06/13/23 03:59:08.177
Jun 13 03:59:08.212: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/13/23 03:59:10.243
STEP: Verifying the service has paired with the endpoint 06/13/23 03:59:10.282
Jun 13 03:59:11.283: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 06/13/23 03:59:11.293
STEP: Creating a custom resource definition that should be denied by the webhook 06/13/23 03:59:11.325
Jun 13 03:59:11.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:59:11.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7070" for this suite. 06/13/23 03:59:11.389
STEP: Destroying namespace "webhook-7070-markers" for this suite. 06/13/23 03:59:11.408
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":355,"skipped":6607,"failed":0}
------------------------------
• [4.143 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:59:07.536
    Jun 13 03:59:07.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename webhook 06/13/23 03:59:07.538
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:59:07.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:59:07.599
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/13/23 03:59:07.638
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/13/23 03:59:08.126
    STEP: Deploying the webhook pod 06/13/23 03:59:08.146
    STEP: Wait for the deployment to be ready 06/13/23 03:59:08.177
    Jun 13 03:59:08.212: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/13/23 03:59:10.243
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:59:10.282
    Jun 13 03:59:11.283: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 06/13/23 03:59:11.293
    STEP: Creating a custom resource definition that should be denied by the webhook 06/13/23 03:59:11.325
    Jun 13 03:59:11.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:59:11.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7070" for this suite. 06/13/23 03:59:11.389
    STEP: Destroying namespace "webhook-7070-markers" for this suite. 06/13/23 03:59:11.408
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:59:11.68
Jun 13 03:59:11.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename events 06/13/23 03:59:11.682
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:59:11.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:59:11.76
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 06/13/23 03:59:11.773
STEP: listing all events in all namespaces 06/13/23 03:59:11.813
STEP: patching the test event 06/13/23 03:59:11.829
STEP: fetching the test event 06/13/23 03:59:11.862
STEP: updating the test event 06/13/23 03:59:11.876
STEP: getting the test event 06/13/23 03:59:11.912
STEP: deleting the test event 06/13/23 03:59:11.926
STEP: listing all events in all namespaces 06/13/23 03:59:11.957
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jun 13 03:59:11.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6385" for this suite. 06/13/23 03:59:11.985
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":356,"skipped":6625,"failed":0}
------------------------------
• [0.334 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:59:11.68
    Jun 13 03:59:11.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename events 06/13/23 03:59:11.682
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:59:11.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:59:11.76
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 06/13/23 03:59:11.773
    STEP: listing all events in all namespaces 06/13/23 03:59:11.813
    STEP: patching the test event 06/13/23 03:59:11.829
    STEP: fetching the test event 06/13/23 03:59:11.862
    STEP: updating the test event 06/13/23 03:59:11.876
    STEP: getting the test event 06/13/23 03:59:11.912
    STEP: deleting the test event 06/13/23 03:59:11.926
    STEP: listing all events in all namespaces 06/13/23 03:59:11.957
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jun 13 03:59:11.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6385" for this suite. 06/13/23 03:59:11.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:59:12.017
Jun 13 03:59:12.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename services 06/13/23 03:59:12.018
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:59:12.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:59:12.063
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-1104 06/13/23 03:59:12.069
STEP: creating replication controller nodeport-test in namespace services-1104 06/13/23 03:59:12.155
I0613 03:59:12.177473      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1104, replica count: 2
I0613 03:59:15.229359      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 13 03:59:15.229: INFO: Creating new exec pod
Jun 13 03:59:15.258: INFO: Waiting up to 5m0s for pod "execpodrvmsr" in namespace "services-1104" to be "running"
Jun 13 03:59:15.279: INFO: Pod "execpodrvmsr": Phase="Pending", Reason="", readiness=false. Elapsed: 20.633948ms
Jun 13 03:59:17.291: INFO: Pod "execpodrvmsr": Phase="Running", Reason="", readiness=true. Elapsed: 2.032634068s
Jun 13 03:59:17.291: INFO: Pod "execpodrvmsr" satisfied condition "running"
Jun 13 03:59:18.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 13 03:59:18.573: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 13 03:59:18.573: INFO: stdout: ""
Jun 13 03:59:19.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 13 03:59:19.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 13 03:59:19.860: INFO: stdout: ""
Jun 13 03:59:20.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 13 03:59:20.785: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 13 03:59:20.785: INFO: stdout: ""
Jun 13 03:59:21.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 13 03:59:21.824: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 13 03:59:21.825: INFO: stdout: ""
Jun 13 03:59:22.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 13 03:59:22.801: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 13 03:59:22.801: INFO: stdout: "nodeport-test-pm7sv"
Jun 13 03:59:22.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.176.90 80'
Jun 13 03:59:22.996: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.176.90 80\nConnection to 10.110.176.90 80 port [tcp/http] succeeded!\n"
Jun 13 03:59:22.996: INFO: stdout: ""
Jun 13 03:59:23.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.176.90 80'
Jun 13 03:59:24.215: INFO: stderr: "+ nc -v -t -w 2 10.110.176.90 80\nConnection to 10.110.176.90 80 port [tcp/http] succeeded!\n+ echo hostName\n"
Jun 13 03:59:24.215: INFO: stdout: "nodeport-test-dmng4"
Jun 13 03:59:24.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.103 30751'
Jun 13 03:59:24.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.103 30751\nConnection to 10.255.64.103 30751 port [tcp/*] succeeded!\n"
Jun 13 03:59:24.461: INFO: stdout: ""
Jun 13 03:59:25.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.103 30751'
Jun 13 03:59:25.745: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.103 30751\nConnection to 10.255.64.103 30751 port [tcp/*] succeeded!\n"
Jun 13 03:59:25.745: INFO: stdout: ""
Jun 13 03:59:26.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.103 30751'
Jun 13 03:59:26.652: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.103 30751\nConnection to 10.255.64.103 30751 port [tcp/*] succeeded!\n"
Jun 13 03:59:26.652: INFO: stdout: "nodeport-test-dmng4"
Jun 13 03:59:26.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.104 30751'
Jun 13 03:59:26.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.104 30751\nConnection to 10.255.64.104 30751 port [tcp/*] succeeded!\n"
Jun 13 03:59:26.891: INFO: stdout: "nodeport-test-dmng4"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 13 03:59:26.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1104" for this suite. 06/13/23 03:59:26.904
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":357,"skipped":6641,"failed":0}
------------------------------
• [SLOW TEST] [14.904 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:59:12.017
    Jun 13 03:59:12.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename services 06/13/23 03:59:12.018
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:59:12.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:59:12.063
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-1104 06/13/23 03:59:12.069
    STEP: creating replication controller nodeport-test in namespace services-1104 06/13/23 03:59:12.155
    I0613 03:59:12.177473      18 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1104, replica count: 2
    I0613 03:59:15.229359      18 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 13 03:59:15.229: INFO: Creating new exec pod
    Jun 13 03:59:15.258: INFO: Waiting up to 5m0s for pod "execpodrvmsr" in namespace "services-1104" to be "running"
    Jun 13 03:59:15.279: INFO: Pod "execpodrvmsr": Phase="Pending", Reason="", readiness=false. Elapsed: 20.633948ms
    Jun 13 03:59:17.291: INFO: Pod "execpodrvmsr": Phase="Running", Reason="", readiness=true. Elapsed: 2.032634068s
    Jun 13 03:59:17.291: INFO: Pod "execpodrvmsr" satisfied condition "running"
    Jun 13 03:59:18.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 13 03:59:18.573: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 13 03:59:18.573: INFO: stdout: ""
    Jun 13 03:59:19.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 13 03:59:19.860: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 13 03:59:19.860: INFO: stdout: ""
    Jun 13 03:59:20.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 13 03:59:20.785: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 13 03:59:20.785: INFO: stdout: ""
    Jun 13 03:59:21.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 13 03:59:21.824: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 13 03:59:21.825: INFO: stdout: ""
    Jun 13 03:59:22.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 13 03:59:22.801: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 13 03:59:22.801: INFO: stdout: "nodeport-test-pm7sv"
    Jun 13 03:59:22.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.176.90 80'
    Jun 13 03:59:22.996: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.110.176.90 80\nConnection to 10.110.176.90 80 port [tcp/http] succeeded!\n"
    Jun 13 03:59:22.996: INFO: stdout: ""
    Jun 13 03:59:23.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.110.176.90 80'
    Jun 13 03:59:24.215: INFO: stderr: "+ nc -v -t -w 2 10.110.176.90 80\nConnection to 10.110.176.90 80 port [tcp/http] succeeded!\n+ echo hostName\n"
    Jun 13 03:59:24.215: INFO: stdout: "nodeport-test-dmng4"
    Jun 13 03:59:24.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.103 30751'
    Jun 13 03:59:24.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.103 30751\nConnection to 10.255.64.103 30751 port [tcp/*] succeeded!\n"
    Jun 13 03:59:24.461: INFO: stdout: ""
    Jun 13 03:59:25.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.103 30751'
    Jun 13 03:59:25.745: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.103 30751\nConnection to 10.255.64.103 30751 port [tcp/*] succeeded!\n"
    Jun 13 03:59:25.745: INFO: stdout: ""
    Jun 13 03:59:26.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.103 30751'
    Jun 13 03:59:26.652: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.103 30751\nConnection to 10.255.64.103 30751 port [tcp/*] succeeded!\n"
    Jun 13 03:59:26.652: INFO: stdout: "nodeport-test-dmng4"
    Jun 13 03:59:26.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2279107438 --namespace=services-1104 exec execpodrvmsr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.255.64.104 30751'
    Jun 13 03:59:26.891: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.255.64.104 30751\nConnection to 10.255.64.104 30751 port [tcp/*] succeeded!\n"
    Jun 13 03:59:26.891: INFO: stdout: "nodeport-test-dmng4"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 13 03:59:26.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1104" for this suite. 06/13/23 03:59:26.904
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:59:26.922
Jun 13 03:59:26.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename emptydir 06/13/23 03:59:26.923
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:59:26.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:59:26.968
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 06/13/23 03:59:26.975
Jun 13 03:59:27.006: INFO: Waiting up to 5m0s for pod "pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86" in namespace "emptydir-6929" to be "Succeeded or Failed"
Jun 13 03:59:27.020: INFO: Pod "pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86": Phase="Pending", Reason="", readiness=false. Elapsed: 13.721769ms
Jun 13 03:59:29.029: INFO: Pod "pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022799427s
Jun 13 03:59:31.032: INFO: Pod "pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025741179s
STEP: Saw pod success 06/13/23 03:59:31.032
Jun 13 03:59:31.032: INFO: Pod "pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86" satisfied condition "Succeeded or Failed"
Jun 13 03:59:31.040: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86 container test-container: <nil>
STEP: delete the pod 06/13/23 03:59:31.072
Jun 13 03:59:31.100: INFO: Waiting for pod pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86 to disappear
Jun 13 03:59:31.108: INFO: Pod pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 13 03:59:31.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6929" for this suite. 06/13/23 03:59:31.121
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":358,"skipped":6662,"failed":0}
------------------------------
• [4.219 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:59:26.922
    Jun 13 03:59:26.922: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename emptydir 06/13/23 03:59:26.923
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:59:26.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:59:26.968
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 06/13/23 03:59:26.975
    Jun 13 03:59:27.006: INFO: Waiting up to 5m0s for pod "pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86" in namespace "emptydir-6929" to be "Succeeded or Failed"
    Jun 13 03:59:27.020: INFO: Pod "pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86": Phase="Pending", Reason="", readiness=false. Elapsed: 13.721769ms
    Jun 13 03:59:29.029: INFO: Pod "pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022799427s
    Jun 13 03:59:31.032: INFO: Pod "pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025741179s
    STEP: Saw pod success 06/13/23 03:59:31.032
    Jun 13 03:59:31.032: INFO: Pod "pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86" satisfied condition "Succeeded or Failed"
    Jun 13 03:59:31.040: INFO: Trying to get logs from node sks-test-v1-25-9-workergroup-2q6k2 pod pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86 container test-container: <nil>
    STEP: delete the pod 06/13/23 03:59:31.072
    Jun 13 03:59:31.100: INFO: Waiting for pod pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86 to disappear
    Jun 13 03:59:31.108: INFO: Pod pod-a0e201ee-b5ed-456f-83c7-1163bd7f3b86 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 13 03:59:31.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6929" for this suite. 06/13/23 03:59:31.121
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:59:31.141
Jun 13 03:59:31.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename disruption 06/13/23 03:59:31.142
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:59:31.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:59:31.181
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 06/13/23 03:59:31.188
STEP: Waiting for the pdb to be processed 06/13/23 03:59:31.201
STEP: First trying to evict a pod which shouldn't be evictable 06/13/23 03:59:33.276
STEP: Waiting for all pods to be running 06/13/23 03:59:33.276
Jun 13 03:59:33.286: INFO: pods: 0 < 3
Jun 13 03:59:35.296: INFO: running pods: 1 < 3
STEP: locating a running pod 06/13/23 03:59:37.296
STEP: Updating the pdb to allow a pod to be evicted 06/13/23 03:59:37.317
STEP: Waiting for the pdb to be processed 06/13/23 03:59:37.334
STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/13/23 03:59:39.365
STEP: Waiting for all pods to be running 06/13/23 03:59:39.365
STEP: Waiting for the pdb to observed all healthy pods 06/13/23 03:59:39.374
STEP: Patching the pdb to disallow a pod to be evicted 06/13/23 03:59:39.453
STEP: Waiting for the pdb to be processed 06/13/23 03:59:39.495
STEP: Waiting for all pods to be running 06/13/23 03:59:39.511
Jun 13 03:59:39.522: INFO: running pods: 2 < 3
Jun 13 03:59:41.532: INFO: running pods: 2 < 3
STEP: locating a running pod 06/13/23 03:59:43.533
STEP: Deleting the pdb to allow a pod to be evicted 06/13/23 03:59:43.56
STEP: Waiting for the pdb to be deleted 06/13/23 03:59:43.589
STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/13/23 03:59:43.607
STEP: Waiting for all pods to be running 06/13/23 03:59:43.607
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 13 03:59:43.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-3782" for this suite. 06/13/23 03:59:43.687
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":359,"skipped":6665,"failed":0}
------------------------------
• [SLOW TEST] [12.635 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:59:31.141
    Jun 13 03:59:31.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename disruption 06/13/23 03:59:31.142
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:59:31.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:59:31.181
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 06/13/23 03:59:31.188
    STEP: Waiting for the pdb to be processed 06/13/23 03:59:31.201
    STEP: First trying to evict a pod which shouldn't be evictable 06/13/23 03:59:33.276
    STEP: Waiting for all pods to be running 06/13/23 03:59:33.276
    Jun 13 03:59:33.286: INFO: pods: 0 < 3
    Jun 13 03:59:35.296: INFO: running pods: 1 < 3
    STEP: locating a running pod 06/13/23 03:59:37.296
    STEP: Updating the pdb to allow a pod to be evicted 06/13/23 03:59:37.317
    STEP: Waiting for the pdb to be processed 06/13/23 03:59:37.334
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/13/23 03:59:39.365
    STEP: Waiting for all pods to be running 06/13/23 03:59:39.365
    STEP: Waiting for the pdb to observed all healthy pods 06/13/23 03:59:39.374
    STEP: Patching the pdb to disallow a pod to be evicted 06/13/23 03:59:39.453
    STEP: Waiting for the pdb to be processed 06/13/23 03:59:39.495
    STEP: Waiting for all pods to be running 06/13/23 03:59:39.511
    Jun 13 03:59:39.522: INFO: running pods: 2 < 3
    Jun 13 03:59:41.532: INFO: running pods: 2 < 3
    STEP: locating a running pod 06/13/23 03:59:43.533
    STEP: Deleting the pdb to allow a pod to be evicted 06/13/23 03:59:43.56
    STEP: Waiting for the pdb to be deleted 06/13/23 03:59:43.589
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/13/23 03:59:43.607
    STEP: Waiting for all pods to be running 06/13/23 03:59:43.607
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 13 03:59:43.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-3782" for this suite. 06/13/23 03:59:43.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/13/23 03:59:43.778
Jun 13 03:59:43.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Building a namespace api object, basename crd-webhook 06/13/23 03:59:43.78
STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:59:43.846
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:59:43.853
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 06/13/23 03:59:43.861
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/13/23 03:59:44.562
STEP: Deploying the custom resource conversion webhook pod 06/13/23 03:59:44.591
STEP: Wait for the deployment to be ready 06/13/23 03:59:44.629
Jun 13 03:59:44.663: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 13 03:59:46.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 59, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 59, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 59, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 59, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/13/23 03:59:48.712
STEP: Verifying the service has paired with the endpoint 06/13/23 03:59:48.758
Jun 13 03:59:49.758: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jun 13 03:59:49.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
STEP: Creating a v1 custom resource 06/13/23 03:59:52.445
STEP: Create a v2 custom resource 06/13/23 03:59:52.487
STEP: List CRs in v1 06/13/23 03:59:52.581
STEP: List CRs in v2 06/13/23 03:59:52.603
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 13 03:59:53.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9819" for this suite. 06/13/23 03:59:53.223
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":360,"skipped":6694,"failed":0}
------------------------------
• [SLOW TEST] [9.684 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/13/23 03:59:43.778
    Jun 13 03:59:43.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Building a namespace api object, basename crd-webhook 06/13/23 03:59:43.78
    STEP: Waiting for a default service account to be provisioned in namespace 06/13/23 03:59:43.846
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/13/23 03:59:43.853
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 06/13/23 03:59:43.861
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/13/23 03:59:44.562
    STEP: Deploying the custom resource conversion webhook pod 06/13/23 03:59:44.591
    STEP: Wait for the deployment to be ready 06/13/23 03:59:44.629
    Jun 13 03:59:44.663: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jun 13 03:59:46.694: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 13, 3, 59, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 59, 44, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 13, 3, 59, 44, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 13, 3, 59, 44, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/13/23 03:59:48.712
    STEP: Verifying the service has paired with the endpoint 06/13/23 03:59:48.758
    Jun 13 03:59:49.758: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jun 13 03:59:49.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2279107438
    STEP: Creating a v1 custom resource 06/13/23 03:59:52.445
    STEP: Create a v2 custom resource 06/13/23 03:59:52.487
    STEP: List CRs in v1 06/13/23 03:59:52.581
    STEP: List CRs in v2 06/13/23 03:59:52.603
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 13 03:59:53.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-9819" for this suite. 06/13/23 03:59:53.223
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":360,"skipped":6706,"failed":0}
Jun 13 03:59:53.464: INFO: Running AfterSuite actions on all nodes
Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jun 13 03:59:53.464: INFO: Running AfterSuite actions on node 1
Jun 13 03:59:53.464: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jun 13 03:59:53.464: INFO: Running AfterSuite actions on all nodes
    Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jun 13 03:59:53.464: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jun 13 03:59:53.464: INFO: Running AfterSuite actions on node 1
    Jun 13 03:59:53.464: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.169 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 360 of 7066 Specs in 6271.276 seconds
SUCCESS! -- 360 Passed | 0 Failed | 0 Pending | 6706 Skipped
PASS

Ginkgo ran 1 suite in 1h44m31.762352302s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

