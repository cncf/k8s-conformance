I0119 20:29:35.729263      22 e2e.go:116] Starting e2e run "f28d6c8f-a821-4fde-a3c3-79708116053c" on Ginkgo node 1
Jan 19 20:29:35.740: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1674160175 - will randomize all specs

Will run 362 of 7067 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jan 19 20:29:35.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
E0119 20:29:35.818801      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
E0119 20:29:35.818801      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Jan 19 20:29:35.819: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan 19 20:29:35.842: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan 19 20:29:35.851: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan 19 20:29:35.851: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
Jan 19 20:29:35.851: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan 19 20:29:35.853: INFO: e2e test version: v1.25.4
Jan 19 20:29:35.854: INFO: kube-apiserver version: v1.25.4+77bec7a
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jan 19 20:29:35.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:29:35.857: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.040 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 19 20:29:35.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    E0119 20:29:35.818801      22 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Jan 19 20:29:35.819: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan 19 20:29:35.842: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan 19 20:29:35.851: INFO: 0 / 0 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan 19 20:29:35.851: INFO: expected 0 pod replicas in namespace 'kube-system', 0 are Running and Ready.
    Jan 19 20:29:35.851: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan 19 20:29:35.853: INFO: e2e test version: v1.25.4
    Jan 19 20:29:35.854: INFO: kube-apiserver version: v1.25.4+77bec7a
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jan 19 20:29:35.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:29:35.857: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:29:35.875
Jan 19 20:29:35.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 20:29:35.876
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:29:35.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:29:35.891
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jan 19 20:29:35.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/19/23 20:29:44.742
Jan 19 20:29:44.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-1205 --namespace=crd-publish-openapi-1205 create -f -'
Jan 19 20:29:46.218: INFO: stderr: ""
Jan 19 20:29:46.219: INFO: stdout: "e2e-test-crd-publish-openapi-7907-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 19 20:29:46.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-1205 --namespace=crd-publish-openapi-1205 delete e2e-test-crd-publish-openapi-7907-crds test-cr'
Jan 19 20:29:46.306: INFO: stderr: ""
Jan 19 20:29:46.306: INFO: stdout: "e2e-test-crd-publish-openapi-7907-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan 19 20:29:46.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-1205 --namespace=crd-publish-openapi-1205 apply -f -'
Jan 19 20:29:47.780: INFO: stderr: ""
Jan 19 20:29:47.780: INFO: stdout: "e2e-test-crd-publish-openapi-7907-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan 19 20:29:47.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-1205 --namespace=crd-publish-openapi-1205 delete e2e-test-crd-publish-openapi-7907-crds test-cr'
Jan 19 20:29:47.840: INFO: stderr: ""
Jan 19 20:29:47.841: INFO: stdout: "e2e-test-crd-publish-openapi-7907-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/19/23 20:29:47.841
Jan 19 20:29:47.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-1205 explain e2e-test-crd-publish-openapi-7907-crds'
Jan 19 20:29:48.221: INFO: stderr: ""
Jan 19 20:29:48.221: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7907-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 20:29:57.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1205" for this suite. 01/19/23 20:29:57.119
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":1,"skipped":6,"failed":0}
------------------------------
• [SLOW TEST] [21.249 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:29:35.875
    Jan 19 20:29:35.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 20:29:35.876
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:29:35.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:29:35.891
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jan 19 20:29:35.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/19/23 20:29:44.742
    Jan 19 20:29:44.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-1205 --namespace=crd-publish-openapi-1205 create -f -'
    Jan 19 20:29:46.218: INFO: stderr: ""
    Jan 19 20:29:46.219: INFO: stdout: "e2e-test-crd-publish-openapi-7907-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 19 20:29:46.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-1205 --namespace=crd-publish-openapi-1205 delete e2e-test-crd-publish-openapi-7907-crds test-cr'
    Jan 19 20:29:46.306: INFO: stderr: ""
    Jan 19 20:29:46.306: INFO: stdout: "e2e-test-crd-publish-openapi-7907-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan 19 20:29:46.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-1205 --namespace=crd-publish-openapi-1205 apply -f -'
    Jan 19 20:29:47.780: INFO: stderr: ""
    Jan 19 20:29:47.780: INFO: stdout: "e2e-test-crd-publish-openapi-7907-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan 19 20:29:47.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-1205 --namespace=crd-publish-openapi-1205 delete e2e-test-crd-publish-openapi-7907-crds test-cr'
    Jan 19 20:29:47.840: INFO: stderr: ""
    Jan 19 20:29:47.841: INFO: stdout: "e2e-test-crd-publish-openapi-7907-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/19/23 20:29:47.841
    Jan 19 20:29:47.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-1205 explain e2e-test-crd-publish-openapi-7907-crds'
    Jan 19 20:29:48.221: INFO: stderr: ""
    Jan 19 20:29:48.221: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7907-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 20:29:57.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1205" for this suite. 01/19/23 20:29:57.119
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:29:57.125
Jan 19 20:29:57.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename lease-test 01/19/23 20:29:57.126
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:29:57.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:29:57.146
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jan 19 20:29:57.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-1572" for this suite. 01/19/23 20:29:57.281
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":2,"skipped":12,"failed":0}
------------------------------
• [0.165 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:29:57.125
    Jan 19 20:29:57.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename lease-test 01/19/23 20:29:57.126
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:29:57.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:29:57.146
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jan 19 20:29:57.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-1572" for this suite. 01/19/23 20:29:57.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:29:57.292
Jan 19 20:29:57.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename var-expansion 01/19/23 20:29:57.293
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:29:57.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:29:57.325
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 01/19/23 20:29:57.328
Jan 19 20:29:57.375: INFO: Waiting up to 5m0s for pod "var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23" in namespace "var-expansion-65" to be "Succeeded or Failed"
Jan 19 20:29:57.378: INFO: Pod "var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.594038ms
Jan 19 20:29:59.382: INFO: Pod "var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006713887s
Jan 19 20:30:01.405: INFO: Pod "var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029834855s
Jan 19 20:30:03.382: INFO: Pod "var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006289095s
STEP: Saw pod success 01/19/23 20:30:03.382
Jan 19 20:30:03.382: INFO: Pod "var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23" satisfied condition "Succeeded or Failed"
Jan 19 20:30:03.395: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23 container dapi-container: <nil>
STEP: delete the pod 01/19/23 20:30:03.407
Jan 19 20:30:03.419: INFO: Waiting for pod var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23 to disappear
Jan 19 20:30:03.421: INFO: Pod var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 20:30:03.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-65" for this suite. 01/19/23 20:30:03.431
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":3,"skipped":53,"failed":0}
------------------------------
• [SLOW TEST] [6.145 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:29:57.292
    Jan 19 20:29:57.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename var-expansion 01/19/23 20:29:57.293
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:29:57.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:29:57.325
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 01/19/23 20:29:57.328
    Jan 19 20:29:57.375: INFO: Waiting up to 5m0s for pod "var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23" in namespace "var-expansion-65" to be "Succeeded or Failed"
    Jan 19 20:29:57.378: INFO: Pod "var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.594038ms
    Jan 19 20:29:59.382: INFO: Pod "var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006713887s
    Jan 19 20:30:01.405: INFO: Pod "var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029834855s
    Jan 19 20:30:03.382: INFO: Pod "var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006289095s
    STEP: Saw pod success 01/19/23 20:30:03.382
    Jan 19 20:30:03.382: INFO: Pod "var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23" satisfied condition "Succeeded or Failed"
    Jan 19 20:30:03.395: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23 container dapi-container: <nil>
    STEP: delete the pod 01/19/23 20:30:03.407
    Jan 19 20:30:03.419: INFO: Waiting for pod var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23 to disappear
    Jan 19 20:30:03.421: INFO: Pod var-expansion-171e84ec-96e3-4004-8937-7cef813e7a23 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 20:30:03.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-65" for this suite. 01/19/23 20:30:03.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:30:03.438
Jan 19 20:30:03.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename tables 01/19/23 20:30:03.439
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:30:03.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:30:03.525
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jan 19 20:30:03.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-6004" for this suite. 01/19/23 20:30:03.547
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":4,"skipped":87,"failed":0}
------------------------------
• [0.120 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:30:03.438
    Jan 19 20:30:03.438: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename tables 01/19/23 20:30:03.439
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:30:03.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:30:03.525
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jan 19 20:30:03.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-6004" for this suite. 01/19/23 20:30:03.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:30:03.558
Jan 19 20:30:03.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 20:30:03.559
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:30:03.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:30:03.591
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173
STEP: creating service in namespace services-3090 01/19/23 20:30:03.594
Jan 19 20:30:03.703: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3090" to be "running and ready"
Jan 19 20:30:03.727: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 23.81663ms
Jan 19 20:30:03.727: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 19 20:30:05.733: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029345835s
Jan 19 20:30:05.733: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 19 20:30:07.732: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 4.028366691s
Jan 19 20:30:07.732: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 19 20:30:07.732: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 19 20:30:07.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 19 20:30:07.884: INFO: rc: 7
Jan 19 20:30:07.894: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 19 20:30:07.897: INFO: Pod kube-proxy-mode-detector no longer exists
Jan 19 20:30:07.897: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-clusterip-timeout in namespace services-3090 01/19/23 20:30:07.897
STEP: creating replication controller affinity-clusterip-timeout in namespace services-3090 01/19/23 20:30:07.906
I0119 20:30:07.915378      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3090, replica count: 3
I0119 20:30:10.966695      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 20:30:13.967375      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 20:30:13.972: INFO: Creating new exec pod
Jan 19 20:30:13.983: INFO: Waiting up to 5m0s for pod "execpod-affinityd9rjg" in namespace "services-3090" to be "running"
Jan 19 20:30:13.986: INFO: Pod "execpod-affinityd9rjg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718559ms
Jan 19 20:30:15.990: INFO: Pod "execpod-affinityd9rjg": Phase="Running", Reason="", readiness=true. Elapsed: 2.006763477s
Jan 19 20:30:15.990: INFO: Pod "execpod-affinityd9rjg" satisfied condition "running"
Jan 19 20:30:16.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec execpod-affinityd9rjg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
Jan 19 20:30:18.157: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
Jan 19 20:30:18.157: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 20:30:18.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec execpod-affinityd9rjg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.45.248 80'
Jan 19 20:30:18.268: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.45.248 80\nConnection to 172.30.45.248 80 port [tcp/http] succeeded!\n"
Jan 19 20:30:18.268: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 20:30:18.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec execpod-affinityd9rjg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.45.248:80/ ; done'
Jan 19 20:30:18.434: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n"
Jan 19 20:30:18.434: INFO: stdout: "\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4"
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
Jan 19 20:30:18.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec execpod-affinityd9rjg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.45.248:80/'
Jan 19 20:30:18.545: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n"
Jan 19 20:30:18.545: INFO: stdout: "affinity-clusterip-timeout-gdjp4"
Jan 19 20:30:38.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec execpod-affinityd9rjg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.45.248:80/'
Jan 19 20:30:39.725: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n"
Jan 19 20:30:39.725: INFO: stdout: "affinity-clusterip-timeout-7tt55"
Jan 19 20:30:39.725: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3090, will wait for the garbage collector to delete the pods 01/19/23 20:30:39.736
Jan 19 20:30:39.795: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.968918ms
Jan 19 20:30:39.895: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.308293ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 20:30:42.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3090" for this suite. 01/19/23 20:30:42.12
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]","completed":5,"skipped":98,"failed":0}
------------------------------
• [SLOW TEST] [38.568 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:30:03.558
    Jan 19 20:30:03.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 20:30:03.559
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:30:03.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:30:03.591
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2173
    STEP: creating service in namespace services-3090 01/19/23 20:30:03.594
    Jan 19 20:30:03.703: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-3090" to be "running and ready"
    Jan 19 20:30:03.727: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 23.81663ms
    Jan 19 20:30:03.727: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 20:30:05.733: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029345835s
    Jan 19 20:30:05.733: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 20:30:07.732: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 4.028366691s
    Jan 19 20:30:07.732: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 19 20:30:07.732: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 19 20:30:07.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 19 20:30:07.884: INFO: rc: 7
    Jan 19 20:30:07.894: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 19 20:30:07.897: INFO: Pod kube-proxy-mode-detector no longer exists
    Jan 19 20:30:07.897: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
    Command stdout:

    stderr:
    + curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
    command terminated with exit code 7

    error:
    exit status 7
    STEP: creating service affinity-clusterip-timeout in namespace services-3090 01/19/23 20:30:07.897
    STEP: creating replication controller affinity-clusterip-timeout in namespace services-3090 01/19/23 20:30:07.906
    I0119 20:30:07.915378      22 runners.go:193] Created replication controller with name: affinity-clusterip-timeout, namespace: services-3090, replica count: 3
    I0119 20:30:10.966695      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0119 20:30:13.967375      22 runners.go:193] affinity-clusterip-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 20:30:13.972: INFO: Creating new exec pod
    Jan 19 20:30:13.983: INFO: Waiting up to 5m0s for pod "execpod-affinityd9rjg" in namespace "services-3090" to be "running"
    Jan 19 20:30:13.986: INFO: Pod "execpod-affinityd9rjg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718559ms
    Jan 19 20:30:15.990: INFO: Pod "execpod-affinityd9rjg": Phase="Running", Reason="", readiness=true. Elapsed: 2.006763477s
    Jan 19 20:30:15.990: INFO: Pod "execpod-affinityd9rjg" satisfied condition "running"
    Jan 19 20:30:16.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec execpod-affinityd9rjg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-timeout 80'
    Jan 19 20:30:18.157: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-timeout 80\nConnection to affinity-clusterip-timeout 80 port [tcp/http] succeeded!\n"
    Jan 19 20:30:18.157: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 20:30:18.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec execpod-affinityd9rjg -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.45.248 80'
    Jan 19 20:30:18.268: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.45.248 80\nConnection to 172.30.45.248 80 port [tcp/http] succeeded!\n"
    Jan 19 20:30:18.268: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 20:30:18.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec execpod-affinityd9rjg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.45.248:80/ ; done'
    Jan 19 20:30:18.434: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n"
    Jan 19 20:30:18.434: INFO: stdout: "\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4\naffinity-clusterip-timeout-gdjp4"
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Received response from host: affinity-clusterip-timeout-gdjp4
    Jan 19 20:30:18.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec execpod-affinityd9rjg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.45.248:80/'
    Jan 19 20:30:18.545: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n"
    Jan 19 20:30:18.545: INFO: stdout: "affinity-clusterip-timeout-gdjp4"
    Jan 19 20:30:38.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-3090 exec execpod-affinityd9rjg -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://172.30.45.248:80/'
    Jan 19 20:30:39.725: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://172.30.45.248:80/\n"
    Jan 19 20:30:39.725: INFO: stdout: "affinity-clusterip-timeout-7tt55"
    Jan 19 20:30:39.725: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-timeout in namespace services-3090, will wait for the garbage collector to delete the pods 01/19/23 20:30:39.736
    Jan 19 20:30:39.795: INFO: Deleting ReplicationController affinity-clusterip-timeout took: 5.968918ms
    Jan 19 20:30:39.895: INFO: Terminating ReplicationController affinity-clusterip-timeout pods took: 100.308293ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 20:30:42.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-3090" for this suite. 01/19/23 20:30:42.12
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:30:42.127
Jan 19 20:30:42.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename hostport 01/19/23 20:30:42.128
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:30:42.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:30:42.234
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/19/23 20:30:42.265
Jan 19 20:30:42.292: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5047" to be "running and ready"
Jan 19 20:30:42.295: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.60548ms
Jan 19 20:30:42.295: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 20:30:44.298: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006012561s
Jan 19 20:30:44.298: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 19 20:30:44.298: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.146.42 on the node which pod1 resides and expect scheduled 01/19/23 20:30:44.298
Jan 19 20:30:44.310: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5047" to be "running and ready"
Jan 19 20:30:44.312: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.613031ms
Jan 19 20:30:44.312: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 20:30:46.315: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00550817s
Jan 19 20:30:46.315: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 19 20:30:46.315: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.146.42 but use UDP protocol on the node which pod2 resides 01/19/23 20:30:46.315
Jan 19 20:30:46.329: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5047" to be "running and ready"
Jan 19 20:30:46.331: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.159418ms
Jan 19 20:30:46.331: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 20:30:48.334: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.005700938s
Jan 19 20:30:48.334: INFO: The phase of Pod pod3 is Running (Ready = false)
Jan 19 20:30:50.337: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.00852672s
Jan 19 20:30:50.337: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan 19 20:30:50.337: INFO: Pod "pod3" satisfied condition "running and ready"
Jan 19 20:30:50.349: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5047" to be "running and ready"
Jan 19 20:30:50.351: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084547ms
Jan 19 20:30:50.351: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan 19 20:30:52.354: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004976657s
Jan 19 20:30:52.354: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan 19 20:30:52.354: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/19/23 20:30:52.356
Jan 19 20:30:52.356: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.146.42 http://127.0.0.1:54323/hostname] Namespace:hostport-5047 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:30:52.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:30:52.356: INFO: ExecWithOptions: Clientset creation
Jan 19 20:30:52.357: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-5047/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.146.42+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.146.42, port: 54323 01/19/23 20:30:52.477
Jan 19 20:30:52.477: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.146.42:54323/hostname] Namespace:hostport-5047 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:30:52.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:30:52.478: INFO: ExecWithOptions: Clientset creation
Jan 19 20:30:52.478: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-5047/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.146.42%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.146.42, port: 54323 UDP 01/19/23 20:30:52.581
Jan 19 20:30:52.581: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.146.42 54323] Namespace:hostport-5047 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:30:52.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:30:52.581: INFO: ExecWithOptions: Clientset creation
Jan 19 20:30:52.581: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-5047/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.146.42+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jan 19 20:30:57.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-5047" for this suite. 01/19/23 20:30:57.677
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":6,"skipped":122,"failed":0}
------------------------------
• [SLOW TEST] [15.556 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:30:42.127
    Jan 19 20:30:42.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename hostport 01/19/23 20:30:42.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:30:42.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:30:42.234
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/19/23 20:30:42.265
    Jan 19 20:30:42.292: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5047" to be "running and ready"
    Jan 19 20:30:42.295: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.60548ms
    Jan 19 20:30:42.295: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 20:30:44.298: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006012561s
    Jan 19 20:30:44.298: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 19 20:30:44.298: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.146.42 on the node which pod1 resides and expect scheduled 01/19/23 20:30:44.298
    Jan 19 20:30:44.310: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5047" to be "running and ready"
    Jan 19 20:30:44.312: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.613031ms
    Jan 19 20:30:44.312: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 20:30:46.315: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.00550817s
    Jan 19 20:30:46.315: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 19 20:30:46.315: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.146.42 but use UDP protocol on the node which pod2 resides 01/19/23 20:30:46.315
    Jan 19 20:30:46.329: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5047" to be "running and ready"
    Jan 19 20:30:46.331: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.159418ms
    Jan 19 20:30:46.331: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 20:30:48.334: INFO: Pod "pod3": Phase="Running", Reason="", readiness=false. Elapsed: 2.005700938s
    Jan 19 20:30:48.334: INFO: The phase of Pod pod3 is Running (Ready = false)
    Jan 19 20:30:50.337: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.00852672s
    Jan 19 20:30:50.337: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan 19 20:30:50.337: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan 19 20:30:50.349: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5047" to be "running and ready"
    Jan 19 20:30:50.351: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084547ms
    Jan 19 20:30:50.351: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 20:30:52.354: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.004976657s
    Jan 19 20:30:52.354: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan 19 20:30:52.354: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/19/23 20:30:52.356
    Jan 19 20:30:52.356: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.146.42 http://127.0.0.1:54323/hostname] Namespace:hostport-5047 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:30:52.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:30:52.356: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:30:52.357: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-5047/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.146.42+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.146.42, port: 54323 01/19/23 20:30:52.477
    Jan 19 20:30:52.477: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.146.42:54323/hostname] Namespace:hostport-5047 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:30:52.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:30:52.478: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:30:52.478: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-5047/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.146.42%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.146.42, port: 54323 UDP 01/19/23 20:30:52.581
    Jan 19 20:30:52.581: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.146.42 54323] Namespace:hostport-5047 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:30:52.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:30:52.581: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:30:52.581: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/hostport-5047/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.146.42+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jan 19 20:30:57.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-5047" for this suite. 01/19/23 20:30:57.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:30:57.684
Jan 19 20:30:57.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename cronjob 01/19/23 20:30:57.685
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:30:57.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:30:57.714
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/19/23 20:30:57.718
W0119 20:30:57.755062      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled 01/19/23 20:30:57.755
STEP: Ensuring exactly one is scheduled 01/19/23 20:31:01.758
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/19/23 20:31:01.761
STEP: Ensuring the job is replaced with a new one 01/19/23 20:31:01.763
STEP: Removing cronjob 01/19/23 20:32:01.766
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 19 20:32:01.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5954" for this suite. 01/19/23 20:32:01.775
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":7,"skipped":136,"failed":0}
------------------------------
• [SLOW TEST] [64.096 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:30:57.684
    Jan 19 20:30:57.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename cronjob 01/19/23 20:30:57.685
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:30:57.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:30:57.714
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/19/23 20:30:57.718
    W0119 20:30:57.755062      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a job is scheduled 01/19/23 20:30:57.755
    STEP: Ensuring exactly one is scheduled 01/19/23 20:31:01.758
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/19/23 20:31:01.761
    STEP: Ensuring the job is replaced with a new one 01/19/23 20:31:01.763
    STEP: Removing cronjob 01/19/23 20:32:01.766
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 19 20:32:01.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5954" for this suite. 01/19/23 20:32:01.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:32:01.78
Jan 19 20:32:01.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 20:32:01.781
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:01.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:01.805
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan 19 20:32:01.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 20:32:02.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7916" for this suite. 01/19/23 20:32:02.85
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":8,"skipped":143,"failed":0}
------------------------------
• [1.078 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:32:01.78
    Jan 19 20:32:01.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 20:32:01.781
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:01.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:01.805
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan 19 20:32:01.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 20:32:02.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7916" for this suite. 01/19/23 20:32:02.85
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:32:02.859
Jan 19 20:32:02.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename statefulset 01/19/23 20:32:02.86
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:02.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:02.9
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3348 01/19/23 20:32:02.904
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 01/19/23 20:32:02.914
STEP: Creating pod with conflicting port in namespace statefulset-3348 01/19/23 20:32:02.95
STEP: Waiting until pod test-pod will start running in namespace statefulset-3348 01/19/23 20:32:03.016
Jan 19 20:32:03.016: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3348" to be "running"
Jan 19 20:32:03.019: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.69798ms
Jan 19 20:32:05.023: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006732309s
Jan 19 20:32:07.025: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008715398s
Jan 19 20:32:09.024: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 6.008061107s
Jan 19 20:32:09.024: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-3348 01/19/23 20:32:09.024
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3348 01/19/23 20:32:09.03
Jan 19 20:32:09.085: INFO: Observed stateful pod in namespace: statefulset-3348, name: ss-0, uid: 378abef0-ba81-4a2c-9e05-564b45874089, status phase: Pending. Waiting for statefulset controller to delete.
Jan 19 20:32:09.094: INFO: Observed stateful pod in namespace: statefulset-3348, name: ss-0, uid: 378abef0-ba81-4a2c-9e05-564b45874089, status phase: Failed. Waiting for statefulset controller to delete.
Jan 19 20:32:09.109: INFO: Observed stateful pod in namespace: statefulset-3348, name: ss-0, uid: 378abef0-ba81-4a2c-9e05-564b45874089, status phase: Failed. Waiting for statefulset controller to delete.
Jan 19 20:32:09.112: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3348
STEP: Removing pod with conflicting port in namespace statefulset-3348 01/19/23 20:32:09.112
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3348 and will be in running state 01/19/23 20:32:09.125
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 20:32:13.133: INFO: Deleting all statefulset in ns statefulset-3348
Jan 19 20:32:13.136: INFO: Scaling statefulset ss to 0
Jan 19 20:32:23.151: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 20:32:23.154: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 20:32:23.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3348" for this suite. 01/19/23 20:32:23.172
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":9,"skipped":146,"failed":0}
------------------------------
• [SLOW TEST] [20.319 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:32:02.859
    Jan 19 20:32:02.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename statefulset 01/19/23 20:32:02.86
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:02.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:02.9
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3348 01/19/23 20:32:02.904
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 01/19/23 20:32:02.914
    STEP: Creating pod with conflicting port in namespace statefulset-3348 01/19/23 20:32:02.95
    STEP: Waiting until pod test-pod will start running in namespace statefulset-3348 01/19/23 20:32:03.016
    Jan 19 20:32:03.016: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-3348" to be "running"
    Jan 19 20:32:03.019: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.69798ms
    Jan 19 20:32:05.023: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006732309s
    Jan 19 20:32:07.025: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008715398s
    Jan 19 20:32:09.024: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 6.008061107s
    Jan 19 20:32:09.024: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-3348 01/19/23 20:32:09.024
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3348 01/19/23 20:32:09.03
    Jan 19 20:32:09.085: INFO: Observed stateful pod in namespace: statefulset-3348, name: ss-0, uid: 378abef0-ba81-4a2c-9e05-564b45874089, status phase: Pending. Waiting for statefulset controller to delete.
    Jan 19 20:32:09.094: INFO: Observed stateful pod in namespace: statefulset-3348, name: ss-0, uid: 378abef0-ba81-4a2c-9e05-564b45874089, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 19 20:32:09.109: INFO: Observed stateful pod in namespace: statefulset-3348, name: ss-0, uid: 378abef0-ba81-4a2c-9e05-564b45874089, status phase: Failed. Waiting for statefulset controller to delete.
    Jan 19 20:32:09.112: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3348
    STEP: Removing pod with conflicting port in namespace statefulset-3348 01/19/23 20:32:09.112
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3348 and will be in running state 01/19/23 20:32:09.125
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 20:32:13.133: INFO: Deleting all statefulset in ns statefulset-3348
    Jan 19 20:32:13.136: INFO: Scaling statefulset ss to 0
    Jan 19 20:32:23.151: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 20:32:23.154: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 20:32:23.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3348" for this suite. 01/19/23 20:32:23.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:32:23.178
Jan 19 20:32:23.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename deployment 01/19/23 20:32:23.178
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:23.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:23.209
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan 19 20:32:23.216: INFO: Creating deployment "webserver-deployment"
W0119 20:32:23.260899      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 20:32:23.260: INFO: Waiting for observed generation 1
Jan 19 20:32:25.278: INFO: Waiting for all required pods to come up
Jan 19 20:32:25.281: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/19/23 20:32:25.281
Jan 19 20:32:25.281: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-txdrc" in namespace "deployment-4274" to be "running"
Jan 19 20:32:25.281: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-62lsk" in namespace "deployment-4274" to be "running"
Jan 19 20:32:25.281: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-8vd8m" in namespace "deployment-4274" to be "running"
Jan 19 20:32:25.281: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-c2d5k" in namespace "deployment-4274" to be "running"
Jan 19 20:32:25.281: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-gfg7p" in namespace "deployment-4274" to be "running"
Jan 19 20:32:25.281: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hg97t" in namespace "deployment-4274" to be "running"
Jan 19 20:32:25.282: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jghvz" in namespace "deployment-4274" to be "running"
Jan 19 20:32:25.282: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-p96vh" in namespace "deployment-4274" to be "running"
Jan 19 20:32:25.282: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rpjmx" in namespace "deployment-4274" to be "running"
Jan 19 20:32:25.282: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-t8kss" in namespace "deployment-4274" to be "running"
Jan 19 20:32:25.283: INFO: Pod "webserver-deployment-845c8977d9-8vd8m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.160799ms
Jan 19 20:32:25.284: INFO: Pod "webserver-deployment-845c8977d9-txdrc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.724563ms
Jan 19 20:32:25.284: INFO: Pod "webserver-deployment-845c8977d9-62lsk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.073243ms
Jan 19 20:32:25.284: INFO: Pod "webserver-deployment-845c8977d9-gfg7p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.872907ms
Jan 19 20:32:25.284: INFO: Pod "webserver-deployment-845c8977d9-p96vh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580277ms
Jan 19 20:32:25.284: INFO: Pod "webserver-deployment-845c8977d9-jghvz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773329ms
Jan 19 20:32:25.285: INFO: Pod "webserver-deployment-845c8977d9-rpjmx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.76662ms
Jan 19 20:32:25.285: INFO: Pod "webserver-deployment-845c8977d9-hg97t": Phase="Pending", Reason="", readiness=false. Elapsed: 3.075573ms
Jan 19 20:32:25.285: INFO: Pod "webserver-deployment-845c8977d9-c2d5k": Phase="Pending", Reason="", readiness=false. Elapsed: 3.384813ms
Jan 19 20:32:25.285: INFO: Pod "webserver-deployment-845c8977d9-t8kss": Phase="Pending", Reason="", readiness=false. Elapsed: 3.273388ms
Jan 19 20:32:27.288: INFO: Pod "webserver-deployment-845c8977d9-62lsk": Phase="Running", Reason="", readiness=true. Elapsed: 2.006469767s
Jan 19 20:32:27.288: INFO: Pod "webserver-deployment-845c8977d9-62lsk" satisfied condition "running"
Jan 19 20:32:27.288: INFO: Pod "webserver-deployment-845c8977d9-hg97t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006240822s
Jan 19 20:32:27.288: INFO: Pod "webserver-deployment-845c8977d9-p96vh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00655637s
Jan 19 20:32:27.288: INFO: Pod "webserver-deployment-845c8977d9-rpjmx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00648312s
Jan 19 20:32:27.288: INFO: Pod "webserver-deployment-845c8977d9-jghvz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006711892s
Jan 19 20:32:27.289: INFO: Pod "webserver-deployment-845c8977d9-gfg7p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007853121s
Jan 19 20:32:27.289: INFO: Pod "webserver-deployment-845c8977d9-t8kss": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007378122s
Jan 19 20:32:27.289: INFO: Pod "webserver-deployment-845c8977d9-c2d5k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007989189s
Jan 19 20:32:27.289: INFO: Pod "webserver-deployment-845c8977d9-8vd8m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008075133s
Jan 19 20:32:27.289: INFO: Pod "webserver-deployment-845c8977d9-txdrc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008356915s
Jan 19 20:32:27.289: INFO: Pod "webserver-deployment-845c8977d9-txdrc" satisfied condition "running"
Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-hg97t": Phase="Running", Reason="", readiness=true. Elapsed: 4.007891998s
Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-hg97t" satisfied condition "running"
Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-rpjmx": Phase="Running", Reason="", readiness=true. Elapsed: 4.007713133s
Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-t8kss": Phase="Running", Reason="", readiness=true. Elapsed: 4.007634241s
Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-rpjmx" satisfied condition "running"
Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-t8kss" satisfied condition "running"
Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-gfg7p": Phase="Running", Reason="", readiness=true. Elapsed: 4.008117142s
Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-gfg7p" satisfied condition "running"
Jan 19 20:32:29.290: INFO: Pod "webserver-deployment-845c8977d9-jghvz": Phase="Running", Reason="", readiness=true. Elapsed: 4.00797743s
Jan 19 20:32:29.290: INFO: Pod "webserver-deployment-845c8977d9-jghvz" satisfied condition "running"
Jan 19 20:32:29.290: INFO: Pod "webserver-deployment-845c8977d9-p96vh": Phase="Running", Reason="", readiness=true. Elapsed: 4.007882854s
Jan 19 20:32:29.290: INFO: Pod "webserver-deployment-845c8977d9-p96vh" satisfied condition "running"
Jan 19 20:32:29.290: INFO: Pod "webserver-deployment-845c8977d9-8vd8m": Phase="Running", Reason="", readiness=true. Elapsed: 4.009196455s
Jan 19 20:32:29.290: INFO: Pod "webserver-deployment-845c8977d9-8vd8m" satisfied condition "running"
Jan 19 20:32:29.291: INFO: Pod "webserver-deployment-845c8977d9-c2d5k": Phase="Running", Reason="", readiness=true. Elapsed: 4.009277792s
Jan 19 20:32:29.291: INFO: Pod "webserver-deployment-845c8977d9-c2d5k" satisfied condition "running"
Jan 19 20:32:29.291: INFO: Waiting for deployment "webserver-deployment" to complete
Jan 19 20:32:29.295: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan 19 20:32:29.316: INFO: Updating deployment webserver-deployment
Jan 19 20:32:29.316: INFO: Waiting for observed generation 2
Jan 19 20:32:31.320: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan 19 20:32:31.322: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan 19 20:32:31.325: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 19 20:32:31.331: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan 19 20:32:31.331: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan 19 20:32:31.334: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan 19 20:32:31.367: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan 19 20:32:31.367: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan 19 20:32:31.377: INFO: Updating deployment webserver-deployment
Jan 19 20:32:31.377: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan 19 20:32:31.381: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan 19 20:32:31.384: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 20:32:31.391: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4274  c9a4adfd-dc9f-4e2a-bbfa-00d102d23873 130965 3 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-19 20:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00508c8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 20:32:27 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-19 20:32:29 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan 19 20:32:31.396: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-4274  a2f1d84d-04fe-4698-8a5d-ee77a2bc5286 130968 3 2023-01-19 20:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c9a4adfd-dc9f-4e2a-bbfa-00d102d23873 0xc00b1e8267 0xc00b1e8268}] [] [{kube-controller-manager Update apps/v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-19 20:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9a4adfd-dc9f-4e2a-bbfa-00d102d23873\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b1e8308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 20:32:31.396: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan 19 20:32:31.396: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-4274  6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 130966 3 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c9a4adfd-dc9f-4e2a-bbfa-00d102d23873 0xc00b1e8367 0xc00b1e8368}] [] [{kube-controller-manager Update apps/v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-19 20:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9a4adfd-dc9f-4e2a-bbfa-00d102d23873\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b1e83f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan 19 20:32:31.403: INFO: Pod "webserver-deployment-69b7448995-6kmgm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6kmgm webserver-deployment-69b7448995- deployment-4274  cde78f28-3ea9-4663-8935-581cf63e5f75 130961 0 2023-01-19 20:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.10.20/23"],"mac_address":"0a:58:0a:80:0a:14","gateway_ips":["10.128.10.1"],"ip_address":"10.128.10.20/23","gateway_ip":"10.128.10.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.10.20"
    ],
    "mac": "0a:58:0a:80:0a:14",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.10.20"
    ],
    "mac": "0a:58:0a:80:0a:14",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a2f1d84d-04fe-4698-8a5d-ee77a2bc5286 0xc002780547 0xc002780548}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2f1d84d-04fe-4698-8a5d-ee77a2bc5286\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.10.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8tc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8tc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-146-42.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.146.42,PodIP:10.128.10.20,StartTime:2023-01-19 20:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.10.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.403: INFO: Pod "webserver-deployment-69b7448995-rjljm" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-rjljm webserver-deployment-69b7448995- deployment-4274  4403fc1c-47d1-406d-b1d2-dd01fd176b38 130960 0 2023-01-19 20:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.62/23"],"mac_address":"0a:58:0a:80:10:3e","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.62/23","gateway_ip":"10.128.16.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.16.62"
    ],
    "mac": "0a:58:0a:80:10:3e",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.16.62"
    ],
    "mac": "0a:58:0a:80:10:3e",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a2f1d84d-04fe-4698-8a5d-ee77a2bc5286 0xc0027807d7 0xc0027807d8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2f1d84d-04fe-4698-8a5d-ee77a2bc5286\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.16.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjbx4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjbx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-207-77.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.207.77,PodIP:10.128.16.62,StartTime:2023-01-19 20:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.16.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.404: INFO: Pod "webserver-deployment-69b7448995-s5k5m" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-s5k5m webserver-deployment-69b7448995- deployment-4274  cc422cf9-77a5-4ea0-905a-425ecd0e5f78 130948 0 2023-01-19 20:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.20/23"],"mac_address":"0a:58:0a:80:08:14","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.20/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.20"
    ],
    "mac": "0a:58:0a:80:08:14",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.20"
    ],
    "mac": "0a:58:0a:80:08:14",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a2f1d84d-04fe-4698-8a5d-ee77a2bc5286 0xc002780a67 0xc002780a68}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2f1d84d-04fe-4698-8a5d-ee77a2bc5286\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k9lt8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9lt8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.20,StartTime:2023-01-19 20:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.404: INFO: Pod "webserver-deployment-69b7448995-sr8c5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-sr8c5 webserver-deployment-69b7448995- deployment-4274  44490fe2-950b-4812-908c-ea3af3c48243 130941 0 2023-01-19 20:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.6.8/23"],"mac_address":"0a:58:0a:80:06:08","gateway_ips":["10.128.6.1"],"ip_address":"10.128.6.8/23","gateway_ip":"10.128.6.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.6.8"
    ],
    "mac": "0a:58:0a:80:06:08",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.6.8"
    ],
    "mac": "0a:58:0a:80:06:08",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a2f1d84d-04fe-4698-8a5d-ee77a2bc5286 0xc002780cf7 0xc002780cf8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2f1d84d-04fe-4698-8a5d-ee77a2bc5286\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 20:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.6.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-19 20:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-568qf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-568qf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-188-71.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.188.71,PodIP:10.128.6.8,StartTime:2023-01-19 20:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.6.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.404: INFO: Pod "webserver-deployment-69b7448995-w6j5n" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-w6j5n webserver-deployment-69b7448995- deployment-4274  b437b36f-584a-4427-aca2-dcda8cd2eb51 130950 0 2023-01-19 20:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.17/23"],"mac_address":"0a:58:0a:80:0c:11","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.17/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.17"
    ],
    "mac": "0a:58:0a:80:0c:11",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.17"
    ],
    "mac": "0a:58:0a:80:0c:11",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a2f1d84d-04fe-4698-8a5d-ee77a2bc5286 0xc002780f87 0xc002780f88}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2f1d84d-04fe-4698-8a5d-ee77a2bc5286\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 20:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-19 20:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xh5bg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xh5bg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-171-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.171.213,PodIP:10.128.12.17,StartTime:2023-01-19 20:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.12.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.404: INFO: Pod "webserver-deployment-845c8977d9-62lsk" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-62lsk webserver-deployment-845c8977d9- deployment-4274  b000a7e2-886b-4c82-93f1-f39b99694a72 130695 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.10.18/23"],"mac_address":"0a:58:0a:80:0a:12","gateway_ips":["10.128.10.1"],"ip_address":"10.128.10.18/23","gateway_ip":"10.128.10.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.10.18"
    ],
    "mac": "0a:58:0a:80:0a:12",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.10.18"
    ],
    "mac": "0a:58:0a:80:0a:12",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc002781217 0xc002781218}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.10.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jxtk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jxtk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-146-42.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.146.42,PodIP:10.128.10.18,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://5a8a264e44deeb65182d0f1d1d3942b5f13dc99641e7dd8192e7680096187eb8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.10.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.404: INFO: Pod "webserver-deployment-845c8977d9-8vd8m" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8vd8m webserver-deployment-845c8977d9- deployment-4274  21fbc6e1-15c3-41a4-b3fc-a4f4e07c770a 130764 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.15/23"],"mac_address":"0a:58:0a:80:0c:0f","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.15/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.15"
    ],
    "mac": "0a:58:0a:80:0c:0f",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.15"
    ],
    "mac": "0a:58:0a:80:0c:0f",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc002781477 0xc002781478}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wkgcz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wkgcz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-171-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.171.213,PodIP:10.128.12.15,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://cd60e3dfe3059220173f2e263910b13c0e0404ed11a911d47bbc9a47c458378a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.12.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.404: INFO: Pod "webserver-deployment-845c8977d9-gfg7p" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gfg7p webserver-deployment-845c8977d9- deployment-4274  2b343d84-3a02-47f0-a4fd-0e6d3339f88a 130758 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.18/23"],"mac_address":"0a:58:0a:80:08:12","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.18/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.18"
    ],
    "mac": "0a:58:0a:80:08:12",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.18"
    ],
    "mac": "0a:58:0a:80:08:12",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc0027816d7 0xc0027816d8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mmdbq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mmdbq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.18,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://23cc50f865b078175f3e259c621e9c78b295f0c7a20a34791ab95dd0ed42a554,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.405: INFO: Pod "webserver-deployment-845c8977d9-hg97t" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hg97t webserver-deployment-845c8977d9- deployment-4274  1a457ba0-6a3f-4374-ab4e-cdce2eda35f6 130755 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.19/23"],"mac_address":"0a:58:0a:80:08:13","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.19/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.19"
    ],
    "mac": "0a:58:0a:80:08:13",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.19"
    ],
    "mac": "0a:58:0a:80:08:13",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc002781947 0xc002781948}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rtclp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rtclp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.19,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://487c7c33617a42f9eaefb492d8893a652092d8ce929392ed5af4127312a5a1be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.405: INFO: Pod "webserver-deployment-845c8977d9-jghvz" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-jghvz webserver-deployment-845c8977d9- deployment-4274  74dc07b0-c1b1-4808-9416-7ae511c5f285 130736 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.61/23"],"mac_address":"0a:58:0a:80:10:3d","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.61/23","gateway_ip":"10.128.16.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.16.61"
    ],
    "mac": "0a:58:0a:80:10:3d",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.16.61"
    ],
    "mac": "0a:58:0a:80:10:3d",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc002781ba7 0xc002781ba8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.16.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dlxsf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dlxsf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-207-77.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.207.77,PodIP:10.128.16.61,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d216b9e9783f8465d3635a89ea594913cb5f6dc4afab70eda521e58b8e23a95f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.16.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.405: INFO: Pod "webserver-deployment-845c8977d9-lpcsn" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lpcsn webserver-deployment-845c8977d9- deployment-4274  e3fc2620-47a4-4ed1-9420-350fe5d25394 130971 0 2023-01-19 20:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc002781e07 0xc002781e08}] [] [{kube-controller-manager Update v1 2023-01-19 20:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8n22,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8n22,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.405: INFO: Pod "webserver-deployment-845c8977d9-p96vh" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-p96vh webserver-deployment-845c8977d9- deployment-4274  0da54a41-e038-4e19-85ef-dd5125407e52 130762 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.16/23"],"mac_address":"0a:58:0a:80:0c:10","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.16/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.16"
    ],
    "mac": "0a:58:0a:80:0c:10",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.16"
    ],
    "mac": "0a:58:0a:80:0c:10",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc00b214127 0xc00b214128}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-979v8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-979v8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-171-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.171.213,PodIP:10.128.12.16,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://2974505f75e27e966610382d56c69d2f8b2e9c7604f9663f462468a2a3cd1597,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.12.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.405: INFO: Pod "webserver-deployment-845c8977d9-rpjmx" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rpjmx webserver-deployment-845c8977d9- deployment-4274  426a279f-1dd5-4214-897f-ff135c6aa076 130768 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.14.37/23"],"mac_address":"0a:58:0a:80:0e:25","gateway_ips":["10.128.14.1"],"ip_address":"10.128.14.37/23","gateway_ip":"10.128.14.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.14.37"
    ],
    "mac": "0a:58:0a:80:0e:25",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.14.37"
    ],
    "mac": "0a:58:0a:80:0e:25",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc00b214387 0xc00b214388}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.14.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6896j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6896j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-158.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.158,PodIP:10.128.14.37,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://94a85901f0a9f8b7f2e8aab949ee04c9b1b792d01c6a9877c238b456cdf2e50b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.14.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:32:31.405: INFO: Pod "webserver-deployment-845c8977d9-txdrc" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-txdrc webserver-deployment-845c8977d9- deployment-4274  67d59026-5383-4648-b856-c36f0a76c6d1 130697 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.10.19/23"],"mac_address":"0a:58:0a:80:0a:13","gateway_ips":["10.128.10.1"],"ip_address":"10.128.10.19/23","gateway_ip":"10.128.10.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.10.19"
    ],
    "mac": "0a:58:0a:80:0a:13",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.10.19"
    ],
    "mac": "0a:58:0a:80:0a:13",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc00b214607 0xc00b214608}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.10.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vt56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vt56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-146-42.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.146.42,PodIP:10.128.10.19,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8b65c78235c5e63b6d5707e8d4ce7079edd984ec662c8d6494acba2ca0f0ba25,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.10.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 20:32:31.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4274" for this suite. 01/19/23 20:32:31.411
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":10,"skipped":152,"failed":0}
------------------------------
• [SLOW TEST] [8.243 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:32:23.178
    Jan 19 20:32:23.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename deployment 01/19/23 20:32:23.178
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:23.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:23.209
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan 19 20:32:23.216: INFO: Creating deployment "webserver-deployment"
    W0119 20:32:23.260899      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 20:32:23.260: INFO: Waiting for observed generation 1
    Jan 19 20:32:25.278: INFO: Waiting for all required pods to come up
    Jan 19 20:32:25.281: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/19/23 20:32:25.281
    Jan 19 20:32:25.281: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-txdrc" in namespace "deployment-4274" to be "running"
    Jan 19 20:32:25.281: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-62lsk" in namespace "deployment-4274" to be "running"
    Jan 19 20:32:25.281: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-8vd8m" in namespace "deployment-4274" to be "running"
    Jan 19 20:32:25.281: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-c2d5k" in namespace "deployment-4274" to be "running"
    Jan 19 20:32:25.281: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-gfg7p" in namespace "deployment-4274" to be "running"
    Jan 19 20:32:25.281: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-hg97t" in namespace "deployment-4274" to be "running"
    Jan 19 20:32:25.282: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-jghvz" in namespace "deployment-4274" to be "running"
    Jan 19 20:32:25.282: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-p96vh" in namespace "deployment-4274" to be "running"
    Jan 19 20:32:25.282: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rpjmx" in namespace "deployment-4274" to be "running"
    Jan 19 20:32:25.282: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-t8kss" in namespace "deployment-4274" to be "running"
    Jan 19 20:32:25.283: INFO: Pod "webserver-deployment-845c8977d9-8vd8m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.160799ms
    Jan 19 20:32:25.284: INFO: Pod "webserver-deployment-845c8977d9-txdrc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.724563ms
    Jan 19 20:32:25.284: INFO: Pod "webserver-deployment-845c8977d9-62lsk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.073243ms
    Jan 19 20:32:25.284: INFO: Pod "webserver-deployment-845c8977d9-gfg7p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.872907ms
    Jan 19 20:32:25.284: INFO: Pod "webserver-deployment-845c8977d9-p96vh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580277ms
    Jan 19 20:32:25.284: INFO: Pod "webserver-deployment-845c8977d9-jghvz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773329ms
    Jan 19 20:32:25.285: INFO: Pod "webserver-deployment-845c8977d9-rpjmx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.76662ms
    Jan 19 20:32:25.285: INFO: Pod "webserver-deployment-845c8977d9-hg97t": Phase="Pending", Reason="", readiness=false. Elapsed: 3.075573ms
    Jan 19 20:32:25.285: INFO: Pod "webserver-deployment-845c8977d9-c2d5k": Phase="Pending", Reason="", readiness=false. Elapsed: 3.384813ms
    Jan 19 20:32:25.285: INFO: Pod "webserver-deployment-845c8977d9-t8kss": Phase="Pending", Reason="", readiness=false. Elapsed: 3.273388ms
    Jan 19 20:32:27.288: INFO: Pod "webserver-deployment-845c8977d9-62lsk": Phase="Running", Reason="", readiness=true. Elapsed: 2.006469767s
    Jan 19 20:32:27.288: INFO: Pod "webserver-deployment-845c8977d9-62lsk" satisfied condition "running"
    Jan 19 20:32:27.288: INFO: Pod "webserver-deployment-845c8977d9-hg97t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006240822s
    Jan 19 20:32:27.288: INFO: Pod "webserver-deployment-845c8977d9-p96vh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00655637s
    Jan 19 20:32:27.288: INFO: Pod "webserver-deployment-845c8977d9-rpjmx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00648312s
    Jan 19 20:32:27.288: INFO: Pod "webserver-deployment-845c8977d9-jghvz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006711892s
    Jan 19 20:32:27.289: INFO: Pod "webserver-deployment-845c8977d9-gfg7p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007853121s
    Jan 19 20:32:27.289: INFO: Pod "webserver-deployment-845c8977d9-t8kss": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007378122s
    Jan 19 20:32:27.289: INFO: Pod "webserver-deployment-845c8977d9-c2d5k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007989189s
    Jan 19 20:32:27.289: INFO: Pod "webserver-deployment-845c8977d9-8vd8m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008075133s
    Jan 19 20:32:27.289: INFO: Pod "webserver-deployment-845c8977d9-txdrc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008356915s
    Jan 19 20:32:27.289: INFO: Pod "webserver-deployment-845c8977d9-txdrc" satisfied condition "running"
    Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-hg97t": Phase="Running", Reason="", readiness=true. Elapsed: 4.007891998s
    Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-hg97t" satisfied condition "running"
    Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-rpjmx": Phase="Running", Reason="", readiness=true. Elapsed: 4.007713133s
    Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-t8kss": Phase="Running", Reason="", readiness=true. Elapsed: 4.007634241s
    Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-rpjmx" satisfied condition "running"
    Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-t8kss" satisfied condition "running"
    Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-gfg7p": Phase="Running", Reason="", readiness=true. Elapsed: 4.008117142s
    Jan 19 20:32:29.289: INFO: Pod "webserver-deployment-845c8977d9-gfg7p" satisfied condition "running"
    Jan 19 20:32:29.290: INFO: Pod "webserver-deployment-845c8977d9-jghvz": Phase="Running", Reason="", readiness=true. Elapsed: 4.00797743s
    Jan 19 20:32:29.290: INFO: Pod "webserver-deployment-845c8977d9-jghvz" satisfied condition "running"
    Jan 19 20:32:29.290: INFO: Pod "webserver-deployment-845c8977d9-p96vh": Phase="Running", Reason="", readiness=true. Elapsed: 4.007882854s
    Jan 19 20:32:29.290: INFO: Pod "webserver-deployment-845c8977d9-p96vh" satisfied condition "running"
    Jan 19 20:32:29.290: INFO: Pod "webserver-deployment-845c8977d9-8vd8m": Phase="Running", Reason="", readiness=true. Elapsed: 4.009196455s
    Jan 19 20:32:29.290: INFO: Pod "webserver-deployment-845c8977d9-8vd8m" satisfied condition "running"
    Jan 19 20:32:29.291: INFO: Pod "webserver-deployment-845c8977d9-c2d5k": Phase="Running", Reason="", readiness=true. Elapsed: 4.009277792s
    Jan 19 20:32:29.291: INFO: Pod "webserver-deployment-845c8977d9-c2d5k" satisfied condition "running"
    Jan 19 20:32:29.291: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan 19 20:32:29.295: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan 19 20:32:29.316: INFO: Updating deployment webserver-deployment
    Jan 19 20:32:29.316: INFO: Waiting for observed generation 2
    Jan 19 20:32:31.320: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan 19 20:32:31.322: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan 19 20:32:31.325: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 19 20:32:31.331: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan 19 20:32:31.331: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan 19 20:32:31.334: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan 19 20:32:31.367: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan 19 20:32:31.367: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan 19 20:32:31.377: INFO: Updating deployment webserver-deployment
    Jan 19 20:32:31.377: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan 19 20:32:31.381: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan 19 20:32:31.384: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 20:32:31.391: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-4274  c9a4adfd-dc9f-4e2a-bbfa-00d102d23873 130965 3 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-01-19 20:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00508c8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 20:32:27 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-01-19 20:32:29 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan 19 20:32:31.396: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-4274  a2f1d84d-04fe-4698-8a5d-ee77a2bc5286 130968 3 2023-01-19 20:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment c9a4adfd-dc9f-4e2a-bbfa-00d102d23873 0xc00b1e8267 0xc00b1e8268}] [] [{kube-controller-manager Update apps/v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-19 20:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9a4adfd-dc9f-4e2a-bbfa-00d102d23873\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b1e8308 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 20:32:31.396: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan 19 20:32:31.396: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-4274  6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 130966 3 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment c9a4adfd-dc9f-4e2a-bbfa-00d102d23873 0xc00b1e8367 0xc00b1e8368}] [] [{kube-controller-manager Update apps/v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-19 20:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c9a4adfd-dc9f-4e2a-bbfa-00d102d23873\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b1e83f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 20:32:31.403: INFO: Pod "webserver-deployment-69b7448995-6kmgm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6kmgm webserver-deployment-69b7448995- deployment-4274  cde78f28-3ea9-4663-8935-581cf63e5f75 130961 0 2023-01-19 20:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.10.20/23"],"mac_address":"0a:58:0a:80:0a:14","gateway_ips":["10.128.10.1"],"ip_address":"10.128.10.20/23","gateway_ip":"10.128.10.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.10.20"
        ],
        "mac": "0a:58:0a:80:0a:14",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.10.20"
        ],
        "mac": "0a:58:0a:80:0a:14",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a2f1d84d-04fe-4698-8a5d-ee77a2bc5286 0xc002780547 0xc002780548}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2f1d84d-04fe-4698-8a5d-ee77a2bc5286\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.10.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8tc2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8tc2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-146-42.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.146.42,PodIP:10.128.10.20,StartTime:2023-01-19 20:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.10.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.403: INFO: Pod "webserver-deployment-69b7448995-rjljm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-rjljm webserver-deployment-69b7448995- deployment-4274  4403fc1c-47d1-406d-b1d2-dd01fd176b38 130960 0 2023-01-19 20:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.62/23"],"mac_address":"0a:58:0a:80:10:3e","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.62/23","gateway_ip":"10.128.16.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.16.62"
        ],
        "mac": "0a:58:0a:80:10:3e",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.16.62"
        ],
        "mac": "0a:58:0a:80:10:3e",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a2f1d84d-04fe-4698-8a5d-ee77a2bc5286 0xc0027807d7 0xc0027807d8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2f1d84d-04fe-4698-8a5d-ee77a2bc5286\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.16.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjbx4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjbx4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-207-77.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.207.77,PodIP:10.128.16.62,StartTime:2023-01-19 20:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.16.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.404: INFO: Pod "webserver-deployment-69b7448995-s5k5m" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-s5k5m webserver-deployment-69b7448995- deployment-4274  cc422cf9-77a5-4ea0-905a-425ecd0e5f78 130948 0 2023-01-19 20:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.20/23"],"mac_address":"0a:58:0a:80:08:14","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.20/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.20"
        ],
        "mac": "0a:58:0a:80:08:14",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.20"
        ],
        "mac": "0a:58:0a:80:08:14",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a2f1d84d-04fe-4698-8a5d-ee77a2bc5286 0xc002780a67 0xc002780a68}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2f1d84d-04fe-4698-8a5d-ee77a2bc5286\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k9lt8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9lt8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.20,StartTime:2023-01-19 20:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.20,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.404: INFO: Pod "webserver-deployment-69b7448995-sr8c5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-sr8c5 webserver-deployment-69b7448995- deployment-4274  44490fe2-950b-4812-908c-ea3af3c48243 130941 0 2023-01-19 20:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.6.8/23"],"mac_address":"0a:58:0a:80:06:08","gateway_ips":["10.128.6.1"],"ip_address":"10.128.6.8/23","gateway_ip":"10.128.6.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.6.8"
        ],
        "mac": "0a:58:0a:80:06:08",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.6.8"
        ],
        "mac": "0a:58:0a:80:06:08",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a2f1d84d-04fe-4698-8a5d-ee77a2bc5286 0xc002780cf7 0xc002780cf8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2f1d84d-04fe-4698-8a5d-ee77a2bc5286\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 20:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.6.8\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-19 20:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-568qf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-568qf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-188-71.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.188.71,PodIP:10.128.6.8,StartTime:2023-01-19 20:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.6.8,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.404: INFO: Pod "webserver-deployment-69b7448995-w6j5n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-w6j5n webserver-deployment-69b7448995- deployment-4274  b437b36f-584a-4427-aca2-dcda8cd2eb51 130950 0 2023-01-19 20:32:29 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.17/23"],"mac_address":"0a:58:0a:80:0c:11","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.17/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.12.17"
        ],
        "mac": "0a:58:0a:80:0c:11",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.12.17"
        ],
        "mac": "0a:58:0a:80:0c:11",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a2f1d84d-04fe-4698-8a5d-ee77a2bc5286 0xc002780f87 0xc002780f88}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:29 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2f1d84d-04fe-4698-8a5d-ee77a2bc5286\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 20:32:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-19 20:32:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xh5bg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xh5bg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-171-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:29 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.171.213,PodIP:10.128.12.17,StartTime:2023-01-19 20:32:29 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.12.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.404: INFO: Pod "webserver-deployment-845c8977d9-62lsk" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-62lsk webserver-deployment-845c8977d9- deployment-4274  b000a7e2-886b-4c82-93f1-f39b99694a72 130695 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.10.18/23"],"mac_address":"0a:58:0a:80:0a:12","gateway_ips":["10.128.10.1"],"ip_address":"10.128.10.18/23","gateway_ip":"10.128.10.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.10.18"
        ],
        "mac": "0a:58:0a:80:0a:12",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.10.18"
        ],
        "mac": "0a:58:0a:80:0a:12",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc002781217 0xc002781218}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.10.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6jxtk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6jxtk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-146-42.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.146.42,PodIP:10.128.10.18,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://5a8a264e44deeb65182d0f1d1d3942b5f13dc99641e7dd8192e7680096187eb8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.10.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.404: INFO: Pod "webserver-deployment-845c8977d9-8vd8m" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8vd8m webserver-deployment-845c8977d9- deployment-4274  21fbc6e1-15c3-41a4-b3fc-a4f4e07c770a 130764 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.15/23"],"mac_address":"0a:58:0a:80:0c:0f","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.15/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.12.15"
        ],
        "mac": "0a:58:0a:80:0c:0f",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.12.15"
        ],
        "mac": "0a:58:0a:80:0c:0f",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc002781477 0xc002781478}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wkgcz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wkgcz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-171-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.171.213,PodIP:10.128.12.15,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://cd60e3dfe3059220173f2e263910b13c0e0404ed11a911d47bbc9a47c458378a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.12.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.404: INFO: Pod "webserver-deployment-845c8977d9-gfg7p" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gfg7p webserver-deployment-845c8977d9- deployment-4274  2b343d84-3a02-47f0-a4fd-0e6d3339f88a 130758 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.18/23"],"mac_address":"0a:58:0a:80:08:12","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.18/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.18"
        ],
        "mac": "0a:58:0a:80:08:12",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.18"
        ],
        "mac": "0a:58:0a:80:08:12",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc0027816d7 0xc0027816d8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mmdbq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mmdbq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.18,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://23cc50f865b078175f3e259c621e9c78b295f0c7a20a34791ab95dd0ed42a554,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.405: INFO: Pod "webserver-deployment-845c8977d9-hg97t" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hg97t webserver-deployment-845c8977d9- deployment-4274  1a457ba0-6a3f-4374-ab4e-cdce2eda35f6 130755 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.19/23"],"mac_address":"0a:58:0a:80:08:13","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.19/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.19"
        ],
        "mac": "0a:58:0a:80:08:13",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.19"
        ],
        "mac": "0a:58:0a:80:08:13",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc002781947 0xc002781948}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rtclp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rtclp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.19,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://487c7c33617a42f9eaefb492d8893a652092d8ce929392ed5af4127312a5a1be,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.405: INFO: Pod "webserver-deployment-845c8977d9-jghvz" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-jghvz webserver-deployment-845c8977d9- deployment-4274  74dc07b0-c1b1-4808-9416-7ae511c5f285 130736 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.16.61/23"],"mac_address":"0a:58:0a:80:10:3d","gateway_ips":["10.128.16.1"],"ip_address":"10.128.16.61/23","gateway_ip":"10.128.16.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.16.61"
        ],
        "mac": "0a:58:0a:80:10:3d",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.16.61"
        ],
        "mac": "0a:58:0a:80:10:3d",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc002781ba7 0xc002781ba8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.16.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dlxsf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dlxsf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-207-77.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.207.77,PodIP:10.128.16.61,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d216b9e9783f8465d3635a89ea594913cb5f6dc4afab70eda521e58b8e23a95f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.16.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.405: INFO: Pod "webserver-deployment-845c8977d9-lpcsn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lpcsn webserver-deployment-845c8977d9- deployment-4274  e3fc2620-47a4-4ed1-9420-350fe5d25394 130971 0 2023-01-19 20:32:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc002781e07 0xc002781e08}] [] [{kube-controller-manager Update v1 2023-01-19 20:32:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w8n22,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w8n22,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.405: INFO: Pod "webserver-deployment-845c8977d9-p96vh" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-p96vh webserver-deployment-845c8977d9- deployment-4274  0da54a41-e038-4e19-85ef-dd5125407e52 130762 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.16/23"],"mac_address":"0a:58:0a:80:0c:10","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.16/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.12.16"
        ],
        "mac": "0a:58:0a:80:0c:10",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.12.16"
        ],
        "mac": "0a:58:0a:80:0c:10",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc00b214127 0xc00b214128}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-979v8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-979v8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-171-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.171.213,PodIP:10.128.12.16,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://2974505f75e27e966610382d56c69d2f8b2e9c7604f9663f462468a2a3cd1597,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.12.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.405: INFO: Pod "webserver-deployment-845c8977d9-rpjmx" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rpjmx webserver-deployment-845c8977d9- deployment-4274  426a279f-1dd5-4214-897f-ff135c6aa076 130768 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.14.37/23"],"mac_address":"0a:58:0a:80:0e:25","gateway_ips":["10.128.14.1"],"ip_address":"10.128.14.37/23","gateway_ip":"10.128.14.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.14.37"
        ],
        "mac": "0a:58:0a:80:0e:25",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.14.37"
        ],
        "mac": "0a:58:0a:80:0e:25",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc00b214387 0xc00b214388}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.14.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6896j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6896j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-151-158.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.151.158,PodIP:10.128.14.37,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://94a85901f0a9f8b7f2e8aab949ee04c9b1b792d01c6a9877c238b456cdf2e50b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.14.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:32:31.405: INFO: Pod "webserver-deployment-845c8977d9-txdrc" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-txdrc webserver-deployment-845c8977d9- deployment-4274  67d59026-5383-4648-b856-c36f0a76c6d1 130697 0 2023-01-19 20:32:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.10.19/23"],"mac_address":"0a:58:0a:80:0a:13","gateway_ips":["10.128.10.1"],"ip_address":"10.128.10.19/23","gateway_ip":"10.128.10.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.10.19"
        ],
        "mac": "0a:58:0a:80:0a:13",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.10.19"
        ],
        "mac": "0a:58:0a:80:0a:13",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77 0xc00b214607 0xc00b214608}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d5a98f2-e8d4-4fe6-b0eb-c7968f1f3f77\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 20:32:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 20:32:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.10.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vt56,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vt56,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-146-42.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c34,c4,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-5qc4x,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:32:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.146.42,PodIP:10.128.10.19,StartTime:2023-01-19 20:32:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:32:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8b65c78235c5e63b6d5707e8d4ce7079edd984ec662c8d6494acba2ca0f0ba25,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.10.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 20:32:31.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4274" for this suite. 01/19/23 20:32:31.411
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:32:31.421
Jan 19 20:32:31.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename init-container 01/19/23 20:32:31.422
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:31.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:31.46
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 01/19/23 20:32:31.462
Jan 19 20:32:31.462: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 20:32:36.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8320" for this suite. 01/19/23 20:32:36.754
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":11,"skipped":154,"failed":0}
------------------------------
• [SLOW TEST] [5.344 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:32:31.421
    Jan 19 20:32:31.422: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename init-container 01/19/23 20:32:31.422
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:31.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:31.46
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 01/19/23 20:32:31.462
    Jan 19 20:32:31.462: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 20:32:36.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-8320" for this suite. 01/19/23 20:32:36.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:32:36.766
Jan 19 20:32:36.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 20:32:36.768
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:36.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:36.824
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 01/19/23 20:32:36.833
Jan 19 20:32:36.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d" in namespace "downward-api-434" to be "Succeeded or Failed"
Jan 19 20:32:36.863: INFO: Pod "downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.098766ms
Jan 19 20:32:38.865: INFO: Pod "downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01471261s
Jan 19 20:32:40.866: INFO: Pod "downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015014831s
STEP: Saw pod success 01/19/23 20:32:40.866
Jan 19 20:32:40.866: INFO: Pod "downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d" satisfied condition "Succeeded or Failed"
Jan 19 20:32:40.868: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d container client-container: <nil>
STEP: delete the pod 01/19/23 20:32:40.885
Jan 19 20:32:40.895: INFO: Waiting for pod downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d to disappear
Jan 19 20:32:40.897: INFO: Pod downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 20:32:40.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-434" for this suite. 01/19/23 20:32:40.904
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":12,"skipped":159,"failed":0}
------------------------------
• [4.145 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:32:36.766
    Jan 19 20:32:36.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 20:32:36.768
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:36.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:36.824
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 01/19/23 20:32:36.833
    Jan 19 20:32:36.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d" in namespace "downward-api-434" to be "Succeeded or Failed"
    Jan 19 20:32:36.863: INFO: Pod "downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d": Phase="Pending", Reason="", readiness=false. Elapsed: 12.098766ms
    Jan 19 20:32:38.865: INFO: Pod "downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01471261s
    Jan 19 20:32:40.866: INFO: Pod "downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015014831s
    STEP: Saw pod success 01/19/23 20:32:40.866
    Jan 19 20:32:40.866: INFO: Pod "downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d" satisfied condition "Succeeded or Failed"
    Jan 19 20:32:40.868: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d container client-container: <nil>
    STEP: delete the pod 01/19/23 20:32:40.885
    Jan 19 20:32:40.895: INFO: Waiting for pod downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d to disappear
    Jan 19 20:32:40.897: INFO: Pod downwardapi-volume-f4a2e0c3-06fa-447d-8b7a-38f07617500d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 20:32:40.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-434" for this suite. 01/19/23 20:32:40.904
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:32:40.912
Jan 19 20:32:40.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename svc-latency 01/19/23 20:32:40.912
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:40.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:40.939
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan 19 20:32:40.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: creating replication controller svc-latency-rc in namespace svc-latency-556 01/19/23 20:32:40.946
W0119 20:32:40.958856      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0119 20:32:40.959035      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-556, replica count: 1
I0119 20:32:42.010163      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 20:32:43.010323      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 20:32:43.121: INFO: Created: latency-svc-rdzrk
Jan 19 20:32:43.126: INFO: Got endpoints: latency-svc-rdzrk [15.528689ms]
Jan 19 20:32:43.154: INFO: Created: latency-svc-td79k
Jan 19 20:32:43.165: INFO: Created: latency-svc-s4gzl
Jan 19 20:32:43.165: INFO: Got endpoints: latency-svc-td79k [39.428179ms]
Jan 19 20:32:43.180: INFO: Got endpoints: latency-svc-s4gzl [53.787295ms]
Jan 19 20:32:43.186: INFO: Created: latency-svc-4vhp6
Jan 19 20:32:43.207: INFO: Got endpoints: latency-svc-4vhp6 [80.978951ms]
Jan 19 20:32:43.210: INFO: Created: latency-svc-8dfvr
Jan 19 20:32:43.229: INFO: Got endpoints: latency-svc-8dfvr [103.107872ms]
Jan 19 20:32:43.243: INFO: Created: latency-svc-b54j2
Jan 19 20:32:43.266: INFO: Got endpoints: latency-svc-b54j2 [139.398932ms]
Jan 19 20:32:43.277: INFO: Created: latency-svc-9b4tx
Jan 19 20:32:43.292: INFO: Got endpoints: latency-svc-9b4tx [165.197142ms]
Jan 19 20:32:43.292: INFO: Created: latency-svc-gdwwq
Jan 19 20:32:43.303: INFO: Got endpoints: latency-svc-gdwwq [176.600383ms]
Jan 19 20:32:43.305: INFO: Created: latency-svc-5bcln
Jan 19 20:32:43.314: INFO: Got endpoints: latency-svc-5bcln [187.211607ms]
Jan 19 20:32:43.314: INFO: Created: latency-svc-qfxvx
Jan 19 20:32:43.325: INFO: Got endpoints: latency-svc-qfxvx [198.13625ms]
Jan 19 20:32:43.325: INFO: Created: latency-svc-vl9fb
Jan 19 20:32:43.332: INFO: Got endpoints: latency-svc-vl9fb [205.146756ms]
Jan 19 20:32:43.342: INFO: Created: latency-svc-h66g2
Jan 19 20:32:43.348: INFO: Got endpoints: latency-svc-h66g2 [221.318771ms]
Jan 19 20:32:43.348: INFO: Created: latency-svc-8hfxm
Jan 19 20:32:43.352: INFO: Got endpoints: latency-svc-8hfxm [225.707864ms]
Jan 19 20:32:43.362: INFO: Created: latency-svc-hkrj2
Jan 19 20:32:43.365: INFO: Got endpoints: latency-svc-hkrj2 [237.981002ms]
Jan 19 20:32:43.367: INFO: Created: latency-svc-ks4wt
Jan 19 20:32:43.374: INFO: Got endpoints: latency-svc-ks4wt [247.455415ms]
Jan 19 20:32:43.377: INFO: Created: latency-svc-9c4rg
Jan 19 20:32:43.382: INFO: Got endpoints: latency-svc-9c4rg [255.166674ms]
Jan 19 20:32:43.387: INFO: Created: latency-svc-8x8g2
Jan 19 20:32:43.394: INFO: Got endpoints: latency-svc-8x8g2 [228.350017ms]
Jan 19 20:32:43.410: INFO: Created: latency-svc-kzl8v
Jan 19 20:32:43.412: INFO: Created: latency-svc-chfpd
Jan 19 20:32:43.414: INFO: Got endpoints: latency-svc-kzl8v [233.767808ms]
Jan 19 20:32:43.422: INFO: Got endpoints: latency-svc-chfpd [214.763442ms]
Jan 19 20:32:43.425: INFO: Created: latency-svc-pvqvk
Jan 19 20:32:43.431: INFO: Got endpoints: latency-svc-pvqvk [201.914165ms]
Jan 19 20:32:43.437: INFO: Created: latency-svc-cqf52
Jan 19 20:32:43.441: INFO: Created: latency-svc-gq9kj
Jan 19 20:32:43.443: INFO: Got endpoints: latency-svc-cqf52 [177.000658ms]
Jan 19 20:32:43.445: INFO: Created: latency-svc-jsd88
Jan 19 20:32:43.448: INFO: Got endpoints: latency-svc-gq9kj [155.910421ms]
Jan 19 20:32:43.455: INFO: Got endpoints: latency-svc-jsd88 [151.775337ms]
Jan 19 20:32:43.465: INFO: Created: latency-svc-w6x27
Jan 19 20:32:43.469: INFO: Got endpoints: latency-svc-w6x27 [154.79563ms]
Jan 19 20:32:43.470: INFO: Created: latency-svc-cvgjl
Jan 19 20:32:43.483: INFO: Got endpoints: latency-svc-cvgjl [157.891509ms]
Jan 19 20:32:43.489: INFO: Created: latency-svc-gxccw
Jan 19 20:32:43.501: INFO: Got endpoints: latency-svc-gxccw [169.596463ms]
Jan 19 20:32:43.509: INFO: Created: latency-svc-shgfn
Jan 19 20:32:43.513: INFO: Got endpoints: latency-svc-shgfn [165.075683ms]
Jan 19 20:32:43.517: INFO: Created: latency-svc-629p9
Jan 19 20:32:43.522: INFO: Got endpoints: latency-svc-629p9 [169.909911ms]
Jan 19 20:32:43.525: INFO: Created: latency-svc-9bqrt
Jan 19 20:32:43.530: INFO: Got endpoints: latency-svc-9bqrt [165.522476ms]
Jan 19 20:32:43.542: INFO: Created: latency-svc-xhnmc
Jan 19 20:32:43.551: INFO: Got endpoints: latency-svc-xhnmc [176.470247ms]
Jan 19 20:32:43.555: INFO: Created: latency-svc-bdkj8
Jan 19 20:32:43.562: INFO: Got endpoints: latency-svc-bdkj8 [179.8971ms]
Jan 19 20:32:43.563: INFO: Created: latency-svc-dbsfb
Jan 19 20:32:43.569: INFO: Got endpoints: latency-svc-dbsfb [175.553399ms]
Jan 19 20:32:43.572: INFO: Created: latency-svc-rtcgm
Jan 19 20:32:43.579: INFO: Got endpoints: latency-svc-rtcgm [164.653303ms]
Jan 19 20:32:43.583: INFO: Created: latency-svc-9p28m
Jan 19 20:32:43.592: INFO: Got endpoints: latency-svc-9p28m [170.054548ms]
Jan 19 20:32:43.596: INFO: Created: latency-svc-g7ntg
Jan 19 20:32:43.605: INFO: Created: latency-svc-sdbrt
Jan 19 20:32:43.607: INFO: Got endpoints: latency-svc-g7ntg [175.474907ms]
Jan 19 20:32:43.612: INFO: Got endpoints: latency-svc-sdbrt [168.488632ms]
Jan 19 20:32:43.616: INFO: Created: latency-svc-p9667
Jan 19 20:32:43.621: INFO: Got endpoints: latency-svc-p9667 [173.381518ms]
Jan 19 20:32:43.633: INFO: Created: latency-svc-fbh7v
Jan 19 20:32:43.639: INFO: Got endpoints: latency-svc-fbh7v [183.929244ms]
Jan 19 20:32:43.643: INFO: Created: latency-svc-97qz8
Jan 19 20:32:43.651: INFO: Got endpoints: latency-svc-97qz8 [181.963466ms]
Jan 19 20:32:43.651: INFO: Created: latency-svc-c8b55
Jan 19 20:32:43.655: INFO: Created: latency-svc-tlh87
Jan 19 20:32:43.663: INFO: Got endpoints: latency-svc-c8b55 [179.489246ms]
Jan 19 20:32:43.664: INFO: Got endpoints: latency-svc-tlh87 [163.113735ms]
Jan 19 20:32:43.676: INFO: Created: latency-svc-jlmnx
Jan 19 20:32:43.690: INFO: Got endpoints: latency-svc-jlmnx [176.541158ms]
Jan 19 20:32:43.695: INFO: Created: latency-svc-cvsxl
Jan 19 20:32:43.703: INFO: Created: latency-svc-4ltrv
Jan 19 20:32:43.710: INFO: Got endpoints: latency-svc-cvsxl [187.171371ms]
Jan 19 20:32:43.710: INFO: Got endpoints: latency-svc-4ltrv [179.332618ms]
Jan 19 20:32:43.719: INFO: Created: latency-svc-g27tb
Jan 19 20:32:43.732: INFO: Got endpoints: latency-svc-g27tb [180.983107ms]
Jan 19 20:32:43.760: INFO: Created: latency-svc-pgsx4
Jan 19 20:32:43.767: INFO: Got endpoints: latency-svc-pgsx4 [205.3198ms]
Jan 19 20:32:43.772: INFO: Created: latency-svc-79kd7
Jan 19 20:32:43.779: INFO: Got endpoints: latency-svc-79kd7 [209.500031ms]
Jan 19 20:32:43.789: INFO: Created: latency-svc-sn9b2
Jan 19 20:32:43.805: INFO: Got endpoints: latency-svc-sn9b2 [226.196062ms]
Jan 19 20:32:43.810: INFO: Created: latency-svc-lzlkq
Jan 19 20:32:43.817: INFO: Got endpoints: latency-svc-lzlkq [225.123028ms]
Jan 19 20:32:43.826: INFO: Created: latency-svc-zrnbn
Jan 19 20:32:43.832: INFO: Got endpoints: latency-svc-zrnbn [225.285153ms]
Jan 19 20:32:43.843: INFO: Created: latency-svc-2dwsc
Jan 19 20:32:43.853: INFO: Got endpoints: latency-svc-2dwsc [241.02005ms]
Jan 19 20:32:43.863: INFO: Created: latency-svc-hdrf9
Jan 19 20:32:43.872: INFO: Got endpoints: latency-svc-hdrf9 [251.026191ms]
Jan 19 20:32:43.875: INFO: Created: latency-svc-wrhm8
Jan 19 20:32:43.884: INFO: Got endpoints: latency-svc-wrhm8 [245.237404ms]
Jan 19 20:32:43.893: INFO: Created: latency-svc-vt5b9
Jan 19 20:32:43.896: INFO: Created: latency-svc-5n6vg
Jan 19 20:32:43.907: INFO: Got endpoints: latency-svc-vt5b9 [256.157065ms]
Jan 19 20:32:43.910: INFO: Got endpoints: latency-svc-5n6vg [247.011916ms]
Jan 19 20:32:43.920: INFO: Created: latency-svc-dg26f
Jan 19 20:32:43.931: INFO: Created: latency-svc-2m89z
Jan 19 20:32:43.937: INFO: Got endpoints: latency-svc-dg26f [272.768519ms]
Jan 19 20:32:43.940: INFO: Got endpoints: latency-svc-2m89z [250.43373ms]
Jan 19 20:32:43.953: INFO: Created: latency-svc-jwl87
Jan 19 20:32:43.959: INFO: Got endpoints: latency-svc-jwl87 [249.359104ms]
Jan 19 20:32:43.959: INFO: Created: latency-svc-qx4fw
Jan 19 20:32:43.964: INFO: Got endpoints: latency-svc-qx4fw [254.305288ms]
Jan 19 20:32:43.979: INFO: Created: latency-svc-wzbg8
Jan 19 20:32:43.990: INFO: Created: latency-svc-bjgll
Jan 19 20:32:44.005: INFO: Got endpoints: latency-svc-wzbg8 [273.1688ms]
Jan 19 20:32:44.009: INFO: Got endpoints: latency-svc-bjgll [241.768364ms]
Jan 19 20:32:44.021: INFO: Created: latency-svc-jvrsr
Jan 19 20:32:44.032: INFO: Got endpoints: latency-svc-jvrsr [253.051315ms]
Jan 19 20:32:44.033: INFO: Created: latency-svc-7vst5
Jan 19 20:32:44.043: INFO: Got endpoints: latency-svc-7vst5 [238.103667ms]
Jan 19 20:32:44.045: INFO: Created: latency-svc-46xcn
Jan 19 20:32:44.053: INFO: Created: latency-svc-xxp2r
Jan 19 20:32:44.070: INFO: Got endpoints: latency-svc-46xcn [252.856269ms]
Jan 19 20:32:44.076: INFO: Got endpoints: latency-svc-xxp2r [243.569429ms]
Jan 19 20:32:44.119: INFO: Created: latency-svc-7gs9m
Jan 19 20:32:44.129: INFO: Created: latency-svc-9zwgc
Jan 19 20:32:44.130: INFO: Got endpoints: latency-svc-7gs9m [277.179311ms]
Jan 19 20:32:44.146: INFO: Created: latency-svc-7s8sl
Jan 19 20:32:44.159: INFO: Got endpoints: latency-svc-9zwgc [286.886715ms]
Jan 19 20:32:44.161: INFO: Created: latency-svc-cmm26
Jan 19 20:32:44.161: INFO: Got endpoints: latency-svc-7s8sl [277.291228ms]
Jan 19 20:32:44.172: INFO: Got endpoints: latency-svc-cmm26 [264.709898ms]
Jan 19 20:32:44.219: INFO: Created: latency-svc-xqcfw
Jan 19 20:32:44.227: INFO: Created: latency-svc-kcbzg
Jan 19 20:32:44.235: INFO: Got endpoints: latency-svc-xqcfw [325.125873ms]
Jan 19 20:32:44.259: INFO: Created: latency-svc-69294
Jan 19 20:32:44.268: INFO: Created: latency-svc-bkclk
Jan 19 20:32:44.272: INFO: Got endpoints: latency-svc-69294 [331.95297ms]
Jan 19 20:32:44.272: INFO: Got endpoints: latency-svc-bkclk [313.407603ms]
Jan 19 20:32:44.280: INFO: Got endpoints: latency-svc-kcbzg [342.645677ms]
Jan 19 20:32:44.308: INFO: Created: latency-svc-s7ztb
Jan 19 20:32:44.316: INFO: Created: latency-svc-z6mvv
Jan 19 20:32:44.321: INFO: Got endpoints: latency-svc-s7ztb [357.370419ms]
Jan 19 20:32:44.331: INFO: Got endpoints: latency-svc-z6mvv [325.611543ms]
Jan 19 20:32:44.340: INFO: Created: latency-svc-jcq8r
Jan 19 20:32:44.347: INFO: Created: latency-svc-5c7kv
Jan 19 20:32:44.348: INFO: Got endpoints: latency-svc-jcq8r [339.348851ms]
Jan 19 20:32:44.358: INFO: Got endpoints: latency-svc-5c7kv [325.805798ms]
Jan 19 20:32:44.366: INFO: Created: latency-svc-hs9pz
Jan 19 20:32:44.372: INFO: Got endpoints: latency-svc-hs9pz [328.95533ms]
Jan 19 20:32:44.374: INFO: Created: latency-svc-ttqlk
Jan 19 20:32:44.384: INFO: Got endpoints: latency-svc-ttqlk [313.534253ms]
Jan 19 20:32:44.407: INFO: Created: latency-svc-w42hz
Jan 19 20:32:44.415: INFO: Got endpoints: latency-svc-w42hz [339.049901ms]
Jan 19 20:32:44.420: INFO: Created: latency-svc-75zzc
Jan 19 20:32:44.430: INFO: Got endpoints: latency-svc-75zzc [300.641025ms]
Jan 19 20:32:44.441: INFO: Created: latency-svc-qbmkm
Jan 19 20:32:44.441: INFO: Created: latency-svc-pbv2x
Jan 19 20:32:44.457: INFO: Got endpoints: latency-svc-pbv2x [297.522757ms]
Jan 19 20:32:44.457: INFO: Got endpoints: latency-svc-qbmkm [295.379449ms]
Jan 19 20:32:44.463: INFO: Created: latency-svc-pbf7q
Jan 19 20:32:44.472: INFO: Got endpoints: latency-svc-pbf7q [300.12786ms]
Jan 19 20:32:44.480: INFO: Created: latency-svc-nrqpl
Jan 19 20:32:44.489: INFO: Got endpoints: latency-svc-nrqpl [254.590288ms]
Jan 19 20:32:44.507: INFO: Created: latency-svc-qp9ts
Jan 19 20:32:44.520: INFO: Got endpoints: latency-svc-qp9ts [248.410012ms]
Jan 19 20:32:44.527: INFO: Created: latency-svc-mrqvp
Jan 19 20:32:44.535: INFO: Got endpoints: latency-svc-mrqvp [263.049713ms]
Jan 19 20:32:44.539: INFO: Created: latency-svc-87pwn
Jan 19 20:32:44.549: INFO: Created: latency-svc-m8wvp
Jan 19 20:32:44.549: INFO: Got endpoints: latency-svc-87pwn [269.38942ms]
Jan 19 20:32:44.557: INFO: Got endpoints: latency-svc-m8wvp [235.305532ms]
Jan 19 20:32:44.560: INFO: Created: latency-svc-82jx2
Jan 19 20:32:44.567: INFO: Got endpoints: latency-svc-82jx2 [235.731747ms]
Jan 19 20:32:44.810: INFO: Created: latency-svc-bb4rr
Jan 19 20:32:44.811: INFO: Created: latency-svc-rpqjj
Jan 19 20:32:44.812: INFO: Created: latency-svc-nkfl5
Jan 19 20:32:44.813: INFO: Created: latency-svc-sctw5
Jan 19 20:32:44.815: INFO: Created: latency-svc-xmxn4
Jan 19 20:32:44.815: INFO: Created: latency-svc-x2pbb
Jan 19 20:32:44.815: INFO: Created: latency-svc-6crpm
Jan 19 20:32:44.815: INFO: Created: latency-svc-rg578
Jan 19 20:32:44.815: INFO: Created: latency-svc-6mlk9
Jan 19 20:32:44.816: INFO: Created: latency-svc-v4cjx
Jan 19 20:32:44.816: INFO: Created: latency-svc-fzw2p
Jan 19 20:32:44.816: INFO: Created: latency-svc-vlfzz
Jan 19 20:32:44.822: INFO: Created: latency-svc-s85mq
Jan 19 20:32:44.822: INFO: Created: latency-svc-6252m
Jan 19 20:32:44.824: INFO: Got endpoints: latency-svc-rpqjj [440.338784ms]
Jan 19 20:32:44.824: INFO: Got endpoints: latency-svc-bb4rr [409.203971ms]
Jan 19 20:32:44.827: INFO: Got endpoints: latency-svc-nkfl5 [454.778514ms]
Jan 19 20:32:44.827: INFO: Got endpoints: latency-svc-vlfzz [355.153217ms]
Jan 19 20:32:44.827: INFO: Got endpoints: latency-svc-v4cjx [270.375848ms]
Jan 19 20:32:44.832: INFO: Created: latency-svc-5scqv
Jan 19 20:32:44.837: INFO: Got endpoints: latency-svc-sctw5 [316.332926ms]
Jan 19 20:32:44.840: INFO: Got endpoints: latency-svc-xmxn4 [290.577065ms]
Jan 19 20:32:44.840: INFO: Got endpoints: latency-svc-fzw2p [409.46397ms]
Jan 19 20:32:44.841: INFO: Got endpoints: latency-svc-rg578 [483.138245ms]
Jan 19 20:32:44.841: INFO: Got endpoints: latency-svc-x2pbb [274.579382ms]
Jan 19 20:32:44.844: INFO: Got endpoints: latency-svc-6mlk9 [308.306382ms]
Jan 19 20:32:44.864: INFO: Got endpoints: latency-svc-6crpm [374.973546ms]
Jan 19 20:32:44.865: INFO: Got endpoints: latency-svc-s85mq [516.012504ms]
Jan 19 20:32:44.865: INFO: Got endpoints: latency-svc-6252m [407.756061ms]
Jan 19 20:32:44.868: INFO: Got endpoints: latency-svc-5scqv [411.19767ms]
Jan 19 20:32:44.884: INFO: Created: latency-svc-cgc2v
Jan 19 20:32:44.896: INFO: Got endpoints: latency-svc-cgc2v [72.249813ms]
Jan 19 20:32:44.925: INFO: Created: latency-svc-bgln2
Jan 19 20:32:44.925: INFO: Got endpoints: latency-svc-bgln2 [97.700736ms]
Jan 19 20:32:44.936: INFO: Created: latency-svc-b7wmp
Jan 19 20:32:44.942: INFO: Got endpoints: latency-svc-b7wmp [114.489347ms]
Jan 19 20:32:44.945: INFO: Created: latency-svc-nvks8
Jan 19 20:32:44.954: INFO: Got endpoints: latency-svc-nvks8 [127.311886ms]
Jan 19 20:32:44.962: INFO: Created: latency-svc-dzvc5
Jan 19 20:32:44.972: INFO: Got endpoints: latency-svc-dzvc5 [148.335042ms]
Jan 19 20:32:44.979: INFO: Created: latency-svc-6h4sr
Jan 19 20:32:44.990: INFO: Got endpoints: latency-svc-6h4sr [152.949713ms]
Jan 19 20:32:45.009: INFO: Created: latency-svc-d68k7
Jan 19 20:32:45.030: INFO: Got endpoints: latency-svc-d68k7 [189.815151ms]
Jan 19 20:32:45.030: INFO: Created: latency-svc-js9nf
Jan 19 20:32:45.044: INFO: Created: latency-svc-b6pgn
Jan 19 20:32:45.044: INFO: Got endpoints: latency-svc-js9nf [204.410833ms]
Jan 19 20:32:45.052: INFO: Got endpoints: latency-svc-b6pgn [210.714788ms]
Jan 19 20:32:45.059: INFO: Created: latency-svc-mlvfw
Jan 19 20:32:45.063: INFO: Got endpoints: latency-svc-mlvfw [222.07929ms]
Jan 19 20:32:45.071: INFO: Created: latency-svc-8rwpb
Jan 19 20:32:45.080: INFO: Got endpoints: latency-svc-8rwpb [235.98249ms]
Jan 19 20:32:45.088: INFO: Created: latency-svc-v9lm6
Jan 19 20:32:45.107: INFO: Got endpoints: latency-svc-v9lm6 [242.341556ms]
Jan 19 20:32:45.116: INFO: Created: latency-svc-hwpms
Jan 19 20:32:45.121: INFO: Got endpoints: latency-svc-hwpms [256.047569ms]
Jan 19 20:32:45.127: INFO: Created: latency-svc-74d6s
Jan 19 20:32:45.135: INFO: Got endpoints: latency-svc-74d6s [270.791271ms]
Jan 19 20:32:45.137: INFO: Created: latency-svc-snm92
Jan 19 20:32:45.145: INFO: Got endpoints: latency-svc-snm92 [277.042888ms]
Jan 19 20:32:45.149: INFO: Created: latency-svc-hflf8
Jan 19 20:32:45.155: INFO: Got endpoints: latency-svc-hflf8 [258.345156ms]
Jan 19 20:32:45.165: INFO: Created: latency-svc-5w9tj
Jan 19 20:32:45.171: INFO: Got endpoints: latency-svc-5w9tj [246.24937ms]
Jan 19 20:32:45.172: INFO: Created: latency-svc-rz9hj
Jan 19 20:32:45.183: INFO: Got endpoints: latency-svc-rz9hj [241.293056ms]
Jan 19 20:32:45.184: INFO: Created: latency-svc-9829l
Jan 19 20:32:45.192: INFO: Got endpoints: latency-svc-9829l [237.395205ms]
Jan 19 20:32:45.199: INFO: Created: latency-svc-2std2
Jan 19 20:32:45.205: INFO: Got endpoints: latency-svc-2std2 [232.050148ms]
Jan 19 20:32:45.212: INFO: Created: latency-svc-ts8xk
Jan 19 20:32:45.213: INFO: Created: latency-svc-99dl9
Jan 19 20:32:45.217: INFO: Got endpoints: latency-svc-99dl9 [227.430095ms]
Jan 19 20:32:45.226: INFO: Created: latency-svc-9fbrw
Jan 19 20:32:45.231: INFO: Got endpoints: latency-svc-ts8xk [201.670534ms]
Jan 19 20:32:45.232: INFO: Got endpoints: latency-svc-9fbrw [187.485211ms]
Jan 19 20:32:45.239: INFO: Created: latency-svc-2mr4s
Jan 19 20:32:45.243: INFO: Got endpoints: latency-svc-2mr4s [191.589516ms]
Jan 19 20:32:45.258: INFO: Created: latency-svc-w5slw
Jan 19 20:32:45.262: INFO: Created: latency-svc-52nmf
Jan 19 20:32:45.263: INFO: Got endpoints: latency-svc-w5slw [199.898692ms]
Jan 19 20:32:45.270: INFO: Got endpoints: latency-svc-52nmf [190.474455ms]
Jan 19 20:32:45.275: INFO: Created: latency-svc-966xl
Jan 19 20:32:45.283: INFO: Got endpoints: latency-svc-966xl [175.883041ms]
Jan 19 20:32:45.289: INFO: Created: latency-svc-cd62m
Jan 19 20:32:45.295: INFO: Got endpoints: latency-svc-cd62m [174.251471ms]
Jan 19 20:32:45.300: INFO: Created: latency-svc-j2kcn
Jan 19 20:32:45.311: INFO: Got endpoints: latency-svc-j2kcn [175.284763ms]
Jan 19 20:32:45.311: INFO: Created: latency-svc-df6tl
Jan 19 20:32:45.319: INFO: Got endpoints: latency-svc-df6tl [173.58839ms]
Jan 19 20:32:45.321: INFO: Created: latency-svc-x9xkd
Jan 19 20:32:45.328: INFO: Got endpoints: latency-svc-x9xkd [172.804206ms]
Jan 19 20:32:45.332: INFO: Created: latency-svc-fkmkh
Jan 19 20:32:45.339: INFO: Got endpoints: latency-svc-fkmkh [168.218754ms]
Jan 19 20:32:45.343: INFO: Created: latency-svc-tzbxg
Jan 19 20:32:45.354: INFO: Got endpoints: latency-svc-tzbxg [170.968926ms]
Jan 19 20:32:45.354: INFO: Created: latency-svc-m6rdw
Jan 19 20:32:45.364: INFO: Got endpoints: latency-svc-m6rdw [171.937677ms]
Jan 19 20:32:45.367: INFO: Created: latency-svc-92sb4
Jan 19 20:32:45.374: INFO: Got endpoints: latency-svc-92sb4 [169.396503ms]
Jan 19 20:32:45.376: INFO: Created: latency-svc-cg8z9
Jan 19 20:32:45.383: INFO: Got endpoints: latency-svc-cg8z9 [165.486223ms]
Jan 19 20:32:45.390: INFO: Created: latency-svc-gx6tm
Jan 19 20:32:45.403: INFO: Got endpoints: latency-svc-gx6tm [171.161369ms]
Jan 19 20:32:45.403: INFO: Created: latency-svc-fjvgp
Jan 19 20:32:45.408: INFO: Got endpoints: latency-svc-fjvgp [176.3752ms]
Jan 19 20:32:45.415: INFO: Created: latency-svc-wxkvt
Jan 19 20:32:45.421: INFO: Got endpoints: latency-svc-wxkvt [177.720112ms]
Jan 19 20:32:45.430: INFO: Created: latency-svc-hqnrf
Jan 19 20:32:45.436: INFO: Got endpoints: latency-svc-hqnrf [172.730706ms]
Jan 19 20:32:45.443: INFO: Created: latency-svc-qpgjd
Jan 19 20:32:45.445: INFO: Created: latency-svc-qkfpc
Jan 19 20:32:45.446: INFO: Got endpoints: latency-svc-qpgjd [175.548171ms]
Jan 19 20:32:45.452: INFO: Got endpoints: latency-svc-qkfpc [169.132441ms]
Jan 19 20:32:45.458: INFO: Created: latency-svc-2xcbp
Jan 19 20:32:45.466: INFO: Got endpoints: latency-svc-2xcbp [171.060732ms]
Jan 19 20:32:45.575: INFO: Created: latency-svc-dfvjv
Jan 19 20:32:45.576: INFO: Created: latency-svc-5qpbx
Jan 19 20:32:45.576: INFO: Created: latency-svc-25wlc
Jan 19 20:32:45.576: INFO: Created: latency-svc-9fr86
Jan 19 20:32:45.576: INFO: Created: latency-svc-7rxrh
Jan 19 20:32:45.577: INFO: Created: latency-svc-895xv
Jan 19 20:32:45.577: INFO: Created: latency-svc-vgqh6
Jan 19 20:32:45.577: INFO: Created: latency-svc-jc2j5
Jan 19 20:32:45.577: INFO: Created: latency-svc-jtrzq
Jan 19 20:32:45.577: INFO: Created: latency-svc-2vlpd
Jan 19 20:32:45.579: INFO: Created: latency-svc-jm5mp
Jan 19 20:32:45.579: INFO: Created: latency-svc-crb6g
Jan 19 20:32:45.579: INFO: Created: latency-svc-ngrqt
Jan 19 20:32:45.580: INFO: Created: latency-svc-cvrsv
Jan 19 20:32:45.580: INFO: Created: latency-svc-hxx57
Jan 19 20:32:45.586: INFO: Got endpoints: latency-svc-895xv [150.075863ms]
Jan 19 20:32:45.591: INFO: Got endpoints: latency-svc-5qpbx [125.294862ms]
Jan 19 20:32:45.595: INFO: Got endpoints: latency-svc-dfvjv [276.039939ms]
Jan 19 20:32:45.595: INFO: Got endpoints: latency-svc-hxx57 [284.133329ms]
Jan 19 20:32:45.596: INFO: Got endpoints: latency-svc-cvrsv [242.103295ms]
Jan 19 20:32:45.596: INFO: Got endpoints: latency-svc-jm5mp [144.300334ms]
Jan 19 20:32:45.600: INFO: Got endpoints: latency-svc-2vlpd [260.595992ms]
Jan 19 20:32:45.604: INFO: Got endpoints: latency-svc-jtrzq [275.896107ms]
Jan 19 20:32:45.608: INFO: Got endpoints: latency-svc-vgqh6 [243.913835ms]
Jan 19 20:32:45.608: INFO: Got endpoints: latency-svc-ngrqt [233.806535ms]
Jan 19 20:32:45.611: INFO: Got endpoints: latency-svc-7rxrh [203.08489ms]
Jan 19 20:32:45.611: INFO: Created: latency-svc-mvnvb
Jan 19 20:32:45.613: INFO: Got endpoints: latency-svc-crb6g [166.771457ms]
Jan 19 20:32:45.613: INFO: Got endpoints: latency-svc-9fr86 [191.527714ms]
Jan 19 20:32:45.619: INFO: Got endpoints: latency-svc-jc2j5 [215.831057ms]
Jan 19 20:32:45.621: INFO: Got endpoints: latency-svc-25wlc [238.103394ms]
Jan 19 20:32:45.621: INFO: Got endpoints: latency-svc-mvnvb [34.642448ms]
Jan 19 20:32:45.640: INFO: Created: latency-svc-s5wzw
Jan 19 20:32:45.640: INFO: Got endpoints: latency-svc-s5wzw [48.931489ms]
Jan 19 20:32:45.646: INFO: Created: latency-svc-5wwwm
Jan 19 20:32:45.651: INFO: Got endpoints: latency-svc-5wwwm [55.610716ms]
Jan 19 20:32:45.654: INFO: Created: latency-svc-8ptl6
Jan 19 20:32:45.665: INFO: Got endpoints: latency-svc-8ptl6 [68.357933ms]
Jan 19 20:32:45.671: INFO: Created: latency-svc-ncjq8
Jan 19 20:32:45.679: INFO: Got endpoints: latency-svc-ncjq8 [84.241862ms]
Jan 19 20:32:45.702: INFO: Created: latency-svc-87nqw
Jan 19 20:32:45.710: INFO: Got endpoints: latency-svc-87nqw [113.766209ms]
Jan 19 20:32:45.718: INFO: Created: latency-svc-crh2r
Jan 19 20:32:45.728: INFO: Got endpoints: latency-svc-crh2r [128.127674ms]
Jan 19 20:32:45.733: INFO: Created: latency-svc-b2pv2
Jan 19 20:32:45.739: INFO: Got endpoints: latency-svc-b2pv2 [135.755098ms]
Jan 19 20:32:45.742: INFO: Created: latency-svc-wcm5c
Jan 19 20:32:45.749: INFO: Got endpoints: latency-svc-wcm5c [141.095336ms]
Jan 19 20:32:45.765: INFO: Created: latency-svc-scfwq
Jan 19 20:32:45.765: INFO: Got endpoints: latency-svc-scfwq [157.273166ms]
Jan 19 20:32:45.780: INFO: Created: latency-svc-vdd2m
Jan 19 20:32:45.786: INFO: Created: latency-svc-lc89v
Jan 19 20:32:45.787: INFO: Got endpoints: latency-svc-vdd2m [175.45206ms]
Jan 19 20:32:45.801: INFO: Got endpoints: latency-svc-lc89v [188.368465ms]
Jan 19 20:32:45.807: INFO: Created: latency-svc-mxwxd
Jan 19 20:32:45.815: INFO: Got endpoints: latency-svc-mxwxd [202.205422ms]
Jan 19 20:32:45.815: INFO: Created: latency-svc-ztzfj
Jan 19 20:32:45.823: INFO: Got endpoints: latency-svc-ztzfj [204.123902ms]
Jan 19 20:32:45.835: INFO: Created: latency-svc-gw2sq
Jan 19 20:32:45.849: INFO: Got endpoints: latency-svc-gw2sq [228.081063ms]
Jan 19 20:32:45.849: INFO: Created: latency-svc-bchts
Jan 19 20:32:45.854: INFO: Got endpoints: latency-svc-bchts [232.794728ms]
Jan 19 20:32:45.858: INFO: Created: latency-svc-54crs
Jan 19 20:32:45.864: INFO: Got endpoints: latency-svc-54crs [223.612063ms]
Jan 19 20:32:45.866: INFO: Created: latency-svc-tqtmx
Jan 19 20:32:45.871: INFO: Got endpoints: latency-svc-tqtmx [220.527316ms]
Jan 19 20:32:45.879: INFO: Created: latency-svc-f7rcw
Jan 19 20:32:45.885: INFO: Got endpoints: latency-svc-f7rcw [220.743229ms]
Jan 19 20:32:45.887: INFO: Created: latency-svc-px2sg
Jan 19 20:32:45.894: INFO: Got endpoints: latency-svc-px2sg [215.016336ms]
Jan 19 20:32:45.904: INFO: Created: latency-svc-smqgb
Jan 19 20:32:45.908: INFO: Got endpoints: latency-svc-smqgb [198.063195ms]
Jan 19 20:32:45.913: INFO: Created: latency-svc-cf6fl
Jan 19 20:32:45.921: INFO: Got endpoints: latency-svc-cf6fl [192.974542ms]
Jan 19 20:32:45.926: INFO: Created: latency-svc-s8lwv
Jan 19 20:32:45.933: INFO: Created: latency-svc-vr9hj
Jan 19 20:32:45.934: INFO: Got endpoints: latency-svc-s8lwv [195.049699ms]
Jan 19 20:32:45.947: INFO: Got endpoints: latency-svc-vr9hj [198.61199ms]
Jan 19 20:32:45.953: INFO: Created: latency-svc-mzvjm
Jan 19 20:32:45.966: INFO: Got endpoints: latency-svc-mzvjm [200.914004ms]
Jan 19 20:32:45.972: INFO: Created: latency-svc-mfspf
Jan 19 20:32:45.986: INFO: Created: latency-svc-2ctnn
Jan 19 20:32:45.990: INFO: Got endpoints: latency-svc-mfspf [203.232138ms]
Jan 19 20:32:45.993: INFO: Created: latency-svc-4g7vw
Jan 19 20:32:45.994: INFO: Got endpoints: latency-svc-2ctnn [192.787875ms]
Jan 19 20:32:46.005: INFO: Got endpoints: latency-svc-4g7vw [190.013257ms]
Jan 19 20:32:46.012: INFO: Created: latency-svc-btnmj
Jan 19 20:32:46.018: INFO: Got endpoints: latency-svc-btnmj [195.289826ms]
Jan 19 20:32:46.024: INFO: Created: latency-svc-9tbdr
Jan 19 20:32:46.039: INFO: Got endpoints: latency-svc-9tbdr [190.499573ms]
Jan 19 20:32:46.042: INFO: Created: latency-svc-t9qb6
Jan 19 20:32:46.043: INFO: Got endpoints: latency-svc-t9qb6 [189.620469ms]
Jan 19 20:32:46.046: INFO: Created: latency-svc-zmbjv
Jan 19 20:32:46.058: INFO: Got endpoints: latency-svc-zmbjv [194.365501ms]
Jan 19 20:32:46.059: INFO: Created: latency-svc-b2s46
Jan 19 20:32:46.066: INFO: Got endpoints: latency-svc-b2s46 [194.417409ms]
Jan 19 20:32:46.074: INFO: Created: latency-svc-jgvg5
Jan 19 20:32:46.076: INFO: Got endpoints: latency-svc-jgvg5 [191.068525ms]
Jan 19 20:32:46.089: INFO: Created: latency-svc-msb2d
Jan 19 20:32:46.101: INFO: Got endpoints: latency-svc-msb2d [206.638945ms]
Jan 19 20:32:46.102: INFO: Created: latency-svc-jgkvh
Jan 19 20:32:46.117: INFO: Created: latency-svc-6lgxb
Jan 19 20:32:46.120: INFO: Got endpoints: latency-svc-jgkvh [211.928439ms]
Jan 19 20:32:46.127: INFO: Created: latency-svc-c7xxk
Jan 19 20:32:46.128: INFO: Got endpoints: latency-svc-6lgxb [206.623456ms]
Jan 19 20:32:46.138: INFO: Got endpoints: latency-svc-c7xxk [204.037114ms]
Jan 19 20:32:46.139: INFO: Latencies: [34.642448ms 39.428179ms 48.931489ms 53.787295ms 55.610716ms 68.357933ms 72.249813ms 80.978951ms 84.241862ms 97.700736ms 103.107872ms 113.766209ms 114.489347ms 125.294862ms 127.311886ms 128.127674ms 135.755098ms 139.398932ms 141.095336ms 144.300334ms 148.335042ms 150.075863ms 151.775337ms 152.949713ms 154.79563ms 155.910421ms 157.273166ms 157.891509ms 163.113735ms 164.653303ms 165.075683ms 165.197142ms 165.486223ms 165.522476ms 166.771457ms 168.218754ms 168.488632ms 169.132441ms 169.396503ms 169.596463ms 169.909911ms 170.054548ms 170.968926ms 171.060732ms 171.161369ms 171.937677ms 172.730706ms 172.804206ms 173.381518ms 173.58839ms 174.251471ms 175.284763ms 175.45206ms 175.474907ms 175.548171ms 175.553399ms 175.883041ms 176.3752ms 176.470247ms 176.541158ms 176.600383ms 177.000658ms 177.720112ms 179.332618ms 179.489246ms 179.8971ms 180.983107ms 181.963466ms 183.929244ms 187.171371ms 187.211607ms 187.485211ms 188.368465ms 189.620469ms 189.815151ms 190.013257ms 190.474455ms 190.499573ms 191.068525ms 191.527714ms 191.589516ms 192.787875ms 192.974542ms 194.365501ms 194.417409ms 195.049699ms 195.289826ms 198.063195ms 198.13625ms 198.61199ms 199.898692ms 200.914004ms 201.670534ms 201.914165ms 202.205422ms 203.08489ms 203.232138ms 204.037114ms 204.123902ms 204.410833ms 205.146756ms 205.3198ms 206.623456ms 206.638945ms 209.500031ms 210.714788ms 211.928439ms 214.763442ms 215.016336ms 215.831057ms 220.527316ms 220.743229ms 221.318771ms 222.07929ms 223.612063ms 225.123028ms 225.285153ms 225.707864ms 226.196062ms 227.430095ms 228.081063ms 228.350017ms 232.050148ms 232.794728ms 233.767808ms 233.806535ms 235.305532ms 235.731747ms 235.98249ms 237.395205ms 237.981002ms 238.103394ms 238.103667ms 241.02005ms 241.293056ms 241.768364ms 242.103295ms 242.341556ms 243.569429ms 243.913835ms 245.237404ms 246.24937ms 247.011916ms 247.455415ms 248.410012ms 249.359104ms 250.43373ms 251.026191ms 252.856269ms 253.051315ms 254.305288ms 254.590288ms 255.166674ms 256.047569ms 256.157065ms 258.345156ms 260.595992ms 263.049713ms 264.709898ms 269.38942ms 270.375848ms 270.791271ms 272.768519ms 273.1688ms 274.579382ms 275.896107ms 276.039939ms 277.042888ms 277.179311ms 277.291228ms 284.133329ms 286.886715ms 290.577065ms 295.379449ms 297.522757ms 300.12786ms 300.641025ms 308.306382ms 313.407603ms 313.534253ms 316.332926ms 325.125873ms 325.611543ms 325.805798ms 328.95533ms 331.95297ms 339.049901ms 339.348851ms 342.645677ms 355.153217ms 357.370419ms 374.973546ms 407.756061ms 409.203971ms 409.46397ms 411.19767ms 440.338784ms 454.778514ms 483.138245ms 516.012504ms]
Jan 19 20:32:46.139: INFO: 50 %ile: 205.146756ms
Jan 19 20:32:46.139: INFO: 90 %ile: 316.332926ms
Jan 19 20:32:46.139: INFO: 99 %ile: 483.138245ms
Jan 19 20:32:46.139: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jan 19 20:32:46.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-556" for this suite. 01/19/23 20:32:46.172
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":13,"skipped":162,"failed":0}
------------------------------
• [SLOW TEST] [5.266 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:32:40.912
    Jan 19 20:32:40.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename svc-latency 01/19/23 20:32:40.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:40.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:40.939
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan 19 20:32:40.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-556 01/19/23 20:32:40.946
    W0119 20:32:40.958856      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "svc-latency-rc" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "svc-latency-rc" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "svc-latency-rc" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "svc-latency-rc" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0119 20:32:40.959035      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-556, replica count: 1
    I0119 20:32:42.010163      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0119 20:32:43.010323      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 20:32:43.121: INFO: Created: latency-svc-rdzrk
    Jan 19 20:32:43.126: INFO: Got endpoints: latency-svc-rdzrk [15.528689ms]
    Jan 19 20:32:43.154: INFO: Created: latency-svc-td79k
    Jan 19 20:32:43.165: INFO: Created: latency-svc-s4gzl
    Jan 19 20:32:43.165: INFO: Got endpoints: latency-svc-td79k [39.428179ms]
    Jan 19 20:32:43.180: INFO: Got endpoints: latency-svc-s4gzl [53.787295ms]
    Jan 19 20:32:43.186: INFO: Created: latency-svc-4vhp6
    Jan 19 20:32:43.207: INFO: Got endpoints: latency-svc-4vhp6 [80.978951ms]
    Jan 19 20:32:43.210: INFO: Created: latency-svc-8dfvr
    Jan 19 20:32:43.229: INFO: Got endpoints: latency-svc-8dfvr [103.107872ms]
    Jan 19 20:32:43.243: INFO: Created: latency-svc-b54j2
    Jan 19 20:32:43.266: INFO: Got endpoints: latency-svc-b54j2 [139.398932ms]
    Jan 19 20:32:43.277: INFO: Created: latency-svc-9b4tx
    Jan 19 20:32:43.292: INFO: Got endpoints: latency-svc-9b4tx [165.197142ms]
    Jan 19 20:32:43.292: INFO: Created: latency-svc-gdwwq
    Jan 19 20:32:43.303: INFO: Got endpoints: latency-svc-gdwwq [176.600383ms]
    Jan 19 20:32:43.305: INFO: Created: latency-svc-5bcln
    Jan 19 20:32:43.314: INFO: Got endpoints: latency-svc-5bcln [187.211607ms]
    Jan 19 20:32:43.314: INFO: Created: latency-svc-qfxvx
    Jan 19 20:32:43.325: INFO: Got endpoints: latency-svc-qfxvx [198.13625ms]
    Jan 19 20:32:43.325: INFO: Created: latency-svc-vl9fb
    Jan 19 20:32:43.332: INFO: Got endpoints: latency-svc-vl9fb [205.146756ms]
    Jan 19 20:32:43.342: INFO: Created: latency-svc-h66g2
    Jan 19 20:32:43.348: INFO: Got endpoints: latency-svc-h66g2 [221.318771ms]
    Jan 19 20:32:43.348: INFO: Created: latency-svc-8hfxm
    Jan 19 20:32:43.352: INFO: Got endpoints: latency-svc-8hfxm [225.707864ms]
    Jan 19 20:32:43.362: INFO: Created: latency-svc-hkrj2
    Jan 19 20:32:43.365: INFO: Got endpoints: latency-svc-hkrj2 [237.981002ms]
    Jan 19 20:32:43.367: INFO: Created: latency-svc-ks4wt
    Jan 19 20:32:43.374: INFO: Got endpoints: latency-svc-ks4wt [247.455415ms]
    Jan 19 20:32:43.377: INFO: Created: latency-svc-9c4rg
    Jan 19 20:32:43.382: INFO: Got endpoints: latency-svc-9c4rg [255.166674ms]
    Jan 19 20:32:43.387: INFO: Created: latency-svc-8x8g2
    Jan 19 20:32:43.394: INFO: Got endpoints: latency-svc-8x8g2 [228.350017ms]
    Jan 19 20:32:43.410: INFO: Created: latency-svc-kzl8v
    Jan 19 20:32:43.412: INFO: Created: latency-svc-chfpd
    Jan 19 20:32:43.414: INFO: Got endpoints: latency-svc-kzl8v [233.767808ms]
    Jan 19 20:32:43.422: INFO: Got endpoints: latency-svc-chfpd [214.763442ms]
    Jan 19 20:32:43.425: INFO: Created: latency-svc-pvqvk
    Jan 19 20:32:43.431: INFO: Got endpoints: latency-svc-pvqvk [201.914165ms]
    Jan 19 20:32:43.437: INFO: Created: latency-svc-cqf52
    Jan 19 20:32:43.441: INFO: Created: latency-svc-gq9kj
    Jan 19 20:32:43.443: INFO: Got endpoints: latency-svc-cqf52 [177.000658ms]
    Jan 19 20:32:43.445: INFO: Created: latency-svc-jsd88
    Jan 19 20:32:43.448: INFO: Got endpoints: latency-svc-gq9kj [155.910421ms]
    Jan 19 20:32:43.455: INFO: Got endpoints: latency-svc-jsd88 [151.775337ms]
    Jan 19 20:32:43.465: INFO: Created: latency-svc-w6x27
    Jan 19 20:32:43.469: INFO: Got endpoints: latency-svc-w6x27 [154.79563ms]
    Jan 19 20:32:43.470: INFO: Created: latency-svc-cvgjl
    Jan 19 20:32:43.483: INFO: Got endpoints: latency-svc-cvgjl [157.891509ms]
    Jan 19 20:32:43.489: INFO: Created: latency-svc-gxccw
    Jan 19 20:32:43.501: INFO: Got endpoints: latency-svc-gxccw [169.596463ms]
    Jan 19 20:32:43.509: INFO: Created: latency-svc-shgfn
    Jan 19 20:32:43.513: INFO: Got endpoints: latency-svc-shgfn [165.075683ms]
    Jan 19 20:32:43.517: INFO: Created: latency-svc-629p9
    Jan 19 20:32:43.522: INFO: Got endpoints: latency-svc-629p9 [169.909911ms]
    Jan 19 20:32:43.525: INFO: Created: latency-svc-9bqrt
    Jan 19 20:32:43.530: INFO: Got endpoints: latency-svc-9bqrt [165.522476ms]
    Jan 19 20:32:43.542: INFO: Created: latency-svc-xhnmc
    Jan 19 20:32:43.551: INFO: Got endpoints: latency-svc-xhnmc [176.470247ms]
    Jan 19 20:32:43.555: INFO: Created: latency-svc-bdkj8
    Jan 19 20:32:43.562: INFO: Got endpoints: latency-svc-bdkj8 [179.8971ms]
    Jan 19 20:32:43.563: INFO: Created: latency-svc-dbsfb
    Jan 19 20:32:43.569: INFO: Got endpoints: latency-svc-dbsfb [175.553399ms]
    Jan 19 20:32:43.572: INFO: Created: latency-svc-rtcgm
    Jan 19 20:32:43.579: INFO: Got endpoints: latency-svc-rtcgm [164.653303ms]
    Jan 19 20:32:43.583: INFO: Created: latency-svc-9p28m
    Jan 19 20:32:43.592: INFO: Got endpoints: latency-svc-9p28m [170.054548ms]
    Jan 19 20:32:43.596: INFO: Created: latency-svc-g7ntg
    Jan 19 20:32:43.605: INFO: Created: latency-svc-sdbrt
    Jan 19 20:32:43.607: INFO: Got endpoints: latency-svc-g7ntg [175.474907ms]
    Jan 19 20:32:43.612: INFO: Got endpoints: latency-svc-sdbrt [168.488632ms]
    Jan 19 20:32:43.616: INFO: Created: latency-svc-p9667
    Jan 19 20:32:43.621: INFO: Got endpoints: latency-svc-p9667 [173.381518ms]
    Jan 19 20:32:43.633: INFO: Created: latency-svc-fbh7v
    Jan 19 20:32:43.639: INFO: Got endpoints: latency-svc-fbh7v [183.929244ms]
    Jan 19 20:32:43.643: INFO: Created: latency-svc-97qz8
    Jan 19 20:32:43.651: INFO: Got endpoints: latency-svc-97qz8 [181.963466ms]
    Jan 19 20:32:43.651: INFO: Created: latency-svc-c8b55
    Jan 19 20:32:43.655: INFO: Created: latency-svc-tlh87
    Jan 19 20:32:43.663: INFO: Got endpoints: latency-svc-c8b55 [179.489246ms]
    Jan 19 20:32:43.664: INFO: Got endpoints: latency-svc-tlh87 [163.113735ms]
    Jan 19 20:32:43.676: INFO: Created: latency-svc-jlmnx
    Jan 19 20:32:43.690: INFO: Got endpoints: latency-svc-jlmnx [176.541158ms]
    Jan 19 20:32:43.695: INFO: Created: latency-svc-cvsxl
    Jan 19 20:32:43.703: INFO: Created: latency-svc-4ltrv
    Jan 19 20:32:43.710: INFO: Got endpoints: latency-svc-cvsxl [187.171371ms]
    Jan 19 20:32:43.710: INFO: Got endpoints: latency-svc-4ltrv [179.332618ms]
    Jan 19 20:32:43.719: INFO: Created: latency-svc-g27tb
    Jan 19 20:32:43.732: INFO: Got endpoints: latency-svc-g27tb [180.983107ms]
    Jan 19 20:32:43.760: INFO: Created: latency-svc-pgsx4
    Jan 19 20:32:43.767: INFO: Got endpoints: latency-svc-pgsx4 [205.3198ms]
    Jan 19 20:32:43.772: INFO: Created: latency-svc-79kd7
    Jan 19 20:32:43.779: INFO: Got endpoints: latency-svc-79kd7 [209.500031ms]
    Jan 19 20:32:43.789: INFO: Created: latency-svc-sn9b2
    Jan 19 20:32:43.805: INFO: Got endpoints: latency-svc-sn9b2 [226.196062ms]
    Jan 19 20:32:43.810: INFO: Created: latency-svc-lzlkq
    Jan 19 20:32:43.817: INFO: Got endpoints: latency-svc-lzlkq [225.123028ms]
    Jan 19 20:32:43.826: INFO: Created: latency-svc-zrnbn
    Jan 19 20:32:43.832: INFO: Got endpoints: latency-svc-zrnbn [225.285153ms]
    Jan 19 20:32:43.843: INFO: Created: latency-svc-2dwsc
    Jan 19 20:32:43.853: INFO: Got endpoints: latency-svc-2dwsc [241.02005ms]
    Jan 19 20:32:43.863: INFO: Created: latency-svc-hdrf9
    Jan 19 20:32:43.872: INFO: Got endpoints: latency-svc-hdrf9 [251.026191ms]
    Jan 19 20:32:43.875: INFO: Created: latency-svc-wrhm8
    Jan 19 20:32:43.884: INFO: Got endpoints: latency-svc-wrhm8 [245.237404ms]
    Jan 19 20:32:43.893: INFO: Created: latency-svc-vt5b9
    Jan 19 20:32:43.896: INFO: Created: latency-svc-5n6vg
    Jan 19 20:32:43.907: INFO: Got endpoints: latency-svc-vt5b9 [256.157065ms]
    Jan 19 20:32:43.910: INFO: Got endpoints: latency-svc-5n6vg [247.011916ms]
    Jan 19 20:32:43.920: INFO: Created: latency-svc-dg26f
    Jan 19 20:32:43.931: INFO: Created: latency-svc-2m89z
    Jan 19 20:32:43.937: INFO: Got endpoints: latency-svc-dg26f [272.768519ms]
    Jan 19 20:32:43.940: INFO: Got endpoints: latency-svc-2m89z [250.43373ms]
    Jan 19 20:32:43.953: INFO: Created: latency-svc-jwl87
    Jan 19 20:32:43.959: INFO: Got endpoints: latency-svc-jwl87 [249.359104ms]
    Jan 19 20:32:43.959: INFO: Created: latency-svc-qx4fw
    Jan 19 20:32:43.964: INFO: Got endpoints: latency-svc-qx4fw [254.305288ms]
    Jan 19 20:32:43.979: INFO: Created: latency-svc-wzbg8
    Jan 19 20:32:43.990: INFO: Created: latency-svc-bjgll
    Jan 19 20:32:44.005: INFO: Got endpoints: latency-svc-wzbg8 [273.1688ms]
    Jan 19 20:32:44.009: INFO: Got endpoints: latency-svc-bjgll [241.768364ms]
    Jan 19 20:32:44.021: INFO: Created: latency-svc-jvrsr
    Jan 19 20:32:44.032: INFO: Got endpoints: latency-svc-jvrsr [253.051315ms]
    Jan 19 20:32:44.033: INFO: Created: latency-svc-7vst5
    Jan 19 20:32:44.043: INFO: Got endpoints: latency-svc-7vst5 [238.103667ms]
    Jan 19 20:32:44.045: INFO: Created: latency-svc-46xcn
    Jan 19 20:32:44.053: INFO: Created: latency-svc-xxp2r
    Jan 19 20:32:44.070: INFO: Got endpoints: latency-svc-46xcn [252.856269ms]
    Jan 19 20:32:44.076: INFO: Got endpoints: latency-svc-xxp2r [243.569429ms]
    Jan 19 20:32:44.119: INFO: Created: latency-svc-7gs9m
    Jan 19 20:32:44.129: INFO: Created: latency-svc-9zwgc
    Jan 19 20:32:44.130: INFO: Got endpoints: latency-svc-7gs9m [277.179311ms]
    Jan 19 20:32:44.146: INFO: Created: latency-svc-7s8sl
    Jan 19 20:32:44.159: INFO: Got endpoints: latency-svc-9zwgc [286.886715ms]
    Jan 19 20:32:44.161: INFO: Created: latency-svc-cmm26
    Jan 19 20:32:44.161: INFO: Got endpoints: latency-svc-7s8sl [277.291228ms]
    Jan 19 20:32:44.172: INFO: Got endpoints: latency-svc-cmm26 [264.709898ms]
    Jan 19 20:32:44.219: INFO: Created: latency-svc-xqcfw
    Jan 19 20:32:44.227: INFO: Created: latency-svc-kcbzg
    Jan 19 20:32:44.235: INFO: Got endpoints: latency-svc-xqcfw [325.125873ms]
    Jan 19 20:32:44.259: INFO: Created: latency-svc-69294
    Jan 19 20:32:44.268: INFO: Created: latency-svc-bkclk
    Jan 19 20:32:44.272: INFO: Got endpoints: latency-svc-69294 [331.95297ms]
    Jan 19 20:32:44.272: INFO: Got endpoints: latency-svc-bkclk [313.407603ms]
    Jan 19 20:32:44.280: INFO: Got endpoints: latency-svc-kcbzg [342.645677ms]
    Jan 19 20:32:44.308: INFO: Created: latency-svc-s7ztb
    Jan 19 20:32:44.316: INFO: Created: latency-svc-z6mvv
    Jan 19 20:32:44.321: INFO: Got endpoints: latency-svc-s7ztb [357.370419ms]
    Jan 19 20:32:44.331: INFO: Got endpoints: latency-svc-z6mvv [325.611543ms]
    Jan 19 20:32:44.340: INFO: Created: latency-svc-jcq8r
    Jan 19 20:32:44.347: INFO: Created: latency-svc-5c7kv
    Jan 19 20:32:44.348: INFO: Got endpoints: latency-svc-jcq8r [339.348851ms]
    Jan 19 20:32:44.358: INFO: Got endpoints: latency-svc-5c7kv [325.805798ms]
    Jan 19 20:32:44.366: INFO: Created: latency-svc-hs9pz
    Jan 19 20:32:44.372: INFO: Got endpoints: latency-svc-hs9pz [328.95533ms]
    Jan 19 20:32:44.374: INFO: Created: latency-svc-ttqlk
    Jan 19 20:32:44.384: INFO: Got endpoints: latency-svc-ttqlk [313.534253ms]
    Jan 19 20:32:44.407: INFO: Created: latency-svc-w42hz
    Jan 19 20:32:44.415: INFO: Got endpoints: latency-svc-w42hz [339.049901ms]
    Jan 19 20:32:44.420: INFO: Created: latency-svc-75zzc
    Jan 19 20:32:44.430: INFO: Got endpoints: latency-svc-75zzc [300.641025ms]
    Jan 19 20:32:44.441: INFO: Created: latency-svc-qbmkm
    Jan 19 20:32:44.441: INFO: Created: latency-svc-pbv2x
    Jan 19 20:32:44.457: INFO: Got endpoints: latency-svc-pbv2x [297.522757ms]
    Jan 19 20:32:44.457: INFO: Got endpoints: latency-svc-qbmkm [295.379449ms]
    Jan 19 20:32:44.463: INFO: Created: latency-svc-pbf7q
    Jan 19 20:32:44.472: INFO: Got endpoints: latency-svc-pbf7q [300.12786ms]
    Jan 19 20:32:44.480: INFO: Created: latency-svc-nrqpl
    Jan 19 20:32:44.489: INFO: Got endpoints: latency-svc-nrqpl [254.590288ms]
    Jan 19 20:32:44.507: INFO: Created: latency-svc-qp9ts
    Jan 19 20:32:44.520: INFO: Got endpoints: latency-svc-qp9ts [248.410012ms]
    Jan 19 20:32:44.527: INFO: Created: latency-svc-mrqvp
    Jan 19 20:32:44.535: INFO: Got endpoints: latency-svc-mrqvp [263.049713ms]
    Jan 19 20:32:44.539: INFO: Created: latency-svc-87pwn
    Jan 19 20:32:44.549: INFO: Created: latency-svc-m8wvp
    Jan 19 20:32:44.549: INFO: Got endpoints: latency-svc-87pwn [269.38942ms]
    Jan 19 20:32:44.557: INFO: Got endpoints: latency-svc-m8wvp [235.305532ms]
    Jan 19 20:32:44.560: INFO: Created: latency-svc-82jx2
    Jan 19 20:32:44.567: INFO: Got endpoints: latency-svc-82jx2 [235.731747ms]
    Jan 19 20:32:44.810: INFO: Created: latency-svc-bb4rr
    Jan 19 20:32:44.811: INFO: Created: latency-svc-rpqjj
    Jan 19 20:32:44.812: INFO: Created: latency-svc-nkfl5
    Jan 19 20:32:44.813: INFO: Created: latency-svc-sctw5
    Jan 19 20:32:44.815: INFO: Created: latency-svc-xmxn4
    Jan 19 20:32:44.815: INFO: Created: latency-svc-x2pbb
    Jan 19 20:32:44.815: INFO: Created: latency-svc-6crpm
    Jan 19 20:32:44.815: INFO: Created: latency-svc-rg578
    Jan 19 20:32:44.815: INFO: Created: latency-svc-6mlk9
    Jan 19 20:32:44.816: INFO: Created: latency-svc-v4cjx
    Jan 19 20:32:44.816: INFO: Created: latency-svc-fzw2p
    Jan 19 20:32:44.816: INFO: Created: latency-svc-vlfzz
    Jan 19 20:32:44.822: INFO: Created: latency-svc-s85mq
    Jan 19 20:32:44.822: INFO: Created: latency-svc-6252m
    Jan 19 20:32:44.824: INFO: Got endpoints: latency-svc-rpqjj [440.338784ms]
    Jan 19 20:32:44.824: INFO: Got endpoints: latency-svc-bb4rr [409.203971ms]
    Jan 19 20:32:44.827: INFO: Got endpoints: latency-svc-nkfl5 [454.778514ms]
    Jan 19 20:32:44.827: INFO: Got endpoints: latency-svc-vlfzz [355.153217ms]
    Jan 19 20:32:44.827: INFO: Got endpoints: latency-svc-v4cjx [270.375848ms]
    Jan 19 20:32:44.832: INFO: Created: latency-svc-5scqv
    Jan 19 20:32:44.837: INFO: Got endpoints: latency-svc-sctw5 [316.332926ms]
    Jan 19 20:32:44.840: INFO: Got endpoints: latency-svc-xmxn4 [290.577065ms]
    Jan 19 20:32:44.840: INFO: Got endpoints: latency-svc-fzw2p [409.46397ms]
    Jan 19 20:32:44.841: INFO: Got endpoints: latency-svc-rg578 [483.138245ms]
    Jan 19 20:32:44.841: INFO: Got endpoints: latency-svc-x2pbb [274.579382ms]
    Jan 19 20:32:44.844: INFO: Got endpoints: latency-svc-6mlk9 [308.306382ms]
    Jan 19 20:32:44.864: INFO: Got endpoints: latency-svc-6crpm [374.973546ms]
    Jan 19 20:32:44.865: INFO: Got endpoints: latency-svc-s85mq [516.012504ms]
    Jan 19 20:32:44.865: INFO: Got endpoints: latency-svc-6252m [407.756061ms]
    Jan 19 20:32:44.868: INFO: Got endpoints: latency-svc-5scqv [411.19767ms]
    Jan 19 20:32:44.884: INFO: Created: latency-svc-cgc2v
    Jan 19 20:32:44.896: INFO: Got endpoints: latency-svc-cgc2v [72.249813ms]
    Jan 19 20:32:44.925: INFO: Created: latency-svc-bgln2
    Jan 19 20:32:44.925: INFO: Got endpoints: latency-svc-bgln2 [97.700736ms]
    Jan 19 20:32:44.936: INFO: Created: latency-svc-b7wmp
    Jan 19 20:32:44.942: INFO: Got endpoints: latency-svc-b7wmp [114.489347ms]
    Jan 19 20:32:44.945: INFO: Created: latency-svc-nvks8
    Jan 19 20:32:44.954: INFO: Got endpoints: latency-svc-nvks8 [127.311886ms]
    Jan 19 20:32:44.962: INFO: Created: latency-svc-dzvc5
    Jan 19 20:32:44.972: INFO: Got endpoints: latency-svc-dzvc5 [148.335042ms]
    Jan 19 20:32:44.979: INFO: Created: latency-svc-6h4sr
    Jan 19 20:32:44.990: INFO: Got endpoints: latency-svc-6h4sr [152.949713ms]
    Jan 19 20:32:45.009: INFO: Created: latency-svc-d68k7
    Jan 19 20:32:45.030: INFO: Got endpoints: latency-svc-d68k7 [189.815151ms]
    Jan 19 20:32:45.030: INFO: Created: latency-svc-js9nf
    Jan 19 20:32:45.044: INFO: Created: latency-svc-b6pgn
    Jan 19 20:32:45.044: INFO: Got endpoints: latency-svc-js9nf [204.410833ms]
    Jan 19 20:32:45.052: INFO: Got endpoints: latency-svc-b6pgn [210.714788ms]
    Jan 19 20:32:45.059: INFO: Created: latency-svc-mlvfw
    Jan 19 20:32:45.063: INFO: Got endpoints: latency-svc-mlvfw [222.07929ms]
    Jan 19 20:32:45.071: INFO: Created: latency-svc-8rwpb
    Jan 19 20:32:45.080: INFO: Got endpoints: latency-svc-8rwpb [235.98249ms]
    Jan 19 20:32:45.088: INFO: Created: latency-svc-v9lm6
    Jan 19 20:32:45.107: INFO: Got endpoints: latency-svc-v9lm6 [242.341556ms]
    Jan 19 20:32:45.116: INFO: Created: latency-svc-hwpms
    Jan 19 20:32:45.121: INFO: Got endpoints: latency-svc-hwpms [256.047569ms]
    Jan 19 20:32:45.127: INFO: Created: latency-svc-74d6s
    Jan 19 20:32:45.135: INFO: Got endpoints: latency-svc-74d6s [270.791271ms]
    Jan 19 20:32:45.137: INFO: Created: latency-svc-snm92
    Jan 19 20:32:45.145: INFO: Got endpoints: latency-svc-snm92 [277.042888ms]
    Jan 19 20:32:45.149: INFO: Created: latency-svc-hflf8
    Jan 19 20:32:45.155: INFO: Got endpoints: latency-svc-hflf8 [258.345156ms]
    Jan 19 20:32:45.165: INFO: Created: latency-svc-5w9tj
    Jan 19 20:32:45.171: INFO: Got endpoints: latency-svc-5w9tj [246.24937ms]
    Jan 19 20:32:45.172: INFO: Created: latency-svc-rz9hj
    Jan 19 20:32:45.183: INFO: Got endpoints: latency-svc-rz9hj [241.293056ms]
    Jan 19 20:32:45.184: INFO: Created: latency-svc-9829l
    Jan 19 20:32:45.192: INFO: Got endpoints: latency-svc-9829l [237.395205ms]
    Jan 19 20:32:45.199: INFO: Created: latency-svc-2std2
    Jan 19 20:32:45.205: INFO: Got endpoints: latency-svc-2std2 [232.050148ms]
    Jan 19 20:32:45.212: INFO: Created: latency-svc-ts8xk
    Jan 19 20:32:45.213: INFO: Created: latency-svc-99dl9
    Jan 19 20:32:45.217: INFO: Got endpoints: latency-svc-99dl9 [227.430095ms]
    Jan 19 20:32:45.226: INFO: Created: latency-svc-9fbrw
    Jan 19 20:32:45.231: INFO: Got endpoints: latency-svc-ts8xk [201.670534ms]
    Jan 19 20:32:45.232: INFO: Got endpoints: latency-svc-9fbrw [187.485211ms]
    Jan 19 20:32:45.239: INFO: Created: latency-svc-2mr4s
    Jan 19 20:32:45.243: INFO: Got endpoints: latency-svc-2mr4s [191.589516ms]
    Jan 19 20:32:45.258: INFO: Created: latency-svc-w5slw
    Jan 19 20:32:45.262: INFO: Created: latency-svc-52nmf
    Jan 19 20:32:45.263: INFO: Got endpoints: latency-svc-w5slw [199.898692ms]
    Jan 19 20:32:45.270: INFO: Got endpoints: latency-svc-52nmf [190.474455ms]
    Jan 19 20:32:45.275: INFO: Created: latency-svc-966xl
    Jan 19 20:32:45.283: INFO: Got endpoints: latency-svc-966xl [175.883041ms]
    Jan 19 20:32:45.289: INFO: Created: latency-svc-cd62m
    Jan 19 20:32:45.295: INFO: Got endpoints: latency-svc-cd62m [174.251471ms]
    Jan 19 20:32:45.300: INFO: Created: latency-svc-j2kcn
    Jan 19 20:32:45.311: INFO: Got endpoints: latency-svc-j2kcn [175.284763ms]
    Jan 19 20:32:45.311: INFO: Created: latency-svc-df6tl
    Jan 19 20:32:45.319: INFO: Got endpoints: latency-svc-df6tl [173.58839ms]
    Jan 19 20:32:45.321: INFO: Created: latency-svc-x9xkd
    Jan 19 20:32:45.328: INFO: Got endpoints: latency-svc-x9xkd [172.804206ms]
    Jan 19 20:32:45.332: INFO: Created: latency-svc-fkmkh
    Jan 19 20:32:45.339: INFO: Got endpoints: latency-svc-fkmkh [168.218754ms]
    Jan 19 20:32:45.343: INFO: Created: latency-svc-tzbxg
    Jan 19 20:32:45.354: INFO: Got endpoints: latency-svc-tzbxg [170.968926ms]
    Jan 19 20:32:45.354: INFO: Created: latency-svc-m6rdw
    Jan 19 20:32:45.364: INFO: Got endpoints: latency-svc-m6rdw [171.937677ms]
    Jan 19 20:32:45.367: INFO: Created: latency-svc-92sb4
    Jan 19 20:32:45.374: INFO: Got endpoints: latency-svc-92sb4 [169.396503ms]
    Jan 19 20:32:45.376: INFO: Created: latency-svc-cg8z9
    Jan 19 20:32:45.383: INFO: Got endpoints: latency-svc-cg8z9 [165.486223ms]
    Jan 19 20:32:45.390: INFO: Created: latency-svc-gx6tm
    Jan 19 20:32:45.403: INFO: Got endpoints: latency-svc-gx6tm [171.161369ms]
    Jan 19 20:32:45.403: INFO: Created: latency-svc-fjvgp
    Jan 19 20:32:45.408: INFO: Got endpoints: latency-svc-fjvgp [176.3752ms]
    Jan 19 20:32:45.415: INFO: Created: latency-svc-wxkvt
    Jan 19 20:32:45.421: INFO: Got endpoints: latency-svc-wxkvt [177.720112ms]
    Jan 19 20:32:45.430: INFO: Created: latency-svc-hqnrf
    Jan 19 20:32:45.436: INFO: Got endpoints: latency-svc-hqnrf [172.730706ms]
    Jan 19 20:32:45.443: INFO: Created: latency-svc-qpgjd
    Jan 19 20:32:45.445: INFO: Created: latency-svc-qkfpc
    Jan 19 20:32:45.446: INFO: Got endpoints: latency-svc-qpgjd [175.548171ms]
    Jan 19 20:32:45.452: INFO: Got endpoints: latency-svc-qkfpc [169.132441ms]
    Jan 19 20:32:45.458: INFO: Created: latency-svc-2xcbp
    Jan 19 20:32:45.466: INFO: Got endpoints: latency-svc-2xcbp [171.060732ms]
    Jan 19 20:32:45.575: INFO: Created: latency-svc-dfvjv
    Jan 19 20:32:45.576: INFO: Created: latency-svc-5qpbx
    Jan 19 20:32:45.576: INFO: Created: latency-svc-25wlc
    Jan 19 20:32:45.576: INFO: Created: latency-svc-9fr86
    Jan 19 20:32:45.576: INFO: Created: latency-svc-7rxrh
    Jan 19 20:32:45.577: INFO: Created: latency-svc-895xv
    Jan 19 20:32:45.577: INFO: Created: latency-svc-vgqh6
    Jan 19 20:32:45.577: INFO: Created: latency-svc-jc2j5
    Jan 19 20:32:45.577: INFO: Created: latency-svc-jtrzq
    Jan 19 20:32:45.577: INFO: Created: latency-svc-2vlpd
    Jan 19 20:32:45.579: INFO: Created: latency-svc-jm5mp
    Jan 19 20:32:45.579: INFO: Created: latency-svc-crb6g
    Jan 19 20:32:45.579: INFO: Created: latency-svc-ngrqt
    Jan 19 20:32:45.580: INFO: Created: latency-svc-cvrsv
    Jan 19 20:32:45.580: INFO: Created: latency-svc-hxx57
    Jan 19 20:32:45.586: INFO: Got endpoints: latency-svc-895xv [150.075863ms]
    Jan 19 20:32:45.591: INFO: Got endpoints: latency-svc-5qpbx [125.294862ms]
    Jan 19 20:32:45.595: INFO: Got endpoints: latency-svc-dfvjv [276.039939ms]
    Jan 19 20:32:45.595: INFO: Got endpoints: latency-svc-hxx57 [284.133329ms]
    Jan 19 20:32:45.596: INFO: Got endpoints: latency-svc-cvrsv [242.103295ms]
    Jan 19 20:32:45.596: INFO: Got endpoints: latency-svc-jm5mp [144.300334ms]
    Jan 19 20:32:45.600: INFO: Got endpoints: latency-svc-2vlpd [260.595992ms]
    Jan 19 20:32:45.604: INFO: Got endpoints: latency-svc-jtrzq [275.896107ms]
    Jan 19 20:32:45.608: INFO: Got endpoints: latency-svc-vgqh6 [243.913835ms]
    Jan 19 20:32:45.608: INFO: Got endpoints: latency-svc-ngrqt [233.806535ms]
    Jan 19 20:32:45.611: INFO: Got endpoints: latency-svc-7rxrh [203.08489ms]
    Jan 19 20:32:45.611: INFO: Created: latency-svc-mvnvb
    Jan 19 20:32:45.613: INFO: Got endpoints: latency-svc-crb6g [166.771457ms]
    Jan 19 20:32:45.613: INFO: Got endpoints: latency-svc-9fr86 [191.527714ms]
    Jan 19 20:32:45.619: INFO: Got endpoints: latency-svc-jc2j5 [215.831057ms]
    Jan 19 20:32:45.621: INFO: Got endpoints: latency-svc-25wlc [238.103394ms]
    Jan 19 20:32:45.621: INFO: Got endpoints: latency-svc-mvnvb [34.642448ms]
    Jan 19 20:32:45.640: INFO: Created: latency-svc-s5wzw
    Jan 19 20:32:45.640: INFO: Got endpoints: latency-svc-s5wzw [48.931489ms]
    Jan 19 20:32:45.646: INFO: Created: latency-svc-5wwwm
    Jan 19 20:32:45.651: INFO: Got endpoints: latency-svc-5wwwm [55.610716ms]
    Jan 19 20:32:45.654: INFO: Created: latency-svc-8ptl6
    Jan 19 20:32:45.665: INFO: Got endpoints: latency-svc-8ptl6 [68.357933ms]
    Jan 19 20:32:45.671: INFO: Created: latency-svc-ncjq8
    Jan 19 20:32:45.679: INFO: Got endpoints: latency-svc-ncjq8 [84.241862ms]
    Jan 19 20:32:45.702: INFO: Created: latency-svc-87nqw
    Jan 19 20:32:45.710: INFO: Got endpoints: latency-svc-87nqw [113.766209ms]
    Jan 19 20:32:45.718: INFO: Created: latency-svc-crh2r
    Jan 19 20:32:45.728: INFO: Got endpoints: latency-svc-crh2r [128.127674ms]
    Jan 19 20:32:45.733: INFO: Created: latency-svc-b2pv2
    Jan 19 20:32:45.739: INFO: Got endpoints: latency-svc-b2pv2 [135.755098ms]
    Jan 19 20:32:45.742: INFO: Created: latency-svc-wcm5c
    Jan 19 20:32:45.749: INFO: Got endpoints: latency-svc-wcm5c [141.095336ms]
    Jan 19 20:32:45.765: INFO: Created: latency-svc-scfwq
    Jan 19 20:32:45.765: INFO: Got endpoints: latency-svc-scfwq [157.273166ms]
    Jan 19 20:32:45.780: INFO: Created: latency-svc-vdd2m
    Jan 19 20:32:45.786: INFO: Created: latency-svc-lc89v
    Jan 19 20:32:45.787: INFO: Got endpoints: latency-svc-vdd2m [175.45206ms]
    Jan 19 20:32:45.801: INFO: Got endpoints: latency-svc-lc89v [188.368465ms]
    Jan 19 20:32:45.807: INFO: Created: latency-svc-mxwxd
    Jan 19 20:32:45.815: INFO: Got endpoints: latency-svc-mxwxd [202.205422ms]
    Jan 19 20:32:45.815: INFO: Created: latency-svc-ztzfj
    Jan 19 20:32:45.823: INFO: Got endpoints: latency-svc-ztzfj [204.123902ms]
    Jan 19 20:32:45.835: INFO: Created: latency-svc-gw2sq
    Jan 19 20:32:45.849: INFO: Got endpoints: latency-svc-gw2sq [228.081063ms]
    Jan 19 20:32:45.849: INFO: Created: latency-svc-bchts
    Jan 19 20:32:45.854: INFO: Got endpoints: latency-svc-bchts [232.794728ms]
    Jan 19 20:32:45.858: INFO: Created: latency-svc-54crs
    Jan 19 20:32:45.864: INFO: Got endpoints: latency-svc-54crs [223.612063ms]
    Jan 19 20:32:45.866: INFO: Created: latency-svc-tqtmx
    Jan 19 20:32:45.871: INFO: Got endpoints: latency-svc-tqtmx [220.527316ms]
    Jan 19 20:32:45.879: INFO: Created: latency-svc-f7rcw
    Jan 19 20:32:45.885: INFO: Got endpoints: latency-svc-f7rcw [220.743229ms]
    Jan 19 20:32:45.887: INFO: Created: latency-svc-px2sg
    Jan 19 20:32:45.894: INFO: Got endpoints: latency-svc-px2sg [215.016336ms]
    Jan 19 20:32:45.904: INFO: Created: latency-svc-smqgb
    Jan 19 20:32:45.908: INFO: Got endpoints: latency-svc-smqgb [198.063195ms]
    Jan 19 20:32:45.913: INFO: Created: latency-svc-cf6fl
    Jan 19 20:32:45.921: INFO: Got endpoints: latency-svc-cf6fl [192.974542ms]
    Jan 19 20:32:45.926: INFO: Created: latency-svc-s8lwv
    Jan 19 20:32:45.933: INFO: Created: latency-svc-vr9hj
    Jan 19 20:32:45.934: INFO: Got endpoints: latency-svc-s8lwv [195.049699ms]
    Jan 19 20:32:45.947: INFO: Got endpoints: latency-svc-vr9hj [198.61199ms]
    Jan 19 20:32:45.953: INFO: Created: latency-svc-mzvjm
    Jan 19 20:32:45.966: INFO: Got endpoints: latency-svc-mzvjm [200.914004ms]
    Jan 19 20:32:45.972: INFO: Created: latency-svc-mfspf
    Jan 19 20:32:45.986: INFO: Created: latency-svc-2ctnn
    Jan 19 20:32:45.990: INFO: Got endpoints: latency-svc-mfspf [203.232138ms]
    Jan 19 20:32:45.993: INFO: Created: latency-svc-4g7vw
    Jan 19 20:32:45.994: INFO: Got endpoints: latency-svc-2ctnn [192.787875ms]
    Jan 19 20:32:46.005: INFO: Got endpoints: latency-svc-4g7vw [190.013257ms]
    Jan 19 20:32:46.012: INFO: Created: latency-svc-btnmj
    Jan 19 20:32:46.018: INFO: Got endpoints: latency-svc-btnmj [195.289826ms]
    Jan 19 20:32:46.024: INFO: Created: latency-svc-9tbdr
    Jan 19 20:32:46.039: INFO: Got endpoints: latency-svc-9tbdr [190.499573ms]
    Jan 19 20:32:46.042: INFO: Created: latency-svc-t9qb6
    Jan 19 20:32:46.043: INFO: Got endpoints: latency-svc-t9qb6 [189.620469ms]
    Jan 19 20:32:46.046: INFO: Created: latency-svc-zmbjv
    Jan 19 20:32:46.058: INFO: Got endpoints: latency-svc-zmbjv [194.365501ms]
    Jan 19 20:32:46.059: INFO: Created: latency-svc-b2s46
    Jan 19 20:32:46.066: INFO: Got endpoints: latency-svc-b2s46 [194.417409ms]
    Jan 19 20:32:46.074: INFO: Created: latency-svc-jgvg5
    Jan 19 20:32:46.076: INFO: Got endpoints: latency-svc-jgvg5 [191.068525ms]
    Jan 19 20:32:46.089: INFO: Created: latency-svc-msb2d
    Jan 19 20:32:46.101: INFO: Got endpoints: latency-svc-msb2d [206.638945ms]
    Jan 19 20:32:46.102: INFO: Created: latency-svc-jgkvh
    Jan 19 20:32:46.117: INFO: Created: latency-svc-6lgxb
    Jan 19 20:32:46.120: INFO: Got endpoints: latency-svc-jgkvh [211.928439ms]
    Jan 19 20:32:46.127: INFO: Created: latency-svc-c7xxk
    Jan 19 20:32:46.128: INFO: Got endpoints: latency-svc-6lgxb [206.623456ms]
    Jan 19 20:32:46.138: INFO: Got endpoints: latency-svc-c7xxk [204.037114ms]
    Jan 19 20:32:46.139: INFO: Latencies: [34.642448ms 39.428179ms 48.931489ms 53.787295ms 55.610716ms 68.357933ms 72.249813ms 80.978951ms 84.241862ms 97.700736ms 103.107872ms 113.766209ms 114.489347ms 125.294862ms 127.311886ms 128.127674ms 135.755098ms 139.398932ms 141.095336ms 144.300334ms 148.335042ms 150.075863ms 151.775337ms 152.949713ms 154.79563ms 155.910421ms 157.273166ms 157.891509ms 163.113735ms 164.653303ms 165.075683ms 165.197142ms 165.486223ms 165.522476ms 166.771457ms 168.218754ms 168.488632ms 169.132441ms 169.396503ms 169.596463ms 169.909911ms 170.054548ms 170.968926ms 171.060732ms 171.161369ms 171.937677ms 172.730706ms 172.804206ms 173.381518ms 173.58839ms 174.251471ms 175.284763ms 175.45206ms 175.474907ms 175.548171ms 175.553399ms 175.883041ms 176.3752ms 176.470247ms 176.541158ms 176.600383ms 177.000658ms 177.720112ms 179.332618ms 179.489246ms 179.8971ms 180.983107ms 181.963466ms 183.929244ms 187.171371ms 187.211607ms 187.485211ms 188.368465ms 189.620469ms 189.815151ms 190.013257ms 190.474455ms 190.499573ms 191.068525ms 191.527714ms 191.589516ms 192.787875ms 192.974542ms 194.365501ms 194.417409ms 195.049699ms 195.289826ms 198.063195ms 198.13625ms 198.61199ms 199.898692ms 200.914004ms 201.670534ms 201.914165ms 202.205422ms 203.08489ms 203.232138ms 204.037114ms 204.123902ms 204.410833ms 205.146756ms 205.3198ms 206.623456ms 206.638945ms 209.500031ms 210.714788ms 211.928439ms 214.763442ms 215.016336ms 215.831057ms 220.527316ms 220.743229ms 221.318771ms 222.07929ms 223.612063ms 225.123028ms 225.285153ms 225.707864ms 226.196062ms 227.430095ms 228.081063ms 228.350017ms 232.050148ms 232.794728ms 233.767808ms 233.806535ms 235.305532ms 235.731747ms 235.98249ms 237.395205ms 237.981002ms 238.103394ms 238.103667ms 241.02005ms 241.293056ms 241.768364ms 242.103295ms 242.341556ms 243.569429ms 243.913835ms 245.237404ms 246.24937ms 247.011916ms 247.455415ms 248.410012ms 249.359104ms 250.43373ms 251.026191ms 252.856269ms 253.051315ms 254.305288ms 254.590288ms 255.166674ms 256.047569ms 256.157065ms 258.345156ms 260.595992ms 263.049713ms 264.709898ms 269.38942ms 270.375848ms 270.791271ms 272.768519ms 273.1688ms 274.579382ms 275.896107ms 276.039939ms 277.042888ms 277.179311ms 277.291228ms 284.133329ms 286.886715ms 290.577065ms 295.379449ms 297.522757ms 300.12786ms 300.641025ms 308.306382ms 313.407603ms 313.534253ms 316.332926ms 325.125873ms 325.611543ms 325.805798ms 328.95533ms 331.95297ms 339.049901ms 339.348851ms 342.645677ms 355.153217ms 357.370419ms 374.973546ms 407.756061ms 409.203971ms 409.46397ms 411.19767ms 440.338784ms 454.778514ms 483.138245ms 516.012504ms]
    Jan 19 20:32:46.139: INFO: 50 %ile: 205.146756ms
    Jan 19 20:32:46.139: INFO: 90 %ile: 316.332926ms
    Jan 19 20:32:46.139: INFO: 99 %ile: 483.138245ms
    Jan 19 20:32:46.139: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jan 19 20:32:46.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-556" for this suite. 01/19/23 20:32:46.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:32:46.179
Jan 19 20:32:46.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename ingress 01/19/23 20:32:46.18
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:46.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:46.205
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/19/23 20:32:46.208
STEP: getting /apis/networking.k8s.io 01/19/23 20:32:46.211
STEP: getting /apis/networking.k8s.iov1 01/19/23 20:32:46.214
STEP: creating 01/19/23 20:32:46.219
STEP: getting 01/19/23 20:32:46.249
STEP: listing 01/19/23 20:32:46.263
STEP: watching 01/19/23 20:32:46.27
Jan 19 20:32:46.270: INFO: starting watch
STEP: cluster-wide listing 01/19/23 20:32:46.271
STEP: cluster-wide watching 01/19/23 20:32:46.28
Jan 19 20:32:46.280: INFO: starting watch
STEP: patching 01/19/23 20:32:46.281
STEP: updating 01/19/23 20:32:46.292
Jan 19 20:32:46.309: INFO: waiting for watch events with expected annotations
Jan 19 20:32:46.309: INFO: saw patched and updated annotations
STEP: patching /status 01/19/23 20:32:46.309
STEP: updating /status 01/19/23 20:32:46.316
STEP: get /status 01/19/23 20:32:46.326
STEP: deleting 01/19/23 20:32:46.329
STEP: deleting a collection 01/19/23 20:32:46.343
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jan 19 20:32:46.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-8254" for this suite. 01/19/23 20:32:46.374
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":14,"skipped":176,"failed":0}
------------------------------
• [0.203 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:32:46.179
    Jan 19 20:32:46.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename ingress 01/19/23 20:32:46.18
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:46.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:46.205
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/19/23 20:32:46.208
    STEP: getting /apis/networking.k8s.io 01/19/23 20:32:46.211
    STEP: getting /apis/networking.k8s.iov1 01/19/23 20:32:46.214
    STEP: creating 01/19/23 20:32:46.219
    STEP: getting 01/19/23 20:32:46.249
    STEP: listing 01/19/23 20:32:46.263
    STEP: watching 01/19/23 20:32:46.27
    Jan 19 20:32:46.270: INFO: starting watch
    STEP: cluster-wide listing 01/19/23 20:32:46.271
    STEP: cluster-wide watching 01/19/23 20:32:46.28
    Jan 19 20:32:46.280: INFO: starting watch
    STEP: patching 01/19/23 20:32:46.281
    STEP: updating 01/19/23 20:32:46.292
    Jan 19 20:32:46.309: INFO: waiting for watch events with expected annotations
    Jan 19 20:32:46.309: INFO: saw patched and updated annotations
    STEP: patching /status 01/19/23 20:32:46.309
    STEP: updating /status 01/19/23 20:32:46.316
    STEP: get /status 01/19/23 20:32:46.326
    STEP: deleting 01/19/23 20:32:46.329
    STEP: deleting a collection 01/19/23 20:32:46.343
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jan 19 20:32:46.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-8254" for this suite. 01/19/23 20:32:46.374
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:32:46.382
Jan 19 20:32:46.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-probe 01/19/23 20:32:46.383
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:46.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:46.43
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d in namespace container-probe-8356 01/19/23 20:32:46.433
W0119 20:32:46.466338      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 20:32:46.466: INFO: Waiting up to 5m0s for pod "test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d" in namespace "container-probe-8356" to be "not pending"
Jan 19 20:32:46.470: INFO: Pod "test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.878665ms
Jan 19 20:32:48.474: INFO: Pod "test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007900348s
Jan 19 20:32:48.474: INFO: Pod "test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d" satisfied condition "not pending"
Jan 19 20:32:48.474: INFO: Started pod test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d in namespace container-probe-8356
STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 20:32:48.474
Jan 19 20:32:48.476: INFO: Initial restart count of pod test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d is 0
STEP: deleting the pod 01/19/23 20:36:48.974
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 20:36:48.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8356" for this suite. 01/19/23 20:36:48.99
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":15,"skipped":177,"failed":0}
------------------------------
• [SLOW TEST] [242.614 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:32:46.382
    Jan 19 20:32:46.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-probe 01/19/23 20:32:46.383
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:32:46.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:32:46.43
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d in namespace container-probe-8356 01/19/23 20:32:46.433
    W0119 20:32:46.466338      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 20:32:46.466: INFO: Waiting up to 5m0s for pod "test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d" in namespace "container-probe-8356" to be "not pending"
    Jan 19 20:32:46.470: INFO: Pod "test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.878665ms
    Jan 19 20:32:48.474: INFO: Pod "test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007900348s
    Jan 19 20:32:48.474: INFO: Pod "test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d" satisfied condition "not pending"
    Jan 19 20:32:48.474: INFO: Started pod test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d in namespace container-probe-8356
    STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 20:32:48.474
    Jan 19 20:32:48.476: INFO: Initial restart count of pod test-webserver-16e7198c-d197-41af-b6af-4fb438bff65d is 0
    STEP: deleting the pod 01/19/23 20:36:48.974
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 20:36:48.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8356" for this suite. 01/19/23 20:36:48.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:36:48.997
Jan 19 20:36:48.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 20:36:48.998
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:36:49.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:36:49.021
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6348 01/19/23 20:36:49.024
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/19/23 20:36:49.05
STEP: creating service externalsvc in namespace services-6348 01/19/23 20:36:49.05
STEP: creating replication controller externalsvc in namespace services-6348 01/19/23 20:36:49.072
I0119 20:36:49.081828      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6348, replica count: 2
I0119 20:36:52.133384      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/19/23 20:36:52.136
Jan 19 20:36:52.148: INFO: Creating new exec pod
Jan 19 20:36:52.183: INFO: Waiting up to 5m0s for pod "execpodp2ssf" in namespace "services-6348" to be "running"
Jan 19 20:36:52.191: INFO: Pod "execpodp2ssf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.316555ms
Jan 19 20:36:54.195: INFO: Pod "execpodp2ssf": Phase="Running", Reason="", readiness=true. Elapsed: 2.01174297s
Jan 19 20:36:54.195: INFO: Pod "execpodp2ssf" satisfied condition "running"
Jan 19 20:36:54.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6348 exec execpodp2ssf -- /bin/sh -x -c nslookup clusterip-service.services-6348.svc.cluster.local'
Jan 19 20:36:54.373: INFO: stderr: "+ nslookup clusterip-service.services-6348.svc.cluster.local\n"
Jan 19 20:36:54.373: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-6348.svc.cluster.local\tcanonical name = externalsvc.services-6348.svc.cluster.local.\nName:\texternalsvc.services-6348.svc.cluster.local\nAddress: 172.30.171.197\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6348, will wait for the garbage collector to delete the pods 01/19/23 20:36:54.373
Jan 19 20:36:54.432: INFO: Deleting ReplicationController externalsvc took: 5.113438ms
Jan 19 20:36:54.533: INFO: Terminating ReplicationController externalsvc pods took: 100.696793ms
Jan 19 20:36:56.382: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 20:36:56.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6348" for this suite. 01/19/23 20:36:56.407
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":16,"skipped":191,"failed":0}
------------------------------
• [SLOW TEST] [7.417 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:36:48.997
    Jan 19 20:36:48.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 20:36:48.998
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:36:49.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:36:49.021
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6348 01/19/23 20:36:49.024
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/19/23 20:36:49.05
    STEP: creating service externalsvc in namespace services-6348 01/19/23 20:36:49.05
    STEP: creating replication controller externalsvc in namespace services-6348 01/19/23 20:36:49.072
    I0119 20:36:49.081828      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6348, replica count: 2
    I0119 20:36:52.133384      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/19/23 20:36:52.136
    Jan 19 20:36:52.148: INFO: Creating new exec pod
    Jan 19 20:36:52.183: INFO: Waiting up to 5m0s for pod "execpodp2ssf" in namespace "services-6348" to be "running"
    Jan 19 20:36:52.191: INFO: Pod "execpodp2ssf": Phase="Pending", Reason="", readiness=false. Elapsed: 8.316555ms
    Jan 19 20:36:54.195: INFO: Pod "execpodp2ssf": Phase="Running", Reason="", readiness=true. Elapsed: 2.01174297s
    Jan 19 20:36:54.195: INFO: Pod "execpodp2ssf" satisfied condition "running"
    Jan 19 20:36:54.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6348 exec execpodp2ssf -- /bin/sh -x -c nslookup clusterip-service.services-6348.svc.cluster.local'
    Jan 19 20:36:54.373: INFO: stderr: "+ nslookup clusterip-service.services-6348.svc.cluster.local\n"
    Jan 19 20:36:54.373: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nclusterip-service.services-6348.svc.cluster.local\tcanonical name = externalsvc.services-6348.svc.cluster.local.\nName:\texternalsvc.services-6348.svc.cluster.local\nAddress: 172.30.171.197\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-6348, will wait for the garbage collector to delete the pods 01/19/23 20:36:54.373
    Jan 19 20:36:54.432: INFO: Deleting ReplicationController externalsvc took: 5.113438ms
    Jan 19 20:36:54.533: INFO: Terminating ReplicationController externalsvc pods took: 100.696793ms
    Jan 19 20:36:56.382: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 20:36:56.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6348" for this suite. 01/19/23 20:36:56.407
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:36:56.415
Jan 19 20:36:56.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 20:36:56.416
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:36:56.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:36:56.453
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/19/23 20:36:56.463
Jan 19 20:36:56.493: INFO: Waiting up to 5m0s for pod "pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77" in namespace "emptydir-6290" to be "Succeeded or Failed"
Jan 19 20:36:56.512: INFO: Pod "pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77": Phase="Pending", Reason="", readiness=false. Elapsed: 18.37675ms
Jan 19 20:36:58.516: INFO: Pod "pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022191511s
Jan 19 20:37:00.516: INFO: Pod "pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022598987s
STEP: Saw pod success 01/19/23 20:37:00.516
Jan 19 20:37:00.516: INFO: Pod "pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77" satisfied condition "Succeeded or Failed"
Jan 19 20:37:00.518: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77 container test-container: <nil>
STEP: delete the pod 01/19/23 20:37:00.528
Jan 19 20:37:00.538: INFO: Waiting for pod pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77 to disappear
Jan 19 20:37:00.540: INFO: Pod pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 20:37:00.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6290" for this suite. 01/19/23 20:37:00.545
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":17,"skipped":220,"failed":0}
------------------------------
• [4.139 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:36:56.415
    Jan 19 20:36:56.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 20:36:56.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:36:56.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:36:56.453
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/19/23 20:36:56.463
    Jan 19 20:36:56.493: INFO: Waiting up to 5m0s for pod "pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77" in namespace "emptydir-6290" to be "Succeeded or Failed"
    Jan 19 20:36:56.512: INFO: Pod "pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77": Phase="Pending", Reason="", readiness=false. Elapsed: 18.37675ms
    Jan 19 20:36:58.516: INFO: Pod "pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022191511s
    Jan 19 20:37:00.516: INFO: Pod "pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022598987s
    STEP: Saw pod success 01/19/23 20:37:00.516
    Jan 19 20:37:00.516: INFO: Pod "pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77" satisfied condition "Succeeded or Failed"
    Jan 19 20:37:00.518: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77 container test-container: <nil>
    STEP: delete the pod 01/19/23 20:37:00.528
    Jan 19 20:37:00.538: INFO: Waiting for pod pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77 to disappear
    Jan 19 20:37:00.540: INFO: Pod pod-54d8079a-f7c5-4cb0-b725-5c6469b97b77 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 20:37:00.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6290" for this suite. 01/19/23 20:37:00.545
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:37:00.554
Jan 19 20:37:00.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename var-expansion 01/19/23 20:37:00.554
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:37:00.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:37:00.571
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 01/19/23 20:37:00.592
Jan 19 20:37:00.705: INFO: Waiting up to 2m0s for pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c" in namespace "var-expansion-221" to be "running"
Jan 19 20:37:00.727: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.406125ms
Jan 19 20:37:02.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025851683s
Jan 19 20:37:04.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026385164s
Jan 19 20:37:06.741: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036817799s
Jan 19 20:37:08.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026812937s
Jan 19 20:37:10.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027739374s
Jan 19 20:37:12.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.027305279s
Jan 19 20:37:14.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026553025s
Jan 19 20:37:16.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.025718821s
Jan 19 20:37:18.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.02620834s
Jan 19 20:37:20.733: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.028658683s
Jan 19 20:37:22.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.025866859s
Jan 19 20:37:24.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 24.026413198s
Jan 19 20:37:26.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 26.025173961s
Jan 19 20:37:28.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 28.026593486s
Jan 19 20:37:30.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.02503668s
Jan 19 20:37:32.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 32.027461369s
Jan 19 20:37:34.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 34.027653627s
Jan 19 20:37:36.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025456492s
Jan 19 20:37:38.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 38.026905442s
Jan 19 20:37:40.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025744033s
Jan 19 20:37:42.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 42.02546292s
Jan 19 20:37:44.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 44.027855775s
Jan 19 20:37:46.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 46.025764279s
Jan 19 20:37:48.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 48.026777907s
Jan 19 20:37:50.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 50.026622076s
Jan 19 20:37:52.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 52.026613782s
Jan 19 20:37:54.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 54.027892444s
Jan 19 20:37:56.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 56.026621002s
Jan 19 20:37:58.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 58.026817801s
Jan 19 20:38:00.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.026539303s
Jan 19 20:38:02.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.025257207s
Jan 19 20:38:04.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.025519685s
Jan 19 20:38:06.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.025197943s
Jan 19 20:38:08.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.026206047s
Jan 19 20:38:10.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.027348379s
Jan 19 20:38:12.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.027519446s
Jan 19 20:38:14.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.027629961s
Jan 19 20:38:16.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.025212904s
Jan 19 20:38:18.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.027191555s
Jan 19 20:38:20.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.027020886s
Jan 19 20:38:22.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.026379201s
Jan 19 20:38:24.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.026785048s
Jan 19 20:38:26.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.025669446s
Jan 19 20:38:28.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.026551473s
Jan 19 20:38:30.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.025906226s
Jan 19 20:38:32.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.027017771s
Jan 19 20:38:34.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.027100064s
Jan 19 20:38:36.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.025802912s
Jan 19 20:38:38.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.027129494s
Jan 19 20:38:40.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.027321553s
Jan 19 20:38:42.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.026838975s
Jan 19 20:38:44.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.027111489s
Jan 19 20:38:46.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.026577704s
Jan 19 20:38:48.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.0255884s
Jan 19 20:38:50.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.026782741s
Jan 19 20:38:52.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.027044396s
Jan 19 20:38:54.729: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.024921959s
Jan 19 20:38:56.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.025606407s
Jan 19 20:38:58.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.026376378s
Jan 19 20:39:00.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.027475299s
Jan 19 20:39:00.735: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.030505608s
STEP: updating the pod 01/19/23 20:39:00.735
Jan 19 20:39:01.253: INFO: Successfully updated pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c"
STEP: waiting for pod running 01/19/23 20:39:01.253
Jan 19 20:39:01.253: INFO: Waiting up to 2m0s for pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c" in namespace "var-expansion-221" to be "running"
Jan 19 20:39:01.256: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580866ms
Jan 19 20:39:03.259: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Running", Reason="", readiness=true. Elapsed: 2.006410891s
Jan 19 20:39:03.259: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c" satisfied condition "running"
STEP: deleting the pod gracefully 01/19/23 20:39:03.259
Jan 19 20:39:03.259: INFO: Deleting pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c" in namespace "var-expansion-221"
Jan 19 20:39:03.265: INFO: Wait up to 5m0s for pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 20:39:35.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-221" for this suite. 01/19/23 20:39:35.277
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":18,"skipped":223,"failed":0}
------------------------------
• [SLOW TEST] [154.729 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:37:00.554
    Jan 19 20:37:00.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename var-expansion 01/19/23 20:37:00.554
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:37:00.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:37:00.571
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 01/19/23 20:37:00.592
    Jan 19 20:37:00.705: INFO: Waiting up to 2m0s for pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c" in namespace "var-expansion-221" to be "running"
    Jan 19 20:37:00.727: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.406125ms
    Jan 19 20:37:02.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025851683s
    Jan 19 20:37:04.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026385164s
    Jan 19 20:37:06.741: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.036817799s
    Jan 19 20:37:08.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026812937s
    Jan 19 20:37:10.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 10.027739374s
    Jan 19 20:37:12.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 12.027305279s
    Jan 19 20:37:14.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 14.026553025s
    Jan 19 20:37:16.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.025718821s
    Jan 19 20:37:18.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 18.02620834s
    Jan 19 20:37:20.733: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.028658683s
    Jan 19 20:37:22.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 22.025866859s
    Jan 19 20:37:24.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 24.026413198s
    Jan 19 20:37:26.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 26.025173961s
    Jan 19 20:37:28.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 28.026593486s
    Jan 19 20:37:30.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 30.02503668s
    Jan 19 20:37:32.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 32.027461369s
    Jan 19 20:37:34.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 34.027653627s
    Jan 19 20:37:36.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 36.025456492s
    Jan 19 20:37:38.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 38.026905442s
    Jan 19 20:37:40.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025744033s
    Jan 19 20:37:42.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 42.02546292s
    Jan 19 20:37:44.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 44.027855775s
    Jan 19 20:37:46.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 46.025764279s
    Jan 19 20:37:48.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 48.026777907s
    Jan 19 20:37:50.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 50.026622076s
    Jan 19 20:37:52.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 52.026613782s
    Jan 19 20:37:54.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 54.027892444s
    Jan 19 20:37:56.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 56.026621002s
    Jan 19 20:37:58.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 58.026817801s
    Jan 19 20:38:00.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.026539303s
    Jan 19 20:38:02.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.025257207s
    Jan 19 20:38:04.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.025519685s
    Jan 19 20:38:06.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.025197943s
    Jan 19 20:38:08.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.026206047s
    Jan 19 20:38:10.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.027348379s
    Jan 19 20:38:12.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.027519446s
    Jan 19 20:38:14.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.027629961s
    Jan 19 20:38:16.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.025212904s
    Jan 19 20:38:18.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.027191555s
    Jan 19 20:38:20.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.027020886s
    Jan 19 20:38:22.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.026379201s
    Jan 19 20:38:24.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.026785048s
    Jan 19 20:38:26.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.025669446s
    Jan 19 20:38:28.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.026551473s
    Jan 19 20:38:30.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.025906226s
    Jan 19 20:38:32.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.027017771s
    Jan 19 20:38:34.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.027100064s
    Jan 19 20:38:36.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.025802912s
    Jan 19 20:38:38.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.027129494s
    Jan 19 20:38:40.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.027321553s
    Jan 19 20:38:42.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.026838975s
    Jan 19 20:38:44.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.027111489s
    Jan 19 20:38:46.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.026577704s
    Jan 19 20:38:48.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.0255884s
    Jan 19 20:38:50.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.026782741s
    Jan 19 20:38:52.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.027044396s
    Jan 19 20:38:54.729: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.024921959s
    Jan 19 20:38:56.730: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.025606407s
    Jan 19 20:38:58.731: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.026376378s
    Jan 19 20:39:00.732: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.027475299s
    Jan 19 20:39:00.735: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.030505608s
    STEP: updating the pod 01/19/23 20:39:00.735
    Jan 19 20:39:01.253: INFO: Successfully updated pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c"
    STEP: waiting for pod running 01/19/23 20:39:01.253
    Jan 19 20:39:01.253: INFO: Waiting up to 2m0s for pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c" in namespace "var-expansion-221" to be "running"
    Jan 19 20:39:01.256: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580866ms
    Jan 19 20:39:03.259: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c": Phase="Running", Reason="", readiness=true. Elapsed: 2.006410891s
    Jan 19 20:39:03.259: INFO: Pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c" satisfied condition "running"
    STEP: deleting the pod gracefully 01/19/23 20:39:03.259
    Jan 19 20:39:03.259: INFO: Deleting pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c" in namespace "var-expansion-221"
    Jan 19 20:39:03.265: INFO: Wait up to 5m0s for pod "var-expansion-f1c27aac-3bc8-4c9e-9db0-203e142a894c" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 20:39:35.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-221" for this suite. 01/19/23 20:39:35.277
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:39:35.283
Jan 19 20:39:35.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pods 01/19/23 20:39:35.284
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:39:35.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:39:35.316
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/19/23 20:39:35.318
STEP: submitting the pod to kubernetes 01/19/23 20:39:35.318
STEP: verifying QOS class is set on the pod 01/19/23 20:39:35.367
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jan 19 20:39:35.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4012" for this suite. 01/19/23 20:39:35.383
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":19,"skipped":224,"failed":0}
------------------------------
• [0.117 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:39:35.283
    Jan 19 20:39:35.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pods 01/19/23 20:39:35.284
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:39:35.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:39:35.316
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/19/23 20:39:35.318
    STEP: submitting the pod to kubernetes 01/19/23 20:39:35.318
    STEP: verifying QOS class is set on the pod 01/19/23 20:39:35.367
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jan 19 20:39:35.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4012" for this suite. 01/19/23 20:39:35.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:39:35.401
Jan 19 20:39:35.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename deployment 01/19/23 20:39:35.402
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:39:35.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:39:35.467
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/19/23 20:39:35.502
Jan 19 20:39:35.502: INFO: Creating simple deployment test-deployment-vl2j5
Jan 19 20:39:35.563: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 20, 39, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 20, 39, 35, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-deployment-vl2j5-777898ffcc\""}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 01/19/23 20:39:37.569
Jan 19 20:39:37.571: INFO: Deployment test-deployment-vl2j5 has Conditions: [{Available True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vl2j5-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 01/19/23 20:39:37.571
Jan 19 20:39:37.580: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 20, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 20, 39, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 20, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 20, 39, 35, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-vl2j5-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/19/23 20:39:37.58
Jan 19 20:39:37.582: INFO: Observed &Deployment event: ADDED
Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vl2j5-777898ffcc"}
Jan 19 20:39:37.582: INFO: Observed &Deployment event: MODIFIED
Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vl2j5-777898ffcc"}
Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 19 20:39:37.582: INFO: Observed &Deployment event: MODIFIED
Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vl2j5-777898ffcc" is progressing.}
Jan 19 20:39:37.582: INFO: Observed &Deployment event: MODIFIED
Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vl2j5-777898ffcc" has successfully progressed.}
Jan 19 20:39:37.582: INFO: Observed &Deployment event: MODIFIED
Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vl2j5-777898ffcc" has successfully progressed.}
Jan 19 20:39:37.583: INFO: Found Deployment test-deployment-vl2j5 in namespace deployment-5883 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 20:39:37.583: INFO: Deployment test-deployment-vl2j5 has an updated status
STEP: patching the Statefulset Status 01/19/23 20:39:37.583
Jan 19 20:39:37.583: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 19 20:39:37.589: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/19/23 20:39:37.589
Jan 19 20:39:37.590: INFO: Observed &Deployment event: ADDED
Jan 19 20:39:37.590: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vl2j5-777898ffcc"}
Jan 19 20:39:37.590: INFO: Observed &Deployment event: MODIFIED
Jan 19 20:39:37.590: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vl2j5-777898ffcc"}
Jan 19 20:39:37.590: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 19 20:39:37.590: INFO: Observed &Deployment event: MODIFIED
Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vl2j5-777898ffcc" is progressing.}
Jan 19 20:39:37.591: INFO: Observed &Deployment event: MODIFIED
Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vl2j5-777898ffcc" has successfully progressed.}
Jan 19 20:39:37.591: INFO: Observed &Deployment event: MODIFIED
Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vl2j5-777898ffcc" has successfully progressed.}
Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 20:39:37.591: INFO: Observed &Deployment event: MODIFIED
Jan 19 20:39:37.591: INFO: Found deployment test-deployment-vl2j5 in namespace deployment-5883 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan 19 20:39:37.591: INFO: Deployment test-deployment-vl2j5 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 20:39:37.595: INFO: Deployment "test-deployment-vl2j5":
&Deployment{ObjectMeta:{test-deployment-vl2j5  deployment-5883  7e1fb040-8e94-4e9c-b415-7b2951e8a566 138274 1 2023-01-19 20:39:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-19 20:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-19 20:39:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-19 20:39:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000854258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-vl2j5-777898ffcc",LastUpdateTime:2023-01-19 20:39:37 +0000 UTC,LastTransitionTime:2023-01-19 20:39:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 20:39:37.598: INFO: New ReplicaSet "test-deployment-vl2j5-777898ffcc" of Deployment "test-deployment-vl2j5":
&ReplicaSet{ObjectMeta:{test-deployment-vl2j5-777898ffcc  deployment-5883  88895e95-e520-4784-9f75-4f1746d206ef 138259 1 2023-01-19 20:39:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-vl2j5 7e1fb040-8e94-4e9c-b415-7b2951e8a566 0xc003dd46a0 0xc003dd46a1}] [] [{kube-controller-manager Update apps/v1 2023-01-19 20:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e1fb040-8e94-4e9c-b415-7b2951e8a566\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 20:39:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dd4748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 20:39:37.601: INFO: Pod "test-deployment-vl2j5-777898ffcc-6npqd" is available:
&Pod{ObjectMeta:{test-deployment-vl2j5-777898ffcc-6npqd test-deployment-vl2j5-777898ffcc- deployment-5883  64cdf1d7-b8ab-4eb3-8c65-bf2c3ff5b41b 138258 0 2023-01-19 20:39:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.28/23"],"mac_address":"0a:58:0a:80:08:1c","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.28/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.28"
    ],
    "mac": "0a:58:0a:80:08:1c",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.28"
    ],
    "mac": "0a:58:0a:80:08:1c",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-vl2j5-777898ffcc 88895e95-e520-4784-9f75-4f1746d206ef 0xc00e4f8f27 0xc00e4f8f28}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88895e95-e520-4784-9f75-4f1746d206ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 20:39:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-19 20:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r59kz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r59kz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c20,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:39:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:39:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.28,StartTime:2023-01-19 20:39:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:39:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8328811caf7857ff7ec1426f14009fa48cdba8c411f565f7dd744eb6a9815d8f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 20:39:37.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5883" for this suite. 01/19/23 20:39:37.605
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":20,"skipped":231,"failed":0}
------------------------------
• [2.208 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:39:35.401
    Jan 19 20:39:35.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename deployment 01/19/23 20:39:35.402
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:39:35.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:39:35.467
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/19/23 20:39:35.502
    Jan 19 20:39:35.502: INFO: Creating simple deployment test-deployment-vl2j5
    Jan 19 20:39:35.563: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 20, 39, 35, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 20, 39, 35, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"test-deployment-vl2j5-777898ffcc\""}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 01/19/23 20:39:37.569
    Jan 19 20:39:37.571: INFO: Deployment test-deployment-vl2j5 has Conditions: [{Available True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vl2j5-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 01/19/23 20:39:37.571
    Jan 19 20:39:37.580: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 20, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 20, 39, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 20, 39, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 20, 39, 35, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-vl2j5-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/19/23 20:39:37.58
    Jan 19 20:39:37.582: INFO: Observed &Deployment event: ADDED
    Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vl2j5-777898ffcc"}
    Jan 19 20:39:37.582: INFO: Observed &Deployment event: MODIFIED
    Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vl2j5-777898ffcc"}
    Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 19 20:39:37.582: INFO: Observed &Deployment event: MODIFIED
    Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vl2j5-777898ffcc" is progressing.}
    Jan 19 20:39:37.582: INFO: Observed &Deployment event: MODIFIED
    Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vl2j5-777898ffcc" has successfully progressed.}
    Jan 19 20:39:37.582: INFO: Observed &Deployment event: MODIFIED
    Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 19 20:39:37.582: INFO: Observed Deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vl2j5-777898ffcc" has successfully progressed.}
    Jan 19 20:39:37.583: INFO: Found Deployment test-deployment-vl2j5 in namespace deployment-5883 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 19 20:39:37.583: INFO: Deployment test-deployment-vl2j5 has an updated status
    STEP: patching the Statefulset Status 01/19/23 20:39:37.583
    Jan 19 20:39:37.583: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 19 20:39:37.589: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/19/23 20:39:37.589
    Jan 19 20:39:37.590: INFO: Observed &Deployment event: ADDED
    Jan 19 20:39:37.590: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vl2j5-777898ffcc"}
    Jan 19 20:39:37.590: INFO: Observed &Deployment event: MODIFIED
    Jan 19 20:39:37.590: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-vl2j5-777898ffcc"}
    Jan 19 20:39:37.590: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 19 20:39:37.590: INFO: Observed &Deployment event: MODIFIED
    Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:35 +0000 UTC 2023-01-19 20:39:35 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-vl2j5-777898ffcc" is progressing.}
    Jan 19 20:39:37.591: INFO: Observed &Deployment event: MODIFIED
    Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vl2j5-777898ffcc" has successfully progressed.}
    Jan 19 20:39:37.591: INFO: Observed &Deployment event: MODIFIED
    Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-19 20:39:36 +0000 UTC 2023-01-19 20:39:35 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-vl2j5-777898ffcc" has successfully progressed.}
    Jan 19 20:39:37.591: INFO: Observed deployment test-deployment-vl2j5 in namespace deployment-5883 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 19 20:39:37.591: INFO: Observed &Deployment event: MODIFIED
    Jan 19 20:39:37.591: INFO: Found deployment test-deployment-vl2j5 in namespace deployment-5883 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan 19 20:39:37.591: INFO: Deployment test-deployment-vl2j5 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 20:39:37.595: INFO: Deployment "test-deployment-vl2j5":
    &Deployment{ObjectMeta:{test-deployment-vl2j5  deployment-5883  7e1fb040-8e94-4e9c-b415-7b2951e8a566 138274 1 2023-01-19 20:39:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-19 20:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-19 20:39:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-19 20:39:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000854258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-vl2j5-777898ffcc",LastUpdateTime:2023-01-19 20:39:37 +0000 UTC,LastTransitionTime:2023-01-19 20:39:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 19 20:39:37.598: INFO: New ReplicaSet "test-deployment-vl2j5-777898ffcc" of Deployment "test-deployment-vl2j5":
    &ReplicaSet{ObjectMeta:{test-deployment-vl2j5-777898ffcc  deployment-5883  88895e95-e520-4784-9f75-4f1746d206ef 138259 1 2023-01-19 20:39:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-vl2j5 7e1fb040-8e94-4e9c-b415-7b2951e8a566 0xc003dd46a0 0xc003dd46a1}] [] [{kube-controller-manager Update apps/v1 2023-01-19 20:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e1fb040-8e94-4e9c-b415-7b2951e8a566\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 20:39:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dd4748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 20:39:37.601: INFO: Pod "test-deployment-vl2j5-777898ffcc-6npqd" is available:
    &Pod{ObjectMeta:{test-deployment-vl2j5-777898ffcc-6npqd test-deployment-vl2j5-777898ffcc- deployment-5883  64cdf1d7-b8ab-4eb3-8c65-bf2c3ff5b41b 138258 0 2023-01-19 20:39:35 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.28/23"],"mac_address":"0a:58:0a:80:08:1c","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.28/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.28"
        ],
        "mac": "0a:58:0a:80:08:1c",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.28"
        ],
        "mac": "0a:58:0a:80:08:1c",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-vl2j5-777898ffcc 88895e95-e520-4784-9f75-4f1746d206ef 0xc00e4f8f27 0xc00e4f8f28}] [] [{ip-10-0-194-246 Update v1 2023-01-19 20:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 20:39:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88895e95-e520-4784-9f75-4f1746d206ef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 20:39:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.28\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-19 20:39:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r59kz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r59kz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c35,c20,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:39:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:39:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 20:39:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.28,StartTime:2023-01-19 20:39:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 20:39:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://8328811caf7857ff7ec1426f14009fa48cdba8c411f565f7dd744eb6a9815d8f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.28,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 20:39:37.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5883" for this suite. 01/19/23 20:39:37.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:39:37.61
Jan 19 20:39:37.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename dns 01/19/23 20:39:37.611
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:39:37.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:39:37.632
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/19/23 20:39:37.638
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/19/23 20:39:37.638
STEP: creating a pod to probe DNS 01/19/23 20:39:37.638
STEP: submitting the pod to kubernetes 01/19/23 20:39:37.638
W0119 20:39:37.681938      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 20:39:37.682: INFO: Waiting up to 15m0s for pod "dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8" in namespace "dns-8782" to be "running"
Jan 19 20:39:37.706: INFO: Pod "dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.946801ms
Jan 19 20:39:39.710: INFO: Pod "dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028113489s
Jan 19 20:39:41.710: INFO: Pod "dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02810479s
Jan 19 20:39:43.710: INFO: Pod "dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8": Phase="Running", Reason="", readiness=true. Elapsed: 6.02796915s
Jan 19 20:39:43.710: INFO: Pod "dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8" satisfied condition "running"
STEP: retrieving the pod 01/19/23 20:39:43.71
STEP: looking for the results for each expected name from probers 01/19/23 20:39:43.713
Jan 19 20:39:43.727: INFO: DNS probes using dns-8782/dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8 succeeded

STEP: deleting the pod 01/19/23 20:39:43.727
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 20:39:43.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8782" for this suite. 01/19/23 20:39:43.741
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":21,"skipped":249,"failed":0}
------------------------------
• [SLOW TEST] [6.136 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:39:37.61
    Jan 19 20:39:37.611: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename dns 01/19/23 20:39:37.611
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:39:37.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:39:37.632
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/19/23 20:39:37.638
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/19/23 20:39:37.638
    STEP: creating a pod to probe DNS 01/19/23 20:39:37.638
    STEP: submitting the pod to kubernetes 01/19/23 20:39:37.638
    W0119 20:39:37.681938      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (containers "webserver", "querier", "jessie-querier" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers "webserver", "querier", "jessie-querier" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.runAsNonRoot=true), seccompProfile (pod or containers "webserver", "querier", "jessie-querier" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 20:39:37.682: INFO: Waiting up to 15m0s for pod "dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8" in namespace "dns-8782" to be "running"
    Jan 19 20:39:37.706: INFO: Pod "dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.946801ms
    Jan 19 20:39:39.710: INFO: Pod "dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028113489s
    Jan 19 20:39:41.710: INFO: Pod "dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02810479s
    Jan 19 20:39:43.710: INFO: Pod "dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8": Phase="Running", Reason="", readiness=true. Elapsed: 6.02796915s
    Jan 19 20:39:43.710: INFO: Pod "dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 20:39:43.71
    STEP: looking for the results for each expected name from probers 01/19/23 20:39:43.713
    Jan 19 20:39:43.727: INFO: DNS probes using dns-8782/dns-test-6cbbfb1a-b8a6-4fbf-9095-e27ed693ecd8 succeeded

    STEP: deleting the pod 01/19/23 20:39:43.727
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 20:39:43.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8782" for this suite. 01/19/23 20:39:43.741
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:39:43.747
Jan 19 20:39:43.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename statefulset 01/19/23 20:39:43.748
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:39:43.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:39:43.768
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2205 01/19/23 20:39:43.772
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-2205 01/19/23 20:39:43.793
W0119 20:39:43.806419      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 20:39:43.821: INFO: Found 0 stateful pods, waiting for 1
Jan 19 20:39:53.826: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/19/23 20:39:53.83
STEP: updating a scale subresource 01/19/23 20:39:53.832
STEP: verifying the statefulset Spec.Replicas was modified 01/19/23 20:39:53.838
STEP: Patch a scale subresource 01/19/23 20:39:53.84
STEP: verifying the statefulset Spec.Replicas was modified 01/19/23 20:39:53.846
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 20:39:53.850: INFO: Deleting all statefulset in ns statefulset-2205
Jan 19 20:39:53.852: INFO: Scaling statefulset ss to 0
Jan 19 20:40:03.870: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 20:40:03.872: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 20:40:03.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2205" for this suite. 01/19/23 20:40:03.892
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":22,"skipped":258,"failed":0}
------------------------------
• [SLOW TEST] [20.153 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:39:43.747
    Jan 19 20:39:43.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename statefulset 01/19/23 20:39:43.748
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:39:43.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:39:43.768
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2205 01/19/23 20:39:43.772
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-2205 01/19/23 20:39:43.793
    W0119 20:39:43.806419      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 20:39:43.821: INFO: Found 0 stateful pods, waiting for 1
    Jan 19 20:39:53.826: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/19/23 20:39:53.83
    STEP: updating a scale subresource 01/19/23 20:39:53.832
    STEP: verifying the statefulset Spec.Replicas was modified 01/19/23 20:39:53.838
    STEP: Patch a scale subresource 01/19/23 20:39:53.84
    STEP: verifying the statefulset Spec.Replicas was modified 01/19/23 20:39:53.846
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 20:39:53.850: INFO: Deleting all statefulset in ns statefulset-2205
    Jan 19 20:39:53.852: INFO: Scaling statefulset ss to 0
    Jan 19 20:40:03.870: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 20:40:03.872: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 20:40:03.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2205" for this suite. 01/19/23 20:40:03.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:40:03.901
Jan 19 20:40:03.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename replicaset 01/19/23 20:40:03.902
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:03.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:03.928
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan 19 20:40:04.000: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 19 20:40:09.006: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/19/23 20:40:09.006
STEP: Scaling up "test-rs" replicaset  01/19/23 20:40:09.006
Jan 19 20:40:09.014: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/19/23 20:40:09.014
W0119 20:40:09.021681      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 19 20:40:09.022: INFO: observed ReplicaSet test-rs in namespace replicaset-1142 with ReadyReplicas 1, AvailableReplicas 1
Jan 19 20:40:09.040: INFO: observed ReplicaSet test-rs in namespace replicaset-1142 with ReadyReplicas 1, AvailableReplicas 1
Jan 19 20:40:09.064: INFO: observed ReplicaSet test-rs in namespace replicaset-1142 with ReadyReplicas 1, AvailableReplicas 1
Jan 19 20:40:09.069: INFO: observed ReplicaSet test-rs in namespace replicaset-1142 with ReadyReplicas 1, AvailableReplicas 1
Jan 19 20:40:10.660: INFO: observed ReplicaSet test-rs in namespace replicaset-1142 with ReadyReplicas 2, AvailableReplicas 2
Jan 19 20:40:11.426: INFO: observed Replicaset test-rs in namespace replicaset-1142 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 19 20:40:11.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1142" for this suite. 01/19/23 20:40:11.431
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":23,"skipped":291,"failed":0}
------------------------------
• [SLOW TEST] [7.536 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:40:03.901
    Jan 19 20:40:03.902: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename replicaset 01/19/23 20:40:03.902
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:03.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:03.928
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan 19 20:40:04.000: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 19 20:40:09.006: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/19/23 20:40:09.006
    STEP: Scaling up "test-rs" replicaset  01/19/23 20:40:09.006
    Jan 19 20:40:09.014: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/19/23 20:40:09.014
    W0119 20:40:09.021681      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 19 20:40:09.022: INFO: observed ReplicaSet test-rs in namespace replicaset-1142 with ReadyReplicas 1, AvailableReplicas 1
    Jan 19 20:40:09.040: INFO: observed ReplicaSet test-rs in namespace replicaset-1142 with ReadyReplicas 1, AvailableReplicas 1
    Jan 19 20:40:09.064: INFO: observed ReplicaSet test-rs in namespace replicaset-1142 with ReadyReplicas 1, AvailableReplicas 1
    Jan 19 20:40:09.069: INFO: observed ReplicaSet test-rs in namespace replicaset-1142 with ReadyReplicas 1, AvailableReplicas 1
    Jan 19 20:40:10.660: INFO: observed ReplicaSet test-rs in namespace replicaset-1142 with ReadyReplicas 2, AvailableReplicas 2
    Jan 19 20:40:11.426: INFO: observed Replicaset test-rs in namespace replicaset-1142 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 19 20:40:11.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1142" for this suite. 01/19/23 20:40:11.431
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:40:11.439
Jan 19 20:40:11.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 20:40:11.439
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:11.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:11.462
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-68e0ae23-3cc2-462b-8227-6bd27b873661 01/19/23 20:40:11.465
STEP: Creating a pod to test consume configMaps 01/19/23 20:40:11.482
Jan 19 20:40:11.538: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7" in namespace "projected-1955" to be "Succeeded or Failed"
Jan 19 20:40:11.568: INFO: Pod "pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006064ms
Jan 19 20:40:13.572: INFO: Pod "pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033901742s
Jan 19 20:40:15.573: INFO: Pod "pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034022057s
STEP: Saw pod success 01/19/23 20:40:15.573
Jan 19 20:40:15.573: INFO: Pod "pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7" satisfied condition "Succeeded or Failed"
Jan 19 20:40:15.575: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 20:40:15.585
Jan 19 20:40:15.596: INFO: Waiting for pod pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7 to disappear
Jan 19 20:40:15.601: INFO: Pod pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 20:40:15.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1955" for this suite. 01/19/23 20:40:15.607
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":24,"skipped":311,"failed":0}
------------------------------
• [4.172 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:40:11.439
    Jan 19 20:40:11.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 20:40:11.439
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:11.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:11.462
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-68e0ae23-3cc2-462b-8227-6bd27b873661 01/19/23 20:40:11.465
    STEP: Creating a pod to test consume configMaps 01/19/23 20:40:11.482
    Jan 19 20:40:11.538: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7" in namespace "projected-1955" to be "Succeeded or Failed"
    Jan 19 20:40:11.568: INFO: Pod "pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006064ms
    Jan 19 20:40:13.572: INFO: Pod "pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033901742s
    Jan 19 20:40:15.573: INFO: Pod "pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034022057s
    STEP: Saw pod success 01/19/23 20:40:15.573
    Jan 19 20:40:15.573: INFO: Pod "pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7" satisfied condition "Succeeded or Failed"
    Jan 19 20:40:15.575: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 20:40:15.585
    Jan 19 20:40:15.596: INFO: Waiting for pod pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7 to disappear
    Jan 19 20:40:15.601: INFO: Pod pod-projected-configmaps-776dc05b-99fc-4b88-8382-c14bee3c71b7 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 20:40:15.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1955" for this suite. 01/19/23 20:40:15.607
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:40:15.613
Jan 19 20:40:15.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 20:40:15.614
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:15.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:15.634
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-3e2df1e2-0518-424b-9b56-4b6cb71c70fc 01/19/23 20:40:15.636
STEP: Creating a pod to test consume secrets 01/19/23 20:40:15.645
Jan 19 20:40:15.669: INFO: Waiting up to 5m0s for pod "pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405" in namespace "secrets-2931" to be "Succeeded or Failed"
Jan 19 20:40:15.672: INFO: Pod "pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405": Phase="Pending", Reason="", readiness=false. Elapsed: 2.337307ms
Jan 19 20:40:17.675: INFO: Pod "pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005460896s
Jan 19 20:40:19.676: INFO: Pod "pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006171429s
STEP: Saw pod success 01/19/23 20:40:19.676
Jan 19 20:40:19.676: INFO: Pod "pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405" satisfied condition "Succeeded or Failed"
Jan 19 20:40:19.678: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405 container secret-env-test: <nil>
STEP: delete the pod 01/19/23 20:40:19.685
Jan 19 20:40:19.700: INFO: Waiting for pod pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405 to disappear
Jan 19 20:40:19.703: INFO: Pod pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 19 20:40:19.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2931" for this suite. 01/19/23 20:40:19.709
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":25,"skipped":371,"failed":0}
------------------------------
• [4.101 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:40:15.613
    Jan 19 20:40:15.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 20:40:15.614
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:15.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:15.634
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-3e2df1e2-0518-424b-9b56-4b6cb71c70fc 01/19/23 20:40:15.636
    STEP: Creating a pod to test consume secrets 01/19/23 20:40:15.645
    Jan 19 20:40:15.669: INFO: Waiting up to 5m0s for pod "pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405" in namespace "secrets-2931" to be "Succeeded or Failed"
    Jan 19 20:40:15.672: INFO: Pod "pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405": Phase="Pending", Reason="", readiness=false. Elapsed: 2.337307ms
    Jan 19 20:40:17.675: INFO: Pod "pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005460896s
    Jan 19 20:40:19.676: INFO: Pod "pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006171429s
    STEP: Saw pod success 01/19/23 20:40:19.676
    Jan 19 20:40:19.676: INFO: Pod "pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405" satisfied condition "Succeeded or Failed"
    Jan 19 20:40:19.678: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405 container secret-env-test: <nil>
    STEP: delete the pod 01/19/23 20:40:19.685
    Jan 19 20:40:19.700: INFO: Waiting for pod pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405 to disappear
    Jan 19 20:40:19.703: INFO: Pod pod-secrets-0b77e864-4cf3-440c-83c3-61e5ace3f405 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 20:40:19.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2931" for this suite. 01/19/23 20:40:19.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:40:19.715
Jan 19 20:40:19.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 20:40:19.715
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:19.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:19.745
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 01/19/23 20:40:19.748
Jan 19 20:40:19.776: INFO: Waiting up to 5m0s for pod "downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa" in namespace "downward-api-1819" to be "Succeeded or Failed"
Jan 19 20:40:19.797: INFO: Pod "downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 21.004756ms
Jan 19 20:40:21.800: INFO: Pod "downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024094458s
Jan 19 20:40:23.800: INFO: Pod "downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023917544s
STEP: Saw pod success 01/19/23 20:40:23.8
Jan 19 20:40:23.800: INFO: Pod "downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa" satisfied condition "Succeeded or Failed"
Jan 19 20:40:23.802: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa container dapi-container: <nil>
STEP: delete the pod 01/19/23 20:40:23.814
Jan 19 20:40:23.827: INFO: Waiting for pod downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa to disappear
Jan 19 20:40:23.829: INFO: Pod downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 19 20:40:23.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1819" for this suite. 01/19/23 20:40:23.839
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":26,"skipped":377,"failed":0}
------------------------------
• [4.130 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:40:19.715
    Jan 19 20:40:19.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 20:40:19.715
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:19.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:19.745
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 01/19/23 20:40:19.748
    Jan 19 20:40:19.776: INFO: Waiting up to 5m0s for pod "downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa" in namespace "downward-api-1819" to be "Succeeded or Failed"
    Jan 19 20:40:19.797: INFO: Pod "downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 21.004756ms
    Jan 19 20:40:21.800: INFO: Pod "downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024094458s
    Jan 19 20:40:23.800: INFO: Pod "downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023917544s
    STEP: Saw pod success 01/19/23 20:40:23.8
    Jan 19 20:40:23.800: INFO: Pod "downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa" satisfied condition "Succeeded or Failed"
    Jan 19 20:40:23.802: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa container dapi-container: <nil>
    STEP: delete the pod 01/19/23 20:40:23.814
    Jan 19 20:40:23.827: INFO: Waiting for pod downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa to disappear
    Jan 19 20:40:23.829: INFO: Pod downward-api-73d7c7af-ae3a-410c-ba09-d7b1f96f6ffa no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 19 20:40:23.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1819" for this suite. 01/19/23 20:40:23.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:40:23.846
Jan 19 20:40:23.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename sched-pred 01/19/23 20:40:23.847
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:23.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:23.869
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 19 20:40:23.872: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 20:40:23.911: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 20:40:23.935: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-146-42.ec2.internal before test
Jan 19 20:40:23.984: INFO: aws-ebs-csi-driver-node-vxgq2 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:41 +0000 UTC (3 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 20:40:23.984: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 20:40:23.984: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 20:40:23.984: INFO: tuned-nbzt4 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container tuned ready: true, restart count 1
Jan 19 20:40:23.984: INFO: downloads-8d695cd69-lcsd9 from openshift-console started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container download-server ready: true, restart count 0
Jan 19 20:40:23.984: INFO: dns-default-c5qwq from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container dns ready: true, restart count 1
Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:23.984: INFO: node-resolver-g5jk6 from openshift-dns started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 20:40:23.984: INFO: node-ca-crm7t from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 20:40:23.984: INFO: ingress-canary-nfwhl from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 20:40:23.984: INFO: migrator-5c54d8d69d-l5lss from openshift-kube-storage-version-migrator started at 2023-01-19 18:42:53 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container migrator ready: true, restart count 0
Jan 19 20:40:23.984: INFO: machine-config-daemon-cc29b from openshift-machine-config-operator started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 20:40:23.984: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 20:40:23.984: INFO: node-exporter-9zc87 from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:23.984: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 20:40:23.984: INFO: sre-dns-latency-exporter-jsn9x from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container main ready: true, restart count 1
Jan 19 20:40:23.984: INFO: multus-5zck8 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 20:40:23.984: INFO: multus-additional-cni-plugins-88bc2 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 20:40:23.984: INFO: network-metrics-daemon-zvctv from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:23.984: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 20:40:23.984: INFO: network-check-source-746dd6c885-bmf5k from openshift-network-diagnostics started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 19 20:40:23.984: INFO: network-check-target-g5qsk from openshift-network-diagnostics started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 20:40:23.984: INFO: obo-prometheus-operator-64989cfc68-hdxb9 from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 20:40:23.984: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-gqhlz from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 20:40:23.984: INFO: osd-metrics-exporter-registry-qdjtk from openshift-osd-metrics started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:23.984: INFO: ovnkube-node-ftv9n from openshift-ovn-kubernetes started at 2023-01-19 18:15:41 +0000 UTC (5 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 20:40:23.984: INFO: 	Container ovn-acl-logging ready: true, restart count 2
Jan 19 20:40:23.984: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 20:40:23.984: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 20:40:23.984: INFO: rbac-permissions-operator-registry-rc4js from openshift-rbac-permissions started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:23.984: INFO: splunkforwarder-ds-7rpx7 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 20:40:23.984: INFO: prometheus-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (6 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 20:40:23.984: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 20:40:23.984: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 20:40:23.984: INFO: thanos-ruler-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (3 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 20:40:23.984: INFO: 	Container thanos-ruler ready: true, restart count 0
Jan 19 20:40:23.984: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
Jan 19 20:40:23.984: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-chbvs from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:23.984: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 20:40:23.984: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 20:40:23.984: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-151-158.ec2.internal before test
Jan 19 20:40:24.028: INFO: addon-operator-manager-84ff88fc6-fmhnb from openshift-addon-operator started at 2023-01-19 18:46:50 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container manager ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container metrics-relay-server ready: true, restart count 0
Jan 19 20:40:24.028: INFO: addon-operator-webhooks-67d9d47489-4kbk2 from openshift-addon-operator started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container webhook ready: true, restart count 0
Jan 19 20:40:24.028: INFO: aws-ebs-csi-driver-node-c8m6c from openshift-cluster-csi-drivers started at 2023-01-19 18:32:48 +0000 UTC (3 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 20:40:24.028: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 20:40:24.028: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 20:40:24.028: INFO: tuned-5h69k from openshift-cluster-node-tuning-operator started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container tuned ready: true, restart count 1
Jan 19 20:40:24.028: INFO: custom-domains-operator-5764f4df49-8nbdk from openshift-custom-domains-operator started at 2023-01-19 19:15:08 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container custom-domains-operator ready: true, restart count 0
Jan 19 20:40:24.028: INFO: deployment-validation-operator-7b8fd479cb-wtzg7 from openshift-deployment-validation-operator started at 2023-01-19 18:46:50 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container deployment-validation-operator ready: true, restart count 0
Jan 19 20:40:24.028: INFO: dns-default-rq2vh from openshift-dns started at 2023-01-19 20:30:23 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container dns ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 20:40:24.028: INFO: node-resolver-9cwpx from openshift-dns started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 20:40:24.028: INFO: image-pruner-27902580-lzd8r from openshift-image-registry started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 20:40:24.028: INFO: image-pruner-27902640-c6mlt from openshift-image-registry started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 20:40:24.028: INFO: image-registry-656cfd9bd9-9x67r from openshift-image-registry started at 2023-01-19 19:15:11 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container registry ready: true, restart count 0
Jan 19 20:40:24.028: INFO: node-ca-k89p7 from openshift-image-registry started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 20:40:24.028: INFO: ingress-canary-v2tjj from openshift-ingress-canary started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 20:40:24.028: INFO: router-default-6bf5ff7dcb-5nzvc from openshift-ingress started at 2023-01-19 18:47:44 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container router ready: true, restart count 0
Jan 19 20:40:24.028: INFO: machine-config-daemon-28fzw from openshift-machine-config-operator started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 20:40:24.028: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 20:40:24.028: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container alertmanager ready: true, restart count 1
Jan 19 20:40:24.028: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 20:40:24.028: INFO: configure-alertmanager-operator-7d94bbbcf-8h7xs from openshift-monitoring started at 2023-01-19 19:16:27 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Jan 19 20:40:24.028: INFO: kube-state-metrics-666f4cbf77-5w8rk from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 19 20:40:24.028: INFO: node-exporter-7t5b7 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.028: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 20:40:24.028: INFO: openshift-state-metrics-7d5c9d867c-97gzd from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 19 20:40:24.028: INFO: prometheus-adapter-5bc6487f7c-q2ng7 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 19 20:40:24.028: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 20:40:24.028: INFO: prometheus-operator-564b78d5ff-v4xnk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 20:40:24.028: INFO: prometheus-operator-admission-webhook-749df5cf4f-zj8xj from openshift-monitoring started at 2023-01-19 18:46:40 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 20:40:24.028: INFO: sre-dns-latency-exporter-2q899 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container main ready: true, restart count 1
Jan 19 20:40:24.028: INFO: sre-stuck-ebs-vols-1-vdlrh from openshift-monitoring started at 2023-01-19 19:17:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container main ready: true, restart count 0
Jan 19 20:40:24.028: INFO: telemeter-client-54bbbbf598-d9fdk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (3 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container reload ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 19 20:40:24.028: INFO: thanos-querier-fcd5b6c8d-ql8qb from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container thanos-query ready: true, restart count 0
Jan 19 20:40:24.028: INFO: multus-additional-cni-plugins-vslld from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 20:40:24.028: INFO: multus-zqhtq from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 20:40:24.028: INFO: network-metrics-daemon-677lg from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.028: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 20:40:24.028: INFO: must-gather-operator-56b466776c-qv8sh from openshift-must-gather-operator started at 2023-01-19 19:17:38 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container must-gather-operator ready: true, restart count 0
Jan 19 20:40:24.028: INFO: network-check-target-xrr4d from openshift-network-diagnostics started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 20:40:24.028: INFO: ocm-agent-ddfdc6544-4fxrp from openshift-ocm-agent-operator started at 2023-01-19 19:17:41 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container ocm-agent ready: true, restart count 0
Jan 19 20:40:24.028: INFO: ocm-agent-operator-6cdb45fd86-4clcj from openshift-ocm-agent-operator started at 2023-01-19 19:17:43 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container ocm-agent-operator ready: true, restart count 0
Jan 19 20:40:24.028: INFO: ovnkube-node-gvwrr from openshift-ovn-kubernetes started at 2023-01-19 18:32:48 +0000 UTC (5 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 20:40:24.028: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 20:40:24.028: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 20:40:24.028: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 20:40:24.028: INFO: rbac-permissions-operator-5fb57974d8-htnjj from openshift-rbac-permissions started at 2023-01-19 19:17:50 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Jan 19 20:40:24.028: INFO: splunkforwarder-ds-ztlpn from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 20:40:24.028: INFO: splunk-forwarder-operator-7d66568cf7-bksrn from openshift-splunk-forwarder-operator started at 2023-01-19 19:17:55 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Jan 19 20:40:24.028: INFO: managed-velero-operator-788754d4b6-n89fw from openshift-velero started at 2023-01-19 19:17:56 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container managed-velero-operator ready: true, restart count 0
Jan 19 20:40:24.028: INFO: velero-7c64598b8c-8blx4 from openshift-velero started at 2023-01-19 19:18:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container velero ready: true, restart count 0
Jan 19 20:40:24.028: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-8mc8n from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.028: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 20:40:24.028: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-171-213.ec2.internal before test
Jan 19 20:40:24.069: INFO: aws-ebs-csi-driver-node-svd2s from openshift-cluster-csi-drivers started at 2023-01-19 18:15:58 +0000 UTC (3 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 20:40:24.069: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 20:40:24.069: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 20:40:24.069: INFO: tuned-9p2db from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container tuned ready: true, restart count 1
Jan 19 20:40:24.069: INFO: dns-default-zbx44 from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container dns ready: true, restart count 1
Jan 19 20:40:24.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.069: INFO: node-resolver-5mb95 from openshift-dns started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 20:40:24.069: INFO: node-ca-7c5fq from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 20:40:24.069: INFO: ingress-canary-6sc9q from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 20:40:24.069: INFO: machine-config-daemon-xxm6j from openshift-machine-config-operator started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 20:40:24.069: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 20:40:24.069: INFO: node-exporter-h445b from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.069: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 20:40:24.069: INFO: sre-dns-latency-exporter-vhwwx from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container main ready: true, restart count 1
Jan 19 20:40:24.069: INFO: multus-additional-cni-plugins-nhwfz from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 20:40:24.069: INFO: multus-j4ngm from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 20:40:24.069: INFO: network-metrics-daemon-hp85h from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.069: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 20:40:24.069: INFO: network-check-target-mg7p4 from openshift-network-diagnostics started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 20:40:24.069: INFO: ovnkube-node-kk5t9 from openshift-ovn-kubernetes started at 2023-01-19 18:15:58 +0000 UTC (5 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.069: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 20:40:24.069: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 20:40:24.069: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 20:40:24.069: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 20:40:24.069: INFO: splunkforwarder-ds-66kjq from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 20:40:24.069: INFO: sonobuoy-e2e-job-4d801e056c4540b8 from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container e2e ready: true, restart count 0
Jan 19 20:40:24.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 20:40:24.069: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-m8vnk from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 20:40:24.069: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 20:40:24.069: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-172-44.ec2.internal before test
Jan 19 20:40:24.098: INFO: aws-ebs-csi-driver-node-z6klb from openshift-cluster-csi-drivers started at 2023-01-19 18:15:33 +0000 UTC (3 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 20:40:24.098: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 20:40:24.098: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 20:40:24.098: INFO: tuned-gjnt5 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container tuned ready: true, restart count 1
Jan 19 20:40:24.098: INFO: dns-default-7fx75 from openshift-dns started at 2023-01-19 18:16:04 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container dns ready: true, restart count 1
Jan 19 20:40:24.098: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.098: INFO: node-resolver-b85qs from openshift-dns started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 20:40:24.098: INFO: node-ca-dsxvs from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 20:40:24.098: INFO: ingress-canary-2h6zr from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 20:40:24.098: INFO: machine-config-daemon-cllj8 from openshift-machine-config-operator started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 20:40:24.098: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 20:40:24.098: INFO: node-exporter-2vp9v from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.098: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 20:40:24.098: INFO: sre-dns-latency-exporter-ctmgr from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container main ready: true, restart count 1
Jan 19 20:40:24.098: INFO: multus-additional-cni-plugins-p5pjp from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 20:40:24.098: INFO: multus-xzxwg from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 20:40:24.098: INFO: network-metrics-daemon-hz5pj from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.098: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 20:40:24.098: INFO: network-check-target-nq9ln from openshift-network-diagnostics started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 20:40:24.098: INFO: collect-profiles-27902640-5s7dt from openshift-operator-lifecycle-manager started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 20:40:24.098: INFO: collect-profiles-27902655-tjcq5 from openshift-operator-lifecycle-manager started at 2023-01-19 20:15:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 20:40:24.098: INFO: collect-profiles-27902670-rjh5b from openshift-operator-lifecycle-manager started at 2023-01-19 20:30:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 20:40:24.098: INFO: ovnkube-node-zmvs8 from openshift-ovn-kubernetes started at 2023-01-19 18:15:33 +0000 UTC (5 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.098: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 20:40:24.098: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 20:40:24.098: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 20:40:24.098: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 20:40:24.098: INFO: splunkforwarder-ds-pj5q8 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 20:40:24.098: INFO: pod-qos-class-dfa28a64-9c00-4636-8d99-dccceb2981f4 from pods-4012 started at 2023-01-19 20:39:35 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container agnhost ready: false, restart count 0
Jan 19 20:40:24.098: INFO: sonobuoy from sonobuoy started at 2023-01-19 20:29:23 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 19 20:40:24.098: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-cbfjg from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.098: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 20:40:24.098: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 20:40:24.098: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-188-71.ec2.internal before test
Jan 19 20:40:24.133: INFO: addon-operator-catalog-zxs8v from openshift-addon-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: cloud-ingress-operator-registry-pqwm7 from openshift-cloud-ingress-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: aws-ebs-csi-driver-node-wmvd4 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:13 +0000 UTC (3 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 20:40:24.133: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 20:40:24.133: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 20:40:24.133: INFO: tuned-7x9t2 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container tuned ready: true, restart count 1
Jan 19 20:40:24.133: INFO: downloads-8d695cd69-hltgd from openshift-console started at 2023-01-19 18:39:20 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container download-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: custom-domains-operator-registry-kdw7s from openshift-custom-domains-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: deployment-validation-operator-catalog-nq94w from openshift-deployment-validation-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: dns-default-tzrx6 from openshift-dns started at 2023-01-19 18:15:43 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container dns ready: true, restart count 1
Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.133: INFO: node-resolver-l2fzw from openshift-dns started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 20:40:24.133: INFO: node-ca-kzsvk from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 20:40:24.133: INFO: ingress-canary-t6tqp from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 20:40:24.133: INFO: machine-config-daemon-zhllf from openshift-machine-config-operator started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 20:40:24.133: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 20:40:24.133: INFO: managed-node-metadata-operator-registry-m6dxz from openshift-managed-node-metadata-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: managed-upgrade-operator-catalog-thxts from openshift-managed-upgrade-operator started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: configure-alertmanager-operator-registry-sjcb5 from openshift-monitoring started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: node-exporter-dc59d from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.133: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 20:40:24.133: INFO: osd-cluster-ready-nb8xk from openshift-monitoring started at 2023-01-19 18:39:21 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container osd-cluster-ready ready: false, restart count 7
Jan 19 20:40:24.133: INFO: sre-dns-latency-exporter-5dz25 from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container main ready: true, restart count 1
Jan 19 20:40:24.133: INFO: multus-additional-cni-plugins-hvpd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 20:40:24.133: INFO: multus-mptd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 20:40:24.133: INFO: network-metrics-daemon-s2l94 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.133: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 20:40:24.133: INFO: must-gather-operator-registry-n7fvg from openshift-must-gather-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: network-check-target-d8gvt from openshift-network-diagnostics started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 20:40:24.133: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-b4r6s from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 20:40:24.133: INFO: observability-operator-6dd79df7dc-z7frh from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container operator ready: true, restart count 0
Jan 19 20:40:24.133: INFO: observability-operator-catalog-vrnkr from openshift-observability-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: ocm-agent-operator-registry-dwfc4 from openshift-ocm-agent-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: ovnkube-node-wcrc4 from openshift-ovn-kubernetes started at 2023-01-19 18:15:13 +0000 UTC (5 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 20:40:24.133: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 20:40:24.133: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 20:40:24.133: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 20:40:24.133: INFO: route-monitor-operator-registry-whpgh from openshift-route-monitor-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: splunkforwarder-ds-xzbdk from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 20:40:24.133: INFO: splunk-forwarder-operator-catalog-fqp2b from openshift-splunk-forwarder-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: prometheus-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:37:05 +0000 UTC (6 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 20:40:24.133: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 20:40:24.133: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 20:40:24.133: INFO: thanos-ruler-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:37:04 +0000 UTC (3 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 20:40:24.133: INFO: 	Container thanos-ruler ready: true, restart count 0
Jan 19 20:40:24.133: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
Jan 19 20:40:24.133: INFO: managed-velero-operator-registry-6lc6b from openshift-velero started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 20:40:24.133: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-q5j5c from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.133: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 20:40:24.133: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 20:40:24.133: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-207-77.ec2.internal before test
Jan 19 20:40:24.175: INFO: addon-operator-webhooks-67d9d47489-f6z86 from openshift-addon-operator started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container webhook ready: true, restart count 0
Jan 19 20:40:24.175: INFO: osd-delete-ownerrefs-serviceaccounts-27902617-wh9q8 from openshift-backplane-srep started at 2023-01-19 19:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 20:40:24.175: INFO: osd-delete-ownerrefs-serviceaccounts-27902647-2672p from openshift-backplane-srep started at 2023-01-19 20:07:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 20:40:24.175: INFO: osd-delete-ownerrefs-serviceaccounts-27902677-st5fv from openshift-backplane-srep started at 2023-01-19 20:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 20:40:24.175: INFO: osd-delete-backplane-serviceaccounts-27902660-hr6ts from openshift-backplane started at 2023-01-19 20:20:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 20:40:24.175: INFO: osd-delete-backplane-serviceaccounts-27902670-zwws9 from openshift-backplane started at 2023-01-19 20:30:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 20:40:24.175: INFO: osd-delete-backplane-serviceaccounts-27902680-cthn6 from openshift-backplane started at 2023-01-19 20:40:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 20:40:24.175: INFO: sre-build-test-27902651-jvtzk from openshift-build-test started at 2023-01-19 20:11:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container sre-build-test ready: false, restart count 0
Jan 19 20:40:24.175: INFO: cloud-ingress-operator-5b5df787b5-bdk4j from openshift-cloud-ingress-operator started at 2023-01-19 19:15:07 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Jan 19 20:40:24.175: INFO: aws-ebs-csi-driver-node-9sjjv from openshift-cluster-csi-drivers started at 2023-01-19 18:33:36 +0000 UTC (3 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 20:40:24.175: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 20:40:24.175: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 20:40:24.175: INFO: tuned-qkjmb from openshift-cluster-node-tuning-operator started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container tuned ready: true, restart count 1
Jan 19 20:40:24.175: INFO: dns-default-zzp2r from openshift-dns started at 2023-01-19 20:31:07 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container dns ready: true, restart count 0
Jan 19 20:40:24.175: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 20:40:24.175: INFO: node-resolver-m4bvp from openshift-dns started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 20:40:24.175: INFO: image-registry-656cfd9bd9-bj4wn from openshift-image-registry started at 2023-01-19 19:15:47 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container registry ready: true, restart count 0
Jan 19 20:40:24.175: INFO: node-ca-kwgfx from openshift-image-registry started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 20:40:24.175: INFO: ingress-canary-6xnfp from openshift-ingress-canary started at 2023-01-19 18:34:10 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 20:40:24.175: INFO: router-default-6bf5ff7dcb-6vrcg from openshift-ingress started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container router ready: true, restart count 0
Jan 19 20:40:24.175: INFO: machine-config-daemon-2bt5b from openshift-machine-config-operator started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 20:40:24.175: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 20:40:24.175: INFO: managed-node-metadata-operator-649f646d54-smsh7 from openshift-managed-node-metadata-operator started at 2023-01-19 19:16:23 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
Jan 19 20:40:24.175: INFO: osd-patch-subscription-source-27902580-hz9tz from openshift-marketplace started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.175: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 20:40:24.176: INFO: osd-patch-subscription-source-27902640-qrd58 from openshift-marketplace started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 20:40:24.176: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container alertmanager ready: true, restart count 1
Jan 19 20:40:24.176: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 20:40:24.176: INFO: node-exporter-s77nf from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.176: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 20:40:24.176: INFO: osd-rebalance-infra-nodes-27902640-lpbrs from openshift-monitoring started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 20:40:24.176: INFO: osd-rebalance-infra-nodes-27902655-77whr from openshift-monitoring started at 2023-01-19 20:15:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 20:40:24.176: INFO: osd-rebalance-infra-nodes-27902670-v89hg from openshift-monitoring started at 2023-01-19 20:30:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 20:40:24.176: INFO: prometheus-adapter-5bc6487f7c-sv96c from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 19 20:40:24.176: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 20:40:24.176: INFO: prometheus-operator-admission-webhook-749df5cf4f-bmj4n from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 20:40:24.176: INFO: sre-dns-latency-exporter-4lksg from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container main ready: true, restart count 1
Jan 19 20:40:24.176: INFO: sre-ebs-iops-reporter-1-sg7b2 from openshift-monitoring started at 2023-01-19 19:16:28 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container main ready: true, restart count 0
Jan 19 20:40:24.176: INFO: thanos-querier-fcd5b6c8d-9jjkp from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container thanos-query ready: true, restart count 0
Jan 19 20:40:24.176: INFO: token-refresher-6f7c46d758-mld9t from openshift-monitoring started at 2023-01-19 19:17:36 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container token-refresher ready: true, restart count 0
Jan 19 20:40:24.176: INFO: multus-additional-cni-plugins-476gw from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 20:40:24.176: INFO: multus-mczpf from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 20:40:24.176: INFO: network-metrics-daemon-ddkb6 from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.176: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 20:40:24.176: INFO: network-check-target-d8k5s from openshift-network-diagnostics started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 20:40:24.176: INFO: osd-metrics-exporter-679d75d598-rsv49 from openshift-osd-metrics started at 2023-01-19 19:17:47 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
Jan 19 20:40:24.176: INFO: ovnkube-node-gq9vc from openshift-ovn-kubernetes started at 2023-01-19 18:33:36 +0000 UTC (5 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 20:40:24.176: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 20:40:24.176: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 20:40:24.176: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 20:40:24.176: INFO: blackbox-exporter-5f7f7bf859-l8ptw from openshift-route-monitor-operator started at 2023-01-19 19:17:51 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 19 20:40:24.176: INFO: route-monitor-operator-controller-manager-bfbff575-pfqc4 from openshift-route-monitor-operator started at 2023-01-19 19:17:53 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container manager ready: true, restart count 0
Jan 19 20:40:24.176: INFO: splunkforwarder-ds-h56tr from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 20:40:24.176: INFO: builds-pruner-27902580-p9bnq from openshift-sre-pruning started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 20:40:24.176: INFO: builds-pruner-27902640-bj57l from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 20:40:24.176: INFO: deployments-pruner-27902580-qfgtd from openshift-sre-pruning started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 20:40:24.176: INFO: deployments-pruner-27902640-vhqsv from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 20:40:24.176: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-hj9fv from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 20:40:24.176: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 20:40:24.176: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/19/23 20:40:24.176
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173bd0664c2d6e66], Reason = [FailedScheduling], Message = [0/9 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 9 node(s) didn't match Pod's node affinity/selector. preemption: 0/9 nodes are available: 9 Preemption is not helpful for scheduling.] 01/19/23 20:40:24.288
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 19 20:40:25.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1397" for this suite. 01/19/23 20:40:25.292
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":27,"skipped":397,"failed":0}
------------------------------
• [1.451 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:40:23.846
    Jan 19 20:40:23.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename sched-pred 01/19/23 20:40:23.847
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:23.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:23.869
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 19 20:40:23.872: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 19 20:40:23.911: INFO: Waiting for terminating namespaces to be deleted...
    Jan 19 20:40:23.935: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-146-42.ec2.internal before test
    Jan 19 20:40:23.984: INFO: aws-ebs-csi-driver-node-vxgq2 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:41 +0000 UTC (3 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: tuned-nbzt4 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: downloads-8d695cd69-lcsd9 from openshift-console started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container download-server ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: dns-default-c5qwq from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container dns ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: node-resolver-g5jk6 from openshift-dns started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: node-ca-crm7t from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: ingress-canary-nfwhl from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: migrator-5c54d8d69d-l5lss from openshift-kube-storage-version-migrator started at 2023-01-19 18:42:53 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container migrator ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: machine-config-daemon-cc29b from openshift-machine-config-operator started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: node-exporter-9zc87 from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: sre-dns-latency-exporter-jsn9x from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container main ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: multus-5zck8 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: multus-additional-cni-plugins-88bc2 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: network-metrics-daemon-zvctv from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: network-check-source-746dd6c885-bmf5k from openshift-network-diagnostics started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container check-endpoints ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: network-check-target-g5qsk from openshift-network-diagnostics started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: obo-prometheus-operator-64989cfc68-hdxb9 from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container prometheus-operator ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-gqhlz from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: osd-metrics-exporter-registry-qdjtk from openshift-osd-metrics started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: ovnkube-node-ftv9n from openshift-ovn-kubernetes started at 2023-01-19 18:15:41 +0000 UTC (5 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: 	Container ovn-acl-logging ready: true, restart count 2
    Jan 19 20:40:23.984: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: rbac-permissions-operator-registry-rc4js from openshift-rbac-permissions started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: splunkforwarder-ds-7rpx7 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 20:40:23.984: INFO: prometheus-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (6 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: thanos-ruler-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (3 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: 	Container thanos-ruler ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-chbvs from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:23.984: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 20:40:23.984: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-151-158.ec2.internal before test
    Jan 19 20:40:24.028: INFO: addon-operator-manager-84ff88fc6-fmhnb from openshift-addon-operator started at 2023-01-19 18:46:50 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container manager ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container metrics-relay-server ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: addon-operator-webhooks-67d9d47489-4kbk2 from openshift-addon-operator started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container webhook ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: aws-ebs-csi-driver-node-c8m6c from openshift-cluster-csi-drivers started at 2023-01-19 18:32:48 +0000 UTC (3 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: tuned-5h69k from openshift-cluster-node-tuning-operator started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: custom-domains-operator-5764f4df49-8nbdk from openshift-custom-domains-operator started at 2023-01-19 19:15:08 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container custom-domains-operator ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: deployment-validation-operator-7b8fd479cb-wtzg7 from openshift-deployment-validation-operator started at 2023-01-19 18:46:50 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container deployment-validation-operator ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: dns-default-rq2vh from openshift-dns started at 2023-01-19 20:30:23 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container dns ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: node-resolver-9cwpx from openshift-dns started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: image-pruner-27902580-lzd8r from openshift-image-registry started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container image-pruner ready: false, restart count 0
    Jan 19 20:40:24.028: INFO: image-pruner-27902640-c6mlt from openshift-image-registry started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container image-pruner ready: false, restart count 0
    Jan 19 20:40:24.028: INFO: image-registry-656cfd9bd9-9x67r from openshift-image-registry started at 2023-01-19 19:15:11 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container registry ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: node-ca-k89p7 from openshift-image-registry started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: ingress-canary-v2tjj from openshift-ingress-canary started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: router-default-6bf5ff7dcb-5nzvc from openshift-ingress started at 2023-01-19 18:47:44 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container router ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: machine-config-daemon-28fzw from openshift-machine-config-operator started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: configure-alertmanager-operator-7d94bbbcf-8h7xs from openshift-monitoring started at 2023-01-19 19:16:27 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: kube-state-metrics-666f4cbf77-5w8rk from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: node-exporter-7t5b7 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: openshift-state-metrics-7d5c9d867c-97gzd from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: prometheus-adapter-5bc6487f7c-q2ng7 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: prometheus-operator-564b78d5ff-v4xnk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container prometheus-operator ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: prometheus-operator-admission-webhook-749df5cf4f-zj8xj from openshift-monitoring started at 2023-01-19 18:46:40 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: sre-dns-latency-exporter-2q899 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container main ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: sre-stuck-ebs-vols-1-vdlrh from openshift-monitoring started at 2023-01-19 19:17:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container main ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: telemeter-client-54bbbbf598-d9fdk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (3 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container reload ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container telemeter-client ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: thanos-querier-fcd5b6c8d-ql8qb from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: multus-additional-cni-plugins-vslld from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: multus-zqhtq from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: network-metrics-daemon-677lg from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: must-gather-operator-56b466776c-qv8sh from openshift-must-gather-operator started at 2023-01-19 19:17:38 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container must-gather-operator ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: network-check-target-xrr4d from openshift-network-diagnostics started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: ocm-agent-ddfdc6544-4fxrp from openshift-ocm-agent-operator started at 2023-01-19 19:17:41 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container ocm-agent ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: ocm-agent-operator-6cdb45fd86-4clcj from openshift-ocm-agent-operator started at 2023-01-19 19:17:43 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container ocm-agent-operator ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: ovnkube-node-gvwrr from openshift-ovn-kubernetes started at 2023-01-19 18:32:48 +0000 UTC (5 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: rbac-permissions-operator-5fb57974d8-htnjj from openshift-rbac-permissions started at 2023-01-19 19:17:50 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: splunkforwarder-ds-ztlpn from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 20:40:24.028: INFO: splunk-forwarder-operator-7d66568cf7-bksrn from openshift-splunk-forwarder-operator started at 2023-01-19 19:17:55 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: managed-velero-operator-788754d4b6-n89fw from openshift-velero started at 2023-01-19 19:17:56 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container managed-velero-operator ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: velero-7c64598b8c-8blx4 from openshift-velero started at 2023-01-19 19:18:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container velero ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-8mc8n from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.028: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 20:40:24.028: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-171-213.ec2.internal before test
    Jan 19 20:40:24.069: INFO: aws-ebs-csi-driver-node-svd2s from openshift-cluster-csi-drivers started at 2023-01-19 18:15:58 +0000 UTC (3 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: tuned-9p2db from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: dns-default-zbx44 from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container dns ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: node-resolver-5mb95 from openshift-dns started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: node-ca-7c5fq from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: ingress-canary-6sc9q from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: machine-config-daemon-xxm6j from openshift-machine-config-operator started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: node-exporter-h445b from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: sre-dns-latency-exporter-vhwwx from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container main ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: multus-additional-cni-plugins-nhwfz from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: multus-j4ngm from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: network-metrics-daemon-hp85h from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: network-check-target-mg7p4 from openshift-network-diagnostics started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: ovnkube-node-kk5t9 from openshift-ovn-kubernetes started at 2023-01-19 18:15:58 +0000 UTC (5 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: splunkforwarder-ds-66kjq from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 20:40:24.069: INFO: sonobuoy-e2e-job-4d801e056c4540b8 from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container e2e ready: true, restart count 0
    Jan 19 20:40:24.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 20:40:24.069: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-m8vnk from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.069: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 20:40:24.069: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 20:40:24.069: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-172-44.ec2.internal before test
    Jan 19 20:40:24.098: INFO: aws-ebs-csi-driver-node-z6klb from openshift-cluster-csi-drivers started at 2023-01-19 18:15:33 +0000 UTC (3 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: tuned-gjnt5 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: dns-default-7fx75 from openshift-dns started at 2023-01-19 18:16:04 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container dns ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: node-resolver-b85qs from openshift-dns started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: node-ca-dsxvs from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: ingress-canary-2h6zr from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: machine-config-daemon-cllj8 from openshift-machine-config-operator started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: node-exporter-2vp9v from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: sre-dns-latency-exporter-ctmgr from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container main ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: multus-additional-cni-plugins-p5pjp from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: multus-xzxwg from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: network-metrics-daemon-hz5pj from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: network-check-target-nq9ln from openshift-network-diagnostics started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: collect-profiles-27902640-5s7dt from openshift-operator-lifecycle-manager started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 19 20:40:24.098: INFO: collect-profiles-27902655-tjcq5 from openshift-operator-lifecycle-manager started at 2023-01-19 20:15:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 19 20:40:24.098: INFO: collect-profiles-27902670-rjh5b from openshift-operator-lifecycle-manager started at 2023-01-19 20:30:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 19 20:40:24.098: INFO: ovnkube-node-zmvs8 from openshift-ovn-kubernetes started at 2023-01-19 18:15:33 +0000 UTC (5 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: splunkforwarder-ds-pj5q8 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 20:40:24.098: INFO: pod-qos-class-dfa28a64-9c00-4636-8d99-dccceb2981f4 from pods-4012 started at 2023-01-19 20:39:35 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container agnhost ready: false, restart count 0
    Jan 19 20:40:24.098: INFO: sonobuoy from sonobuoy started at 2023-01-19 20:29:23 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 19 20:40:24.098: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-cbfjg from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.098: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 20:40:24.098: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 20:40:24.098: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-188-71.ec2.internal before test
    Jan 19 20:40:24.133: INFO: addon-operator-catalog-zxs8v from openshift-addon-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: cloud-ingress-operator-registry-pqwm7 from openshift-cloud-ingress-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: aws-ebs-csi-driver-node-wmvd4 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:13 +0000 UTC (3 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: tuned-7x9t2 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: downloads-8d695cd69-hltgd from openshift-console started at 2023-01-19 18:39:20 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container download-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: custom-domains-operator-registry-kdw7s from openshift-custom-domains-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: deployment-validation-operator-catalog-nq94w from openshift-deployment-validation-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: dns-default-tzrx6 from openshift-dns started at 2023-01-19 18:15:43 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container dns ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: node-resolver-l2fzw from openshift-dns started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: node-ca-kzsvk from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: ingress-canary-t6tqp from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: machine-config-daemon-zhllf from openshift-machine-config-operator started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: managed-node-metadata-operator-registry-m6dxz from openshift-managed-node-metadata-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: managed-upgrade-operator-catalog-thxts from openshift-managed-upgrade-operator started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: configure-alertmanager-operator-registry-sjcb5 from openshift-monitoring started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: node-exporter-dc59d from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: osd-cluster-ready-nb8xk from openshift-monitoring started at 2023-01-19 18:39:21 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container osd-cluster-ready ready: false, restart count 7
    Jan 19 20:40:24.133: INFO: sre-dns-latency-exporter-5dz25 from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container main ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: multus-additional-cni-plugins-hvpd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: multus-mptd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: network-metrics-daemon-s2l94 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: must-gather-operator-registry-n7fvg from openshift-must-gather-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: network-check-target-d8gvt from openshift-network-diagnostics started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-b4r6s from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: observability-operator-6dd79df7dc-z7frh from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container operator ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: observability-operator-catalog-vrnkr from openshift-observability-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: ocm-agent-operator-registry-dwfc4 from openshift-ocm-agent-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: ovnkube-node-wcrc4 from openshift-ovn-kubernetes started at 2023-01-19 18:15:13 +0000 UTC (5 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: route-monitor-operator-registry-whpgh from openshift-route-monitor-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: splunkforwarder-ds-xzbdk from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 20:40:24.133: INFO: splunk-forwarder-operator-catalog-fqp2b from openshift-splunk-forwarder-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: prometheus-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:37:05 +0000 UTC (6 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: thanos-ruler-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:37:04 +0000 UTC (3 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: 	Container thanos-ruler ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: managed-velero-operator-registry-6lc6b from openshift-velero started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-q5j5c from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.133: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 20:40:24.133: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-207-77.ec2.internal before test
    Jan 19 20:40:24.175: INFO: addon-operator-webhooks-67d9d47489-f6z86 from openshift-addon-operator started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container webhook ready: true, restart count 0
    Jan 19 20:40:24.175: INFO: osd-delete-ownerrefs-serviceaccounts-27902617-wh9q8 from openshift-backplane-srep started at 2023-01-19 19:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
    Jan 19 20:40:24.175: INFO: osd-delete-ownerrefs-serviceaccounts-27902647-2672p from openshift-backplane-srep started at 2023-01-19 20:07:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
    Jan 19 20:40:24.175: INFO: osd-delete-ownerrefs-serviceaccounts-27902677-st5fv from openshift-backplane-srep started at 2023-01-19 20:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
    Jan 19 20:40:24.175: INFO: osd-delete-backplane-serviceaccounts-27902660-hr6ts from openshift-backplane started at 2023-01-19 20:20:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
    Jan 19 20:40:24.175: INFO: osd-delete-backplane-serviceaccounts-27902670-zwws9 from openshift-backplane started at 2023-01-19 20:30:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
    Jan 19 20:40:24.175: INFO: osd-delete-backplane-serviceaccounts-27902680-cthn6 from openshift-backplane started at 2023-01-19 20:40:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
    Jan 19 20:40:24.175: INFO: sre-build-test-27902651-jvtzk from openshift-build-test started at 2023-01-19 20:11:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container sre-build-test ready: false, restart count 0
    Jan 19 20:40:24.175: INFO: cloud-ingress-operator-5b5df787b5-bdk4j from openshift-cloud-ingress-operator started at 2023-01-19 19:15:07 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
    Jan 19 20:40:24.175: INFO: aws-ebs-csi-driver-node-9sjjv from openshift-cluster-csi-drivers started at 2023-01-19 18:33:36 +0000 UTC (3 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 20:40:24.175: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 20:40:24.175: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 20:40:24.175: INFO: tuned-qkjmb from openshift-cluster-node-tuning-operator started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 20:40:24.175: INFO: dns-default-zzp2r from openshift-dns started at 2023-01-19 20:31:07 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container dns ready: true, restart count 0
    Jan 19 20:40:24.175: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 20:40:24.175: INFO: node-resolver-m4bvp from openshift-dns started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 20:40:24.175: INFO: image-registry-656cfd9bd9-bj4wn from openshift-image-registry started at 2023-01-19 19:15:47 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container registry ready: true, restart count 0
    Jan 19 20:40:24.175: INFO: node-ca-kwgfx from openshift-image-registry started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 20:40:24.175: INFO: ingress-canary-6xnfp from openshift-ingress-canary started at 2023-01-19 18:34:10 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 20:40:24.175: INFO: router-default-6bf5ff7dcb-6vrcg from openshift-ingress started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container router ready: true, restart count 0
    Jan 19 20:40:24.175: INFO: machine-config-daemon-2bt5b from openshift-machine-config-operator started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 20:40:24.175: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 20:40:24.175: INFO: managed-node-metadata-operator-649f646d54-smsh7 from openshift-managed-node-metadata-operator started at 2023-01-19 19:16:23 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
    Jan 19 20:40:24.175: INFO: osd-patch-subscription-source-27902580-hz9tz from openshift-marketplace started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.175: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
    Jan 19 20:40:24.176: INFO: osd-patch-subscription-source-27902640-qrd58 from openshift-marketplace started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
    Jan 19 20:40:24.176: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: node-exporter-s77nf from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: osd-rebalance-infra-nodes-27902640-lpbrs from openshift-monitoring started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
    Jan 19 20:40:24.176: INFO: osd-rebalance-infra-nodes-27902655-77whr from openshift-monitoring started at 2023-01-19 20:15:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
    Jan 19 20:40:24.176: INFO: osd-rebalance-infra-nodes-27902670-v89hg from openshift-monitoring started at 2023-01-19 20:30:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
    Jan 19 20:40:24.176: INFO: prometheus-adapter-5bc6487f7c-sv96c from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: prometheus-operator-admission-webhook-749df5cf4f-bmj4n from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: sre-dns-latency-exporter-4lksg from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container main ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: sre-ebs-iops-reporter-1-sg7b2 from openshift-monitoring started at 2023-01-19 19:16:28 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container main ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: thanos-querier-fcd5b6c8d-9jjkp from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: token-refresher-6f7c46d758-mld9t from openshift-monitoring started at 2023-01-19 19:17:36 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container token-refresher ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: multus-additional-cni-plugins-476gw from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: multus-mczpf from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: network-metrics-daemon-ddkb6 from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: network-check-target-d8k5s from openshift-network-diagnostics started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: osd-metrics-exporter-679d75d598-rsv49 from openshift-osd-metrics started at 2023-01-19 19:17:47 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: ovnkube-node-gq9vc from openshift-ovn-kubernetes started at 2023-01-19 18:33:36 +0000 UTC (5 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: blackbox-exporter-5f7f7bf859-l8ptw from openshift-route-monitor-operator started at 2023-01-19 19:17:51 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: route-monitor-operator-controller-manager-bfbff575-pfqc4 from openshift-route-monitor-operator started at 2023-01-19 19:17:53 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container manager ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: splunkforwarder-ds-h56tr from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 20:40:24.176: INFO: builds-pruner-27902580-p9bnq from openshift-sre-pruning started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container builds-pruner ready: false, restart count 0
    Jan 19 20:40:24.176: INFO: builds-pruner-27902640-bj57l from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container builds-pruner ready: false, restart count 0
    Jan 19 20:40:24.176: INFO: deployments-pruner-27902580-qfgtd from openshift-sre-pruning started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container deployments-pruner ready: false, restart count 0
    Jan 19 20:40:24.176: INFO: deployments-pruner-27902640-vhqsv from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container deployments-pruner ready: false, restart count 0
    Jan 19 20:40:24.176: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-hj9fv from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 20:40:24.176: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 20:40:24.176: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/19/23 20:40:24.176
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173bd0664c2d6e66], Reason = [FailedScheduling], Message = [0/9 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 9 node(s) didn't match Pod's node affinity/selector. preemption: 0/9 nodes are available: 9 Preemption is not helpful for scheduling.] 01/19/23 20:40:24.288
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 20:40:25.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-1397" for this suite. 01/19/23 20:40:25.292
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:40:25.298
Jan 19 20:40:25.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename events 01/19/23 20:40:25.299
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:25.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:25.331
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/19/23 20:40:25.336
STEP: listing events in all namespaces 01/19/23 20:40:25.361
STEP: listing events in test namespace 01/19/23 20:40:25.542
STEP: listing events with field selection filtering on source 01/19/23 20:40:25.544
STEP: listing events with field selection filtering on reportingController 01/19/23 20:40:25.547
STEP: getting the test event 01/19/23 20:40:25.549
STEP: patching the test event 01/19/23 20:40:25.551
STEP: getting the test event 01/19/23 20:40:25.559
STEP: updating the test event 01/19/23 20:40:25.561
STEP: getting the test event 01/19/23 20:40:25.566
STEP: deleting the test event 01/19/23 20:40:25.569
STEP: listing events in all namespaces 01/19/23 20:40:25.576
STEP: listing events in test namespace 01/19/23 20:40:25.743
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 19 20:40:25.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-737" for this suite. 01/19/23 20:40:25.752
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":28,"skipped":420,"failed":0}
------------------------------
• [0.460 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:40:25.298
    Jan 19 20:40:25.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename events 01/19/23 20:40:25.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:25.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:25.331
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/19/23 20:40:25.336
    STEP: listing events in all namespaces 01/19/23 20:40:25.361
    STEP: listing events in test namespace 01/19/23 20:40:25.542
    STEP: listing events with field selection filtering on source 01/19/23 20:40:25.544
    STEP: listing events with field selection filtering on reportingController 01/19/23 20:40:25.547
    STEP: getting the test event 01/19/23 20:40:25.549
    STEP: patching the test event 01/19/23 20:40:25.551
    STEP: getting the test event 01/19/23 20:40:25.559
    STEP: updating the test event 01/19/23 20:40:25.561
    STEP: getting the test event 01/19/23 20:40:25.566
    STEP: deleting the test event 01/19/23 20:40:25.569
    STEP: listing events in all namespaces 01/19/23 20:40:25.576
    STEP: listing events in test namespace 01/19/23 20:40:25.743
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 19 20:40:25.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-737" for this suite. 01/19/23 20:40:25.752
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:40:25.758
Jan 19 20:40:25.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 20:40:25.759
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:25.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:25.801
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 20:40:25.839
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 20:40:26.398
STEP: Deploying the webhook pod 01/19/23 20:40:26.408
STEP: Wait for the deployment to be ready 01/19/23 20:40:26.422
Jan 19 20:40:26.429: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/19/23 20:40:28.448
STEP: Verifying the service has paired with the endpoint 01/19/23 20:40:28.457
Jan 19 20:40:29.458: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 01/19/23 20:40:29.46
STEP: create a pod that should be denied by the webhook 01/19/23 20:40:29.479
STEP: create a pod that causes the webhook to hang 01/19/23 20:40:29.498
STEP: create a configmap that should be denied by the webhook 01/19/23 20:40:39.51
STEP: create a configmap that should be admitted by the webhook 01/19/23 20:40:39.531
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/19/23 20:40:39.54
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/19/23 20:40:39.547
STEP: create a namespace that bypass the webhook 01/19/23 20:40:39.552
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/19/23 20:40:39.558
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 20:40:39.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5074" for this suite. 01/19/23 20:40:39.661
STEP: Destroying namespace "webhook-5074-markers" for this suite. 01/19/23 20:40:39.677
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":29,"skipped":422,"failed":0}
------------------------------
• [SLOW TEST] [13.995 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:40:25.758
    Jan 19 20:40:25.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 20:40:25.759
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:25.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:25.801
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 20:40:25.839
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 20:40:26.398
    STEP: Deploying the webhook pod 01/19/23 20:40:26.408
    STEP: Wait for the deployment to be ready 01/19/23 20:40:26.422
    Jan 19 20:40:26.429: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/19/23 20:40:28.448
    STEP: Verifying the service has paired with the endpoint 01/19/23 20:40:28.457
    Jan 19 20:40:29.458: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 01/19/23 20:40:29.46
    STEP: create a pod that should be denied by the webhook 01/19/23 20:40:29.479
    STEP: create a pod that causes the webhook to hang 01/19/23 20:40:29.498
    STEP: create a configmap that should be denied by the webhook 01/19/23 20:40:39.51
    STEP: create a configmap that should be admitted by the webhook 01/19/23 20:40:39.531
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/19/23 20:40:39.54
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/19/23 20:40:39.547
    STEP: create a namespace that bypass the webhook 01/19/23 20:40:39.552
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/19/23 20:40:39.558
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 20:40:39.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5074" for this suite. 01/19/23 20:40:39.661
    STEP: Destroying namespace "webhook-5074-markers" for this suite. 01/19/23 20:40:39.677
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:40:39.755
Jan 19 20:40:39.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename containers 01/19/23 20:40:39.756
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:39.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:39.805
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 01/19/23 20:40:39.807
Jan 19 20:40:39.866: INFO: Waiting up to 5m0s for pod "client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001" in namespace "containers-4741" to be "Succeeded or Failed"
Jan 19 20:40:39.877: INFO: Pod "client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001": Phase="Pending", Reason="", readiness=false. Elapsed: 11.291475ms
Jan 19 20:40:41.882: INFO: Pod "client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015630512s
Jan 19 20:40:43.882: INFO: Pod "client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015511803s
STEP: Saw pod success 01/19/23 20:40:43.882
Jan 19 20:40:43.882: INFO: Pod "client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001" satisfied condition "Succeeded or Failed"
Jan 19 20:40:43.886: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 20:40:43.892
Jan 19 20:40:43.901: INFO: Waiting for pod client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001 to disappear
Jan 19 20:40:43.903: INFO: Pod client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 19 20:40:43.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4741" for this suite. 01/19/23 20:40:43.908
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":30,"skipped":497,"failed":0}
------------------------------
• [4.159 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:40:39.755
    Jan 19 20:40:39.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename containers 01/19/23 20:40:39.756
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:39.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:39.805
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 01/19/23 20:40:39.807
    Jan 19 20:40:39.866: INFO: Waiting up to 5m0s for pod "client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001" in namespace "containers-4741" to be "Succeeded or Failed"
    Jan 19 20:40:39.877: INFO: Pod "client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001": Phase="Pending", Reason="", readiness=false. Elapsed: 11.291475ms
    Jan 19 20:40:41.882: INFO: Pod "client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015630512s
    Jan 19 20:40:43.882: INFO: Pod "client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015511803s
    STEP: Saw pod success 01/19/23 20:40:43.882
    Jan 19 20:40:43.882: INFO: Pod "client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001" satisfied condition "Succeeded or Failed"
    Jan 19 20:40:43.886: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 20:40:43.892
    Jan 19 20:40:43.901: INFO: Waiting for pod client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001 to disappear
    Jan 19 20:40:43.903: INFO: Pod client-containers-a0dffe07-223d-4ddf-a7ae-a5aa2b265001 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 19 20:40:43.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-4741" for this suite. 01/19/23 20:40:43.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:40:43.915
Jan 19 20:40:43.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename sched-preemption 01/19/23 20:40:43.916
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:43.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:43.942
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 19 20:40:44.009: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 20:41:44.237: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 01/19/23 20:41:44.243
Jan 19 20:41:44.288: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 19 20:41:44.303: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 19 20:41:44.342: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 19 20:41:44.362: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 19 20:41:44.400: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 19 20:41:44.422: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jan 19 20:41:44.458: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jan 19 20:41:44.475: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Jan 19 20:41:44.511: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Jan 19 20:41:44.527: INFO: Created pod: pod4-1-sched-preemption-medium-priority
Jan 19 20:41:44.564: INFO: Created pod: pod5-0-sched-preemption-medium-priority
Jan 19 20:41:44.580: INFO: Created pod: pod5-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/19/23 20:41:44.58
Jan 19 20:41:44.580: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4786" to be "running"
Jan 19 20:41:44.583: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315624ms
Jan 19 20:41:46.586: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005910947s
Jan 19 20:41:48.586: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005986932s
Jan 19 20:41:50.587: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.006697194s
Jan 19 20:41:50.587: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 19 20:41:50.587: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
Jan 19 20:41:50.590: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.741173ms
Jan 19 20:41:50.590: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 20:41:50.590: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
Jan 19 20:41:50.605: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.730961ms
Jan 19 20:41:50.605: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 20:41:50.605: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
Jan 19 20:41:50.608: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.305587ms
Jan 19 20:41:50.608: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 20:41:50.608: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
Jan 19 20:41:50.611: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.913861ms
Jan 19 20:41:52.616: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007801706s
Jan 19 20:41:52.616: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 20:41:52.616: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
Jan 19 20:41:52.618: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.356248ms
Jan 19 20:41:52.618: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 20:41:52.618: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
Jan 19 20:41:52.620: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.090085ms
Jan 19 20:41:52.620: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 20:41:52.620: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
Jan 19 20:41:52.623: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.417517ms
Jan 19 20:41:52.623: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 20:41:52.623: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
Jan 19 20:41:52.625: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.394297ms
Jan 19 20:41:52.625: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 20:41:52.625: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
Jan 19 20:41:52.628: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.275489ms
Jan 19 20:41:52.628: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 20:41:52.628: INFO: Waiting up to 5m0s for pod "pod5-0-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
Jan 19 20:41:52.630: INFO: Pod "pod5-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.039882ms
Jan 19 20:41:52.630: INFO: Pod "pod5-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 20:41:52.630: INFO: Waiting up to 5m0s for pod "pod5-1-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
Jan 19 20:41:52.632: INFO: Pod "pod5-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.501831ms
Jan 19 20:41:52.632: INFO: Pod "pod5-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/19/23 20:41:52.632
Jan 19 20:41:52.642: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan 19 20:41:52.644: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.894794ms
Jan 19 20:41:54.648: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005519189s
Jan 19 20:41:56.649: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007229178s
Jan 19 20:41:56.649: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 19 20:41:56.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4786" for this suite. 01/19/23 20:41:56.709
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":31,"skipped":505,"failed":0}
------------------------------
• [SLOW TEST] [72.922 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:40:43.915
    Jan 19 20:40:43.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename sched-preemption 01/19/23 20:40:43.916
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:40:43.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:40:43.942
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 19 20:40:44.009: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 19 20:41:44.237: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 01/19/23 20:41:44.243
    Jan 19 20:41:44.288: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 19 20:41:44.303: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 19 20:41:44.342: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 19 20:41:44.362: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan 19 20:41:44.400: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan 19 20:41:44.422: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Jan 19 20:41:44.458: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Jan 19 20:41:44.475: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Jan 19 20:41:44.511: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Jan 19 20:41:44.527: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    Jan 19 20:41:44.564: INFO: Created pod: pod5-0-sched-preemption-medium-priority
    Jan 19 20:41:44.580: INFO: Created pod: pod5-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/19/23 20:41:44.58
    Jan 19 20:41:44.580: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4786" to be "running"
    Jan 19 20:41:44.583: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315624ms
    Jan 19 20:41:46.586: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005910947s
    Jan 19 20:41:48.586: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005986932s
    Jan 19 20:41:50.587: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 6.006697194s
    Jan 19 20:41:50.587: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 19 20:41:50.587: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
    Jan 19 20:41:50.590: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.741173ms
    Jan 19 20:41:50.590: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 20:41:50.590: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
    Jan 19 20:41:50.605: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 14.730961ms
    Jan 19 20:41:50.605: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 20:41:50.605: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
    Jan 19 20:41:50.608: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.305587ms
    Jan 19 20:41:50.608: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 20:41:50.608: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
    Jan 19 20:41:50.611: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.913861ms
    Jan 19 20:41:52.616: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007801706s
    Jan 19 20:41:52.616: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 20:41:52.616: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
    Jan 19 20:41:52.618: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.356248ms
    Jan 19 20:41:52.618: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 20:41:52.618: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
    Jan 19 20:41:52.620: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.090085ms
    Jan 19 20:41:52.620: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 20:41:52.620: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
    Jan 19 20:41:52.623: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.417517ms
    Jan 19 20:41:52.623: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 20:41:52.623: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
    Jan 19 20:41:52.625: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.394297ms
    Jan 19 20:41:52.625: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 20:41:52.625: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
    Jan 19 20:41:52.628: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.275489ms
    Jan 19 20:41:52.628: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 20:41:52.628: INFO: Waiting up to 5m0s for pod "pod5-0-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
    Jan 19 20:41:52.630: INFO: Pod "pod5-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.039882ms
    Jan 19 20:41:52.630: INFO: Pod "pod5-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 20:41:52.630: INFO: Waiting up to 5m0s for pod "pod5-1-sched-preemption-medium-priority" in namespace "sched-preemption-4786" to be "running"
    Jan 19 20:41:52.632: INFO: Pod "pod5-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.501831ms
    Jan 19 20:41:52.632: INFO: Pod "pod5-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/19/23 20:41:52.632
    Jan 19 20:41:52.642: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan 19 20:41:52.644: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.894794ms
    Jan 19 20:41:54.648: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005519189s
    Jan 19 20:41:56.649: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007229178s
    Jan 19 20:41:56.649: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 20:41:56.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4786" for this suite. 01/19/23 20:41:56.709
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:41:56.838
Jan 19 20:41:56.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 20:41:56.838
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:41:56.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:41:56.86
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 20:41:56.899
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 20:41:57.354
STEP: Deploying the webhook pod 01/19/23 20:41:57.36
STEP: Wait for the deployment to be ready 01/19/23 20:41:57.373
Jan 19 20:41:57.378: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/19/23 20:41:59.385
STEP: Verifying the service has paired with the endpoint 01/19/23 20:41:59.395
Jan 19 20:42:00.396: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 01/19/23 20:42:00.399
STEP: Creating a custom resource definition that should be denied by the webhook 01/19/23 20:42:00.413
Jan 19 20:42:00.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 20:42:00.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2706" for this suite. 01/19/23 20:42:00.43
STEP: Destroying namespace "webhook-2706-markers" for this suite. 01/19/23 20:42:00.436
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":32,"skipped":509,"failed":0}
------------------------------
• [3.648 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:41:56.838
    Jan 19 20:41:56.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 20:41:56.838
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:41:56.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:41:56.86
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 20:41:56.899
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 20:41:57.354
    STEP: Deploying the webhook pod 01/19/23 20:41:57.36
    STEP: Wait for the deployment to be ready 01/19/23 20:41:57.373
    Jan 19 20:41:57.378: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/19/23 20:41:59.385
    STEP: Verifying the service has paired with the endpoint 01/19/23 20:41:59.395
    Jan 19 20:42:00.396: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/19/23 20:42:00.399
    STEP: Creating a custom resource definition that should be denied by the webhook 01/19/23 20:42:00.413
    Jan 19 20:42:00.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 20:42:00.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2706" for this suite. 01/19/23 20:42:00.43
    STEP: Destroying namespace "webhook-2706-markers" for this suite. 01/19/23 20:42:00.436
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:42:00.486
Jan 19 20:42:00.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 20:42:00.487
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:42:00.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:42:00.527
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 01/19/23 20:42:00.533
Jan 19 20:42:00.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: mark a version not serverd 01/19/23 20:42:19.984
STEP: check the unserved version gets removed 01/19/23 20:42:19.998
STEP: check the other version is not changed 01/19/23 20:42:28.403
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 20:42:43.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7985" for this suite. 01/19/23 20:42:43.529
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":33,"skipped":516,"failed":0}
------------------------------
• [SLOW TEST] [43.049 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:42:00.486
    Jan 19 20:42:00.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 20:42:00.487
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:42:00.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:42:00.527
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 01/19/23 20:42:00.533
    Jan 19 20:42:00.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: mark a version not serverd 01/19/23 20:42:19.984
    STEP: check the unserved version gets removed 01/19/23 20:42:19.998
    STEP: check the other version is not changed 01/19/23 20:42:28.403
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 20:42:43.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7985" for this suite. 01/19/23 20:42:43.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:42:43.536
Jan 19 20:42:43.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename var-expansion 01/19/23 20:42:43.537
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:42:43.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:42:43.563
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 01/19/23 20:42:43.567
STEP: waiting for pod running 01/19/23 20:42:43.622
Jan 19 20:42:43.622: INFO: Waiting up to 2m0s for pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b" in namespace "var-expansion-8349" to be "running"
Jan 19 20:42:43.629: INFO: Pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.809241ms
Jan 19 20:42:45.634: INFO: Pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011744057s
Jan 19 20:42:45.634: INFO: Pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b" satisfied condition "running"
STEP: creating a file in subpath 01/19/23 20:42:45.634
Jan 19 20:42:45.637: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8349 PodName:var-expansion-f3a85193-4355-427b-a178-caab81028b3b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:42:45.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:42:45.638: INFO: ExecWithOptions: Clientset creation
Jan 19 20:42:45.638: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-8349/pods/var-expansion-f3a85193-4355-427b-a178-caab81028b3b/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/19/23 20:42:45.707
Jan 19 20:42:45.710: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8349 PodName:var-expansion-f3a85193-4355-427b-a178-caab81028b3b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:42:45.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:42:45.710: INFO: ExecWithOptions: Clientset creation
Jan 19 20:42:45.710: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-8349/pods/var-expansion-f3a85193-4355-427b-a178-caab81028b3b/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/19/23 20:42:45.783
Jan 19 20:42:46.299: INFO: Successfully updated pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b"
STEP: waiting for annotated pod running 01/19/23 20:42:46.299
Jan 19 20:42:46.299: INFO: Waiting up to 2m0s for pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b" in namespace "var-expansion-8349" to be "running"
Jan 19 20:42:46.302: INFO: Pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.314543ms
Jan 19 20:42:46.302: INFO: Pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b" satisfied condition "running"
STEP: deleting the pod gracefully 01/19/23 20:42:46.302
Jan 19 20:42:46.302: INFO: Deleting pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b" in namespace "var-expansion-8349"
Jan 19 20:42:46.308: INFO: Wait up to 5m0s for pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 20:43:20.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8349" for this suite. 01/19/23 20:43:20.32
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":34,"skipped":530,"failed":0}
------------------------------
• [SLOW TEST] [36.789 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:42:43.536
    Jan 19 20:42:43.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename var-expansion 01/19/23 20:42:43.537
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:42:43.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:42:43.563
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 01/19/23 20:42:43.567
    STEP: waiting for pod running 01/19/23 20:42:43.622
    Jan 19 20:42:43.622: INFO: Waiting up to 2m0s for pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b" in namespace "var-expansion-8349" to be "running"
    Jan 19 20:42:43.629: INFO: Pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.809241ms
    Jan 19 20:42:45.634: INFO: Pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011744057s
    Jan 19 20:42:45.634: INFO: Pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b" satisfied condition "running"
    STEP: creating a file in subpath 01/19/23 20:42:45.634
    Jan 19 20:42:45.637: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8349 PodName:var-expansion-f3a85193-4355-427b-a178-caab81028b3b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:42:45.637: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:42:45.638: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:42:45.638: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-8349/pods/var-expansion-f3a85193-4355-427b-a178-caab81028b3b/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/19/23 20:42:45.707
    Jan 19 20:42:45.710: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8349 PodName:var-expansion-f3a85193-4355-427b-a178-caab81028b3b ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:42:45.710: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:42:45.710: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:42:45.710: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/var-expansion-8349/pods/var-expansion-f3a85193-4355-427b-a178-caab81028b3b/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/19/23 20:42:45.783
    Jan 19 20:42:46.299: INFO: Successfully updated pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b"
    STEP: waiting for annotated pod running 01/19/23 20:42:46.299
    Jan 19 20:42:46.299: INFO: Waiting up to 2m0s for pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b" in namespace "var-expansion-8349" to be "running"
    Jan 19 20:42:46.302: INFO: Pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b": Phase="Running", Reason="", readiness=true. Elapsed: 2.314543ms
    Jan 19 20:42:46.302: INFO: Pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b" satisfied condition "running"
    STEP: deleting the pod gracefully 01/19/23 20:42:46.302
    Jan 19 20:42:46.302: INFO: Deleting pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b" in namespace "var-expansion-8349"
    Jan 19 20:42:46.308: INFO: Wait up to 5m0s for pod "var-expansion-f3a85193-4355-427b-a178-caab81028b3b" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 20:43:20.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8349" for this suite. 01/19/23 20:43:20.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:43:20.326
Jan 19 20:43:20.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pods 01/19/23 20:43:20.327
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:20.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:20.358
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jan 19 20:43:20.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: creating the pod 01/19/23 20:43:20.361
STEP: submitting the pod to kubernetes 01/19/23 20:43:20.361
Jan 19 20:43:20.390: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-fe4b33c4-bff6-4d8f-a70e-e4f3d9d31219" in namespace "pods-9609" to be "running and ready"
Jan 19 20:43:20.392: INFO: Pod "pod-exec-websocket-fe4b33c4-bff6-4d8f-a70e-e4f3d9d31219": Phase="Pending", Reason="", readiness=false. Elapsed: 2.175607ms
Jan 19 20:43:20.392: INFO: The phase of Pod pod-exec-websocket-fe4b33c4-bff6-4d8f-a70e-e4f3d9d31219 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 20:43:22.394: INFO: Pod "pod-exec-websocket-fe4b33c4-bff6-4d8f-a70e-e4f3d9d31219": Phase="Running", Reason="", readiness=true. Elapsed: 2.004652318s
Jan 19 20:43:22.394: INFO: The phase of Pod pod-exec-websocket-fe4b33c4-bff6-4d8f-a70e-e4f3d9d31219 is Running (Ready = true)
Jan 19 20:43:22.394: INFO: Pod "pod-exec-websocket-fe4b33c4-bff6-4d8f-a70e-e4f3d9d31219" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 20:43:22.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9609" for this suite. 01/19/23 20:43:22.569
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":35,"skipped":564,"failed":0}
------------------------------
• [2.248 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:43:20.326
    Jan 19 20:43:20.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pods 01/19/23 20:43:20.327
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:20.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:20.358
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jan 19 20:43:20.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: creating the pod 01/19/23 20:43:20.361
    STEP: submitting the pod to kubernetes 01/19/23 20:43:20.361
    Jan 19 20:43:20.390: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-fe4b33c4-bff6-4d8f-a70e-e4f3d9d31219" in namespace "pods-9609" to be "running and ready"
    Jan 19 20:43:20.392: INFO: Pod "pod-exec-websocket-fe4b33c4-bff6-4d8f-a70e-e4f3d9d31219": Phase="Pending", Reason="", readiness=false. Elapsed: 2.175607ms
    Jan 19 20:43:20.392: INFO: The phase of Pod pod-exec-websocket-fe4b33c4-bff6-4d8f-a70e-e4f3d9d31219 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 20:43:22.394: INFO: Pod "pod-exec-websocket-fe4b33c4-bff6-4d8f-a70e-e4f3d9d31219": Phase="Running", Reason="", readiness=true. Elapsed: 2.004652318s
    Jan 19 20:43:22.394: INFO: The phase of Pod pod-exec-websocket-fe4b33c4-bff6-4d8f-a70e-e4f3d9d31219 is Running (Ready = true)
    Jan 19 20:43:22.394: INFO: Pod "pod-exec-websocket-fe4b33c4-bff6-4d8f-a70e-e4f3d9d31219" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 20:43:22.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9609" for this suite. 01/19/23 20:43:22.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:43:22.574
Jan 19 20:43:22.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 20:43:22.575
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:22.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:22.61
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415
STEP: creating a Service 01/19/23 20:43:22.621
STEP: watching for the Service to be added 01/19/23 20:43:22.655
Jan 19 20:43:22.656: INFO: Found Service test-service-k6kdw in namespace services-6085 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan 19 20:43:22.656: INFO: Service test-service-k6kdw created
STEP: Getting /status 01/19/23 20:43:22.656
Jan 19 20:43:22.669: INFO: Service test-service-k6kdw has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/19/23 20:43:22.669
STEP: watching for the Service to be patched 01/19/23 20:43:22.685
Jan 19 20:43:22.686: INFO: observed Service test-service-k6kdw in namespace services-6085 with annotations: map[] & LoadBalancer: {[]}
Jan 19 20:43:22.686: INFO: Found Service test-service-k6kdw in namespace services-6085 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan 19 20:43:22.686: INFO: Service test-service-k6kdw has service status patched
STEP: updating the ServiceStatus 01/19/23 20:43:22.686
Jan 19 20:43:22.701: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/19/23 20:43:22.701
Jan 19 20:43:22.702: INFO: Observed Service test-service-k6kdw in namespace services-6085 with annotations: map[] & Conditions: {[]}
Jan 19 20:43:22.702: INFO: Observed event: &Service{ObjectMeta:{test-service-k6kdw  services-6085  089c48eb-85de-474f-abf6-d16ddbfa4758 142787 0 2023-01-19 20:43:22 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-19 20:43:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-19 20:43:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.30.245.9,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.30.245.9],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan 19 20:43:22.703: INFO: Found Service test-service-k6kdw in namespace services-6085 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 19 20:43:22.703: INFO: Service test-service-k6kdw has service status updated
STEP: patching the service 01/19/23 20:43:22.703
STEP: watching for the Service to be patched 01/19/23 20:43:22.721
Jan 19 20:43:22.723: INFO: observed Service test-service-k6kdw in namespace services-6085 with labels: map[test-service-static:true]
Jan 19 20:43:22.723: INFO: observed Service test-service-k6kdw in namespace services-6085 with labels: map[test-service-static:true]
Jan 19 20:43:22.723: INFO: observed Service test-service-k6kdw in namespace services-6085 with labels: map[test-service-static:true]
Jan 19 20:43:22.723: INFO: Found Service test-service-k6kdw in namespace services-6085 with labels: map[test-service:patched test-service-static:true]
Jan 19 20:43:22.723: INFO: Service test-service-k6kdw patched
STEP: deleting the service 01/19/23 20:43:22.723
STEP: watching for the Service to be deleted 01/19/23 20:43:22.764
Jan 19 20:43:22.765: INFO: Observed event: ADDED
Jan 19 20:43:22.765: INFO: Observed event: MODIFIED
Jan 19 20:43:22.765: INFO: Observed event: MODIFIED
Jan 19 20:43:22.765: INFO: Observed event: MODIFIED
Jan 19 20:43:22.765: INFO: Found Service test-service-k6kdw in namespace services-6085 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jan 19 20:43:22.765: INFO: Service test-service-k6kdw deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 20:43:22.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6085" for this suite. 01/19/23 20:43:22.777
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":36,"skipped":571,"failed":0}
------------------------------
• [0.218 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:43:22.574
    Jan 19 20:43:22.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 20:43:22.575
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:22.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:22.61
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3415
    STEP: creating a Service 01/19/23 20:43:22.621
    STEP: watching for the Service to be added 01/19/23 20:43:22.655
    Jan 19 20:43:22.656: INFO: Found Service test-service-k6kdw in namespace services-6085 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan 19 20:43:22.656: INFO: Service test-service-k6kdw created
    STEP: Getting /status 01/19/23 20:43:22.656
    Jan 19 20:43:22.669: INFO: Service test-service-k6kdw has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/19/23 20:43:22.669
    STEP: watching for the Service to be patched 01/19/23 20:43:22.685
    Jan 19 20:43:22.686: INFO: observed Service test-service-k6kdw in namespace services-6085 with annotations: map[] & LoadBalancer: {[]}
    Jan 19 20:43:22.686: INFO: Found Service test-service-k6kdw in namespace services-6085 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan 19 20:43:22.686: INFO: Service test-service-k6kdw has service status patched
    STEP: updating the ServiceStatus 01/19/23 20:43:22.686
    Jan 19 20:43:22.701: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/19/23 20:43:22.701
    Jan 19 20:43:22.702: INFO: Observed Service test-service-k6kdw in namespace services-6085 with annotations: map[] & Conditions: {[]}
    Jan 19 20:43:22.702: INFO: Observed event: &Service{ObjectMeta:{test-service-k6kdw  services-6085  089c48eb-85de-474f-abf6-d16ddbfa4758 142787 0 2023-01-19 20:43:22 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-19 20:43:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-19 20:43:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.30.245.9,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.30.245.9],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan 19 20:43:22.703: INFO: Found Service test-service-k6kdw in namespace services-6085 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 19 20:43:22.703: INFO: Service test-service-k6kdw has service status updated
    STEP: patching the service 01/19/23 20:43:22.703
    STEP: watching for the Service to be patched 01/19/23 20:43:22.721
    Jan 19 20:43:22.723: INFO: observed Service test-service-k6kdw in namespace services-6085 with labels: map[test-service-static:true]
    Jan 19 20:43:22.723: INFO: observed Service test-service-k6kdw in namespace services-6085 with labels: map[test-service-static:true]
    Jan 19 20:43:22.723: INFO: observed Service test-service-k6kdw in namespace services-6085 with labels: map[test-service-static:true]
    Jan 19 20:43:22.723: INFO: Found Service test-service-k6kdw in namespace services-6085 with labels: map[test-service:patched test-service-static:true]
    Jan 19 20:43:22.723: INFO: Service test-service-k6kdw patched
    STEP: deleting the service 01/19/23 20:43:22.723
    STEP: watching for the Service to be deleted 01/19/23 20:43:22.764
    Jan 19 20:43:22.765: INFO: Observed event: ADDED
    Jan 19 20:43:22.765: INFO: Observed event: MODIFIED
    Jan 19 20:43:22.765: INFO: Observed event: MODIFIED
    Jan 19 20:43:22.765: INFO: Observed event: MODIFIED
    Jan 19 20:43:22.765: INFO: Found Service test-service-k6kdw in namespace services-6085 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jan 19 20:43:22.765: INFO: Service test-service-k6kdw deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 20:43:22.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6085" for this suite. 01/19/23 20:43:22.777
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:43:22.794
Jan 19 20:43:22.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename dns 01/19/23 20:43:22.794
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:22.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:22.879
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/19/23 20:43:22.883
W0119 20:43:22.930841      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 20:43:22.930: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3811  7f029e94-c0fc-49a5-a4c1-bacc181a892b 142820 0 2023-01-19 20:43:22 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-01-19 20:43:22 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v4mzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v4mzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c12,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 20:43:22.931: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3811" to be "running and ready"
Jan 19 20:43:22.952: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 21.004142ms
Jan 19 20:43:22.952: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan 19 20:43:24.956: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.0249204s
Jan 19 20:43:24.956: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan 19 20:43:24.956: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/19/23 20:43:24.956
Jan 19 20:43:24.956: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3811 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:43:24.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:43:24.956: INFO: ExecWithOptions: Clientset creation
Jan 19 20:43:24.956: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-3811/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/19/23 20:43:25.037
Jan 19 20:43:25.037: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3811 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:43:25.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:43:25.037: INFO: ExecWithOptions: Clientset creation
Jan 19 20:43:25.037: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-3811/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 20:43:25.113: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 20:43:25.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3811" for this suite. 01/19/23 20:43:25.129
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":37,"skipped":620,"failed":0}
------------------------------
• [2.341 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:43:22.794
    Jan 19 20:43:22.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename dns 01/19/23 20:43:22.794
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:22.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:22.879
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/19/23 20:43:22.883
    W0119 20:43:22.930841      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 20:43:22.930: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3811  7f029e94-c0fc-49a5-a4c1-bacc181a892b 142820 0 2023-01-19 20:43:22 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-01-19 20:43:22 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v4mzm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v4mzm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c12,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 20:43:22.931: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3811" to be "running and ready"
    Jan 19 20:43:22.952: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 21.004142ms
    Jan 19 20:43:22.952: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 20:43:24.956: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.0249204s
    Jan 19 20:43:24.956: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan 19 20:43:24.956: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/19/23 20:43:24.956
    Jan 19 20:43:24.956: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3811 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:43:24.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:43:24.956: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:43:24.956: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-3811/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/19/23 20:43:25.037
    Jan 19 20:43:25.037: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3811 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:43:25.037: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:43:25.037: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:43:25.037: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/dns-3811/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 20:43:25.113: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 20:43:25.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3811" for this suite. 01/19/23 20:43:25.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:43:25.135
Jan 19 20:43:25.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename replication-controller 01/19/23 20:43:25.136
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:25.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:25.157
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 01/19/23 20:43:25.159
Jan 19 20:43:25.199: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2225" to be "running and ready"
Jan 19 20:43:25.202: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.13134ms
Jan 19 20:43:25.202: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan 19 20:43:27.205: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.006541963s
Jan 19 20:43:27.205: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan 19 20:43:27.205: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/19/23 20:43:27.208
STEP: Then the orphan pod is adopted 01/19/23 20:43:27.214
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 19 20:43:28.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2225" for this suite. 01/19/23 20:43:28.224
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":38,"skipped":635,"failed":0}
------------------------------
• [3.094 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:43:25.135
    Jan 19 20:43:25.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename replication-controller 01/19/23 20:43:25.136
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:25.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:25.157
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/19/23 20:43:25.159
    Jan 19 20:43:25.199: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2225" to be "running and ready"
    Jan 19 20:43:25.202: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 3.13134ms
    Jan 19 20:43:25.202: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 20:43:27.205: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.006541963s
    Jan 19 20:43:27.205: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan 19 20:43:27.205: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/19/23 20:43:27.208
    STEP: Then the orphan pod is adopted 01/19/23 20:43:27.214
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 19 20:43:28.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2225" for this suite. 01/19/23 20:43:28.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:43:28.23
Jan 19 20:43:28.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 20:43:28.231
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:28.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:28.259
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 01/19/23 20:43:28.268
Jan 19 20:43:28.305: INFO: Waiting up to 5m0s for pod "annotationupdatea02a3721-0292-4682-8445-411c34146160" in namespace "downward-api-5438" to be "running and ready"
Jan 19 20:43:28.310: INFO: Pod "annotationupdatea02a3721-0292-4682-8445-411c34146160": Phase="Pending", Reason="", readiness=false. Elapsed: 5.720234ms
Jan 19 20:43:28.311: INFO: The phase of Pod annotationupdatea02a3721-0292-4682-8445-411c34146160 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 20:43:30.315: INFO: Pod "annotationupdatea02a3721-0292-4682-8445-411c34146160": Phase="Running", Reason="", readiness=true. Elapsed: 2.010352753s
Jan 19 20:43:30.315: INFO: The phase of Pod annotationupdatea02a3721-0292-4682-8445-411c34146160 is Running (Ready = true)
Jan 19 20:43:30.315: INFO: Pod "annotationupdatea02a3721-0292-4682-8445-411c34146160" satisfied condition "running and ready"
Jan 19 20:43:30.941: INFO: Successfully updated pod "annotationupdatea02a3721-0292-4682-8445-411c34146160"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 20:43:34.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5438" for this suite. 01/19/23 20:43:34.965
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":39,"skipped":640,"failed":0}
------------------------------
• [SLOW TEST] [6.740 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:43:28.23
    Jan 19 20:43:28.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 20:43:28.231
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:28.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:28.259
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 01/19/23 20:43:28.268
    Jan 19 20:43:28.305: INFO: Waiting up to 5m0s for pod "annotationupdatea02a3721-0292-4682-8445-411c34146160" in namespace "downward-api-5438" to be "running and ready"
    Jan 19 20:43:28.310: INFO: Pod "annotationupdatea02a3721-0292-4682-8445-411c34146160": Phase="Pending", Reason="", readiness=false. Elapsed: 5.720234ms
    Jan 19 20:43:28.311: INFO: The phase of Pod annotationupdatea02a3721-0292-4682-8445-411c34146160 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 20:43:30.315: INFO: Pod "annotationupdatea02a3721-0292-4682-8445-411c34146160": Phase="Running", Reason="", readiness=true. Elapsed: 2.010352753s
    Jan 19 20:43:30.315: INFO: The phase of Pod annotationupdatea02a3721-0292-4682-8445-411c34146160 is Running (Ready = true)
    Jan 19 20:43:30.315: INFO: Pod "annotationupdatea02a3721-0292-4682-8445-411c34146160" satisfied condition "running and ready"
    Jan 19 20:43:30.941: INFO: Successfully updated pod "annotationupdatea02a3721-0292-4682-8445-411c34146160"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 20:43:34.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5438" for this suite. 01/19/23 20:43:34.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:43:34.97
Jan 19 20:43:34.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename init-container 01/19/23 20:43:34.971
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:34.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:34.992
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 01/19/23 20:43:34.995
Jan 19 20:43:34.995: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 20:43:38.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6" for this suite. 01/19/23 20:43:38.208
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":40,"skipped":650,"failed":0}
------------------------------
• [3.243 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:43:34.97
    Jan 19 20:43:34.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename init-container 01/19/23 20:43:34.971
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:34.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:34.992
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 01/19/23 20:43:34.995
    Jan 19 20:43:34.995: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 20:43:38.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-6" for this suite. 01/19/23 20:43:38.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:43:38.214
Jan 19 20:43:38.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename cronjob 01/19/23 20:43:38.215
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:38.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:38.237
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/19/23 20:43:38.241
W0119 20:43:38.252726      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring no jobs are scheduled 01/19/23 20:43:38.252
STEP: Ensuring no job exists by listing jobs explicitly 01/19/23 20:48:38.258
STEP: Removing cronjob 01/19/23 20:48:38.26
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 19 20:48:38.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2715" for this suite. 01/19/23 20:48:38.27
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":41,"skipped":662,"failed":0}
------------------------------
• [SLOW TEST] [300.061 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:43:38.214
    Jan 19 20:43:38.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename cronjob 01/19/23 20:43:38.215
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:43:38.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:43:38.237
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/19/23 20:43:38.241
    W0119 20:43:38.252726      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring no jobs are scheduled 01/19/23 20:43:38.252
    STEP: Ensuring no job exists by listing jobs explicitly 01/19/23 20:48:38.258
    STEP: Removing cronjob 01/19/23 20:48:38.26
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 19 20:48:38.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2715" for this suite. 01/19/23 20:48:38.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:48:38.275
Jan 19 20:48:38.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename csistoragecapacity 01/19/23 20:48:38.276
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:48:38.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:48:38.297
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/19/23 20:48:38.299
STEP: getting /apis/storage.k8s.io 01/19/23 20:48:38.302
STEP: getting /apis/storage.k8s.io/v1 01/19/23 20:48:38.303
STEP: creating 01/19/23 20:48:38.304
STEP: watching 01/19/23 20:48:38.334
Jan 19 20:48:38.334: INFO: starting watch
STEP: getting 01/19/23 20:48:38.343
STEP: listing in namespace 01/19/23 20:48:38.352
STEP: listing across namespaces 01/19/23 20:48:38.357
STEP: patching 01/19/23 20:48:38.36
STEP: updating 01/19/23 20:48:38.366
Jan 19 20:48:38.372: INFO: waiting for watch events with expected annotations in namespace
Jan 19 20:48:38.372: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/19/23 20:48:38.372
STEP: deleting a collection 01/19/23 20:48:38.382
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jan 19 20:48:38.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-1751" for this suite. 01/19/23 20:48:38.401
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":42,"skipped":679,"failed":0}
------------------------------
• [0.131 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:48:38.275
    Jan 19 20:48:38.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename csistoragecapacity 01/19/23 20:48:38.276
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:48:38.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:48:38.297
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/19/23 20:48:38.299
    STEP: getting /apis/storage.k8s.io 01/19/23 20:48:38.302
    STEP: getting /apis/storage.k8s.io/v1 01/19/23 20:48:38.303
    STEP: creating 01/19/23 20:48:38.304
    STEP: watching 01/19/23 20:48:38.334
    Jan 19 20:48:38.334: INFO: starting watch
    STEP: getting 01/19/23 20:48:38.343
    STEP: listing in namespace 01/19/23 20:48:38.352
    STEP: listing across namespaces 01/19/23 20:48:38.357
    STEP: patching 01/19/23 20:48:38.36
    STEP: updating 01/19/23 20:48:38.366
    Jan 19 20:48:38.372: INFO: waiting for watch events with expected annotations in namespace
    Jan 19 20:48:38.372: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/19/23 20:48:38.372
    STEP: deleting a collection 01/19/23 20:48:38.382
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jan 19 20:48:38.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-1751" for this suite. 01/19/23 20:48:38.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:48:38.407
Jan 19 20:48:38.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 20:48:38.408
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:48:38.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:48:38.431
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 20:48:38.472
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 20:48:38.768
STEP: Deploying the webhook pod 01/19/23 20:48:38.778
STEP: Wait for the deployment to be ready 01/19/23 20:48:38.79
Jan 19 20:48:38.799: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/19/23 20:48:40.808
STEP: Verifying the service has paired with the endpoint 01/19/23 20:48:40.818
Jan 19 20:48:41.819: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/19/23 20:48:41.822
STEP: create a configmap that should be updated by the webhook 01/19/23 20:48:41.836
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 20:48:41.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-0" for this suite. 01/19/23 20:48:41.858
STEP: Destroying namespace "webhook-0-markers" for this suite. 01/19/23 20:48:41.867
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":43,"skipped":691,"failed":0}
------------------------------
• [3.568 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:48:38.407
    Jan 19 20:48:38.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 20:48:38.408
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:48:38.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:48:38.431
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 20:48:38.472
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 20:48:38.768
    STEP: Deploying the webhook pod 01/19/23 20:48:38.778
    STEP: Wait for the deployment to be ready 01/19/23 20:48:38.79
    Jan 19 20:48:38.799: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/19/23 20:48:40.808
    STEP: Verifying the service has paired with the endpoint 01/19/23 20:48:40.818
    Jan 19 20:48:41.819: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/19/23 20:48:41.822
    STEP: create a configmap that should be updated by the webhook 01/19/23 20:48:41.836
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 20:48:41.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-0" for this suite. 01/19/23 20:48:41.858
    STEP: Destroying namespace "webhook-0-markers" for this suite. 01/19/23 20:48:41.867
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:48:41.976
Jan 19 20:48:41.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 20:48:41.976
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:48:42.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:48:42.018
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan 19 20:48:42.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 20:48:48.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1346" for this suite. 01/19/23 20:48:48.929
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":44,"skipped":702,"failed":0}
------------------------------
• [SLOW TEST] [6.963 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:48:41.976
    Jan 19 20:48:41.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 20:48:41.976
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:48:42.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:48:42.018
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan 19 20:48:42.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 20:48:48.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1346" for this suite. 01/19/23 20:48:48.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:48:48.94
Jan 19 20:48:48.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pod-network-test 01/19/23 20:48:48.941
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:48:48.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:48:48.959
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-8153 01/19/23 20:48:48.967
STEP: creating a selector 01/19/23 20:48:48.967
STEP: Creating the service pods in kubernetes 01/19/23 20:48:48.967
Jan 19 20:48:48.967: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 20:48:49.193: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8153" to be "running and ready"
Jan 19 20:48:49.195: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136014ms
Jan 19 20:48:49.195: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 20:48:51.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004873645s
Jan 19 20:48:51.198: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 20:48:53.199: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006086129s
Jan 19 20:48:53.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 20:48:55.200: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007115156s
Jan 19 20:48:55.200: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 20:48:57.199: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005804571s
Jan 19 20:48:57.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 20:48:59.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005374729s
Jan 19 20:48:59.198: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 20:49:01.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.005063935s
Jan 19 20:49:01.198: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 19 20:49:01.198: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 19 20:49:01.201: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8153" to be "running and ready"
Jan 19 20:49:01.203: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.598944ms
Jan 19 20:49:01.203: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan 19 20:49:03.207: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.006289383s
Jan 19 20:49:03.207: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan 19 20:49:05.207: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.006860766s
Jan 19 20:49:05.207: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan 19 20:49:07.209: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.008808027s
Jan 19 20:49:07.209: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan 19 20:49:09.206: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.005405094s
Jan 19 20:49:09.206: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan 19 20:49:11.207: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.0061591s
Jan 19 20:49:11.207: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 19 20:49:11.207: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 19 20:49:11.209: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8153" to be "running and ready"
Jan 19 20:49:11.212: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.715083ms
Jan 19 20:49:11.212: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 19 20:49:11.212: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan 19 20:49:11.224: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8153" to be "running and ready"
Jan 19 20:49:11.227: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.889197ms
Jan 19 20:49:11.227: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan 19 20:49:11.227: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jan 19 20:49:11.230: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-8153" to be "running and ready"
Jan 19 20:49:11.233: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 2.537022ms
Jan 19 20:49:11.233: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jan 19 20:49:11.233: INFO: Pod "netserver-4" satisfied condition "running and ready"
Jan 19 20:49:11.240: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-8153" to be "running and ready"
Jan 19 20:49:11.242: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 2.154356ms
Jan 19 20:49:11.242: INFO: The phase of Pod netserver-5 is Running (Ready = true)
Jan 19 20:49:11.242: INFO: Pod "netserver-5" satisfied condition "running and ready"
STEP: Creating test pods 01/19/23 20:49:11.244
Jan 19 20:49:11.267: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8153" to be "running"
Jan 19 20:49:11.271: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.316619ms
Jan 19 20:49:13.275: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007369127s
Jan 19 20:49:13.275: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 19 20:49:13.277: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8153" to be "running"
Jan 19 20:49:13.280: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.510602ms
Jan 19 20:49:13.280: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 19 20:49:13.282: INFO: Setting MaxTries for pod polling to 66 for networking test based on endpoint count 6
Jan 19 20:49:13.282: INFO: Going to poll 10.128.10.25 on port 8083 at least 0 times, with a maximum of 66 tries before failing
Jan 19 20:49:13.284: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.10.25:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8153 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:49:13.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:49:13.284: INFO: ExecWithOptions: Clientset creation
Jan 19 20:49:13.285: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8153/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.10.25%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 20:49:13.408: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 19 20:49:13.408: INFO: Going to poll 10.128.14.42 on port 8083 at least 0 times, with a maximum of 66 tries before failing
Jan 19 20:49:13.411: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.14.42:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8153 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:49:13.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:49:13.411: INFO: ExecWithOptions: Clientset creation
Jan 19 20:49:13.411: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8153/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.14.42%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 20:49:13.513: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 19 20:49:13.513: INFO: Going to poll 10.128.12.27 on port 8083 at least 0 times, with a maximum of 66 tries before failing
Jan 19 20:49:13.515: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.12.27:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8153 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:49:13.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:49:13.516: INFO: ExecWithOptions: Clientset creation
Jan 19 20:49:13.516: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8153/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.12.27%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 20:49:13.601: INFO: Found all 1 expected endpoints: [netserver-2]
Jan 19 20:49:13.601: INFO: Going to poll 10.128.8.47 on port 8083 at least 0 times, with a maximum of 66 tries before failing
Jan 19 20:49:13.604: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.8.47:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8153 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:49:13.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:49:13.605: INFO: ExecWithOptions: Clientset creation
Jan 19 20:49:13.605: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8153/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.8.47%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 20:49:13.691: INFO: Found all 1 expected endpoints: [netserver-3]
Jan 19 20:49:13.691: INFO: Going to poll 10.128.6.14 on port 8083 at least 0 times, with a maximum of 66 tries before failing
Jan 19 20:49:13.693: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.6.14:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8153 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:49:13.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:49:13.694: INFO: ExecWithOptions: Clientset creation
Jan 19 20:49:13.694: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8153/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.6.14%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 20:49:13.786: INFO: Found all 1 expected endpoints: [netserver-4]
Jan 19 20:49:13.786: INFO: Going to poll 10.128.16.70 on port 8083 at least 0 times, with a maximum of 66 tries before failing
Jan 19 20:49:13.788: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.16.70:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8153 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 20:49:13.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 20:49:13.789: INFO: ExecWithOptions: Clientset creation
Jan 19 20:49:13.789: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8153/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.16.70%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 20:49:13.875: INFO: Found all 1 expected endpoints: [netserver-5]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 19 20:49:13.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8153" for this suite. 01/19/23 20:49:13.88
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":45,"skipped":726,"failed":0}
------------------------------
• [SLOW TEST] [24.946 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:48:48.94
    Jan 19 20:48:48.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pod-network-test 01/19/23 20:48:48.941
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:48:48.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:48:48.959
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-8153 01/19/23 20:48:48.967
    STEP: creating a selector 01/19/23 20:48:48.967
    STEP: Creating the service pods in kubernetes 01/19/23 20:48:48.967
    Jan 19 20:48:48.967: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 19 20:48:49.193: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8153" to be "running and ready"
    Jan 19 20:48:49.195: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.136014ms
    Jan 19 20:48:49.195: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 20:48:51.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004873645s
    Jan 19 20:48:51.198: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 20:48:53.199: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006086129s
    Jan 19 20:48:53.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 20:48:55.200: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.007115156s
    Jan 19 20:48:55.200: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 20:48:57.199: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005804571s
    Jan 19 20:48:57.199: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 20:48:59.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005374729s
    Jan 19 20:48:59.198: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 20:49:01.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.005063935s
    Jan 19 20:49:01.198: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 19 20:49:01.198: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 19 20:49:01.201: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8153" to be "running and ready"
    Jan 19 20:49:01.203: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.598944ms
    Jan 19 20:49:01.203: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan 19 20:49:03.207: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.006289383s
    Jan 19 20:49:03.207: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan 19 20:49:05.207: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.006860766s
    Jan 19 20:49:05.207: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan 19 20:49:07.209: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.008808027s
    Jan 19 20:49:07.209: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan 19 20:49:09.206: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.005405094s
    Jan 19 20:49:09.206: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan 19 20:49:11.207: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.0061591s
    Jan 19 20:49:11.207: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 19 20:49:11.207: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 19 20:49:11.209: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8153" to be "running and ready"
    Jan 19 20:49:11.212: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.715083ms
    Jan 19 20:49:11.212: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 19 20:49:11.212: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan 19 20:49:11.224: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8153" to be "running and ready"
    Jan 19 20:49:11.227: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.889197ms
    Jan 19 20:49:11.227: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan 19 20:49:11.227: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jan 19 20:49:11.230: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-8153" to be "running and ready"
    Jan 19 20:49:11.233: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 2.537022ms
    Jan 19 20:49:11.233: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jan 19 20:49:11.233: INFO: Pod "netserver-4" satisfied condition "running and ready"
    Jan 19 20:49:11.240: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-8153" to be "running and ready"
    Jan 19 20:49:11.242: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 2.154356ms
    Jan 19 20:49:11.242: INFO: The phase of Pod netserver-5 is Running (Ready = true)
    Jan 19 20:49:11.242: INFO: Pod "netserver-5" satisfied condition "running and ready"
    STEP: Creating test pods 01/19/23 20:49:11.244
    Jan 19 20:49:11.267: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8153" to be "running"
    Jan 19 20:49:11.271: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.316619ms
    Jan 19 20:49:13.275: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007369127s
    Jan 19 20:49:13.275: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 19 20:49:13.277: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8153" to be "running"
    Jan 19 20:49:13.280: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.510602ms
    Jan 19 20:49:13.280: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 19 20:49:13.282: INFO: Setting MaxTries for pod polling to 66 for networking test based on endpoint count 6
    Jan 19 20:49:13.282: INFO: Going to poll 10.128.10.25 on port 8083 at least 0 times, with a maximum of 66 tries before failing
    Jan 19 20:49:13.284: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.10.25:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8153 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:49:13.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:49:13.284: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:49:13.285: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8153/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.10.25%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 20:49:13.408: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 19 20:49:13.408: INFO: Going to poll 10.128.14.42 on port 8083 at least 0 times, with a maximum of 66 tries before failing
    Jan 19 20:49:13.411: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.14.42:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8153 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:49:13.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:49:13.411: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:49:13.411: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8153/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.14.42%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 20:49:13.513: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan 19 20:49:13.513: INFO: Going to poll 10.128.12.27 on port 8083 at least 0 times, with a maximum of 66 tries before failing
    Jan 19 20:49:13.515: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.12.27:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8153 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:49:13.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:49:13.516: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:49:13.516: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8153/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.12.27%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 20:49:13.601: INFO: Found all 1 expected endpoints: [netserver-2]
    Jan 19 20:49:13.601: INFO: Going to poll 10.128.8.47 on port 8083 at least 0 times, with a maximum of 66 tries before failing
    Jan 19 20:49:13.604: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.8.47:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8153 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:49:13.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:49:13.605: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:49:13.605: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8153/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.8.47%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 20:49:13.691: INFO: Found all 1 expected endpoints: [netserver-3]
    Jan 19 20:49:13.691: INFO: Going to poll 10.128.6.14 on port 8083 at least 0 times, with a maximum of 66 tries before failing
    Jan 19 20:49:13.693: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.6.14:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8153 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:49:13.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:49:13.694: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:49:13.694: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8153/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.6.14%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 20:49:13.786: INFO: Found all 1 expected endpoints: [netserver-4]
    Jan 19 20:49:13.786: INFO: Going to poll 10.128.16.70 on port 8083 at least 0 times, with a maximum of 66 tries before failing
    Jan 19 20:49:13.788: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.128.16.70:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8153 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 20:49:13.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 20:49:13.789: INFO: ExecWithOptions: Clientset creation
    Jan 19 20:49:13.789: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8153/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.128.16.70%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 20:49:13.875: INFO: Found all 1 expected endpoints: [netserver-5]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 19 20:49:13.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8153" for this suite. 01/19/23 20:49:13.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:49:13.887
Jan 19 20:49:13.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pods 01/19/23 20:49:13.888
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:49:13.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:49:13.934
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 01/19/23 20:49:13.936
Jan 19 20:49:13.982: INFO: Waiting up to 5m0s for pod "pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47" in namespace "pods-8797" to be "running and ready"
Jan 19 20:49:14.021: INFO: Pod "pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47": Phase="Pending", Reason="", readiness=false. Elapsed: 38.858658ms
Jan 19 20:49:14.021: INFO: The phase of Pod pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 20:49:16.024: INFO: Pod "pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47": Phase="Running", Reason="", readiness=true. Elapsed: 2.041965718s
Jan 19 20:49:16.024: INFO: The phase of Pod pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47 is Running (Ready = true)
Jan 19 20:49:16.024: INFO: Pod "pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47" satisfied condition "running and ready"
Jan 19 20:49:16.029: INFO: Pod pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47 has hostIP: 10.0.172.44
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 20:49:16.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8797" for this suite. 01/19/23 20:49:16.033
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":46,"skipped":747,"failed":0}
------------------------------
• [2.154 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:49:13.887
    Jan 19 20:49:13.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pods 01/19/23 20:49:13.888
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:49:13.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:49:13.934
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 01/19/23 20:49:13.936
    Jan 19 20:49:13.982: INFO: Waiting up to 5m0s for pod "pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47" in namespace "pods-8797" to be "running and ready"
    Jan 19 20:49:14.021: INFO: Pod "pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47": Phase="Pending", Reason="", readiness=false. Elapsed: 38.858658ms
    Jan 19 20:49:14.021: INFO: The phase of Pod pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 20:49:16.024: INFO: Pod "pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47": Phase="Running", Reason="", readiness=true. Elapsed: 2.041965718s
    Jan 19 20:49:16.024: INFO: The phase of Pod pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47 is Running (Ready = true)
    Jan 19 20:49:16.024: INFO: Pod "pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47" satisfied condition "running and ready"
    Jan 19 20:49:16.029: INFO: Pod pod-hostip-ed02e5c1-1424-407f-802f-e4941a250a47 has hostIP: 10.0.172.44
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 20:49:16.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8797" for this suite. 01/19/23 20:49:16.033
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:49:16.041
Jan 19 20:49:16.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 20:49:16.041
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:49:16.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:49:16.064
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 01/19/23 20:49:16.067
W0119 20:49:16.126643      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 20:49:16.126: INFO: Waiting up to 5m0s for pod "pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677" in namespace "emptydir-4847" to be "Succeeded or Failed"
Jan 19 20:49:16.133: INFO: Pod "pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677": Phase="Pending", Reason="", readiness=false. Elapsed: 7.081002ms
Jan 19 20:49:18.137: INFO: Pod "pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011198295s
Jan 19 20:49:20.137: INFO: Pod "pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010299197s
STEP: Saw pod success 01/19/23 20:49:20.137
Jan 19 20:49:20.137: INFO: Pod "pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677" satisfied condition "Succeeded or Failed"
Jan 19 20:49:20.139: INFO: Trying to get logs from node ip-10-0-188-71.ec2.internal pod pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677 container test-container: <nil>
STEP: delete the pod 01/19/23 20:49:20.158
Jan 19 20:49:20.169: INFO: Waiting for pod pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677 to disappear
Jan 19 20:49:20.172: INFO: Pod pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 20:49:20.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4847" for this suite. 01/19/23 20:49:20.176
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":47,"skipped":749,"failed":0}
------------------------------
• [4.141 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:49:16.041
    Jan 19 20:49:16.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 20:49:16.041
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:49:16.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:49:16.064
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/19/23 20:49:16.067
    W0119 20:49:16.126643      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 20:49:16.126: INFO: Waiting up to 5m0s for pod "pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677" in namespace "emptydir-4847" to be "Succeeded or Failed"
    Jan 19 20:49:16.133: INFO: Pod "pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677": Phase="Pending", Reason="", readiness=false. Elapsed: 7.081002ms
    Jan 19 20:49:18.137: INFO: Pod "pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011198295s
    Jan 19 20:49:20.137: INFO: Pod "pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010299197s
    STEP: Saw pod success 01/19/23 20:49:20.137
    Jan 19 20:49:20.137: INFO: Pod "pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677" satisfied condition "Succeeded or Failed"
    Jan 19 20:49:20.139: INFO: Trying to get logs from node ip-10-0-188-71.ec2.internal pod pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677 container test-container: <nil>
    STEP: delete the pod 01/19/23 20:49:20.158
    Jan 19 20:49:20.169: INFO: Waiting for pod pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677 to disappear
    Jan 19 20:49:20.172: INFO: Pod pod-4e67294e-7f2a-4ce0-89c9-da65e9e5f677 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 20:49:20.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4847" for this suite. 01/19/23 20:49:20.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:49:20.183
Jan 19 20:49:20.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename cronjob 01/19/23 20:49:20.184
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:49:20.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:49:20.205
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/19/23 20:49:20.209
W0119 20:49:20.219305      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring a job is scheduled 01/19/23 20:49:20.219
STEP: Ensuring exactly one is scheduled 01/19/23 20:50:00.224
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/19/23 20:50:00.227
STEP: Ensuring no more jobs are scheduled 01/19/23 20:50:00.23
STEP: Removing cronjob 01/19/23 20:55:00.236
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 19 20:55:00.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-1420" for this suite. 01/19/23 20:55:00.244
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":48,"skipped":783,"failed":0}
------------------------------
• [SLOW TEST] [340.069 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:49:20.183
    Jan 19 20:49:20.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename cronjob 01/19/23 20:49:20.184
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:49:20.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:49:20.205
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/19/23 20:49:20.209
    W0119 20:49:20.219305      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring a job is scheduled 01/19/23 20:49:20.219
    STEP: Ensuring exactly one is scheduled 01/19/23 20:50:00.224
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/19/23 20:50:00.227
    STEP: Ensuring no more jobs are scheduled 01/19/23 20:50:00.23
    STEP: Removing cronjob 01/19/23 20:55:00.236
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 19 20:55:00.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-1420" for this suite. 01/19/23 20:55:00.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:55:00.253
Jan 19 20:55:00.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-runtime 01/19/23 20:55:00.254
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:55:00.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:55:00.283
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 01/19/23 20:55:00.287
STEP: wait for the container to reach Failed 01/19/23 20:55:00.39
STEP: get the container status 01/19/23 20:55:03.406
STEP: the container should be terminated 01/19/23 20:55:03.41
STEP: the termination message should be set 01/19/23 20:55:03.41
Jan 19 20:55:03.410: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/19/23 20:55:03.41
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 19 20:55:03.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7203" for this suite. 01/19/23 20:55:03.426
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":49,"skipped":812,"failed":0}
------------------------------
• [3.179 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:55:00.253
    Jan 19 20:55:00.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-runtime 01/19/23 20:55:00.254
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:55:00.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:55:00.283
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 01/19/23 20:55:00.287
    STEP: wait for the container to reach Failed 01/19/23 20:55:00.39
    STEP: get the container status 01/19/23 20:55:03.406
    STEP: the container should be terminated 01/19/23 20:55:03.41
    STEP: the termination message should be set 01/19/23 20:55:03.41
    Jan 19 20:55:03.410: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/19/23 20:55:03.41
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 19 20:55:03.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7203" for this suite. 01/19/23 20:55:03.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:55:03.434
Jan 19 20:55:03.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pods 01/19/23 20:55:03.434
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:55:03.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:55:03.455
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 01/19/23 20:55:03.459
STEP: setting up watch 01/19/23 20:55:03.459
STEP: submitting the pod to kubernetes 01/19/23 20:55:03.571
STEP: verifying the pod is in kubernetes 01/19/23 20:55:03.595
STEP: verifying pod creation was observed 01/19/23 20:55:03.599
Jan 19 20:55:03.599: INFO: Waiting up to 5m0s for pod "pod-submit-remove-b18f10fb-b5af-45c4-8a02-4916651033b7" in namespace "pods-8910" to be "running"
Jan 19 20:55:03.605: INFO: Pod "pod-submit-remove-b18f10fb-b5af-45c4-8a02-4916651033b7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.623557ms
Jan 19 20:55:05.609: INFO: Pod "pod-submit-remove-b18f10fb-b5af-45c4-8a02-4916651033b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009665966s
Jan 19 20:55:05.609: INFO: Pod "pod-submit-remove-b18f10fb-b5af-45c4-8a02-4916651033b7" satisfied condition "running"
STEP: deleting the pod gracefully 01/19/23 20:55:05.611
STEP: verifying pod deletion was observed 01/19/23 20:55:05.617
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 20:55:07.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8910" for this suite. 01/19/23 20:55:07.626
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":50,"skipped":840,"failed":0}
------------------------------
• [4.197 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:55:03.434
    Jan 19 20:55:03.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pods 01/19/23 20:55:03.434
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:55:03.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:55:03.455
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 01/19/23 20:55:03.459
    STEP: setting up watch 01/19/23 20:55:03.459
    STEP: submitting the pod to kubernetes 01/19/23 20:55:03.571
    STEP: verifying the pod is in kubernetes 01/19/23 20:55:03.595
    STEP: verifying pod creation was observed 01/19/23 20:55:03.599
    Jan 19 20:55:03.599: INFO: Waiting up to 5m0s for pod "pod-submit-remove-b18f10fb-b5af-45c4-8a02-4916651033b7" in namespace "pods-8910" to be "running"
    Jan 19 20:55:03.605: INFO: Pod "pod-submit-remove-b18f10fb-b5af-45c4-8a02-4916651033b7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.623557ms
    Jan 19 20:55:05.609: INFO: Pod "pod-submit-remove-b18f10fb-b5af-45c4-8a02-4916651033b7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009665966s
    Jan 19 20:55:05.609: INFO: Pod "pod-submit-remove-b18f10fb-b5af-45c4-8a02-4916651033b7" satisfied condition "running"
    STEP: deleting the pod gracefully 01/19/23 20:55:05.611
    STEP: verifying pod deletion was observed 01/19/23 20:55:05.617
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 20:55:07.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8910" for this suite. 01/19/23 20:55:07.626
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:55:07.631
Jan 19 20:55:07.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename endpointslice 01/19/23 20:55:07.632
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:55:07.681
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:55:07.683
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 01/19/23 20:55:07.685
STEP: getting /apis/discovery.k8s.io 01/19/23 20:55:07.687
STEP: getting /apis/discovery.k8s.iov1 01/19/23 20:55:07.688
STEP: creating 01/19/23 20:55:07.689
STEP: getting 01/19/23 20:55:07.706
STEP: listing 01/19/23 20:55:07.708
STEP: watching 01/19/23 20:55:07.713
Jan 19 20:55:07.713: INFO: starting watch
STEP: cluster-wide listing 01/19/23 20:55:07.714
STEP: cluster-wide watching 01/19/23 20:55:07.719
Jan 19 20:55:07.719: INFO: starting watch
STEP: patching 01/19/23 20:55:07.72
STEP: updating 01/19/23 20:55:07.729
Jan 19 20:55:07.744: INFO: waiting for watch events with expected annotations
Jan 19 20:55:07.744: INFO: saw patched and updated annotations
STEP: deleting 01/19/23 20:55:07.744
STEP: deleting a collection 01/19/23 20:55:07.756
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 19 20:55:07.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6199" for this suite. 01/19/23 20:55:07.819
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":51,"skipped":844,"failed":0}
------------------------------
• [0.215 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:55:07.631
    Jan 19 20:55:07.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename endpointslice 01/19/23 20:55:07.632
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:55:07.681
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:55:07.683
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 01/19/23 20:55:07.685
    STEP: getting /apis/discovery.k8s.io 01/19/23 20:55:07.687
    STEP: getting /apis/discovery.k8s.iov1 01/19/23 20:55:07.688
    STEP: creating 01/19/23 20:55:07.689
    STEP: getting 01/19/23 20:55:07.706
    STEP: listing 01/19/23 20:55:07.708
    STEP: watching 01/19/23 20:55:07.713
    Jan 19 20:55:07.713: INFO: starting watch
    STEP: cluster-wide listing 01/19/23 20:55:07.714
    STEP: cluster-wide watching 01/19/23 20:55:07.719
    Jan 19 20:55:07.719: INFO: starting watch
    STEP: patching 01/19/23 20:55:07.72
    STEP: updating 01/19/23 20:55:07.729
    Jan 19 20:55:07.744: INFO: waiting for watch events with expected annotations
    Jan 19 20:55:07.744: INFO: saw patched and updated annotations
    STEP: deleting 01/19/23 20:55:07.744
    STEP: deleting a collection 01/19/23 20:55:07.756
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 19 20:55:07.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6199" for this suite. 01/19/23 20:55:07.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:55:07.847
Jan 19 20:55:07.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename cronjob 01/19/23 20:55:07.848
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:55:07.938
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:55:07.943
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/19/23 20:55:07.945
W0119 20:55:07.963480      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring more than one job is running at a time 01/19/23 20:55:07.963
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/19/23 20:57:01.966
STEP: Removing cronjob 01/19/23 20:57:01.969
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 19 20:57:01.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-3595" for this suite. 01/19/23 20:57:01.979
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":52,"skipped":850,"failed":0}
------------------------------
• [SLOW TEST] [114.138 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:55:07.847
    Jan 19 20:55:07.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename cronjob 01/19/23 20:55:07.848
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:55:07.938
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:55:07.943
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/19/23 20:55:07.945
    W0119 20:55:07.963480      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring more than one job is running at a time 01/19/23 20:55:07.963
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/19/23 20:57:01.966
    STEP: Removing cronjob 01/19/23 20:57:01.969
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 19 20:57:01.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-3595" for this suite. 01/19/23 20:57:01.979
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:57:01.985
Jan 19 20:57:01.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 20:57:01.986
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:57:02.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:57:02.018
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 01/19/23 20:57:02.022
W0119 20:57:02.069479      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 20:57:02.069: INFO: Waiting up to 5m0s for pod "pod-765d7312-1018-46a4-afaa-bf504c235b13" in namespace "emptydir-600" to be "Succeeded or Failed"
Jan 19 20:57:02.080: INFO: Pod "pod-765d7312-1018-46a4-afaa-bf504c235b13": Phase="Pending", Reason="", readiness=false. Elapsed: 11.224622ms
Jan 19 20:57:04.084: INFO: Pod "pod-765d7312-1018-46a4-afaa-bf504c235b13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014424477s
Jan 19 20:57:06.084: INFO: Pod "pod-765d7312-1018-46a4-afaa-bf504c235b13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014650179s
STEP: Saw pod success 01/19/23 20:57:06.084
Jan 19 20:57:06.084: INFO: Pod "pod-765d7312-1018-46a4-afaa-bf504c235b13" satisfied condition "Succeeded or Failed"
Jan 19 20:57:06.086: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-765d7312-1018-46a4-afaa-bf504c235b13 container test-container: <nil>
STEP: delete the pod 01/19/23 20:57:06.096
Jan 19 20:57:06.115: INFO: Waiting for pod pod-765d7312-1018-46a4-afaa-bf504c235b13 to disappear
Jan 19 20:57:06.118: INFO: Pod pod-765d7312-1018-46a4-afaa-bf504c235b13 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 20:57:06.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-600" for this suite. 01/19/23 20:57:06.122
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":53,"skipped":850,"failed":0}
------------------------------
• [4.141 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:57:01.985
    Jan 19 20:57:01.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 20:57:01.986
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:57:02.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:57:02.018
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/19/23 20:57:02.022
    W0119 20:57:02.069479      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 20:57:02.069: INFO: Waiting up to 5m0s for pod "pod-765d7312-1018-46a4-afaa-bf504c235b13" in namespace "emptydir-600" to be "Succeeded or Failed"
    Jan 19 20:57:02.080: INFO: Pod "pod-765d7312-1018-46a4-afaa-bf504c235b13": Phase="Pending", Reason="", readiness=false. Elapsed: 11.224622ms
    Jan 19 20:57:04.084: INFO: Pod "pod-765d7312-1018-46a4-afaa-bf504c235b13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014424477s
    Jan 19 20:57:06.084: INFO: Pod "pod-765d7312-1018-46a4-afaa-bf504c235b13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014650179s
    STEP: Saw pod success 01/19/23 20:57:06.084
    Jan 19 20:57:06.084: INFO: Pod "pod-765d7312-1018-46a4-afaa-bf504c235b13" satisfied condition "Succeeded or Failed"
    Jan 19 20:57:06.086: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-765d7312-1018-46a4-afaa-bf504c235b13 container test-container: <nil>
    STEP: delete the pod 01/19/23 20:57:06.096
    Jan 19 20:57:06.115: INFO: Waiting for pod pod-765d7312-1018-46a4-afaa-bf504c235b13 to disappear
    Jan 19 20:57:06.118: INFO: Pod pod-765d7312-1018-46a4-afaa-bf504c235b13 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 20:57:06.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-600" for this suite. 01/19/23 20:57:06.122
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 20:57:06.127
Jan 19 20:57:06.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-probe 01/19/23 20:57:06.128
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:57:06.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:57:06.151
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a in namespace container-probe-8542 01/19/23 20:57:06.16
Jan 19 20:57:06.198: INFO: Waiting up to 5m0s for pod "busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a" in namespace "container-probe-8542" to be "not pending"
Jan 19 20:57:06.205: INFO: Pod "busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.287955ms
Jan 19 20:57:08.209: INFO: Pod "busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a": Phase="Running", Reason="", readiness=true. Elapsed: 2.010618007s
Jan 19 20:57:08.209: INFO: Pod "busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a" satisfied condition "not pending"
Jan 19 20:57:08.209: INFO: Started pod busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a in namespace container-probe-8542
STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 20:57:08.209
Jan 19 20:57:08.212: INFO: Initial restart count of pod busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a is 0
STEP: deleting the pod 01/19/23 21:01:08.691
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 21:01:08.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8542" for this suite. 01/19/23 21:01:08.709
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":54,"skipped":853,"failed":0}
------------------------------
• [SLOW TEST] [242.588 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 20:57:06.127
    Jan 19 20:57:06.127: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-probe 01/19/23 20:57:06.128
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 20:57:06.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 20:57:06.151
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a in namespace container-probe-8542 01/19/23 20:57:06.16
    Jan 19 20:57:06.198: INFO: Waiting up to 5m0s for pod "busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a" in namespace "container-probe-8542" to be "not pending"
    Jan 19 20:57:06.205: INFO: Pod "busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.287955ms
    Jan 19 20:57:08.209: INFO: Pod "busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a": Phase="Running", Reason="", readiness=true. Elapsed: 2.010618007s
    Jan 19 20:57:08.209: INFO: Pod "busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a" satisfied condition "not pending"
    Jan 19 20:57:08.209: INFO: Started pod busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a in namespace container-probe-8542
    STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 20:57:08.209
    Jan 19 20:57:08.212: INFO: Initial restart count of pod busybox-18aaff8b-54d6-4b88-9445-3d53dc50565a is 0
    STEP: deleting the pod 01/19/23 21:01:08.691
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 21:01:08.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8542" for this suite. 01/19/23 21:01:08.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:01:08.715
Jan 19 21:01:08.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:01:08.716
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:01:08.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:01:08.801
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 21:01:08.804
Jan 19 21:01:08.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-8950 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 19 21:01:08.886: INFO: stderr: ""
Jan 19 21:01:08.886: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/19/23 21:01:08.886
STEP: verifying the pod e2e-test-httpd-pod was created 01/19/23 21:01:13.94
Jan 19 21:01:13.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-8950 get pod e2e-test-httpd-pod -o json'
Jan 19 21:01:13.997: INFO: stderr: ""
Jan 19 21:01:13.997: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.ovn.org/pod-networks\": \"{\\\"default\\\":{\\\"ip_addresses\\\":[\\\"10.128.8.55/23\\\"],\\\"mac_address\\\":\\\"0a:58:0a:80:08:37\\\",\\\"gateway_ips\\\":[\\\"10.128.8.1\\\"],\\\"ip_address\\\":\\\"10.128.8.55/23\\\",\\\"gateway_ip\\\":\\\"10.128.8.1\\\"}}\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"ovn-kubernetes\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.128.8.55\\\"\\n    ],\\n    \\\"mac\\\": \\\"0a:58:0a:80:08:37\\\",\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"ovn-kubernetes\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.128.8.55\\\"\\n    ],\\n    \\\"mac\\\": \\\"0a:58:0a:80:08:37\\\",\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-01-19T21:01:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8950\",\n        \"resourceVersion\": \"155320\",\n        \"uid\": \"60f0c540-72c2-4937-a7ab-13d2afa6786a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-nkzz6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-172-44.ec2.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c40,c30\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-nkzz6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T21:01:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T21:01:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T21:01:10Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T21:01:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://7a46aabb1b9509ee338ac1aebe7709ac56a11eabd368f2198791a34bf8b502d3\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-19T21:01:09Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.172.44\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.128.8.55\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.128.8.55\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-19T21:01:08Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/19/23 21:01:13.997
Jan 19 21:01:13.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-8950 replace -f -'
Jan 19 21:01:16.481: INFO: stderr: ""
Jan 19 21:01:16.481: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/19/23 21:01:16.481
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jan 19 21:01:16.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-8950 delete pods e2e-test-httpd-pod'
Jan 19 21:01:18.380: INFO: stderr: ""
Jan 19 21:01:18.380: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:01:18.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8950" for this suite. 01/19/23 21:01:18.384
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":55,"skipped":862,"failed":0}
------------------------------
• [SLOW TEST] [9.675 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:01:08.715
    Jan 19 21:01:08.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:01:08.716
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:01:08.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:01:08.801
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 21:01:08.804
    Jan 19 21:01:08.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-8950 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 19 21:01:08.886: INFO: stderr: ""
    Jan 19 21:01:08.886: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/19/23 21:01:08.886
    STEP: verifying the pod e2e-test-httpd-pod was created 01/19/23 21:01:13.94
    Jan 19 21:01:13.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-8950 get pod e2e-test-httpd-pod -o json'
    Jan 19 21:01:13.997: INFO: stderr: ""
    Jan 19 21:01:13.997: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"k8s.ovn.org/pod-networks\": \"{\\\"default\\\":{\\\"ip_addresses\\\":[\\\"10.128.8.55/23\\\"],\\\"mac_address\\\":\\\"0a:58:0a:80:08:37\\\",\\\"gateway_ips\\\":[\\\"10.128.8.1\\\"],\\\"ip_address\\\":\\\"10.128.8.55/23\\\",\\\"gateway_ip\\\":\\\"10.128.8.1\\\"}}\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"ovn-kubernetes\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.128.8.55\\\"\\n    ],\\n    \\\"mac\\\": \\\"0a:58:0a:80:08:37\\\",\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"ovn-kubernetes\\\",\\n    \\\"interface\\\": \\\"eth0\\\",\\n    \\\"ips\\\": [\\n        \\\"10.128.8.55\\\"\\n    ],\\n    \\\"mac\\\": \\\"0a:58:0a:80:08:37\\\",\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-01-19T21:01:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8950\",\n        \"resourceVersion\": \"155320\",\n        \"uid\": \"60f0c540-72c2-4937-a7ab-13d2afa6786a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-nkzz6\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-172-44.ec2.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c40,c30\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-nkzz6\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T21:01:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T21:01:10Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T21:01:10Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-19T21:01:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://7a46aabb1b9509ee338ac1aebe7709ac56a11eabd368f2198791a34bf8b502d3\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-19T21:01:09Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.172.44\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.128.8.55\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.128.8.55\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-19T21:01:08Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/19/23 21:01:13.997
    Jan 19 21:01:13.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-8950 replace -f -'
    Jan 19 21:01:16.481: INFO: stderr: ""
    Jan 19 21:01:16.481: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 01/19/23 21:01:16.481
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jan 19 21:01:16.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-8950 delete pods e2e-test-httpd-pod'
    Jan 19 21:01:18.380: INFO: stderr: ""
    Jan 19 21:01:18.380: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:01:18.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8950" for this suite. 01/19/23 21:01:18.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:01:18.391
Jan 19 21:01:18.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename endpointslice 01/19/23 21:01:18.392
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:01:18.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:01:18.422
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 01/19/23 21:01:23.553
STEP: referencing matching pods with named port 01/19/23 21:01:28.559
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/19/23 21:01:33.565
STEP: recreating EndpointSlices after they've been deleted 01/19/23 21:01:38.573
Jan 19 21:01:38.589: INFO: EndpointSlice for Service endpointslice-1824/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 19 21:01:48.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1824" for this suite. 01/19/23 21:01:48.599
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":56,"skipped":875,"failed":0}
------------------------------
• [SLOW TEST] [30.214 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:01:18.391
    Jan 19 21:01:18.391: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename endpointslice 01/19/23 21:01:18.392
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:01:18.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:01:18.422
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 01/19/23 21:01:23.553
    STEP: referencing matching pods with named port 01/19/23 21:01:28.559
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/19/23 21:01:33.565
    STEP: recreating EndpointSlices after they've been deleted 01/19/23 21:01:38.573
    Jan 19 21:01:38.589: INFO: EndpointSlice for Service endpointslice-1824/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 19 21:01:48.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-1824" for this suite. 01/19/23 21:01:48.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:01:48.605
Jan 19 21:01:48.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 21:01:48.606
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:01:48.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:01:48.635
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/19/23 21:01:48.637
Jan 19 21:01:48.676: INFO: Waiting up to 5m0s for pod "pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91" in namespace "emptydir-8499" to be "Succeeded or Failed"
Jan 19 21:01:48.680: INFO: Pod "pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.509113ms
Jan 19 21:01:50.684: INFO: Pod "pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00785357s
Jan 19 21:01:52.683: INFO: Pod "pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007562932s
STEP: Saw pod success 01/19/23 21:01:52.683
Jan 19 21:01:52.684: INFO: Pod "pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91" satisfied condition "Succeeded or Failed"
Jan 19 21:01:52.686: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91 container test-container: <nil>
STEP: delete the pod 01/19/23 21:01:52.696
Jan 19 21:01:52.706: INFO: Waiting for pod pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91 to disappear
Jan 19 21:01:52.707: INFO: Pod pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 21:01:52.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8499" for this suite. 01/19/23 21:01:52.712
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":57,"skipped":887,"failed":0}
------------------------------
• [4.112 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:01:48.605
    Jan 19 21:01:48.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 21:01:48.606
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:01:48.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:01:48.635
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/19/23 21:01:48.637
    Jan 19 21:01:48.676: INFO: Waiting up to 5m0s for pod "pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91" in namespace "emptydir-8499" to be "Succeeded or Failed"
    Jan 19 21:01:48.680: INFO: Pod "pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91": Phase="Pending", Reason="", readiness=false. Elapsed: 4.509113ms
    Jan 19 21:01:50.684: INFO: Pod "pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00785357s
    Jan 19 21:01:52.683: INFO: Pod "pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007562932s
    STEP: Saw pod success 01/19/23 21:01:52.683
    Jan 19 21:01:52.684: INFO: Pod "pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91" satisfied condition "Succeeded or Failed"
    Jan 19 21:01:52.686: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91 container test-container: <nil>
    STEP: delete the pod 01/19/23 21:01:52.696
    Jan 19 21:01:52.706: INFO: Waiting for pod pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91 to disappear
    Jan 19 21:01:52.707: INFO: Pod pod-ae5199aa-1f6d-40e2-9dbd-1b368146bb91 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:01:52.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8499" for this suite. 01/19/23 21:01:52.712
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:01:52.718
Jan 19 21:01:52.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename subpath 01/19/23 21:01:52.719
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:01:52.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:01:52.741
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/19/23 21:01:52.744
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-c94c 01/19/23 21:01:52.767
STEP: Creating a pod to test atomic-volume-subpath 01/19/23 21:01:52.767
Jan 19 21:01:52.810: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c94c" in namespace "subpath-8005" to be "Succeeded or Failed"
Jan 19 21:01:52.818: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.656733ms
Jan 19 21:01:54.821: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010767418s
Jan 19 21:01:56.821: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 4.010643354s
Jan 19 21:01:58.824: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 6.013567135s
Jan 19 21:02:00.821: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 8.011017193s
Jan 19 21:02:02.821: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 10.010756743s
Jan 19 21:02:04.823: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 12.012206942s
Jan 19 21:02:06.821: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 14.010952s
Jan 19 21:02:08.822: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 16.011714926s
Jan 19 21:02:10.821: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 18.010984961s
Jan 19 21:02:12.823: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 20.012141525s
Jan 19 21:02:14.822: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=false. Elapsed: 22.011571627s
Jan 19 21:02:16.822: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011759867s
STEP: Saw pod success 01/19/23 21:02:16.822
Jan 19 21:02:16.822: INFO: Pod "pod-subpath-test-configmap-c94c" satisfied condition "Succeeded or Failed"
Jan 19 21:02:16.825: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-subpath-test-configmap-c94c container test-container-subpath-configmap-c94c: <nil>
STEP: delete the pod 01/19/23 21:02:16.83
Jan 19 21:02:16.839: INFO: Waiting for pod pod-subpath-test-configmap-c94c to disappear
Jan 19 21:02:16.841: INFO: Pod pod-subpath-test-configmap-c94c no longer exists
STEP: Deleting pod pod-subpath-test-configmap-c94c 01/19/23 21:02:16.841
Jan 19 21:02:16.841: INFO: Deleting pod "pod-subpath-test-configmap-c94c" in namespace "subpath-8005"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 19 21:02:16.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8005" for this suite. 01/19/23 21:02:16.847
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":58,"skipped":920,"failed":0}
------------------------------
• [SLOW TEST] [24.135 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:01:52.718
    Jan 19 21:01:52.718: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename subpath 01/19/23 21:01:52.719
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:01:52.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:01:52.741
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/19/23 21:01:52.744
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-c94c 01/19/23 21:01:52.767
    STEP: Creating a pod to test atomic-volume-subpath 01/19/23 21:01:52.767
    Jan 19 21:01:52.810: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-c94c" in namespace "subpath-8005" to be "Succeeded or Failed"
    Jan 19 21:01:52.818: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.656733ms
    Jan 19 21:01:54.821: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 2.010767418s
    Jan 19 21:01:56.821: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 4.010643354s
    Jan 19 21:01:58.824: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 6.013567135s
    Jan 19 21:02:00.821: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 8.011017193s
    Jan 19 21:02:02.821: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 10.010756743s
    Jan 19 21:02:04.823: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 12.012206942s
    Jan 19 21:02:06.821: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 14.010952s
    Jan 19 21:02:08.822: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 16.011714926s
    Jan 19 21:02:10.821: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 18.010984961s
    Jan 19 21:02:12.823: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=true. Elapsed: 20.012141525s
    Jan 19 21:02:14.822: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Running", Reason="", readiness=false. Elapsed: 22.011571627s
    Jan 19 21:02:16.822: INFO: Pod "pod-subpath-test-configmap-c94c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011759867s
    STEP: Saw pod success 01/19/23 21:02:16.822
    Jan 19 21:02:16.822: INFO: Pod "pod-subpath-test-configmap-c94c" satisfied condition "Succeeded or Failed"
    Jan 19 21:02:16.825: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-subpath-test-configmap-c94c container test-container-subpath-configmap-c94c: <nil>
    STEP: delete the pod 01/19/23 21:02:16.83
    Jan 19 21:02:16.839: INFO: Waiting for pod pod-subpath-test-configmap-c94c to disappear
    Jan 19 21:02:16.841: INFO: Pod pod-subpath-test-configmap-c94c no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-c94c 01/19/23 21:02:16.841
    Jan 19 21:02:16.841: INFO: Deleting pod "pod-subpath-test-configmap-c94c" in namespace "subpath-8005"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 19 21:02:16.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8005" for this suite. 01/19/23 21:02:16.847
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:02:16.853
Jan 19 21:02:16.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-runtime 01/19/23 21:02:16.854
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:02:16.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:02:16.935
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 01/19/23 21:02:16.939
STEP: wait for the container to reach Succeeded 01/19/23 21:02:17.03
STEP: get the container status 01/19/23 21:02:21.075
STEP: the container should be terminated 01/19/23 21:02:21.077
STEP: the termination message should be set 01/19/23 21:02:21.077
Jan 19 21:02:21.077: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/19/23 21:02:21.077
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 19 21:02:21.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3275" for this suite. 01/19/23 21:02:21.093
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":59,"skipped":921,"failed":0}
------------------------------
• [4.244 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:02:16.853
    Jan 19 21:02:16.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-runtime 01/19/23 21:02:16.854
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:02:16.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:02:16.935
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 01/19/23 21:02:16.939
    STEP: wait for the container to reach Succeeded 01/19/23 21:02:17.03
    STEP: get the container status 01/19/23 21:02:21.075
    STEP: the container should be terminated 01/19/23 21:02:21.077
    STEP: the termination message should be set 01/19/23 21:02:21.077
    Jan 19 21:02:21.077: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/19/23 21:02:21.077
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 19 21:02:21.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-3275" for this suite. 01/19/23 21:02:21.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:02:21.098
Jan 19 21:02:21.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:02:21.099
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:02:21.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:02:21.114
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:02:21.182
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:02:21.555
STEP: Deploying the webhook pod 01/19/23 21:02:21.561
STEP: Wait for the deployment to be ready 01/19/23 21:02:21.572
Jan 19 21:02:21.580: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/19/23 21:02:23.589
STEP: Verifying the service has paired with the endpoint 01/19/23 21:02:23.598
Jan 19 21:02:24.599: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/19/23 21:02:24.602
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/19/23 21:02:24.617
STEP: Creating a dummy validating-webhook-configuration object 01/19/23 21:02:24.63
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/19/23 21:02:24.638
STEP: Creating a dummy mutating-webhook-configuration object 01/19/23 21:02:24.643
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/19/23 21:02:24.65
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:02:24.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6774" for this suite. 01/19/23 21:02:24.673
STEP: Destroying namespace "webhook-6774-markers" for this suite. 01/19/23 21:02:24.678
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":60,"skipped":931,"failed":0}
------------------------------
• [3.639 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:02:21.098
    Jan 19 21:02:21.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:02:21.099
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:02:21.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:02:21.114
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:02:21.182
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:02:21.555
    STEP: Deploying the webhook pod 01/19/23 21:02:21.561
    STEP: Wait for the deployment to be ready 01/19/23 21:02:21.572
    Jan 19 21:02:21.580: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/19/23 21:02:23.589
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:02:23.598
    Jan 19 21:02:24.599: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/19/23 21:02:24.602
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/19/23 21:02:24.617
    STEP: Creating a dummy validating-webhook-configuration object 01/19/23 21:02:24.63
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/19/23 21:02:24.638
    STEP: Creating a dummy mutating-webhook-configuration object 01/19/23 21:02:24.643
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/19/23 21:02:24.65
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:02:24.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-6774" for this suite. 01/19/23 21:02:24.673
    STEP: Destroying namespace "webhook-6774-markers" for this suite. 01/19/23 21:02:24.678
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:02:24.738
Jan 19 21:02:24.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename taint-single-pod 01/19/23 21:02:24.739
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:02:24.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:02:24.778
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jan 19 21:02:24.781: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 21:03:25.009: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jan 19 21:03:25.015: INFO: Starting informer...
STEP: Starting pod... 01/19/23 21:03:25.015
Jan 19 21:03:25.231: INFO: Pod is running on ip-10-0-172-44.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node 01/19/23 21:03:25.231
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 21:03:25.246
STEP: Waiting short time to make sure Pod is queued for deletion 01/19/23 21:03:25.261
Jan 19 21:03:25.261: INFO: Pod wasn't evicted. Proceeding
Jan 19 21:03:25.261: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 21:03:25.288
STEP: Waiting some time to make sure that toleration time passed. 01/19/23 21:03:25.304
Jan 19 21:04:40.308: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:04:40.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-755" for this suite. 01/19/23 21:04:40.314
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":61,"skipped":966,"failed":0}
------------------------------
• [SLOW TEST] [135.582 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:02:24.738
    Jan 19 21:02:24.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename taint-single-pod 01/19/23 21:02:24.739
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:02:24.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:02:24.778
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jan 19 21:02:24.781: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 19 21:03:25.009: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jan 19 21:03:25.015: INFO: Starting informer...
    STEP: Starting pod... 01/19/23 21:03:25.015
    Jan 19 21:03:25.231: INFO: Pod is running on ip-10-0-172-44.ec2.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 01/19/23 21:03:25.231
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 21:03:25.246
    STEP: Waiting short time to make sure Pod is queued for deletion 01/19/23 21:03:25.261
    Jan 19 21:03:25.261: INFO: Pod wasn't evicted. Proceeding
    Jan 19 21:03:25.261: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 21:03:25.288
    STEP: Waiting some time to make sure that toleration time passed. 01/19/23 21:03:25.304
    Jan 19 21:04:40.308: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:04:40.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-755" for this suite. 01/19/23 21:04:40.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:04:40.321
Jan 19 21:04:40.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 21:04:40.321
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:04:40.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:04:40.395
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221
STEP: creating service in namespace services-5258 01/19/23 21:04:40.398
Jan 19 21:04:40.412: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-5258" to be "running and ready"
Jan 19 21:04:40.415: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.280117ms
Jan 19 21:04:40.415: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:04:42.419: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.007181687s
Jan 19 21:04:42.419: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
Jan 19 21:04:42.419: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
Jan 19 21:04:42.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
Jan 19 21:04:42.572: INFO: rc: 7
Jan 19 21:04:42.581: INFO: Waiting for pod kube-proxy-mode-detector to disappear
Jan 19 21:04:42.583: INFO: Pod kube-proxy-mode-detector no longer exists
Jan 19 21:04:42.583: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
Command stdout:

stderr:
+ curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
command terminated with exit code 7

error:
exit status 7
STEP: creating service affinity-nodeport-timeout in namespace services-5258 01/19/23 21:04:42.583
STEP: creating replication controller affinity-nodeport-timeout in namespace services-5258 01/19/23 21:04:42.595
I0119 21:04:42.603539      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5258, replica count: 3
I0119 21:04:45.654616      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 21:04:45.663: INFO: Creating new exec pod
Jan 19 21:04:45.674: INFO: Waiting up to 5m0s for pod "execpod-affinity89vv2" in namespace "services-5258" to be "running"
Jan 19 21:04:45.677: INFO: Pod "execpod-affinity89vv2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.390429ms
Jan 19 21:04:47.680: INFO: Pod "execpod-affinity89vv2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005301456s
Jan 19 21:04:47.680: INFO: Pod "execpod-affinity89vv2" satisfied condition "running"
Jan 19 21:04:48.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
Jan 19 21:04:49.875: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
Jan 19 21:04:49.875: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:04:49.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.112.100 80'
Jan 19 21:04:50.010: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.112.100 80\nConnection to 172.30.112.100 80 port [tcp/http] succeeded!\n"
Jan 19 21:04:50.010: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:04:50.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.188.71 32634'
Jan 19 21:04:51.157: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.188.71 32634\nConnection to 10.0.188.71 32634 port [tcp/*] succeeded!\n"
Jan 19 21:04:51.157: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:04:51.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.207.77 32634'
Jan 19 21:04:52.309: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.207.77 32634\nConnection to 10.0.207.77 32634 port [tcp/*] succeeded!\n"
Jan 19 21:04:52.309: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:04:52.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.146.42:32634/ ; done'
Jan 19 21:04:53.519: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n"
Jan 19 21:04:53.519: INFO: stdout: "\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg"
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
Jan 19 21:04:53.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.146.42:32634/'
Jan 19 21:04:53.651: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n"
Jan 19 21:04:53.651: INFO: stdout: "affinity-nodeport-timeout-52vjg"
Jan 19 21:05:13.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.146.42:32634/'
Jan 19 21:05:14.836: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n"
Jan 19 21:05:14.836: INFO: stdout: "affinity-nodeport-timeout-52vjg"
Jan 19 21:05:34.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.146.42:32634/'
Jan 19 21:05:36.021: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n"
Jan 19 21:05:36.021: INFO: stdout: "affinity-nodeport-timeout-hkgqc"
Jan 19 21:05:36.021: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5258, will wait for the garbage collector to delete the pods 01/19/23 21:05:36.03
Jan 19 21:05:36.092: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.757114ms
Jan 19 21:05:36.193: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.010449ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 21:05:38.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5258" for this suite. 01/19/23 21:05:38.73
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]","completed":62,"skipped":973,"failed":0}
------------------------------
• [SLOW TEST] [58.417 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:04:40.321
    Jan 19 21:04:40.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 21:04:40.321
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:04:40.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:04:40.395
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity timeout work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2221
    STEP: creating service in namespace services-5258 01/19/23 21:04:40.398
    Jan 19 21:04:40.412: INFO: Waiting up to 5m0s for pod "kube-proxy-mode-detector" in namespace "services-5258" to be "running and ready"
    Jan 19 21:04:40.415: INFO: Pod "kube-proxy-mode-detector": Phase="Pending", Reason="", readiness=false. Elapsed: 3.280117ms
    Jan 19 21:04:40.415: INFO: The phase of Pod kube-proxy-mode-detector is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:04:42.419: INFO: Pod "kube-proxy-mode-detector": Phase="Running", Reason="", readiness=true. Elapsed: 2.007181687s
    Jan 19 21:04:42.419: INFO: The phase of Pod kube-proxy-mode-detector is Running (Ready = true)
    Jan 19 21:04:42.419: INFO: Pod "kube-proxy-mode-detector" satisfied condition "running and ready"
    Jan 19 21:04:42.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode'
    Jan 19 21:04:42.572: INFO: rc: 7
    Jan 19 21:04:42.581: INFO: Waiting for pod kube-proxy-mode-detector to disappear
    Jan 19 21:04:42.583: INFO: Pod kube-proxy-mode-detector no longer exists
    Jan 19 21:04:42.583: INFO: Couldn't detect KubeProxy mode - test failure may be expected: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec kube-proxy-mode-detector -- /bin/sh -x -c curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode:
    Command stdout:

    stderr:
    + curl -q -s --connect-timeout 1 http://localhost:10249/proxyMode
    command terminated with exit code 7

    error:
    exit status 7
    STEP: creating service affinity-nodeport-timeout in namespace services-5258 01/19/23 21:04:42.583
    STEP: creating replication controller affinity-nodeport-timeout in namespace services-5258 01/19/23 21:04:42.595
    I0119 21:04:42.603539      22 runners.go:193] Created replication controller with name: affinity-nodeport-timeout, namespace: services-5258, replica count: 3
    I0119 21:04:45.654616      22 runners.go:193] affinity-nodeport-timeout Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 21:04:45.663: INFO: Creating new exec pod
    Jan 19 21:04:45.674: INFO: Waiting up to 5m0s for pod "execpod-affinity89vv2" in namespace "services-5258" to be "running"
    Jan 19 21:04:45.677: INFO: Pod "execpod-affinity89vv2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.390429ms
    Jan 19 21:04:47.680: INFO: Pod "execpod-affinity89vv2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005301456s
    Jan 19 21:04:47.680: INFO: Pod "execpod-affinity89vv2" satisfied condition "running"
    Jan 19 21:04:48.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-timeout 80'
    Jan 19 21:04:49.875: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-timeout 80\nConnection to affinity-nodeport-timeout 80 port [tcp/http] succeeded!\n"
    Jan 19 21:04:49.875: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:04:49.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.112.100 80'
    Jan 19 21:04:50.010: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.112.100 80\nConnection to 172.30.112.100 80 port [tcp/http] succeeded!\n"
    Jan 19 21:04:50.010: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:04:50.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.188.71 32634'
    Jan 19 21:04:51.157: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.188.71 32634\nConnection to 10.0.188.71 32634 port [tcp/*] succeeded!\n"
    Jan 19 21:04:51.157: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:04:51.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.207.77 32634'
    Jan 19 21:04:52.309: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.207.77 32634\nConnection to 10.0.207.77 32634 port [tcp/*] succeeded!\n"
    Jan 19 21:04:52.309: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:04:52.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.146.42:32634/ ; done'
    Jan 19 21:04:53.519: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n"
    Jan 19 21:04:53.519: INFO: stdout: "\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg\naffinity-nodeport-timeout-52vjg"
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Received response from host: affinity-nodeport-timeout-52vjg
    Jan 19 21:04:53.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.146.42:32634/'
    Jan 19 21:04:53.651: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n"
    Jan 19 21:04:53.651: INFO: stdout: "affinity-nodeport-timeout-52vjg"
    Jan 19 21:05:13.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.146.42:32634/'
    Jan 19 21:05:14.836: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n"
    Jan 19 21:05:14.836: INFO: stdout: "affinity-nodeport-timeout-52vjg"
    Jan 19 21:05:34.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5258 exec execpod-affinity89vv2 -- /bin/sh -x -c curl -q -s --connect-timeout 2 http://10.0.146.42:32634/'
    Jan 19 21:05:36.021: INFO: stderr: "+ curl -q -s --connect-timeout 2 http://10.0.146.42:32634/\n"
    Jan 19 21:05:36.021: INFO: stdout: "affinity-nodeport-timeout-hkgqc"
    Jan 19 21:05:36.021: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-timeout in namespace services-5258, will wait for the garbage collector to delete the pods 01/19/23 21:05:36.03
    Jan 19 21:05:36.092: INFO: Deleting ReplicationController affinity-nodeport-timeout took: 6.757114ms
    Jan 19 21:05:36.193: INFO: Terminating ReplicationController affinity-nodeport-timeout pods took: 101.010449ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 21:05:38.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5258" for this suite. 01/19/23 21:05:38.73
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:05:38.738
Jan 19 21:05:38.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:05:38.739
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:05:38.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:05:38.772
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:05:38.78
Jan 19 21:05:38.835: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388" in namespace "projected-9185" to be "Succeeded or Failed"
Jan 19 21:05:38.842: INFO: Pod "downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388": Phase="Pending", Reason="", readiness=false. Elapsed: 6.545617ms
Jan 19 21:05:40.845: INFO: Pod "downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00957497s
Jan 19 21:05:42.846: INFO: Pod "downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011458572s
STEP: Saw pod success 01/19/23 21:05:42.846
Jan 19 21:05:42.847: INFO: Pod "downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388" satisfied condition "Succeeded or Failed"
Jan 19 21:05:42.849: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388 container client-container: <nil>
STEP: delete the pod 01/19/23 21:05:42.857
Jan 19 21:05:42.866: INFO: Waiting for pod downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388 to disappear
Jan 19 21:05:42.868: INFO: Pod downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 21:05:42.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9185" for this suite. 01/19/23 21:05:42.872
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":63,"skipped":1007,"failed":0}
------------------------------
• [4.138 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:05:38.738
    Jan 19 21:05:38.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:05:38.739
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:05:38.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:05:38.772
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:05:38.78
    Jan 19 21:05:38.835: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388" in namespace "projected-9185" to be "Succeeded or Failed"
    Jan 19 21:05:38.842: INFO: Pod "downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388": Phase="Pending", Reason="", readiness=false. Elapsed: 6.545617ms
    Jan 19 21:05:40.845: INFO: Pod "downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00957497s
    Jan 19 21:05:42.846: INFO: Pod "downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011458572s
    STEP: Saw pod success 01/19/23 21:05:42.846
    Jan 19 21:05:42.847: INFO: Pod "downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388" satisfied condition "Succeeded or Failed"
    Jan 19 21:05:42.849: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388 container client-container: <nil>
    STEP: delete the pod 01/19/23 21:05:42.857
    Jan 19 21:05:42.866: INFO: Waiting for pod downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388 to disappear
    Jan 19 21:05:42.868: INFO: Pod downwardapi-volume-fd462976-554c-4c13-937f-becad55bc388 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 21:05:42.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9185" for this suite. 01/19/23 21:05:42.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:05:42.877
Jan 19 21:05:42.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 21:05:42.878
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:05:42.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:05:42.902
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-2f79e15a-e3d5-4760-895b-5faacdee481a 01/19/23 21:05:42.905
STEP: Creating a pod to test consume secrets 01/19/23 21:05:42.921
Jan 19 21:05:42.974: INFO: Waiting up to 5m0s for pod "pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2" in namespace "secrets-7633" to be "Succeeded or Failed"
Jan 19 21:05:42.982: INFO: Pod "pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.924543ms
Jan 19 21:05:44.986: INFO: Pod "pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011773043s
Jan 19 21:05:46.985: INFO: Pod "pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011361166s
STEP: Saw pod success 01/19/23 21:05:46.985
Jan 19 21:05:46.986: INFO: Pod "pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2" satisfied condition "Succeeded or Failed"
Jan 19 21:05:46.988: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 21:05:46.993
Jan 19 21:05:47.004: INFO: Waiting for pod pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2 to disappear
Jan 19 21:05:47.006: INFO: Pod pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 21:05:47.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7633" for this suite. 01/19/23 21:05:47.011
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":64,"skipped":1012,"failed":0}
------------------------------
• [4.142 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:05:42.877
    Jan 19 21:05:42.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 21:05:42.878
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:05:42.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:05:42.902
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-2f79e15a-e3d5-4760-895b-5faacdee481a 01/19/23 21:05:42.905
    STEP: Creating a pod to test consume secrets 01/19/23 21:05:42.921
    Jan 19 21:05:42.974: INFO: Waiting up to 5m0s for pod "pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2" in namespace "secrets-7633" to be "Succeeded or Failed"
    Jan 19 21:05:42.982: INFO: Pod "pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2": Phase="Pending", Reason="", readiness=false. Elapsed: 7.924543ms
    Jan 19 21:05:44.986: INFO: Pod "pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011773043s
    Jan 19 21:05:46.985: INFO: Pod "pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011361166s
    STEP: Saw pod success 01/19/23 21:05:46.985
    Jan 19 21:05:46.986: INFO: Pod "pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2" satisfied condition "Succeeded or Failed"
    Jan 19 21:05:46.988: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:05:46.993
    Jan 19 21:05:47.004: INFO: Waiting for pod pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2 to disappear
    Jan 19 21:05:47.006: INFO: Pod pod-secrets-ffa24a96-9112-43ba-8fd0-e0a3d9e922a2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 21:05:47.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7633" for this suite. 01/19/23 21:05:47.011
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:05:47.02
Jan 19 21:05:47.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename init-container 01/19/23 21:05:47.021
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:05:47.036
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:05:47.039
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 01/19/23 21:05:47.05
Jan 19 21:05:47.050: INFO: PodSpec: initContainers in spec.initContainers
Jan 19 21:06:35.075: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-08f962b1-272b-4c86-b53e-ad0bb3e41d13", GenerateName:"", Namespace:"init-container-1675", SelfLink:"", UID:"2ba6731d-82f3-4282-9457-0688337e2e92", ResourceVersion:"160060", Generation:0, CreationTimestamp:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"50374164"}, Annotations:map[string]string{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.8.67/23\"],\"mac_address\":\"0a:58:0a:80:08:43\",\"gateway_ips\":[\"10.128.8.1\"],\"ip_address\":\"10.128.8.67/23\",\"gateway_ip\":\"10.128.8.1\"}}", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.8.67\"\n    ],\n    \"mac\": \"0a:58:0a:80:08:43\",\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.8.67\"\n    ],\n    \"mac\": \"0a:58:0a:80:08:43\",\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f1dd28), Subresource:""}, v1.ManagedFieldsEntry{Manager:"ip-10-0-194-246", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f1dd58), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f1dd88), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 19, 21, 6, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f1ddb8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-ww6f2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc001e79300), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ww6f2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0037ee960), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ww6f2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0037eea80), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ww6f2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0037ee900), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000c39c28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-172-44.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000b7ea10), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c39d30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c39d60)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000c39d8c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000c39d90), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e83850), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.172.44", PodIP:"10.128.8.67", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.128.8.67"}}, StartTime:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000b7eb60)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000b7ebd0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://3b29e69832709db779f254d8990e55722cea7dc3214c286be41fef8f94d1f557", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001e79380), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001e79360), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc000c39e0f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 21:06:35.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1675" for this suite. 01/19/23 21:06:35.081
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":65,"skipped":1043,"failed":0}
------------------------------
• [SLOW TEST] [48.071 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:05:47.02
    Jan 19 21:05:47.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename init-container 01/19/23 21:05:47.021
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:05:47.036
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:05:47.039
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 01/19/23 21:05:47.05
    Jan 19 21:05:47.050: INFO: PodSpec: initContainers in spec.initContainers
    Jan 19 21:06:35.075: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-08f962b1-272b-4c86-b53e-ad0bb3e41d13", GenerateName:"", Namespace:"init-container-1675", SelfLink:"", UID:"2ba6731d-82f3-4282-9457-0688337e2e92", ResourceVersion:"160060", Generation:0, CreationTimestamp:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"50374164"}, Annotations:map[string]string{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.8.67/23\"],\"mac_address\":\"0a:58:0a:80:08:43\",\"gateway_ips\":[\"10.128.8.1\"],\"ip_address\":\"10.128.8.67/23\",\"gateway_ip\":\"10.128.8.1\"}}", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.8.67\"\n    ],\n    \"mac\": \"0a:58:0a:80:08:43\",\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.8.67\"\n    ],\n    \"mac\": \"0a:58:0a:80:08:43\",\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f1dd28), Subresource:""}, v1.ManagedFieldsEntry{Manager:"ip-10-0-194-246", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f1dd58), Subresource:""}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f1dd88), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 19, 21, 6, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004f1ddb8), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-ww6f2", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc001e79300), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ww6f2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0037ee960), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ww6f2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0037eea80), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ww6f2", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0037ee900), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000c39c28), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-172-44.ec2.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000b7ea10), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c39d30)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c39d60)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000c39d8c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000c39d90), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000e83850), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.172.44", PodIP:"10.128.8.67", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.128.8.67"}}, StartTime:time.Date(2023, time.January, 19, 21, 5, 47, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000b7eb60)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000b7ebd0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://3b29e69832709db779f254d8990e55722cea7dc3214c286be41fef8f94d1f557", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001e79380), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc001e79360), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc000c39e0f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 21:06:35.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-1675" for this suite. 01/19/23 21:06:35.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:06:35.092
Jan 19 21:06:35.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 21:06:35.093
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:06:35.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:06:35.125
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/19/23 21:06:35.149
Jan 19 21:06:35.195: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3438" to be "running and ready"
Jan 19 21:06:35.207: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.341382ms
Jan 19 21:06:35.207: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:06:37.210: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015296489s
Jan 19 21:06:37.210: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 19 21:06:37.210: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 01/19/23 21:06:37.212
Jan 19 21:06:37.223: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-3438" to be "running and ready"
Jan 19 21:06:37.226: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464767ms
Jan 19 21:06:37.226: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:06:39.229: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005944658s
Jan 19 21:06:39.229: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan 19 21:06:39.229: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/19/23 21:06:39.231
Jan 19 21:06:39.238: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 19 21:06:39.240: INFO: Pod pod-with-prestop-http-hook still exists
Jan 19 21:06:41.241: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 19 21:06:41.247: INFO: Pod pod-with-prestop-http-hook still exists
Jan 19 21:06:43.242: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan 19 21:06:43.244: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/19/23 21:06:43.244
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 19 21:06:43.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3438" for this suite. 01/19/23 21:06:43.259
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":66,"skipped":1070,"failed":0}
------------------------------
• [SLOW TEST] [8.171 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:06:35.092
    Jan 19 21:06:35.092: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 21:06:35.093
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:06:35.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:06:35.125
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/19/23 21:06:35.149
    Jan 19 21:06:35.195: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3438" to be "running and ready"
    Jan 19 21:06:35.207: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.341382ms
    Jan 19 21:06:35.207: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:06:37.210: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.015296489s
    Jan 19 21:06:37.210: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 19 21:06:37.210: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 01/19/23 21:06:37.212
    Jan 19 21:06:37.223: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-3438" to be "running and ready"
    Jan 19 21:06:37.226: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.464767ms
    Jan 19 21:06:37.226: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:06:39.229: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.005944658s
    Jan 19 21:06:39.229: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan 19 21:06:39.229: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/19/23 21:06:39.231
    Jan 19 21:06:39.238: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 19 21:06:39.240: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 19 21:06:41.241: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 19 21:06:41.247: INFO: Pod pod-with-prestop-http-hook still exists
    Jan 19 21:06:43.242: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan 19 21:06:43.244: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/19/23 21:06:43.244
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 19 21:06:43.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-3438" for this suite. 01/19/23 21:06:43.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:06:43.264
Jan 19 21:06:43.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:06:43.265
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:06:43.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:06:43.29
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:06:43.292
Jan 19 21:06:43.330: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1" in namespace "projected-1925" to be "Succeeded or Failed"
Jan 19 21:06:43.344: INFO: Pod "downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.84092ms
Jan 19 21:06:45.348: INFO: Pod "downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1": Phase="Running", Reason="", readiness=false. Elapsed: 2.018430003s
Jan 19 21:06:47.347: INFO: Pod "downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017416704s
STEP: Saw pod success 01/19/23 21:06:47.347
Jan 19 21:06:47.347: INFO: Pod "downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1" satisfied condition "Succeeded or Failed"
Jan 19 21:06:47.350: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1 container client-container: <nil>
STEP: delete the pod 01/19/23 21:06:47.355
Jan 19 21:06:47.366: INFO: Waiting for pod downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1 to disappear
Jan 19 21:06:47.382: INFO: Pod downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 21:06:47.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1925" for this suite. 01/19/23 21:06:47.387
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":67,"skipped":1082,"failed":0}
------------------------------
• [4.132 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:06:43.264
    Jan 19 21:06:43.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:06:43.265
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:06:43.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:06:43.29
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:06:43.292
    Jan 19 21:06:43.330: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1" in namespace "projected-1925" to be "Succeeded or Failed"
    Jan 19 21:06:43.344: INFO: Pod "downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1": Phase="Pending", Reason="", readiness=false. Elapsed: 13.84092ms
    Jan 19 21:06:45.348: INFO: Pod "downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1": Phase="Running", Reason="", readiness=false. Elapsed: 2.018430003s
    Jan 19 21:06:47.347: INFO: Pod "downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017416704s
    STEP: Saw pod success 01/19/23 21:06:47.347
    Jan 19 21:06:47.347: INFO: Pod "downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1" satisfied condition "Succeeded or Failed"
    Jan 19 21:06:47.350: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1 container client-container: <nil>
    STEP: delete the pod 01/19/23 21:06:47.355
    Jan 19 21:06:47.366: INFO: Waiting for pod downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1 to disappear
    Jan 19 21:06:47.382: INFO: Pod downwardapi-volume-ed24bdfc-fcba-4e27-80c7-1d6c67c62fa1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 21:06:47.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1925" for this suite. 01/19/23 21:06:47.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:06:47.396
Jan 19 21:06:47.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 21:06:47.397
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:06:47.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:06:47.431
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-1520 01/19/23 21:06:47.433
STEP: changing the ExternalName service to type=NodePort 01/19/23 21:06:47.445
STEP: creating replication controller externalname-service in namespace services-1520 01/19/23 21:06:47.531
I0119 21:06:47.549705      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1520, replica count: 2
I0119 21:06:50.601029      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 21:06:50.601: INFO: Creating new exec pod
Jan 19 21:06:50.613: INFO: Waiting up to 5m0s for pod "execpod6drk5" in namespace "services-1520" to be "running"
Jan 19 21:06:50.616: INFO: Pod "execpod6drk5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.827745ms
Jan 19 21:06:52.619: INFO: Pod "execpod6drk5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006142742s
Jan 19 21:06:52.619: INFO: Pod "execpod6drk5" satisfied condition "running"
Jan 19 21:06:53.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1520 exec execpod6drk5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 19 21:06:53.754: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 19 21:06:53.754: INFO: stdout: ""
Jan 19 21:06:54.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1520 exec execpod6drk5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 19 21:06:54.884: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 19 21:06:54.884: INFO: stdout: "externalname-service-fxrpc"
Jan 19 21:06:54.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1520 exec execpod6drk5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.236.40 80'
Jan 19 21:06:54.992: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.236.40 80\nConnection to 172.30.236.40 80 port [tcp/http] succeeded!\n"
Jan 19 21:06:54.992: INFO: stdout: "externalname-service-5z84r"
Jan 19 21:06:54.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1520 exec execpod6drk5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.172.44 30492'
Jan 19 21:06:55.122: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.172.44 30492\nConnection to 10.0.172.44 30492 port [tcp/*] succeeded!\n"
Jan 19 21:06:55.122: INFO: stdout: "externalname-service-fxrpc"
Jan 19 21:06:55.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1520 exec execpod6drk5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.146.42 30492'
Jan 19 21:06:55.238: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.146.42 30492\nConnection to 10.0.146.42 30492 port [tcp/*] succeeded!\n"
Jan 19 21:06:55.238: INFO: stdout: "externalname-service-fxrpc"
Jan 19 21:06:55.238: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 21:06:55.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1520" for this suite. 01/19/23 21:06:55.273
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":68,"skipped":1099,"failed":0}
------------------------------
• [SLOW TEST] [7.882 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:06:47.396
    Jan 19 21:06:47.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 21:06:47.397
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:06:47.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:06:47.431
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-1520 01/19/23 21:06:47.433
    STEP: changing the ExternalName service to type=NodePort 01/19/23 21:06:47.445
    STEP: creating replication controller externalname-service in namespace services-1520 01/19/23 21:06:47.531
    I0119 21:06:47.549705      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-1520, replica count: 2
    I0119 21:06:50.601029      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 21:06:50.601: INFO: Creating new exec pod
    Jan 19 21:06:50.613: INFO: Waiting up to 5m0s for pod "execpod6drk5" in namespace "services-1520" to be "running"
    Jan 19 21:06:50.616: INFO: Pod "execpod6drk5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.827745ms
    Jan 19 21:06:52.619: INFO: Pod "execpod6drk5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006142742s
    Jan 19 21:06:52.619: INFO: Pod "execpod6drk5" satisfied condition "running"
    Jan 19 21:06:53.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1520 exec execpod6drk5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 19 21:06:53.754: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 19 21:06:53.754: INFO: stdout: ""
    Jan 19 21:06:54.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1520 exec execpod6drk5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 19 21:06:54.884: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 19 21:06:54.884: INFO: stdout: "externalname-service-fxrpc"
    Jan 19 21:06:54.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1520 exec execpod6drk5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.236.40 80'
    Jan 19 21:06:54.992: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.236.40 80\nConnection to 172.30.236.40 80 port [tcp/http] succeeded!\n"
    Jan 19 21:06:54.992: INFO: stdout: "externalname-service-5z84r"
    Jan 19 21:06:54.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1520 exec execpod6drk5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.172.44 30492'
    Jan 19 21:06:55.122: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.172.44 30492\nConnection to 10.0.172.44 30492 port [tcp/*] succeeded!\n"
    Jan 19 21:06:55.122: INFO: stdout: "externalname-service-fxrpc"
    Jan 19 21:06:55.122: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1520 exec execpod6drk5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.146.42 30492'
    Jan 19 21:06:55.238: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.146.42 30492\nConnection to 10.0.146.42 30492 port [tcp/*] succeeded!\n"
    Jan 19 21:06:55.238: INFO: stdout: "externalname-service-fxrpc"
    Jan 19 21:06:55.238: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 21:06:55.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1520" for this suite. 01/19/23 21:06:55.273
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:06:55.279
Jan 19 21:06:55.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 21:06:55.279
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:06:55.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:06:55.321
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 21:06:55.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4481" for this suite. 01/19/23 21:06:55.4
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":69,"skipped":1121,"failed":0}
------------------------------
• [0.133 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:06:55.279
    Jan 19 21:06:55.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 21:06:55.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:06:55.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:06:55.321
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 21:06:55.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4481" for this suite. 01/19/23 21:06:55.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:06:55.414
Jan 19 21:06:55.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:06:55.415
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:06:55.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:06:55.442
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-261593d0-f89b-4dea-ab13-bf6af4717887 01/19/23 21:06:55.45
STEP: Creating a pod to test consume secrets 01/19/23 21:06:55.463
Jan 19 21:06:55.503: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f" in namespace "projected-6623" to be "Succeeded or Failed"
Jan 19 21:06:55.532: INFO: Pod "pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.525948ms
Jan 19 21:06:57.535: INFO: Pod "pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031405735s
Jan 19 21:06:59.536: INFO: Pod "pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033073511s
STEP: Saw pod success 01/19/23 21:06:59.536
Jan 19 21:06:59.536: INFO: Pod "pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f" satisfied condition "Succeeded or Failed"
Jan 19 21:06:59.539: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f container projected-secret-volume-test: <nil>
STEP: delete the pod 01/19/23 21:06:59.549
Jan 19 21:06:59.558: INFO: Waiting for pod pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f to disappear
Jan 19 21:06:59.560: INFO: Pod pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 21:06:59.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6623" for this suite. 01/19/23 21:06:59.565
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":70,"skipped":1175,"failed":0}
------------------------------
• [4.158 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:06:55.414
    Jan 19 21:06:55.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:06:55.415
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:06:55.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:06:55.442
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-261593d0-f89b-4dea-ab13-bf6af4717887 01/19/23 21:06:55.45
    STEP: Creating a pod to test consume secrets 01/19/23 21:06:55.463
    Jan 19 21:06:55.503: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f" in namespace "projected-6623" to be "Succeeded or Failed"
    Jan 19 21:06:55.532: INFO: Pod "pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f": Phase="Pending", Reason="", readiness=false. Elapsed: 28.525948ms
    Jan 19 21:06:57.535: INFO: Pod "pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031405735s
    Jan 19 21:06:59.536: INFO: Pod "pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033073511s
    STEP: Saw pod success 01/19/23 21:06:59.536
    Jan 19 21:06:59.536: INFO: Pod "pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f" satisfied condition "Succeeded or Failed"
    Jan 19 21:06:59.539: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:06:59.549
    Jan 19 21:06:59.558: INFO: Waiting for pod pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f to disappear
    Jan 19 21:06:59.560: INFO: Pod pod-projected-secrets-06a25d80-3ddd-44ba-800a-a2b7ae8ee35f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 21:06:59.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6623" for this suite. 01/19/23 21:06:59.565
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:06:59.573
Jan 19 21:06:59.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:06:59.573
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:06:59.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:06:59.595
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:06:59.598
Jan 19 21:06:59.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db" in namespace "projected-955" to be "Succeeded or Failed"
Jan 19 21:06:59.653: INFO: Pod "downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005418ms
Jan 19 21:07:01.655: INFO: Pod "downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010352741s
Jan 19 21:07:03.655: INFO: Pod "downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010583971s
STEP: Saw pod success 01/19/23 21:07:03.655
Jan 19 21:07:03.655: INFO: Pod "downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db" satisfied condition "Succeeded or Failed"
Jan 19 21:07:03.657: INFO: Trying to get logs from node ip-10-0-146-42.ec2.internal pod downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db container client-container: <nil>
STEP: delete the pod 01/19/23 21:07:03.668
Jan 19 21:07:03.677: INFO: Waiting for pod downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db to disappear
Jan 19 21:07:03.680: INFO: Pod downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 21:07:03.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-955" for this suite. 01/19/23 21:07:03.684
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":71,"skipped":1177,"failed":0}
------------------------------
• [4.117 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:06:59.573
    Jan 19 21:06:59.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:06:59.573
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:06:59.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:06:59.595
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:06:59.598
    Jan 19 21:06:59.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db" in namespace "projected-955" to be "Succeeded or Failed"
    Jan 19 21:06:59.653: INFO: Pod "downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005418ms
    Jan 19 21:07:01.655: INFO: Pod "downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010352741s
    Jan 19 21:07:03.655: INFO: Pod "downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010583971s
    STEP: Saw pod success 01/19/23 21:07:03.655
    Jan 19 21:07:03.655: INFO: Pod "downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db" satisfied condition "Succeeded or Failed"
    Jan 19 21:07:03.657: INFO: Trying to get logs from node ip-10-0-146-42.ec2.internal pod downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db container client-container: <nil>
    STEP: delete the pod 01/19/23 21:07:03.668
    Jan 19 21:07:03.677: INFO: Waiting for pod downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db to disappear
    Jan 19 21:07:03.680: INFO: Pod downwardapi-volume-1d337224-dc9b-4780-b855-62cb809e55db no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 21:07:03.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-955" for this suite. 01/19/23 21:07:03.684
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:07:03.69
Jan 19 21:07:03.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename replicaset 01/19/23 21:07:03.69
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:03.716
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:03.72
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan 19 21:07:03.724: INFO: Creating ReplicaSet my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e
W0119 21:07:03.739253      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:07:03.741: INFO: Pod name my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e: Found 0 pods out of 1
Jan 19 21:07:08.748: INFO: Pod name my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e: Found 1 pods out of 1
Jan 19 21:07:08.748: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e" is running
Jan 19 21:07:08.748: INFO: Waiting up to 5m0s for pod "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e-q78ps" in namespace "replicaset-3843" to be "running"
Jan 19 21:07:08.756: INFO: Pod "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e-q78ps": Phase="Running", Reason="", readiness=true. Elapsed: 8.575115ms
Jan 19 21:07:08.756: INFO: Pod "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e-q78ps" satisfied condition "running"
Jan 19 21:07:08.756: INFO: Pod "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e-q78ps" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:07:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:07:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:07:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:07:03 +0000 UTC Reason: Message:}])
Jan 19 21:07:08.757: INFO: Trying to dial the pod
Jan 19 21:07:13.768: INFO: Controller my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e: Got expected result from replica 1 [my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e-q78ps]: "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e-q78ps", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 19 21:07:13.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3843" for this suite. 01/19/23 21:07:13.772
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":72,"skipped":1178,"failed":0}
------------------------------
• [SLOW TEST] [10.088 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:07:03.69
    Jan 19 21:07:03.690: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename replicaset 01/19/23 21:07:03.69
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:03.716
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:03.72
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan 19 21:07:03.724: INFO: Creating ReplicaSet my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e
    W0119 21:07:03.739253      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:07:03.741: INFO: Pod name my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e: Found 0 pods out of 1
    Jan 19 21:07:08.748: INFO: Pod name my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e: Found 1 pods out of 1
    Jan 19 21:07:08.748: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e" is running
    Jan 19 21:07:08.748: INFO: Waiting up to 5m0s for pod "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e-q78ps" in namespace "replicaset-3843" to be "running"
    Jan 19 21:07:08.756: INFO: Pod "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e-q78ps": Phase="Running", Reason="", readiness=true. Elapsed: 8.575115ms
    Jan 19 21:07:08.756: INFO: Pod "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e-q78ps" satisfied condition "running"
    Jan 19 21:07:08.756: INFO: Pod "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e-q78ps" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:07:03 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:07:05 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:07:05 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:07:03 +0000 UTC Reason: Message:}])
    Jan 19 21:07:08.757: INFO: Trying to dial the pod
    Jan 19 21:07:13.768: INFO: Controller my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e: Got expected result from replica 1 [my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e-q78ps]: "my-hostname-basic-90a82261-18e4-4629-8694-e15f527d881e-q78ps", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 19 21:07:13.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3843" for this suite. 01/19/23 21:07:13.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:07:13.778
Jan 19 21:07:13.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename server-version 01/19/23 21:07:13.779
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:13.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:13.81
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/19/23 21:07:13.814
STEP: Confirm major version 01/19/23 21:07:13.816
Jan 19 21:07:13.816: INFO: Major version: 1
STEP: Confirm minor version 01/19/23 21:07:13.816
Jan 19 21:07:13.817: INFO: cleanMinorVersion: 25
Jan 19 21:07:13.817: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jan 19 21:07:13.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-6399" for this suite. 01/19/23 21:07:13.839
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":73,"skipped":1194,"failed":0}
------------------------------
• [0.093 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:07:13.778
    Jan 19 21:07:13.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename server-version 01/19/23 21:07:13.779
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:13.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:13.81
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/19/23 21:07:13.814
    STEP: Confirm major version 01/19/23 21:07:13.816
    Jan 19 21:07:13.816: INFO: Major version: 1
    STEP: Confirm minor version 01/19/23 21:07:13.816
    Jan 19 21:07:13.817: INFO: cleanMinorVersion: 25
    Jan 19 21:07:13.817: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jan 19 21:07:13.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-6399" for this suite. 01/19/23 21:07:13.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:07:13.871
Jan 19 21:07:13.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 21:07:13.872
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:13.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:13.918
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-8872 01/19/23 21:07:13.968
STEP: creating replication controller nodeport-test in namespace services-8872 01/19/23 21:07:14.11
I0119 21:07:14.155132      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8872, replica count: 2
I0119 21:07:17.206849      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 21:07:17.206: INFO: Creating new exec pod
Jan 19 21:07:17.218: INFO: Waiting up to 5m0s for pod "execpod2cnrd" in namespace "services-8872" to be "running"
Jan 19 21:07:17.221: INFO: Pod "execpod2cnrd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.726013ms
Jan 19 21:07:19.225: INFO: Pod "execpod2cnrd": Phase="Running", Reason="", readiness=true. Elapsed: 2.006887239s
Jan 19 21:07:19.225: INFO: Pod "execpod2cnrd" satisfied condition "running"
Jan 19 21:07:20.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-8872 exec execpod2cnrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jan 19 21:07:20.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan 19 21:07:20.361: INFO: stdout: "nodeport-test-r2dfn"
Jan 19 21:07:20.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-8872 exec execpod2cnrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.207.210 80'
Jan 19 21:07:20.481: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.207.210 80\nConnection to 172.30.207.210 80 port [tcp/http] succeeded!\n"
Jan 19 21:07:20.482: INFO: stdout: "nodeport-test-rxbwz"
Jan 19 21:07:20.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-8872 exec execpod2cnrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.207.77 31338'
Jan 19 21:07:20.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.207.77 31338\nConnection to 10.0.207.77 31338 port [tcp/*] succeeded!\n"
Jan 19 21:07:20.619: INFO: stdout: "nodeport-test-r2dfn"
Jan 19 21:07:20.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-8872 exec execpod2cnrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.171.213 31338'
Jan 19 21:07:20.735: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.171.213 31338\nConnection to 10.0.171.213 31338 port [tcp/*] succeeded!\n"
Jan 19 21:07:20.735: INFO: stdout: "nodeport-test-r2dfn"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 21:07:20.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8872" for this suite. 01/19/23 21:07:20.741
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":74,"skipped":1203,"failed":0}
------------------------------
• [SLOW TEST] [6.875 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:07:13.871
    Jan 19 21:07:13.871: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 21:07:13.872
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:13.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:13.918
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-8872 01/19/23 21:07:13.968
    STEP: creating replication controller nodeport-test in namespace services-8872 01/19/23 21:07:14.11
    I0119 21:07:14.155132      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8872, replica count: 2
    I0119 21:07:17.206849      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 21:07:17.206: INFO: Creating new exec pod
    Jan 19 21:07:17.218: INFO: Waiting up to 5m0s for pod "execpod2cnrd" in namespace "services-8872" to be "running"
    Jan 19 21:07:17.221: INFO: Pod "execpod2cnrd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.726013ms
    Jan 19 21:07:19.225: INFO: Pod "execpod2cnrd": Phase="Running", Reason="", readiness=true. Elapsed: 2.006887239s
    Jan 19 21:07:19.225: INFO: Pod "execpod2cnrd" satisfied condition "running"
    Jan 19 21:07:20.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-8872 exec execpod2cnrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jan 19 21:07:20.361: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan 19 21:07:20.361: INFO: stdout: "nodeport-test-r2dfn"
    Jan 19 21:07:20.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-8872 exec execpod2cnrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.207.210 80'
    Jan 19 21:07:20.481: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.207.210 80\nConnection to 172.30.207.210 80 port [tcp/http] succeeded!\n"
    Jan 19 21:07:20.482: INFO: stdout: "nodeport-test-rxbwz"
    Jan 19 21:07:20.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-8872 exec execpod2cnrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.207.77 31338'
    Jan 19 21:07:20.619: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.207.77 31338\nConnection to 10.0.207.77 31338 port [tcp/*] succeeded!\n"
    Jan 19 21:07:20.619: INFO: stdout: "nodeport-test-r2dfn"
    Jan 19 21:07:20.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-8872 exec execpod2cnrd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.171.213 31338'
    Jan 19 21:07:20.735: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.171.213 31338\nConnection to 10.0.171.213 31338 port [tcp/*] succeeded!\n"
    Jan 19 21:07:20.735: INFO: stdout: "nodeport-test-r2dfn"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 21:07:20.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8872" for this suite. 01/19/23 21:07:20.741
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:07:20.747
Jan 19 21:07:20.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename gc 01/19/23 21:07:20.747
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:20.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:20.809
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/19/23 21:07:20.812
STEP: Wait for the Deployment to create new ReplicaSet 01/19/23 21:07:20.821
STEP: delete the deployment 01/19/23 21:07:21.347
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/19/23 21:07:21.352
STEP: Gathering metrics 01/19/23 21:07:21.869
W0119 21:07:21.871332      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0119 21:07:21.871347      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 21:07:21.871: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 21:07:21.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4310" for this suite. 01/19/23 21:07:21.875
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":75,"skipped":1210,"failed":0}
------------------------------
• [1.134 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:07:20.747
    Jan 19 21:07:20.747: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename gc 01/19/23 21:07:20.747
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:20.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:20.809
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/19/23 21:07:20.812
    STEP: Wait for the Deployment to create new ReplicaSet 01/19/23 21:07:20.821
    STEP: delete the deployment 01/19/23 21:07:21.347
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/19/23 21:07:21.352
    STEP: Gathering metrics 01/19/23 21:07:21.869
    W0119 21:07:21.871332      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0119 21:07:21.871347      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 19 21:07:21.871: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 21:07:21.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4310" for this suite. 01/19/23 21:07:21.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:07:21.882
Jan 19 21:07:21.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:07:21.883
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:21.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:21.913
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-c88c5502-f129-4b91-a85f-ea1f9dcdd391 01/19/23 21:07:21.916
STEP: Creating a pod to test consume configMaps 01/19/23 21:07:21.931
W0119 21:07:21.982288      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:07:21.982: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5" in namespace "projected-5511" to be "Succeeded or Failed"
Jan 19 21:07:21.993: INFO: Pod "pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.811896ms
Jan 19 21:07:23.995: INFO: Pod "pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013397404s
Jan 19 21:07:25.998: INFO: Pod "pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015837834s
STEP: Saw pod success 01/19/23 21:07:25.998
Jan 19 21:07:25.998: INFO: Pod "pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5" satisfied condition "Succeeded or Failed"
Jan 19 21:07:26.011: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:07:26.022
Jan 19 21:07:26.050: INFO: Waiting for pod pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5 to disappear
Jan 19 21:07:26.052: INFO: Pod pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 21:07:26.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5511" for this suite. 01/19/23 21:07:26.057
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":76,"skipped":1238,"failed":0}
------------------------------
• [4.181 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:07:21.882
    Jan 19 21:07:21.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:07:21.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:21.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:21.913
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-c88c5502-f129-4b91-a85f-ea1f9dcdd391 01/19/23 21:07:21.916
    STEP: Creating a pod to test consume configMaps 01/19/23 21:07:21.931
    W0119 21:07:21.982288      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:07:21.982: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5" in namespace "projected-5511" to be "Succeeded or Failed"
    Jan 19 21:07:21.993: INFO: Pod "pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.811896ms
    Jan 19 21:07:23.995: INFO: Pod "pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013397404s
    Jan 19 21:07:25.998: INFO: Pod "pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015837834s
    STEP: Saw pod success 01/19/23 21:07:25.998
    Jan 19 21:07:25.998: INFO: Pod "pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5" satisfied condition "Succeeded or Failed"
    Jan 19 21:07:26.011: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:07:26.022
    Jan 19 21:07:26.050: INFO: Waiting for pod pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5 to disappear
    Jan 19 21:07:26.052: INFO: Pod pod-projected-configmaps-ae767c21-65f3-4bed-a10f-b53b8f3e01b5 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 21:07:26.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5511" for this suite. 01/19/23 21:07:26.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:07:26.064
Jan 19 21:07:26.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename dns 01/19/23 21:07:26.065
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:26.088
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:26.091
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/19/23 21:07:26.094
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9085.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9085.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9085.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9085.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 234.48.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.48.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.48.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.48.234_tcp@PTR;sleep 1; done
 01/19/23 21:07:26.147
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9085.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9085.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9085.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9085.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 234.48.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.48.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.48.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.48.234_tcp@PTR;sleep 1; done
 01/19/23 21:07:26.147
STEP: creating a pod to probe DNS 01/19/23 21:07:26.147
STEP: submitting the pod to kubernetes 01/19/23 21:07:26.147
Jan 19 21:07:26.206: INFO: Waiting up to 15m0s for pod "dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c" in namespace "dns-9085" to be "running"
Jan 19 21:07:26.213: INFO: Pod "dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.887854ms
Jan 19 21:07:28.230: INFO: Pod "dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c": Phase="Running", Reason="", readiness=true. Elapsed: 2.023428372s
Jan 19 21:07:28.230: INFO: Pod "dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c" satisfied condition "running"
STEP: retrieving the pod 01/19/23 21:07:28.23
STEP: looking for the results for each expected name from probers 01/19/23 21:07:28.232
Jan 19 21:07:28.239: INFO: Unable to read wheezy_udp@dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
Jan 19 21:07:28.243: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
Jan 19 21:07:28.247: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
Jan 19 21:07:28.250: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
Jan 19 21:07:28.264: INFO: Unable to read jessie_udp@dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
Jan 19 21:07:28.269: INFO: Unable to read jessie_tcp@dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
Jan 19 21:07:28.284: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
Jan 19 21:07:28.286: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
Jan 19 21:07:28.301: INFO: Lookups using dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c failed for: [wheezy_udp@dns-test-service.dns-9085.svc.cluster.local wheezy_tcp@dns-test-service.dns-9085.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local jessie_udp@dns-test-service.dns-9085.svc.cluster.local jessie_tcp@dns-test-service.dns-9085.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local]

Jan 19 21:07:33.358: INFO: DNS probes using dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c succeeded

STEP: deleting the pod 01/19/23 21:07:33.358
STEP: deleting the test service 01/19/23 21:07:33.391
STEP: deleting the test headless service 01/19/23 21:07:33.413
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 21:07:33.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9085" for this suite. 01/19/23 21:07:33.458
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":77,"skipped":1245,"failed":0}
------------------------------
• [SLOW TEST] [7.404 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:07:26.064
    Jan 19 21:07:26.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename dns 01/19/23 21:07:26.065
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:26.088
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:26.091
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/19/23 21:07:26.094
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9085.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9085.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9085.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9085.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 234.48.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.48.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.48.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.48.234_tcp@PTR;sleep 1; done
     01/19/23 21:07:26.147
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9085.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9085.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9085.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9085.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9085.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 234.48.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.48.234_udp@PTR;check="$$(dig +tcp +noall +answer +search 234.48.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.48.234_tcp@PTR;sleep 1; done
     01/19/23 21:07:26.147
    STEP: creating a pod to probe DNS 01/19/23 21:07:26.147
    STEP: submitting the pod to kubernetes 01/19/23 21:07:26.147
    Jan 19 21:07:26.206: INFO: Waiting up to 15m0s for pod "dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c" in namespace "dns-9085" to be "running"
    Jan 19 21:07:26.213: INFO: Pod "dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.887854ms
    Jan 19 21:07:28.230: INFO: Pod "dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c": Phase="Running", Reason="", readiness=true. Elapsed: 2.023428372s
    Jan 19 21:07:28.230: INFO: Pod "dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 21:07:28.23
    STEP: looking for the results for each expected name from probers 01/19/23 21:07:28.232
    Jan 19 21:07:28.239: INFO: Unable to read wheezy_udp@dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
    Jan 19 21:07:28.243: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
    Jan 19 21:07:28.247: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
    Jan 19 21:07:28.250: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
    Jan 19 21:07:28.264: INFO: Unable to read jessie_udp@dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
    Jan 19 21:07:28.269: INFO: Unable to read jessie_tcp@dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
    Jan 19 21:07:28.284: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
    Jan 19 21:07:28.286: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local from pod dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c: the server could not find the requested resource (get pods dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c)
    Jan 19 21:07:28.301: INFO: Lookups using dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c failed for: [wheezy_udp@dns-test-service.dns-9085.svc.cluster.local wheezy_tcp@dns-test-service.dns-9085.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local jessie_udp@dns-test-service.dns-9085.svc.cluster.local jessie_tcp@dns-test-service.dns-9085.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9085.svc.cluster.local]

    Jan 19 21:07:33.358: INFO: DNS probes using dns-9085/dns-test-ac348dc1-7bee-4a2a-8b6f-46a6adc2f43c succeeded

    STEP: deleting the pod 01/19/23 21:07:33.358
    STEP: deleting the test service 01/19/23 21:07:33.391
    STEP: deleting the test headless service 01/19/23 21:07:33.413
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 21:07:33.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9085" for this suite. 01/19/23 21:07:33.458
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:07:33.468
Jan 19 21:07:33.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename statefulset 01/19/23 21:07:33.469
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:33.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:33.547
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-4229 01/19/23 21:07:33.549
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 01/19/23 21:07:33.558
W0119 21:07:33.583912      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:07:33.590: INFO: Found 0 stateful pods, waiting for 3
Jan 19 21:07:43.593: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 21:07:43.593: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 21:07:43.593: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/19/23 21:07:43.6
Jan 19 21:07:43.629: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/19/23 21:07:43.629
STEP: Not applying an update when the partition is greater than the number of replicas 01/19/23 21:07:53.673
STEP: Performing a canary update 01/19/23 21:07:53.673
Jan 19 21:07:53.692: INFO: Updating stateful set ss2
Jan 19 21:07:53.696: INFO: Waiting for Pod statefulset-4229/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 01/19/23 21:08:03.702
Jan 19 21:08:03.733: INFO: Found 1 stateful pods, waiting for 3
Jan 19 21:08:13.738: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 21:08:13.738: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 21:08:13.738: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/19/23 21:08:13.743
Jan 19 21:08:13.761: INFO: Updating stateful set ss2
Jan 19 21:08:13.766: INFO: Waiting for Pod statefulset-4229/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jan 19 21:08:23.791: INFO: Updating stateful set ss2
Jan 19 21:08:23.796: INFO: Waiting for StatefulSet statefulset-4229/ss2 to complete update
Jan 19 21:08:23.796: INFO: Waiting for Pod statefulset-4229/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 21:08:33.802: INFO: Deleting all statefulset in ns statefulset-4229
Jan 19 21:08:33.805: INFO: Scaling statefulset ss2 to 0
Jan 19 21:08:43.820: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 21:08:43.822: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 21:08:43.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4229" for this suite. 01/19/23 21:08:43.837
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":78,"skipped":1251,"failed":0}
------------------------------
• [SLOW TEST] [70.375 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:07:33.468
    Jan 19 21:07:33.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename statefulset 01/19/23 21:07:33.469
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:07:33.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:07:33.547
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-4229 01/19/23 21:07:33.549
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 01/19/23 21:07:33.558
    W0119 21:07:33.583912      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:07:33.590: INFO: Found 0 stateful pods, waiting for 3
    Jan 19 21:07:43.593: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 21:07:43.593: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 21:07:43.593: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/19/23 21:07:43.6
    Jan 19 21:07:43.629: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/19/23 21:07:43.629
    STEP: Not applying an update when the partition is greater than the number of replicas 01/19/23 21:07:53.673
    STEP: Performing a canary update 01/19/23 21:07:53.673
    Jan 19 21:07:53.692: INFO: Updating stateful set ss2
    Jan 19 21:07:53.696: INFO: Waiting for Pod statefulset-4229/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 01/19/23 21:08:03.702
    Jan 19 21:08:03.733: INFO: Found 1 stateful pods, waiting for 3
    Jan 19 21:08:13.738: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 21:08:13.738: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 21:08:13.738: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/19/23 21:08:13.743
    Jan 19 21:08:13.761: INFO: Updating stateful set ss2
    Jan 19 21:08:13.766: INFO: Waiting for Pod statefulset-4229/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jan 19 21:08:23.791: INFO: Updating stateful set ss2
    Jan 19 21:08:23.796: INFO: Waiting for StatefulSet statefulset-4229/ss2 to complete update
    Jan 19 21:08:23.796: INFO: Waiting for Pod statefulset-4229/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 21:08:33.802: INFO: Deleting all statefulset in ns statefulset-4229
    Jan 19 21:08:33.805: INFO: Scaling statefulset ss2 to 0
    Jan 19 21:08:43.820: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 21:08:43.822: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 21:08:43.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-4229" for this suite. 01/19/23 21:08:43.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:08:43.844
Jan 19 21:08:43.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:08:43.845
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:08:43.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:08:43.886
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
Jan 19 21:08:43.904: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-84b11221-24e0-435c-aaf0-71b2fdf991db 01/19/23 21:08:43.904
STEP: Creating configMap with name cm-test-opt-upd-40763683-9722-4025-8143-c8cb32a768fa 01/19/23 21:08:43.911
STEP: Creating the pod 01/19/23 21:08:43.934
Jan 19 21:08:43.977: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d4ef7fb-8b8a-4252-a997-0831856cb022" in namespace "configmap-2644" to be "running and ready"
Jan 19 21:08:43.986: INFO: Pod "pod-configmaps-2d4ef7fb-8b8a-4252-a997-0831856cb022": Phase="Pending", Reason="", readiness=false. Elapsed: 8.617496ms
Jan 19 21:08:43.986: INFO: The phase of Pod pod-configmaps-2d4ef7fb-8b8a-4252-a997-0831856cb022 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:08:45.988: INFO: Pod "pod-configmaps-2d4ef7fb-8b8a-4252-a997-0831856cb022": Phase="Running", Reason="", readiness=true. Elapsed: 2.011446172s
Jan 19 21:08:45.988: INFO: The phase of Pod pod-configmaps-2d4ef7fb-8b8a-4252-a997-0831856cb022 is Running (Ready = true)
Jan 19 21:08:45.988: INFO: Pod "pod-configmaps-2d4ef7fb-8b8a-4252-a997-0831856cb022" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-84b11221-24e0-435c-aaf0-71b2fdf991db 01/19/23 21:08:46.004
STEP: Updating configmap cm-test-opt-upd-40763683-9722-4025-8143-c8cb32a768fa 01/19/23 21:08:46.011
STEP: Creating configMap with name cm-test-opt-create-d138aa42-3b75-4242-a952-d2b999c5b6ea 01/19/23 21:08:46.016
STEP: waiting to observe update in volume 01/19/23 21:08:46.022
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:08:48.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2644" for this suite. 01/19/23 21:08:48.043
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":79,"skipped":1260,"failed":0}
------------------------------
• [4.203 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:08:43.844
    Jan 19 21:08:43.844: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:08:43.845
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:08:43.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:08:43.886
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    Jan 19 21:08:43.904: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-84b11221-24e0-435c-aaf0-71b2fdf991db 01/19/23 21:08:43.904
    STEP: Creating configMap with name cm-test-opt-upd-40763683-9722-4025-8143-c8cb32a768fa 01/19/23 21:08:43.911
    STEP: Creating the pod 01/19/23 21:08:43.934
    Jan 19 21:08:43.977: INFO: Waiting up to 5m0s for pod "pod-configmaps-2d4ef7fb-8b8a-4252-a997-0831856cb022" in namespace "configmap-2644" to be "running and ready"
    Jan 19 21:08:43.986: INFO: Pod "pod-configmaps-2d4ef7fb-8b8a-4252-a997-0831856cb022": Phase="Pending", Reason="", readiness=false. Elapsed: 8.617496ms
    Jan 19 21:08:43.986: INFO: The phase of Pod pod-configmaps-2d4ef7fb-8b8a-4252-a997-0831856cb022 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:08:45.988: INFO: Pod "pod-configmaps-2d4ef7fb-8b8a-4252-a997-0831856cb022": Phase="Running", Reason="", readiness=true. Elapsed: 2.011446172s
    Jan 19 21:08:45.988: INFO: The phase of Pod pod-configmaps-2d4ef7fb-8b8a-4252-a997-0831856cb022 is Running (Ready = true)
    Jan 19 21:08:45.988: INFO: Pod "pod-configmaps-2d4ef7fb-8b8a-4252-a997-0831856cb022" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-84b11221-24e0-435c-aaf0-71b2fdf991db 01/19/23 21:08:46.004
    STEP: Updating configmap cm-test-opt-upd-40763683-9722-4025-8143-c8cb32a768fa 01/19/23 21:08:46.011
    STEP: Creating configMap with name cm-test-opt-create-d138aa42-3b75-4242-a952-d2b999c5b6ea 01/19/23 21:08:46.016
    STEP: waiting to observe update in volume 01/19/23 21:08:46.022
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:08:48.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2644" for this suite. 01/19/23 21:08:48.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:08:48.048
Jan 19 21:08:48.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename containers 01/19/23 21:08:48.049
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:08:48.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:08:48.083
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jan 19 21:08:48.247: INFO: Waiting up to 5m0s for pod "client-containers-d3ee2823-d6a2-4d54-bd4a-0dee38567798" in namespace "containers-3583" to be "running"
Jan 19 21:08:48.270: INFO: Pod "client-containers-d3ee2823-d6a2-4d54-bd4a-0dee38567798": Phase="Pending", Reason="", readiness=false. Elapsed: 22.712005ms
Jan 19 21:08:50.274: INFO: Pod "client-containers-d3ee2823-d6a2-4d54-bd4a-0dee38567798": Phase="Running", Reason="", readiness=true. Elapsed: 2.02699005s
Jan 19 21:08:50.274: INFO: Pod "client-containers-d3ee2823-d6a2-4d54-bd4a-0dee38567798" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 19 21:08:50.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3583" for this suite. 01/19/23 21:08:50.289
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":80,"skipped":1267,"failed":0}
------------------------------
• [2.254 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:08:48.048
    Jan 19 21:08:48.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename containers 01/19/23 21:08:48.049
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:08:48.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:08:48.083
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jan 19 21:08:48.247: INFO: Waiting up to 5m0s for pod "client-containers-d3ee2823-d6a2-4d54-bd4a-0dee38567798" in namespace "containers-3583" to be "running"
    Jan 19 21:08:48.270: INFO: Pod "client-containers-d3ee2823-d6a2-4d54-bd4a-0dee38567798": Phase="Pending", Reason="", readiness=false. Elapsed: 22.712005ms
    Jan 19 21:08:50.274: INFO: Pod "client-containers-d3ee2823-d6a2-4d54-bd4a-0dee38567798": Phase="Running", Reason="", readiness=true. Elapsed: 2.02699005s
    Jan 19 21:08:50.274: INFO: Pod "client-containers-d3ee2823-d6a2-4d54-bd4a-0dee38567798" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 19 21:08:50.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3583" for this suite. 01/19/23 21:08:50.289
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:08:50.302
Jan 19 21:08:50.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename resourcequota 01/19/23 21:08:50.303
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:08:50.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:08:50.336
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 01/19/23 21:08:50.339
STEP: Counting existing ResourceQuota 01/19/23 21:08:56.344
STEP: Creating a ResourceQuota 01/19/23 21:09:01.346
STEP: Ensuring resource quota status is calculated 01/19/23 21:09:01.35
STEP: Creating a Secret 01/19/23 21:09:03.354
STEP: Ensuring resource quota status captures secret creation 01/19/23 21:09:03.363
STEP: Deleting a secret 01/19/23 21:09:05.367
STEP: Ensuring resource quota status released usage 01/19/23 21:09:05.372
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 21:09:07.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6291" for this suite. 01/19/23 21:09:07.381
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":81,"skipped":1274,"failed":0}
------------------------------
• [SLOW TEST] [17.084 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:08:50.302
    Jan 19 21:08:50.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename resourcequota 01/19/23 21:08:50.303
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:08:50.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:08:50.336
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 01/19/23 21:08:50.339
    STEP: Counting existing ResourceQuota 01/19/23 21:08:56.344
    STEP: Creating a ResourceQuota 01/19/23 21:09:01.346
    STEP: Ensuring resource quota status is calculated 01/19/23 21:09:01.35
    STEP: Creating a Secret 01/19/23 21:09:03.354
    STEP: Ensuring resource quota status captures secret creation 01/19/23 21:09:03.363
    STEP: Deleting a secret 01/19/23 21:09:05.367
    STEP: Ensuring resource quota status released usage 01/19/23 21:09:05.372
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 21:09:07.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6291" for this suite. 01/19/23 21:09:07.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:09:07.386
Jan 19 21:09:07.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename aggregator 01/19/23 21:09:07.387
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:07.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:07.416
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan 19 21:09:07.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/19/23 21:09:07.422
Jan 19 21:09:07.694: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan 19 21:09:09.740: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 21:09:11.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 21:09:13.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 21:09:15.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 21:09:17.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 21:09:19.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 21:09:21.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 21:09:23.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 21:09:25.742: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 21:09:27.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 21:09:29.857: INFO: Waited 108.325499ms for the sample-apiserver to be ready to handle requests.
I0119 21:09:30.902571      22 request.go:682] Waited for 1.027237151s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/config.openshift.io/v1
STEP: Read Status for v1alpha1.wardle.example.com 01/19/23 21:09:32.112
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/19/23 21:09:32.156
STEP: List APIServices 01/19/23 21:09:32.208
Jan 19 21:09:32.262: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jan 19 21:09:33.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9819" for this suite. 01/19/23 21:09:33.111
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":82,"skipped":1290,"failed":0}
------------------------------
• [SLOW TEST] [25.772 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:09:07.386
    Jan 19 21:09:07.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename aggregator 01/19/23 21:09:07.387
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:07.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:07.416
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan 19 21:09:07.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/19/23 21:09:07.422
    Jan 19 21:09:07.694: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan 19 21:09:09.740: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 21:09:11.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 21:09:13.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 21:09:15.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 21:09:17.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 21:09:19.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 21:09:21.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 21:09:23.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 21:09:25.742: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 21:09:27.744: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 9, 7, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-b5b45d9d4\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 21:09:29.857: INFO: Waited 108.325499ms for the sample-apiserver to be ready to handle requests.
    I0119 21:09:30.902571      22 request.go:682] Waited for 1.027237151s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/config.openshift.io/v1
    STEP: Read Status for v1alpha1.wardle.example.com 01/19/23 21:09:32.112
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/19/23 21:09:32.156
    STEP: List APIServices 01/19/23 21:09:32.208
    Jan 19 21:09:32.262: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jan 19 21:09:33.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-9819" for this suite. 01/19/23 21:09:33.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:09:33.159
Jan 19 21:09:33.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:09:33.16
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:33.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:33.21
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 01/19/23 21:09:33.217
Jan 19 21:09:33.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-1638 create -f -'
Jan 19 21:09:35.592: INFO: stderr: ""
Jan 19 21:09:35.592: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/19/23 21:09:35.592
Jan 19 21:09:35.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-1638 diff -f -'
Jan 19 21:09:37.690: INFO: rc: 1
Jan 19 21:09:37.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-1638 delete -f -'
Jan 19 21:09:37.748: INFO: stderr: ""
Jan 19 21:09:37.749: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:09:37.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1638" for this suite. 01/19/23 21:09:37.754
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":83,"skipped":1302,"failed":0}
------------------------------
• [4.601 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:09:33.159
    Jan 19 21:09:33.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:09:33.16
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:33.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:33.21
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 01/19/23 21:09:33.217
    Jan 19 21:09:33.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-1638 create -f -'
    Jan 19 21:09:35.592: INFO: stderr: ""
    Jan 19 21:09:35.592: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/19/23 21:09:35.592
    Jan 19 21:09:35.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-1638 diff -f -'
    Jan 19 21:09:37.690: INFO: rc: 1
    Jan 19 21:09:37.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-1638 delete -f -'
    Jan 19 21:09:37.748: INFO: stderr: ""
    Jan 19 21:09:37.749: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:09:37.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1638" for this suite. 01/19/23 21:09:37.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:09:37.761
Jan 19 21:09:37.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:09:37.762
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:37.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:37.785
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-7148cae0-1900-43e6-a4b5-e160998e794a 01/19/23 21:09:37.788
STEP: Creating a pod to test consume configMaps 01/19/23 21:09:37.802
Jan 19 21:09:37.839: INFO: Waiting up to 5m0s for pod "pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7" in namespace "configmap-9051" to be "Succeeded or Failed"
Jan 19 21:09:37.842: INFO: Pod "pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.642015ms
Jan 19 21:09:39.846: INFO: Pod "pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007527555s
Jan 19 21:09:41.846: INFO: Pod "pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007123932s
STEP: Saw pod success 01/19/23 21:09:41.846
Jan 19 21:09:41.846: INFO: Pod "pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7" satisfied condition "Succeeded or Failed"
Jan 19 21:09:41.849: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:09:41.854
Jan 19 21:09:41.866: INFO: Waiting for pod pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7 to disappear
Jan 19 21:09:41.868: INFO: Pod pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:09:41.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9051" for this suite. 01/19/23 21:09:41.872
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":84,"skipped":1324,"failed":0}
------------------------------
• [4.116 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:09:37.761
    Jan 19 21:09:37.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:09:37.762
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:37.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:37.785
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-7148cae0-1900-43e6-a4b5-e160998e794a 01/19/23 21:09:37.788
    STEP: Creating a pod to test consume configMaps 01/19/23 21:09:37.802
    Jan 19 21:09:37.839: INFO: Waiting up to 5m0s for pod "pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7" in namespace "configmap-9051" to be "Succeeded or Failed"
    Jan 19 21:09:37.842: INFO: Pod "pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.642015ms
    Jan 19 21:09:39.846: INFO: Pod "pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007527555s
    Jan 19 21:09:41.846: INFO: Pod "pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007123932s
    STEP: Saw pod success 01/19/23 21:09:41.846
    Jan 19 21:09:41.846: INFO: Pod "pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7" satisfied condition "Succeeded or Failed"
    Jan 19 21:09:41.849: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:09:41.854
    Jan 19 21:09:41.866: INFO: Waiting for pod pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7 to disappear
    Jan 19 21:09:41.868: INFO: Pod pod-configmaps-603c4125-9d9c-4f43-90a0-aef6a775a9e7 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:09:41.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9051" for this suite. 01/19/23 21:09:41.872
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:09:41.878
Jan 19 21:09:41.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename daemonsets 01/19/23 21:09:41.878
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:41.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:41.898
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 01/19/23 21:09:41.953
STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 21:09:41.96
Jan 19 21:09:41.968: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:41.968: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:41.968: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:41.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:09:41.970: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:09:42.975: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:42.975: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:42.975: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:42.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:09:42.978: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:09:43.975: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:43.975: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:43.975: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:43.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 19 21:09:43.983: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/19/23 21:09:43.986
Jan 19 21:09:43.999: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:43.999: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:43.999: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:44.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 19 21:09:44.003: INFO: Node ip-10-0-171-213.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:09:45.007: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:45.007: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:45.007: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:45.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 19 21:09:45.011: INFO: Node ip-10-0-171-213.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:09:46.009: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:46.009: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:46.010: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:46.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 19 21:09:46.012: INFO: Node ip-10-0-171-213.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:09:47.007: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:47.007: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:47.007: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:47.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 19 21:09:47.011: INFO: Node ip-10-0-171-213.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:09:48.009: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:48.009: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:48.009: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:09:48.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 19 21:09:48.012: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/19/23 21:09:48.014
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7880, will wait for the garbage collector to delete the pods 01/19/23 21:09:48.014
Jan 19 21:09:48.072: INFO: Deleting DaemonSet.extensions daemon-set took: 5.667671ms
Jan 19 21:09:48.173: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.538966ms
Jan 19 21:09:50.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:09:50.177: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 21:09:50.179: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"164856"},"items":null}

Jan 19 21:09:50.182: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"164856"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:09:50.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7880" for this suite. 01/19/23 21:09:50.204
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":85,"skipped":1328,"failed":0}
------------------------------
• [SLOW TEST] [8.332 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:09:41.878
    Jan 19 21:09:41.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename daemonsets 01/19/23 21:09:41.878
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:41.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:41.898
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 01/19/23 21:09:41.953
    STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 21:09:41.96
    Jan 19 21:09:41.968: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:41.968: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:41.968: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:41.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:09:41.970: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:09:42.975: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:42.975: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:42.975: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:42.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:09:42.978: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:09:43.975: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:43.975: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:43.975: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:43.983: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
    Jan 19 21:09:43.983: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/19/23 21:09:43.986
    Jan 19 21:09:43.999: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:43.999: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:43.999: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:44.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 19 21:09:44.003: INFO: Node ip-10-0-171-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:09:45.007: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:45.007: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:45.007: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:45.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 19 21:09:45.011: INFO: Node ip-10-0-171-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:09:46.009: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:46.009: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:46.010: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:46.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 19 21:09:46.012: INFO: Node ip-10-0-171-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:09:47.007: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:47.007: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:47.007: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:47.011: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 19 21:09:47.011: INFO: Node ip-10-0-171-213.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:09:48.009: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:48.009: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:48.009: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:09:48.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
    Jan 19 21:09:48.012: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/19/23 21:09:48.014
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7880, will wait for the garbage collector to delete the pods 01/19/23 21:09:48.014
    Jan 19 21:09:48.072: INFO: Deleting DaemonSet.extensions daemon-set took: 5.667671ms
    Jan 19 21:09:48.173: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.538966ms
    Jan 19 21:09:50.177: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:09:50.177: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 19 21:09:50.179: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"164856"},"items":null}

    Jan 19 21:09:50.182: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"164856"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:09:50.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7880" for this suite. 01/19/23 21:09:50.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:09:50.211
Jan 19 21:09:50.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 21:09:50.212
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:50.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:50.236
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:09:50.238
Jan 19 21:09:50.266: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a" in namespace "downward-api-8428" to be "Succeeded or Failed"
Jan 19 21:09:50.278: INFO: Pod "downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.463433ms
Jan 19 21:09:52.282: INFO: Pod "downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016202259s
Jan 19 21:09:54.283: INFO: Pod "downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017193057s
STEP: Saw pod success 01/19/23 21:09:54.283
Jan 19 21:09:54.284: INFO: Pod "downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a" satisfied condition "Succeeded or Failed"
Jan 19 21:09:54.286: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a container client-container: <nil>
STEP: delete the pod 01/19/23 21:09:54.291
Jan 19 21:09:54.305: INFO: Waiting for pod downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a to disappear
Jan 19 21:09:54.307: INFO: Pod downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 21:09:54.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8428" for this suite. 01/19/23 21:09:54.311
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":86,"skipped":1353,"failed":0}
------------------------------
• [4.106 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:09:50.211
    Jan 19 21:09:50.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 21:09:50.212
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:50.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:50.236
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:09:50.238
    Jan 19 21:09:50.266: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a" in namespace "downward-api-8428" to be "Succeeded or Failed"
    Jan 19 21:09:50.278: INFO: Pod "downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.463433ms
    Jan 19 21:09:52.282: INFO: Pod "downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016202259s
    Jan 19 21:09:54.283: INFO: Pod "downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017193057s
    STEP: Saw pod success 01/19/23 21:09:54.283
    Jan 19 21:09:54.284: INFO: Pod "downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a" satisfied condition "Succeeded or Failed"
    Jan 19 21:09:54.286: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a container client-container: <nil>
    STEP: delete the pod 01/19/23 21:09:54.291
    Jan 19 21:09:54.305: INFO: Waiting for pod downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a to disappear
    Jan 19 21:09:54.307: INFO: Pod downwardapi-volume-9d6dee14-48d4-4843-a12c-6199738cd74a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 21:09:54.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8428" for this suite. 01/19/23 21:09:54.311
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:09:54.32
Jan 19 21:09:54.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 21:09:54.321
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:54.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:54.356
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 01/19/23 21:09:54.358
Jan 19 21:09:54.396: INFO: Waiting up to 5m0s for pod "downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31" in namespace "downward-api-3117" to be "Succeeded or Failed"
Jan 19 21:09:54.403: INFO: Pod "downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31": Phase="Pending", Reason="", readiness=false. Elapsed: 7.314011ms
Jan 19 21:09:56.407: INFO: Pod "downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011487053s
Jan 19 21:09:58.408: INFO: Pod "downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01254631s
STEP: Saw pod success 01/19/23 21:09:58.408
Jan 19 21:09:58.408: INFO: Pod "downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31" satisfied condition "Succeeded or Failed"
Jan 19 21:09:58.412: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31 container dapi-container: <nil>
STEP: delete the pod 01/19/23 21:09:58.417
Jan 19 21:09:58.428: INFO: Waiting for pod downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31 to disappear
Jan 19 21:09:58.430: INFO: Pod downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 19 21:09:58.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3117" for this suite. 01/19/23 21:09:58.435
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":87,"skipped":1438,"failed":0}
------------------------------
• [4.120 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:09:54.32
    Jan 19 21:09:54.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 21:09:54.321
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:54.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:54.356
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 01/19/23 21:09:54.358
    Jan 19 21:09:54.396: INFO: Waiting up to 5m0s for pod "downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31" in namespace "downward-api-3117" to be "Succeeded or Failed"
    Jan 19 21:09:54.403: INFO: Pod "downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31": Phase="Pending", Reason="", readiness=false. Elapsed: 7.314011ms
    Jan 19 21:09:56.407: INFO: Pod "downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011487053s
    Jan 19 21:09:58.408: INFO: Pod "downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01254631s
    STEP: Saw pod success 01/19/23 21:09:58.408
    Jan 19 21:09:58.408: INFO: Pod "downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31" satisfied condition "Succeeded or Failed"
    Jan 19 21:09:58.412: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31 container dapi-container: <nil>
    STEP: delete the pod 01/19/23 21:09:58.417
    Jan 19 21:09:58.428: INFO: Waiting for pod downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31 to disappear
    Jan 19 21:09:58.430: INFO: Pod downward-api-ef17fdaf-7bc4-4a12-ad80-c13f44cc6f31 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 19 21:09:58.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3117" for this suite. 01/19/23 21:09:58.435
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:09:58.442
Jan 19 21:09:58.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename runtimeclass 01/19/23 21:09:58.443
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:58.465
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:58.469
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan 19 21:09:58.498: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9792 to be scheduled
Jan 19 21:09:58.500: INFO: 1 pods are not scheduled: [runtimeclass-9792/test-runtimeclass-runtimeclass-9792-preconfigured-handler-cckd5(5761e7b9-81b5-4981-86bf-11862504e6ba)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 19 21:10:00.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9792" for this suite. 01/19/23 21:10:00.521
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":88,"skipped":1478,"failed":0}
------------------------------
• [2.085 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:09:58.442
    Jan 19 21:09:58.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename runtimeclass 01/19/23 21:09:58.443
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:09:58.465
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:09:58.469
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan 19 21:09:58.498: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9792 to be scheduled
    Jan 19 21:09:58.500: INFO: 1 pods are not scheduled: [runtimeclass-9792/test-runtimeclass-runtimeclass-9792-preconfigured-handler-cckd5(5761e7b9-81b5-4981-86bf-11862504e6ba)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 19 21:10:00.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9792" for this suite. 01/19/23 21:10:00.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:10:00.528
Jan 19 21:10:00.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pods 01/19/23 21:10:00.529
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:00.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:00.553
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 01/19/23 21:10:00.56
Jan 19 21:10:00.605: INFO: Waiting up to 5m0s for pod "pod-2fwgb" in namespace "pods-9556" to be "running"
Jan 19 21:10:00.613: INFO: Pod "pod-2fwgb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.118737ms
Jan 19 21:10:02.616: INFO: Pod "pod-2fwgb": Phase="Running", Reason="", readiness=true. Elapsed: 2.011828471s
Jan 19 21:10:02.616: INFO: Pod "pod-2fwgb" satisfied condition "running"
STEP: patching /status 01/19/23 21:10:02.616
Jan 19 21:10:02.624: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 21:10:02.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9556" for this suite. 01/19/23 21:10:02.627
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":89,"skipped":1519,"failed":0}
------------------------------
• [2.104 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:10:00.528
    Jan 19 21:10:00.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pods 01/19/23 21:10:00.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:00.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:00.553
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 01/19/23 21:10:00.56
    Jan 19 21:10:00.605: INFO: Waiting up to 5m0s for pod "pod-2fwgb" in namespace "pods-9556" to be "running"
    Jan 19 21:10:00.613: INFO: Pod "pod-2fwgb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.118737ms
    Jan 19 21:10:02.616: INFO: Pod "pod-2fwgb": Phase="Running", Reason="", readiness=true. Elapsed: 2.011828471s
    Jan 19 21:10:02.616: INFO: Pod "pod-2fwgb" satisfied condition "running"
    STEP: patching /status 01/19/23 21:10:02.616
    Jan 19 21:10:02.624: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 21:10:02.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9556" for this suite. 01/19/23 21:10:02.627
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:10:02.633
Jan 19 21:10:02.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename subpath 01/19/23 21:10:02.634
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:02.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:02.661
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/19/23 21:10:02.666
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-lhj6 01/19/23 21:10:02.686
STEP: Creating a pod to test atomic-volume-subpath 01/19/23 21:10:02.686
Jan 19 21:10:02.701: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lhj6" in namespace "subpath-9074" to be "Succeeded or Failed"
Jan 19 21:10:02.711: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.35131ms
Jan 19 21:10:04.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012405405s
Jan 19 21:10:06.715: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 4.013336565s
Jan 19 21:10:08.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 6.013038232s
Jan 19 21:10:10.715: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 8.013291267s
Jan 19 21:10:12.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 10.012953032s
Jan 19 21:10:14.715: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 12.013872629s
Jan 19 21:10:16.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 14.013069736s
Jan 19 21:10:18.716: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 16.014707122s
Jan 19 21:10:20.715: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 18.013468743s
Jan 19 21:10:22.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 20.013069151s
Jan 19 21:10:24.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=false. Elapsed: 22.012697138s
Jan 19 21:10:26.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012555175s
STEP: Saw pod success 01/19/23 21:10:26.714
Jan 19 21:10:26.714: INFO: Pod "pod-subpath-test-configmap-lhj6" satisfied condition "Succeeded or Failed"
Jan 19 21:10:26.716: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-subpath-test-configmap-lhj6 container test-container-subpath-configmap-lhj6: <nil>
STEP: delete the pod 01/19/23 21:10:26.724
Jan 19 21:10:26.734: INFO: Waiting for pod pod-subpath-test-configmap-lhj6 to disappear
Jan 19 21:10:26.736: INFO: Pod pod-subpath-test-configmap-lhj6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lhj6 01/19/23 21:10:26.736
Jan 19 21:10:26.736: INFO: Deleting pod "pod-subpath-test-configmap-lhj6" in namespace "subpath-9074"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 19 21:10:26.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9074" for this suite. 01/19/23 21:10:26.742
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":90,"skipped":1542,"failed":0}
------------------------------
• [SLOW TEST] [24.114 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:10:02.633
    Jan 19 21:10:02.633: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename subpath 01/19/23 21:10:02.634
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:02.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:02.661
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/19/23 21:10:02.666
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-lhj6 01/19/23 21:10:02.686
    STEP: Creating a pod to test atomic-volume-subpath 01/19/23 21:10:02.686
    Jan 19 21:10:02.701: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lhj6" in namespace "subpath-9074" to be "Succeeded or Failed"
    Jan 19 21:10:02.711: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.35131ms
    Jan 19 21:10:04.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 2.012405405s
    Jan 19 21:10:06.715: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 4.013336565s
    Jan 19 21:10:08.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 6.013038232s
    Jan 19 21:10:10.715: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 8.013291267s
    Jan 19 21:10:12.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 10.012953032s
    Jan 19 21:10:14.715: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 12.013872629s
    Jan 19 21:10:16.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 14.013069736s
    Jan 19 21:10:18.716: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 16.014707122s
    Jan 19 21:10:20.715: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 18.013468743s
    Jan 19 21:10:22.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=true. Elapsed: 20.013069151s
    Jan 19 21:10:24.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Running", Reason="", readiness=false. Elapsed: 22.012697138s
    Jan 19 21:10:26.714: INFO: Pod "pod-subpath-test-configmap-lhj6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012555175s
    STEP: Saw pod success 01/19/23 21:10:26.714
    Jan 19 21:10:26.714: INFO: Pod "pod-subpath-test-configmap-lhj6" satisfied condition "Succeeded or Failed"
    Jan 19 21:10:26.716: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-subpath-test-configmap-lhj6 container test-container-subpath-configmap-lhj6: <nil>
    STEP: delete the pod 01/19/23 21:10:26.724
    Jan 19 21:10:26.734: INFO: Waiting for pod pod-subpath-test-configmap-lhj6 to disappear
    Jan 19 21:10:26.736: INFO: Pod pod-subpath-test-configmap-lhj6 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-lhj6 01/19/23 21:10:26.736
    Jan 19 21:10:26.736: INFO: Deleting pod "pod-subpath-test-configmap-lhj6" in namespace "subpath-9074"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 19 21:10:26.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9074" for this suite. 01/19/23 21:10:26.742
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:10:26.748
Jan 19 21:10:26.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:10:26.748
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:26.769
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:26.776
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-3260733d-f537-4fb4-b9bb-edb9956740bb 01/19/23 21:10:26.778
STEP: Creating a pod to test consume secrets 01/19/23 21:10:26.79
Jan 19 21:10:26.807: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779" in namespace "projected-3602" to be "Succeeded or Failed"
Jan 19 21:10:26.809: INFO: Pod "pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137338ms
Jan 19 21:10:28.812: INFO: Pod "pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005262372s
Jan 19 21:10:30.813: INFO: Pod "pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005494637s
STEP: Saw pod success 01/19/23 21:10:30.813
Jan 19 21:10:30.813: INFO: Pod "pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779" satisfied condition "Succeeded or Failed"
Jan 19 21:10:30.815: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/19/23 21:10:30.82
Jan 19 21:10:30.828: INFO: Waiting for pod pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779 to disappear
Jan 19 21:10:30.833: INFO: Pod pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 21:10:30.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3602" for this suite. 01/19/23 21:10:30.838
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":91,"skipped":1551,"failed":0}
------------------------------
• [4.098 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:10:26.748
    Jan 19 21:10:26.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:10:26.748
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:26.769
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:26.776
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-3260733d-f537-4fb4-b9bb-edb9956740bb 01/19/23 21:10:26.778
    STEP: Creating a pod to test consume secrets 01/19/23 21:10:26.79
    Jan 19 21:10:26.807: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779" in namespace "projected-3602" to be "Succeeded or Failed"
    Jan 19 21:10:26.809: INFO: Pod "pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137338ms
    Jan 19 21:10:28.812: INFO: Pod "pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005262372s
    Jan 19 21:10:30.813: INFO: Pod "pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005494637s
    STEP: Saw pod success 01/19/23 21:10:30.813
    Jan 19 21:10:30.813: INFO: Pod "pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779" satisfied condition "Succeeded or Failed"
    Jan 19 21:10:30.815: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:10:30.82
    Jan 19 21:10:30.828: INFO: Waiting for pod pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779 to disappear
    Jan 19 21:10:30.833: INFO: Pod pod-projected-secrets-6e2753e5-0c37-4f95-a851-be7db4d7f779 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 21:10:30.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3602" for this suite. 01/19/23 21:10:30.838
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:10:30.846
Jan 19 21:10:30.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename crd-webhook 01/19/23 21:10:30.847
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:30.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:30.865
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/19/23 21:10:30.87
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/19/23 21:10:31.116
STEP: Deploying the custom resource conversion webhook pod 01/19/23 21:10:31.135
STEP: Wait for the deployment to be ready 01/19/23 21:10:31.146
Jan 19 21:10:31.156: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/19/23 21:10:33.164
STEP: Verifying the service has paired with the endpoint 01/19/23 21:10:33.174
Jan 19 21:10:34.174: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan 19 21:10:34.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Creating a v1 custom resource 01/19/23 21:10:36.742
STEP: Create a v2 custom resource 01/19/23 21:10:36.76
STEP: List CRs in v1 01/19/23 21:10:36.811
STEP: List CRs in v2 01/19/23 21:10:36.816
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:10:37.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-8118" for this suite. 01/19/23 21:10:37.336
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":92,"skipped":1555,"failed":0}
------------------------------
• [SLOW TEST] [6.559 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:10:30.846
    Jan 19 21:10:30.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename crd-webhook 01/19/23 21:10:30.847
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:30.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:30.865
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/19/23 21:10:30.87
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/19/23 21:10:31.116
    STEP: Deploying the custom resource conversion webhook pod 01/19/23 21:10:31.135
    STEP: Wait for the deployment to be ready 01/19/23 21:10:31.146
    Jan 19 21:10:31.156: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/19/23 21:10:33.164
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:10:33.174
    Jan 19 21:10:34.174: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan 19 21:10:34.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Creating a v1 custom resource 01/19/23 21:10:36.742
    STEP: Create a v2 custom resource 01/19/23 21:10:36.76
    STEP: List CRs in v1 01/19/23 21:10:36.811
    STEP: List CRs in v2 01/19/23 21:10:36.816
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:10:37.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-8118" for this suite. 01/19/23 21:10:37.336
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:10:37.405
Jan 19 21:10:37.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:10:37.406
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:37.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:37.455
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:10:37.494
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:10:38.103
STEP: Deploying the webhook pod 01/19/23 21:10:38.11
STEP: Wait for the deployment to be ready 01/19/23 21:10:38.119
Jan 19 21:10:38.123: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/19/23 21:10:40.133
STEP: Verifying the service has paired with the endpoint 01/19/23 21:10:40.146
Jan 19 21:10:41.147: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jan 19 21:10:41.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9947-crds.webhook.example.com via the AdmissionRegistration API 01/19/23 21:10:41.659
STEP: Creating a custom resource that should be mutated by the webhook 01/19/23 21:10:41.674
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:10:44.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1233" for this suite. 01/19/23 21:10:44.229
STEP: Destroying namespace "webhook-1233-markers" for this suite. 01/19/23 21:10:44.235
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":93,"skipped":1568,"failed":0}
------------------------------
• [SLOW TEST] [6.909 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:10:37.405
    Jan 19 21:10:37.405: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:10:37.406
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:37.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:37.455
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:10:37.494
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:10:38.103
    STEP: Deploying the webhook pod 01/19/23 21:10:38.11
    STEP: Wait for the deployment to be ready 01/19/23 21:10:38.119
    Jan 19 21:10:38.123: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/19/23 21:10:40.133
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:10:40.146
    Jan 19 21:10:41.147: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jan 19 21:10:41.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9947-crds.webhook.example.com via the AdmissionRegistration API 01/19/23 21:10:41.659
    STEP: Creating a custom resource that should be mutated by the webhook 01/19/23 21:10:41.674
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:10:44.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1233" for this suite. 01/19/23 21:10:44.229
    STEP: Destroying namespace "webhook-1233-markers" for this suite. 01/19/23 21:10:44.235
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:10:44.315
Jan 19 21:10:44.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 21:10:44.316
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:44.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:44.383
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/19/23 21:10:44.408
Jan 19 21:10:44.462: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4400" to be "running and ready"
Jan 19 21:10:44.466: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.488781ms
Jan 19 21:10:44.466: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:10:46.469: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.00701554s
Jan 19 21:10:46.469: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 19 21:10:46.469: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 01/19/23 21:10:46.471
Jan 19 21:10:46.483: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4400" to be "running and ready"
Jan 19 21:10:46.489: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.680371ms
Jan 19 21:10:46.489: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:10:48.492: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008227818s
Jan 19 21:10:48.492: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan 19 21:10:48.492: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/19/23 21:10:48.494
STEP: delete the pod with lifecycle hook 01/19/23 21:10:48.499
Jan 19 21:10:48.505: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 19 21:10:48.509: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 19 21:10:50.510: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 19 21:10:50.514: INFO: Pod pod-with-poststart-exec-hook still exists
Jan 19 21:10:52.510: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan 19 21:10:52.513: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 19 21:10:52.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4400" for this suite. 01/19/23 21:10:52.518
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":94,"skipped":1579,"failed":0}
------------------------------
• [SLOW TEST] [8.210 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:10:44.315
    Jan 19 21:10:44.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 21:10:44.316
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:44.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:44.383
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/19/23 21:10:44.408
    Jan 19 21:10:44.462: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4400" to be "running and ready"
    Jan 19 21:10:44.466: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.488781ms
    Jan 19 21:10:44.466: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:10:46.469: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.00701554s
    Jan 19 21:10:46.469: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 19 21:10:46.469: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 01/19/23 21:10:46.471
    Jan 19 21:10:46.483: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4400" to be "running and ready"
    Jan 19 21:10:46.489: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.680371ms
    Jan 19 21:10:46.489: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:10:48.492: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008227818s
    Jan 19 21:10:48.492: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan 19 21:10:48.492: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/19/23 21:10:48.494
    STEP: delete the pod with lifecycle hook 01/19/23 21:10:48.499
    Jan 19 21:10:48.505: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 19 21:10:48.509: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 19 21:10:50.510: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 19 21:10:50.514: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan 19 21:10:52.510: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan 19 21:10:52.513: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 19 21:10:52.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4400" for this suite. 01/19/23 21:10:52.518
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:10:52.526
Jan 19 21:10:52.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 21:10:52.527
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:52.544
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:52.551
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-4191 01/19/23 21:10:52.555
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4191 to expose endpoints map[] 01/19/23 21:10:52.576
Jan 19 21:10:52.581: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan 19 21:10:53.588: INFO: successfully validated that service multi-endpoint-test in namespace services-4191 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4191 01/19/23 21:10:53.588
Jan 19 21:10:53.600: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4191" to be "running and ready"
Jan 19 21:10:53.602: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.202524ms
Jan 19 21:10:53.602: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:10:55.606: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005916075s
Jan 19 21:10:55.606: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 19 21:10:55.606: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4191 to expose endpoints map[pod1:[100]] 01/19/23 21:10:55.608
Jan 19 21:10:55.616: INFO: successfully validated that service multi-endpoint-test in namespace services-4191 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-4191 01/19/23 21:10:55.616
Jan 19 21:10:55.627: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4191" to be "running and ready"
Jan 19 21:10:55.629: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.405781ms
Jan 19 21:10:55.629: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:10:57.633: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006447646s
Jan 19 21:10:57.633: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 19 21:10:57.633: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4191 to expose endpoints map[pod1:[100] pod2:[101]] 01/19/23 21:10:57.636
Jan 19 21:10:57.644: INFO: successfully validated that service multi-endpoint-test in namespace services-4191 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/19/23 21:10:57.644
Jan 19 21:10:57.645: INFO: Creating new exec pod
Jan 19 21:10:57.654: INFO: Waiting up to 5m0s for pod "execpod84jhk" in namespace "services-4191" to be "running"
Jan 19 21:10:57.657: INFO: Pod "execpod84jhk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.391311ms
Jan 19 21:10:59.660: INFO: Pod "execpod84jhk": Phase="Running", Reason="", readiness=true. Elapsed: 2.005556033s
Jan 19 21:10:59.660: INFO: Pod "execpod84jhk" satisfied condition "running"
Jan 19 21:11:00.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-4191 exec execpod84jhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jan 19 21:11:00.791: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan 19 21:11:00.791: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:11:00.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-4191 exec execpod84jhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.232.255 80'
Jan 19 21:11:00.926: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.232.255 80\nConnection to 172.30.232.255 80 port [tcp/http] succeeded!\n"
Jan 19 21:11:00.926: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:11:00.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-4191 exec execpod84jhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jan 19 21:11:01.048: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan 19 21:11:01.048: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:11:01.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-4191 exec execpod84jhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.232.255 81'
Jan 19 21:11:01.168: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.232.255 81\nConnection to 172.30.232.255 81 port [tcp/*] succeeded!\n"
Jan 19 21:11:01.168: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-4191 01/19/23 21:11:01.168
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4191 to expose endpoints map[pod2:[101]] 01/19/23 21:11:01.194
Jan 19 21:11:01.229: INFO: successfully validated that service multi-endpoint-test in namespace services-4191 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-4191 01/19/23 21:11:01.229
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4191 to expose endpoints map[] 01/19/23 21:11:01.261
Jan 19 21:11:02.313: INFO: successfully validated that service multi-endpoint-test in namespace services-4191 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 21:11:02.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4191" for this suite. 01/19/23 21:11:02.352
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":95,"skipped":1619,"failed":0}
------------------------------
• [SLOW TEST] [9.835 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:10:52.526
    Jan 19 21:10:52.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 21:10:52.527
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:10:52.544
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:10:52.551
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-4191 01/19/23 21:10:52.555
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4191 to expose endpoints map[] 01/19/23 21:10:52.576
    Jan 19 21:10:52.581: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan 19 21:10:53.588: INFO: successfully validated that service multi-endpoint-test in namespace services-4191 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4191 01/19/23 21:10:53.588
    Jan 19 21:10:53.600: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4191" to be "running and ready"
    Jan 19 21:10:53.602: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.202524ms
    Jan 19 21:10:53.602: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:10:55.606: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005916075s
    Jan 19 21:10:55.606: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 19 21:10:55.606: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4191 to expose endpoints map[pod1:[100]] 01/19/23 21:10:55.608
    Jan 19 21:10:55.616: INFO: successfully validated that service multi-endpoint-test in namespace services-4191 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-4191 01/19/23 21:10:55.616
    Jan 19 21:10:55.627: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4191" to be "running and ready"
    Jan 19 21:10:55.629: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.405781ms
    Jan 19 21:10:55.629: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:10:57.633: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006447646s
    Jan 19 21:10:57.633: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 19 21:10:57.633: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4191 to expose endpoints map[pod1:[100] pod2:[101]] 01/19/23 21:10:57.636
    Jan 19 21:10:57.644: INFO: successfully validated that service multi-endpoint-test in namespace services-4191 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/19/23 21:10:57.644
    Jan 19 21:10:57.645: INFO: Creating new exec pod
    Jan 19 21:10:57.654: INFO: Waiting up to 5m0s for pod "execpod84jhk" in namespace "services-4191" to be "running"
    Jan 19 21:10:57.657: INFO: Pod "execpod84jhk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.391311ms
    Jan 19 21:10:59.660: INFO: Pod "execpod84jhk": Phase="Running", Reason="", readiness=true. Elapsed: 2.005556033s
    Jan 19 21:10:59.660: INFO: Pod "execpod84jhk" satisfied condition "running"
    Jan 19 21:11:00.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-4191 exec execpod84jhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jan 19 21:11:00.791: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan 19 21:11:00.791: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:11:00.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-4191 exec execpod84jhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.232.255 80'
    Jan 19 21:11:00.926: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.232.255 80\nConnection to 172.30.232.255 80 port [tcp/http] succeeded!\n"
    Jan 19 21:11:00.926: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:11:00.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-4191 exec execpod84jhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jan 19 21:11:01.048: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan 19 21:11:01.048: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:11:01.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-4191 exec execpod84jhk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.232.255 81'
    Jan 19 21:11:01.168: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.232.255 81\nConnection to 172.30.232.255 81 port [tcp/*] succeeded!\n"
    Jan 19 21:11:01.168: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-4191 01/19/23 21:11:01.168
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4191 to expose endpoints map[pod2:[101]] 01/19/23 21:11:01.194
    Jan 19 21:11:01.229: INFO: successfully validated that service multi-endpoint-test in namespace services-4191 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-4191 01/19/23 21:11:01.229
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4191 to expose endpoints map[] 01/19/23 21:11:01.261
    Jan 19 21:11:02.313: INFO: successfully validated that service multi-endpoint-test in namespace services-4191 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 21:11:02.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4191" for this suite. 01/19/23 21:11:02.352
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:11:02.361
Jan 19 21:11:02.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename events 01/19/23 21:11:02.362
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:11:02.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:11:02.409
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/19/23 21:11:02.423
Jan 19 21:11:02.449: INFO: created test-event-1
Jan 19 21:11:02.459: INFO: created test-event-2
Jan 19 21:11:02.471: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/19/23 21:11:02.471
STEP: delete collection of events 01/19/23 21:11:02.476
Jan 19 21:11:02.476: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/19/23 21:11:02.522
Jan 19 21:11:02.522: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 19 21:11:02.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7246" for this suite. 01/19/23 21:11:02.539
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":96,"skipped":1620,"failed":0}
------------------------------
• [0.200 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:11:02.361
    Jan 19 21:11:02.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename events 01/19/23 21:11:02.362
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:11:02.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:11:02.409
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/19/23 21:11:02.423
    Jan 19 21:11:02.449: INFO: created test-event-1
    Jan 19 21:11:02.459: INFO: created test-event-2
    Jan 19 21:11:02.471: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/19/23 21:11:02.471
    STEP: delete collection of events 01/19/23 21:11:02.476
    Jan 19 21:11:02.476: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/19/23 21:11:02.522
    Jan 19 21:11:02.522: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 19 21:11:02.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-7246" for this suite. 01/19/23 21:11:02.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:11:02.563
Jan 19 21:11:02.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename statefulset 01/19/23 21:11:02.563
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:11:02.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:11:02.599
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-3296 01/19/23 21:11:02.608
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 01/19/23 21:11:02.624
Jan 19 21:11:02.640: INFO: Found 0 stateful pods, waiting for 3
Jan 19 21:11:12.645: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 21:11:12.645: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 21:11:12.645: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 21:11:12.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-3296 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 21:11:12.776: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 21:11:12.776: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 21:11:12.776: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/19/23 21:11:22.789
Jan 19 21:11:22.807: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/19/23 21:11:22.807
STEP: Updating Pods in reverse ordinal order 01/19/23 21:11:32.836
Jan 19 21:11:32.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-3296 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 21:11:33.098: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 21:11:33.098: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 21:11:33.098: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 01/19/23 21:11:43.118
Jan 19 21:11:43.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-3296 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 21:11:43.268: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 21:11:43.268: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 21:11:43.268: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 21:11:53.298: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/19/23 21:12:03.31
Jan 19 21:12:03.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-3296 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 21:12:03.432: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 21:12:03.432: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 21:12:03.432: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 21:12:13.450: INFO: Deleting all statefulset in ns statefulset-3296
Jan 19 21:12:13.452: INFO: Scaling statefulset ss2 to 0
Jan 19 21:12:23.469: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 21:12:23.471: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 21:12:23.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3296" for this suite. 01/19/23 21:12:23.485
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":97,"skipped":1678,"failed":0}
------------------------------
• [SLOW TEST] [80.937 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:11:02.563
    Jan 19 21:11:02.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename statefulset 01/19/23 21:11:02.563
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:11:02.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:11:02.599
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-3296 01/19/23 21:11:02.608
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 01/19/23 21:11:02.624
    Jan 19 21:11:02.640: INFO: Found 0 stateful pods, waiting for 3
    Jan 19 21:11:12.645: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 21:11:12.645: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 21:11:12.645: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 21:11:12.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-3296 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 21:11:12.776: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 21:11:12.776: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 21:11:12.776: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 01/19/23 21:11:22.789
    Jan 19 21:11:22.807: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/19/23 21:11:22.807
    STEP: Updating Pods in reverse ordinal order 01/19/23 21:11:32.836
    Jan 19 21:11:32.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-3296 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 21:11:33.098: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 21:11:33.098: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 21:11:33.098: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 01/19/23 21:11:43.118
    Jan 19 21:11:43.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-3296 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 21:11:43.268: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 21:11:43.268: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 21:11:43.268: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 21:11:53.298: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/19/23 21:12:03.31
    Jan 19 21:12:03.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-3296 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 21:12:03.432: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 21:12:03.432: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 21:12:03.432: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 21:12:13.450: INFO: Deleting all statefulset in ns statefulset-3296
    Jan 19 21:12:13.452: INFO: Scaling statefulset ss2 to 0
    Jan 19 21:12:23.469: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 21:12:23.471: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 21:12:23.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-3296" for this suite. 01/19/23 21:12:23.485
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:12:23.5
Jan 19 21:12:23.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-probe 01/19/23 21:12:23.501
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:12:23.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:12:23.525
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 21:13:23.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7575" for this suite. 01/19/23 21:13:23.589
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":98,"skipped":1683,"failed":0}
------------------------------
• [SLOW TEST] [60.095 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:12:23.5
    Jan 19 21:12:23.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-probe 01/19/23 21:12:23.501
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:12:23.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:12:23.525
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 21:13:23.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7575" for this suite. 01/19/23 21:13:23.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:13:23.595
Jan 19 21:13:23.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename resourcequota 01/19/23 21:13:23.596
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:13:23.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:13:23.621
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 01/19/23 21:13:23.623
STEP: Getting a ResourceQuota 01/19/23 21:13:23.637
STEP: Updating a ResourceQuota 01/19/23 21:13:23.647
STEP: Verifying a ResourceQuota was modified 01/19/23 21:13:23.652
STEP: Deleting a ResourceQuota 01/19/23 21:13:23.655
STEP: Verifying the deleted ResourceQuota 01/19/23 21:13:23.67
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 21:13:23.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6775" for this suite. 01/19/23 21:13:23.681
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":99,"skipped":1698,"failed":0}
------------------------------
• [0.095 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:13:23.595
    Jan 19 21:13:23.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename resourcequota 01/19/23 21:13:23.596
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:13:23.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:13:23.621
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 01/19/23 21:13:23.623
    STEP: Getting a ResourceQuota 01/19/23 21:13:23.637
    STEP: Updating a ResourceQuota 01/19/23 21:13:23.647
    STEP: Verifying a ResourceQuota was modified 01/19/23 21:13:23.652
    STEP: Deleting a ResourceQuota 01/19/23 21:13:23.655
    STEP: Verifying the deleted ResourceQuota 01/19/23 21:13:23.67
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 21:13:23.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-6775" for this suite. 01/19/23 21:13:23.681
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:13:23.691
Jan 19 21:13:23.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename endpointslice 01/19/23 21:13:23.692
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:13:23.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:13:23.729
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jan 19 21:13:23.779: INFO: Endpoints addresses: [10.0.145.33 10.0.170.255 10.0.194.246] , ports: [6443]
Jan 19 21:13:23.779: INFO: EndpointSlices addresses: [10.0.145.33 10.0.170.255 10.0.194.246] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 19 21:13:23.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4420" for this suite. 01/19/23 21:13:23.791
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":100,"skipped":1702,"failed":0}
------------------------------
• [0.116 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:13:23.691
    Jan 19 21:13:23.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename endpointslice 01/19/23 21:13:23.692
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:13:23.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:13:23.729
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jan 19 21:13:23.779: INFO: Endpoints addresses: [10.0.145.33 10.0.170.255 10.0.194.246] , ports: [6443]
    Jan 19 21:13:23.779: INFO: EndpointSlices addresses: [10.0.145.33 10.0.170.255 10.0.194.246] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 19 21:13:23.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4420" for this suite. 01/19/23 21:13:23.791
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:13:23.807
Jan 19 21:13:23.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename svcaccounts 01/19/23 21:13:23.808
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:13:23.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:13:23.871
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jan 19 21:13:23.942: INFO: created pod pod-service-account-defaultsa
Jan 19 21:13:23.942: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan 19 21:13:23.978: INFO: created pod pod-service-account-mountsa
Jan 19 21:13:23.978: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan 19 21:13:23.997: INFO: created pod pod-service-account-nomountsa
Jan 19 21:13:23.997: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan 19 21:13:24.013: INFO: created pod pod-service-account-defaultsa-mountspec
Jan 19 21:13:24.013: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan 19 21:13:24.037: INFO: created pod pod-service-account-mountsa-mountspec
Jan 19 21:13:24.037: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan 19 21:13:24.070: INFO: created pod pod-service-account-nomountsa-mountspec
Jan 19 21:13:24.070: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan 19 21:13:24.092: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan 19 21:13:24.092: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan 19 21:13:24.106: INFO: created pod pod-service-account-mountsa-nomountspec
Jan 19 21:13:24.106: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan 19 21:13:24.122: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan 19 21:13:24.122: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 19 21:13:24.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7402" for this suite. 01/19/23 21:13:24.126
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":101,"skipped":1706,"failed":0}
------------------------------
• [0.329 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:13:23.807
    Jan 19 21:13:23.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename svcaccounts 01/19/23 21:13:23.808
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:13:23.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:13:23.871
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jan 19 21:13:23.942: INFO: created pod pod-service-account-defaultsa
    Jan 19 21:13:23.942: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan 19 21:13:23.978: INFO: created pod pod-service-account-mountsa
    Jan 19 21:13:23.978: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan 19 21:13:23.997: INFO: created pod pod-service-account-nomountsa
    Jan 19 21:13:23.997: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan 19 21:13:24.013: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan 19 21:13:24.013: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan 19 21:13:24.037: INFO: created pod pod-service-account-mountsa-mountspec
    Jan 19 21:13:24.037: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan 19 21:13:24.070: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan 19 21:13:24.070: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan 19 21:13:24.092: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan 19 21:13:24.092: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan 19 21:13:24.106: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan 19 21:13:24.106: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan 19 21:13:24.122: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan 19 21:13:24.122: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 19 21:13:24.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7402" for this suite. 01/19/23 21:13:24.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:13:24.138
Jan 19 21:13:24.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename daemonsets 01/19/23 21:13:24.139
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:13:24.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:13:24.199
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jan 19 21:13:24.273: INFO: Create a RollingUpdate DaemonSet
Jan 19 21:13:24.289: INFO: Check that daemon pods launch on every node of the cluster
Jan 19 21:13:24.295: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:24.295: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:24.295: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:24.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:13:24.299: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:13:25.304: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:25.304: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:25.304: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:25.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:13:25.307: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:13:26.306: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:26.306: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:26.306: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:26.311: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 19 21:13:26.311: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
Jan 19 21:13:26.311: INFO: Update the DaemonSet to trigger a rollout
Jan 19 21:13:26.338: INFO: Updating DaemonSet daemon-set
Jan 19 21:13:29.354: INFO: Roll back the DaemonSet before rollout is complete
Jan 19 21:13:29.362: INFO: Updating DaemonSet daemon-set
Jan 19 21:13:29.362: INFO: Make sure DaemonSet rollback is complete
Jan 19 21:13:29.368: INFO: Wrong image for pod: daemon-set-mdz8b. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jan 19 21:13:29.368: INFO: Pod daemon-set-mdz8b is not available
Jan 19 21:13:29.373: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:29.373: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:29.373: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:30.384: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:30.384: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:30.384: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:31.377: INFO: Pod daemon-set-tbvw6 is not available
Jan 19 21:13:31.390: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:31.390: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:13:31.390: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/19/23 21:13:31.415
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9972, will wait for the garbage collector to delete the pods 01/19/23 21:13:31.415
Jan 19 21:13:31.475: INFO: Deleting DaemonSet.extensions daemon-set took: 7.726176ms
Jan 19 21:13:31.576: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.327885ms
Jan 19 21:13:33.979: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:13:33.979: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 21:13:33.981: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"169834"},"items":null}

Jan 19 21:13:33.983: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"169834"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:13:34.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9972" for this suite. 01/19/23 21:13:34.013
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":102,"skipped":1740,"failed":0}
------------------------------
• [SLOW TEST] [9.879 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:13:24.138
    Jan 19 21:13:24.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename daemonsets 01/19/23 21:13:24.139
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:13:24.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:13:24.199
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jan 19 21:13:24.273: INFO: Create a RollingUpdate DaemonSet
    Jan 19 21:13:24.289: INFO: Check that daemon pods launch on every node of the cluster
    Jan 19 21:13:24.295: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:24.295: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:24.295: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:24.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:13:24.299: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:13:25.304: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:25.304: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:25.304: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:25.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:13:25.307: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:13:26.306: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:26.306: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:26.306: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:26.311: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
    Jan 19 21:13:26.311: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
    Jan 19 21:13:26.311: INFO: Update the DaemonSet to trigger a rollout
    Jan 19 21:13:26.338: INFO: Updating DaemonSet daemon-set
    Jan 19 21:13:29.354: INFO: Roll back the DaemonSet before rollout is complete
    Jan 19 21:13:29.362: INFO: Updating DaemonSet daemon-set
    Jan 19 21:13:29.362: INFO: Make sure DaemonSet rollback is complete
    Jan 19 21:13:29.368: INFO: Wrong image for pod: daemon-set-mdz8b. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jan 19 21:13:29.368: INFO: Pod daemon-set-mdz8b is not available
    Jan 19 21:13:29.373: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:29.373: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:29.373: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:30.384: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:30.384: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:30.384: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:31.377: INFO: Pod daemon-set-tbvw6 is not available
    Jan 19 21:13:31.390: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:31.390: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:13:31.390: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/19/23 21:13:31.415
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9972, will wait for the garbage collector to delete the pods 01/19/23 21:13:31.415
    Jan 19 21:13:31.475: INFO: Deleting DaemonSet.extensions daemon-set took: 7.726176ms
    Jan 19 21:13:31.576: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.327885ms
    Jan 19 21:13:33.979: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:13:33.979: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 19 21:13:33.981: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"169834"},"items":null}

    Jan 19 21:13:33.983: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"169834"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:13:34.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9972" for this suite. 01/19/23 21:13:34.013
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:13:34.02
Jan 19 21:13:34.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:13:34.021
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:13:34.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:13:34.047
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/19/23 21:13:34.052
Jan 19 21:13:34.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/19/23 21:14:05.46
Jan 19 21:14:05.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:14:14.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:14:47.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2509" for this suite. 01/19/23 21:14:47.168
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":103,"skipped":1819,"failed":0}
------------------------------
• [SLOW TEST] [73.154 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:13:34.02
    Jan 19 21:13:34.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:13:34.021
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:13:34.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:13:34.047
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/19/23 21:13:34.052
    Jan 19 21:13:34.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/19/23 21:14:05.46
    Jan 19 21:14:05.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:14:14.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:14:47.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2509" for this suite. 01/19/23 21:14:47.168
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:14:47.174
Jan 19 21:14:47.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename controllerrevisions 01/19/23 21:14:47.175
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:14:47.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:14:47.189
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-glbfw-daemon-set" 01/19/23 21:14:47.296
STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 21:14:47.312
Jan 19 21:14:47.318: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:14:47.318: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:14:47.318: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:14:47.330: INFO: Number of nodes with available pods controlled by daemonset e2e-glbfw-daemon-set: 0
Jan 19 21:14:47.330: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:14:48.335: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:14:48.335: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:14:48.335: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:14:48.338: INFO: Number of nodes with available pods controlled by daemonset e2e-glbfw-daemon-set: 1
Jan 19 21:14:48.338: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:14:49.335: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:14:49.335: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:14:49.335: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:14:49.338: INFO: Number of nodes with available pods controlled by daemonset e2e-glbfw-daemon-set: 6
Jan 19 21:14:49.338: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset e2e-glbfw-daemon-set
STEP: Confirm DaemonSet "e2e-glbfw-daemon-set" successfully created with "daemonset-name=e2e-glbfw-daemon-set" label 01/19/23 21:14:49.34
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-glbfw-daemon-set" 01/19/23 21:14:49.346
Jan 19 21:14:49.349: INFO: Located ControllerRevision: "e2e-glbfw-daemon-set-68cccfd6cf"
STEP: Patching ControllerRevision "e2e-glbfw-daemon-set-68cccfd6cf" 01/19/23 21:14:49.351
Jan 19 21:14:49.357: INFO: e2e-glbfw-daemon-set-68cccfd6cf has been patched
STEP: Create a new ControllerRevision 01/19/23 21:14:49.357
Jan 19 21:14:49.362: INFO: Created ControllerRevision: e2e-glbfw-daemon-set-5cbbb78f87
STEP: Confirm that there are two ControllerRevisions 01/19/23 21:14:49.362
Jan 19 21:14:49.362: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 19 21:14:49.364: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-glbfw-daemon-set-68cccfd6cf" 01/19/23 21:14:49.364
STEP: Confirm that there is only one ControllerRevision 01/19/23 21:14:49.368
Jan 19 21:14:49.368: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 19 21:14:49.370: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-glbfw-daemon-set-5cbbb78f87" 01/19/23 21:14:49.372
Jan 19 21:14:49.379: INFO: e2e-glbfw-daemon-set-5cbbb78f87 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/19/23 21:14:49.379
W0119 21:14:49.384483      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/19/23 21:14:49.384
Jan 19 21:14:49.384: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 19 21:14:50.386: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 19 21:14:50.388: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-glbfw-daemon-set-5cbbb78f87=updated" 01/19/23 21:14:50.388
STEP: Confirm that there is only one ControllerRevision 01/19/23 21:14:50.395
Jan 19 21:14:50.395: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan 19 21:14:50.397: INFO: Found 1 ControllerRevisions
Jan 19 21:14:50.399: INFO: ControllerRevision "e2e-glbfw-daemon-set-7fd5b79898" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-glbfw-daemon-set" 01/19/23 21:14:50.401
STEP: deleting DaemonSet.extensions e2e-glbfw-daemon-set in namespace controllerrevisions-9324, will wait for the garbage collector to delete the pods 01/19/23 21:14:50.401
Jan 19 21:14:50.458: INFO: Deleting DaemonSet.extensions e2e-glbfw-daemon-set took: 4.565256ms
Jan 19 21:14:50.558: INFO: Terminating DaemonSet.extensions e2e-glbfw-daemon-set pods took: 100.270512ms
Jan 19 21:14:52.161: INFO: Number of nodes with available pods controlled by daemonset e2e-glbfw-daemon-set: 0
Jan 19 21:14:52.161: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-glbfw-daemon-set
Jan 19 21:14:52.163: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"170946"},"items":null}

Jan 19 21:14:52.165: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"170946"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:14:52.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-9324" for this suite. 01/19/23 21:14:52.189
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":104,"skipped":1823,"failed":0}
------------------------------
• [SLOW TEST] [5.022 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:14:47.174
    Jan 19 21:14:47.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename controllerrevisions 01/19/23 21:14:47.175
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:14:47.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:14:47.189
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-glbfw-daemon-set" 01/19/23 21:14:47.296
    STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 21:14:47.312
    Jan 19 21:14:47.318: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:14:47.318: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:14:47.318: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:14:47.330: INFO: Number of nodes with available pods controlled by daemonset e2e-glbfw-daemon-set: 0
    Jan 19 21:14:47.330: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:14:48.335: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:14:48.335: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:14:48.335: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:14:48.338: INFO: Number of nodes with available pods controlled by daemonset e2e-glbfw-daemon-set: 1
    Jan 19 21:14:48.338: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:14:49.335: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:14:49.335: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:14:49.335: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:14:49.338: INFO: Number of nodes with available pods controlled by daemonset e2e-glbfw-daemon-set: 6
    Jan 19 21:14:49.338: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset e2e-glbfw-daemon-set
    STEP: Confirm DaemonSet "e2e-glbfw-daemon-set" successfully created with "daemonset-name=e2e-glbfw-daemon-set" label 01/19/23 21:14:49.34
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-glbfw-daemon-set" 01/19/23 21:14:49.346
    Jan 19 21:14:49.349: INFO: Located ControllerRevision: "e2e-glbfw-daemon-set-68cccfd6cf"
    STEP: Patching ControllerRevision "e2e-glbfw-daemon-set-68cccfd6cf" 01/19/23 21:14:49.351
    Jan 19 21:14:49.357: INFO: e2e-glbfw-daemon-set-68cccfd6cf has been patched
    STEP: Create a new ControllerRevision 01/19/23 21:14:49.357
    Jan 19 21:14:49.362: INFO: Created ControllerRevision: e2e-glbfw-daemon-set-5cbbb78f87
    STEP: Confirm that there are two ControllerRevisions 01/19/23 21:14:49.362
    Jan 19 21:14:49.362: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 19 21:14:49.364: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-glbfw-daemon-set-68cccfd6cf" 01/19/23 21:14:49.364
    STEP: Confirm that there is only one ControllerRevision 01/19/23 21:14:49.368
    Jan 19 21:14:49.368: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 19 21:14:49.370: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-glbfw-daemon-set-5cbbb78f87" 01/19/23 21:14:49.372
    Jan 19 21:14:49.379: INFO: e2e-glbfw-daemon-set-5cbbb78f87 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/19/23 21:14:49.379
    W0119 21:14:49.384483      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/19/23 21:14:49.384
    Jan 19 21:14:49.384: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 19 21:14:50.386: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 19 21:14:50.388: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-glbfw-daemon-set-5cbbb78f87=updated" 01/19/23 21:14:50.388
    STEP: Confirm that there is only one ControllerRevision 01/19/23 21:14:50.395
    Jan 19 21:14:50.395: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan 19 21:14:50.397: INFO: Found 1 ControllerRevisions
    Jan 19 21:14:50.399: INFO: ControllerRevision "e2e-glbfw-daemon-set-7fd5b79898" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-glbfw-daemon-set" 01/19/23 21:14:50.401
    STEP: deleting DaemonSet.extensions e2e-glbfw-daemon-set in namespace controllerrevisions-9324, will wait for the garbage collector to delete the pods 01/19/23 21:14:50.401
    Jan 19 21:14:50.458: INFO: Deleting DaemonSet.extensions e2e-glbfw-daemon-set took: 4.565256ms
    Jan 19 21:14:50.558: INFO: Terminating DaemonSet.extensions e2e-glbfw-daemon-set pods took: 100.270512ms
    Jan 19 21:14:52.161: INFO: Number of nodes with available pods controlled by daemonset e2e-glbfw-daemon-set: 0
    Jan 19 21:14:52.161: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-glbfw-daemon-set
    Jan 19 21:14:52.163: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"170946"},"items":null}

    Jan 19 21:14:52.165: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"170946"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:14:52.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-9324" for this suite. 01/19/23 21:14:52.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:14:52.196
Jan 19 21:14:52.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 21:14:52.197
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:14:52.211
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:14:52.213
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:14:52.22
Jan 19 21:14:52.249: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a" in namespace "downward-api-1969" to be "Succeeded or Failed"
Jan 19 21:14:52.252: INFO: Pod "downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068663ms
Jan 19 21:14:54.255: INFO: Pod "downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005981326s
Jan 19 21:14:56.256: INFO: Pod "downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006936033s
STEP: Saw pod success 01/19/23 21:14:56.256
Jan 19 21:14:56.257: INFO: Pod "downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a" satisfied condition "Succeeded or Failed"
Jan 19 21:14:56.259: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a container client-container: <nil>
STEP: delete the pod 01/19/23 21:14:56.269
Jan 19 21:14:56.278: INFO: Waiting for pod downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a to disappear
Jan 19 21:14:56.280: INFO: Pod downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 21:14:56.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1969" for this suite. 01/19/23 21:14:56.285
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":105,"skipped":1833,"failed":0}
------------------------------
• [4.094 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:14:52.196
    Jan 19 21:14:52.196: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 21:14:52.197
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:14:52.211
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:14:52.213
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:14:52.22
    Jan 19 21:14:52.249: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a" in namespace "downward-api-1969" to be "Succeeded or Failed"
    Jan 19 21:14:52.252: INFO: Pod "downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.068663ms
    Jan 19 21:14:54.255: INFO: Pod "downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005981326s
    Jan 19 21:14:56.256: INFO: Pod "downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006936033s
    STEP: Saw pod success 01/19/23 21:14:56.256
    Jan 19 21:14:56.257: INFO: Pod "downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a" satisfied condition "Succeeded or Failed"
    Jan 19 21:14:56.259: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a container client-container: <nil>
    STEP: delete the pod 01/19/23 21:14:56.269
    Jan 19 21:14:56.278: INFO: Waiting for pod downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a to disappear
    Jan 19 21:14:56.280: INFO: Pod downwardapi-volume-85e00fd5-5dbd-427b-9673-366dd1f1075a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 21:14:56.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1969" for this suite. 01/19/23 21:14:56.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:14:56.291
Jan 19 21:14:56.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:14:56.292
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:14:56.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:14:56.314
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
Jan 19 21:14:56.330: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-6b947c4e-6565-45ec-acc9-4b204cb03ecf 01/19/23 21:14:56.33
STEP: Creating the pod 01/19/23 21:14:56.344
Jan 19 21:14:56.371: INFO: Waiting up to 5m0s for pod "pod-configmaps-cb4ce185-8948-4173-98ab-7bc65b155015" in namespace "configmap-6149" to be "running"
Jan 19 21:14:56.391: INFO: Pod "pod-configmaps-cb4ce185-8948-4173-98ab-7bc65b155015": Phase="Pending", Reason="", readiness=false. Elapsed: 19.311756ms
Jan 19 21:14:58.394: INFO: Pod "pod-configmaps-cb4ce185-8948-4173-98ab-7bc65b155015": Phase="Running", Reason="", readiness=false. Elapsed: 2.022797859s
Jan 19 21:14:58.394: INFO: Pod "pod-configmaps-cb4ce185-8948-4173-98ab-7bc65b155015" satisfied condition "running"
STEP: Waiting for pod with text data 01/19/23 21:14:58.394
STEP: Waiting for pod with binary data 01/19/23 21:14:58.399
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:14:58.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6149" for this suite. 01/19/23 21:14:58.408
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":106,"skipped":1839,"failed":0}
------------------------------
• [2.124 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:14:56.291
    Jan 19 21:14:56.291: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:14:56.292
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:14:56.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:14:56.314
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    Jan 19 21:14:56.330: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-6b947c4e-6565-45ec-acc9-4b204cb03ecf 01/19/23 21:14:56.33
    STEP: Creating the pod 01/19/23 21:14:56.344
    Jan 19 21:14:56.371: INFO: Waiting up to 5m0s for pod "pod-configmaps-cb4ce185-8948-4173-98ab-7bc65b155015" in namespace "configmap-6149" to be "running"
    Jan 19 21:14:56.391: INFO: Pod "pod-configmaps-cb4ce185-8948-4173-98ab-7bc65b155015": Phase="Pending", Reason="", readiness=false. Elapsed: 19.311756ms
    Jan 19 21:14:58.394: INFO: Pod "pod-configmaps-cb4ce185-8948-4173-98ab-7bc65b155015": Phase="Running", Reason="", readiness=false. Elapsed: 2.022797859s
    Jan 19 21:14:58.394: INFO: Pod "pod-configmaps-cb4ce185-8948-4173-98ab-7bc65b155015" satisfied condition "running"
    STEP: Waiting for pod with text data 01/19/23 21:14:58.394
    STEP: Waiting for pod with binary data 01/19/23 21:14:58.399
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:14:58.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6149" for this suite. 01/19/23 21:14:58.408
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:14:58.415
Jan 19 21:14:58.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename sched-preemption 01/19/23 21:14:58.416
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:14:58.434
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:14:58.436
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 19 21:14:58.460: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 21:15:58.656: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:15:58.662
Jan 19 21:15:58.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename sched-preemption-path 01/19/23 21:15:58.663
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:15:58.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:15:58.679
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 01/19/23 21:15:58.695
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/19/23 21:15:58.695
Jan 19 21:15:58.728: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1780" to be "running"
Jan 19 21:15:58.733: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.040953ms
Jan 19 21:16:00.736: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008252429s
Jan 19 21:16:00.736: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/19/23 21:16:00.738
Jan 19 21:16:00.747: INFO: found a healthy node: ip-10-0-172-44.ec2.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jan 19 21:16:12.820: INFO: pods created so far: [1 1 1]
Jan 19 21:16:12.820: INFO: length of pods created so far: 3
Jan 19 21:16:14.837: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jan 19 21:16:21.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1780" for this suite. 01/19/23 21:16:21.843
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:16:21.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-808" for this suite. 01/19/23 21:16:21.884
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":107,"skipped":1839,"failed":0}
------------------------------
• [SLOW TEST] [83.581 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:14:58.415
    Jan 19 21:14:58.415: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename sched-preemption 01/19/23 21:14:58.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:14:58.434
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:14:58.436
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 19 21:14:58.460: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 19 21:15:58.656: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:15:58.662
    Jan 19 21:15:58.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename sched-preemption-path 01/19/23 21:15:58.663
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:15:58.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:15:58.679
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 01/19/23 21:15:58.695
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/19/23 21:15:58.695
    Jan 19 21:15:58.728: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1780" to be "running"
    Jan 19 21:15:58.733: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 5.040953ms
    Jan 19 21:16:00.736: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008252429s
    Jan 19 21:16:00.736: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/19/23 21:16:00.738
    Jan 19 21:16:00.747: INFO: found a healthy node: ip-10-0-172-44.ec2.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jan 19 21:16:12.820: INFO: pods created so far: [1 1 1]
    Jan 19 21:16:12.820: INFO: length of pods created so far: 3
    Jan 19 21:16:14.837: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jan 19 21:16:21.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-1780" for this suite. 01/19/23 21:16:21.843
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:16:21.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-808" for this suite. 01/19/23 21:16:21.884
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:16:21.996
Jan 19 21:16:21.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 21:16:21.997
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:16:22.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:16:22.03
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 01/19/23 21:16:22.032
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/19/23 21:16:22.039
STEP: patching the secret 01/19/23 21:16:22.269
STEP: deleting the secret using a LabelSelector 01/19/23 21:16:22.278
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/19/23 21:16:22.287
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 19 21:16:22.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-341" for this suite. 01/19/23 21:16:22.392
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":108,"skipped":1855,"failed":0}
------------------------------
• [0.402 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:16:21.996
    Jan 19 21:16:21.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 21:16:21.997
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:16:22.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:16:22.03
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 01/19/23 21:16:22.032
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/19/23 21:16:22.039
    STEP: patching the secret 01/19/23 21:16:22.269
    STEP: deleting the secret using a LabelSelector 01/19/23 21:16:22.278
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/19/23 21:16:22.287
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 21:16:22.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-341" for this suite. 01/19/23 21:16:22.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:16:22.399
Jan 19 21:16:22.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename crd-watch 01/19/23 21:16:22.4
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:16:22.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:16:22.416
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan 19 21:16:22.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Creating first CR  01/19/23 21:16:24.974
Jan 19 21:16:25.001: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T21:16:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T21:16:24Z]] name:name1 resourceVersion:172651 uid:fdfceebc-2bf3-4733-b5f8-1972459b4c4d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/19/23 21:16:35.002
Jan 19 21:16:35.010: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T21:16:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T21:16:35Z]] name:name2 resourceVersion:172934 uid:3c1c881d-8b3a-40a3-b72f-fa9d8eec7a19] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/19/23 21:16:45.013
Jan 19 21:16:45.019: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T21:16:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T21:16:45Z]] name:name1 resourceVersion:173024 uid:fdfceebc-2bf3-4733-b5f8-1972459b4c4d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/19/23 21:16:55.023
Jan 19 21:16:55.029: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T21:16:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T21:16:55Z]] name:name2 resourceVersion:173103 uid:3c1c881d-8b3a-40a3-b72f-fa9d8eec7a19] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/19/23 21:17:05.031
Jan 19 21:17:05.038: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T21:16:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T21:16:45Z]] name:name1 resourceVersion:173189 uid:fdfceebc-2bf3-4733-b5f8-1972459b4c4d] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/19/23 21:17:15.042
Jan 19 21:17:15.048: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T21:16:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T21:16:55Z]] name:name2 resourceVersion:173274 uid:3c1c881d-8b3a-40a3-b72f-fa9d8eec7a19] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:17:25.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5460" for this suite. 01/19/23 21:17:25.566
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":109,"skipped":1861,"failed":0}
------------------------------
• [SLOW TEST] [63.172 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:16:22.399
    Jan 19 21:16:22.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename crd-watch 01/19/23 21:16:22.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:16:22.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:16:22.416
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan 19 21:16:22.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Creating first CR  01/19/23 21:16:24.974
    Jan 19 21:16:25.001: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T21:16:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T21:16:24Z]] name:name1 resourceVersion:172651 uid:fdfceebc-2bf3-4733-b5f8-1972459b4c4d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/19/23 21:16:35.002
    Jan 19 21:16:35.010: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T21:16:35Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T21:16:35Z]] name:name2 resourceVersion:172934 uid:3c1c881d-8b3a-40a3-b72f-fa9d8eec7a19] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/19/23 21:16:45.013
    Jan 19 21:16:45.019: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T21:16:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T21:16:45Z]] name:name1 resourceVersion:173024 uid:fdfceebc-2bf3-4733-b5f8-1972459b4c4d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/19/23 21:16:55.023
    Jan 19 21:16:55.029: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T21:16:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T21:16:55Z]] name:name2 resourceVersion:173103 uid:3c1c881d-8b3a-40a3-b72f-fa9d8eec7a19] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/19/23 21:17:05.031
    Jan 19 21:17:05.038: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T21:16:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T21:16:45Z]] name:name1 resourceVersion:173189 uid:fdfceebc-2bf3-4733-b5f8-1972459b4c4d] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/19/23 21:17:15.042
    Jan 19 21:17:15.048: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-19T21:16:35Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-19T21:16:55Z]] name:name2 resourceVersion:173274 uid:3c1c881d-8b3a-40a3-b72f-fa9d8eec7a19] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:17:25.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-5460" for this suite. 01/19/23 21:17:25.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:17:25.572
Jan 19 21:17:25.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-runtime 01/19/23 21:17:25.573
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:17:25.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:17:25.587
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/19/23 21:17:25.702
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/19/23 21:17:43.801
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/19/23 21:17:43.805
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/19/23 21:17:43.81
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/19/23 21:17:43.81
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/19/23 21:17:43.835
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/19/23 21:17:45.842
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/19/23 21:17:47.85
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/19/23 21:17:47.855
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/19/23 21:17:47.855
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/19/23 21:17:47.878
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/19/23 21:17:48.883
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/19/23 21:17:50.892
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/19/23 21:17:50.896
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/19/23 21:17:50.896
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 19 21:17:50.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5500" for this suite. 01/19/23 21:17:50.921
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":110,"skipped":1893,"failed":0}
------------------------------
• [SLOW TEST] [25.354 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:17:25.572
    Jan 19 21:17:25.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-runtime 01/19/23 21:17:25.573
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:17:25.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:17:25.587
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/19/23 21:17:25.702
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/19/23 21:17:43.801
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/19/23 21:17:43.805
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/19/23 21:17:43.81
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/19/23 21:17:43.81
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/19/23 21:17:43.835
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/19/23 21:17:45.842
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/19/23 21:17:47.85
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/19/23 21:17:47.855
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/19/23 21:17:47.855
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/19/23 21:17:47.878
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/19/23 21:17:48.883
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/19/23 21:17:50.892
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/19/23 21:17:50.896
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/19/23 21:17:50.896
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 19 21:17:50.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5500" for this suite. 01/19/23 21:17:50.921
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:17:50.926
Jan 19 21:17:50.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename prestop 01/19/23 21:17:50.927
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:17:50.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:17:50.943
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9581 01/19/23 21:17:50.945
W0119 21:17:50.982314      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Waiting for pods to come up. 01/19/23 21:17:50.982
Jan 19 21:17:50.982: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9581" to be "running"
Jan 19 21:17:50.986: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293747ms
Jan 19 21:17:52.990: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008012227s
Jan 19 21:17:52.990: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9581 01/19/23 21:17:52.992
Jan 19 21:17:53.002: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9581" to be "running"
Jan 19 21:17:53.005: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346176ms
Jan 19 21:17:55.008: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.005904069s
Jan 19 21:17:55.008: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/19/23 21:17:55.008
Jan 19 21:18:00.020: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/19/23 21:18:00.02
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jan 19 21:18:00.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9581" for this suite. 01/19/23 21:18:00.034
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":111,"skipped":1897,"failed":0}
------------------------------
• [SLOW TEST] [9.113 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:17:50.926
    Jan 19 21:17:50.926: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename prestop 01/19/23 21:17:50.927
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:17:50.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:17:50.943
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9581 01/19/23 21:17:50.945
    W0119 21:17:50.982314      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Waiting for pods to come up. 01/19/23 21:17:50.982
    Jan 19 21:17:50.982: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9581" to be "running"
    Jan 19 21:17:50.986: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293747ms
    Jan 19 21:17:52.990: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.008012227s
    Jan 19 21:17:52.990: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9581 01/19/23 21:17:52.992
    Jan 19 21:17:53.002: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9581" to be "running"
    Jan 19 21:17:53.005: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346176ms
    Jan 19 21:17:55.008: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.005904069s
    Jan 19 21:17:55.008: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/19/23 21:17:55.008
    Jan 19 21:18:00.020: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/19/23 21:18:00.02
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jan 19 21:18:00.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-9581" for this suite. 01/19/23 21:18:00.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:18:00.04
Jan 19 21:18:00.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:18:00.041
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:00.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:00.054
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:18:00.088
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:18:00.541
STEP: Deploying the webhook pod 01/19/23 21:18:00.552
STEP: Wait for the deployment to be ready 01/19/23 21:18:00.563
Jan 19 21:18:00.571: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/19/23 21:18:02.58
STEP: Verifying the service has paired with the endpoint 01/19/23 21:18:02.592
Jan 19 21:18:03.592: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 01/19/23 21:18:03.646
STEP: Creating a configMap that should be mutated 01/19/23 21:18:03.657
STEP: Deleting the collection of validation webhooks 01/19/23 21:18:03.68
STEP: Creating a configMap that should not be mutated 01/19/23 21:18:03.725
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:18:03.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1892" for this suite. 01/19/23 21:18:03.742
STEP: Destroying namespace "webhook-1892-markers" for this suite. 01/19/23 21:18:03.748
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":112,"skipped":1909,"failed":0}
------------------------------
• [3.750 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:18:00.04
    Jan 19 21:18:00.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:18:00.041
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:00.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:00.054
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:18:00.088
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:18:00.541
    STEP: Deploying the webhook pod 01/19/23 21:18:00.552
    STEP: Wait for the deployment to be ready 01/19/23 21:18:00.563
    Jan 19 21:18:00.571: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/19/23 21:18:02.58
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:18:02.592
    Jan 19 21:18:03.592: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 01/19/23 21:18:03.646
    STEP: Creating a configMap that should be mutated 01/19/23 21:18:03.657
    STEP: Deleting the collection of validation webhooks 01/19/23 21:18:03.68
    STEP: Creating a configMap that should not be mutated 01/19/23 21:18:03.725
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:18:03.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1892" for this suite. 01/19/23 21:18:03.742
    STEP: Destroying namespace "webhook-1892-markers" for this suite. 01/19/23 21:18:03.748
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:18:03.791
Jan 19 21:18:03.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:18:03.792
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:03.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:03.815
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-539e54fe-ac77-4100-8e44-34bf68b72afa 01/19/23 21:18:03.825
STEP: Creating secret with name secret-projected-all-test-volume-54e41eda-cee5-496d-a8f4-9cc6328dc73b 01/19/23 21:18:03.84
STEP: Creating a pod to test Check all projections for projected volume plugin 01/19/23 21:18:03.845
Jan 19 21:18:03.873: INFO: Waiting up to 5m0s for pod "projected-volume-58fa9042-25be-4979-af0e-244455102be6" in namespace "projected-4701" to be "Succeeded or Failed"
Jan 19 21:18:03.879: INFO: Pod "projected-volume-58fa9042-25be-4979-af0e-244455102be6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.55745ms
Jan 19 21:18:05.882: INFO: Pod "projected-volume-58fa9042-25be-4979-af0e-244455102be6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009492762s
Jan 19 21:18:07.884: INFO: Pod "projected-volume-58fa9042-25be-4979-af0e-244455102be6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011450558s
STEP: Saw pod success 01/19/23 21:18:07.884
Jan 19 21:18:07.884: INFO: Pod "projected-volume-58fa9042-25be-4979-af0e-244455102be6" satisfied condition "Succeeded or Failed"
Jan 19 21:18:07.887: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod projected-volume-58fa9042-25be-4979-af0e-244455102be6 container projected-all-volume-test: <nil>
STEP: delete the pod 01/19/23 21:18:07.895
Jan 19 21:18:07.904: INFO: Waiting for pod projected-volume-58fa9042-25be-4979-af0e-244455102be6 to disappear
Jan 19 21:18:07.908: INFO: Pod projected-volume-58fa9042-25be-4979-af0e-244455102be6 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jan 19 21:18:07.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4701" for this suite. 01/19/23 21:18:07.912
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":113,"skipped":1924,"failed":0}
------------------------------
• [4.126 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:18:03.791
    Jan 19 21:18:03.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:18:03.792
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:03.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:03.815
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-539e54fe-ac77-4100-8e44-34bf68b72afa 01/19/23 21:18:03.825
    STEP: Creating secret with name secret-projected-all-test-volume-54e41eda-cee5-496d-a8f4-9cc6328dc73b 01/19/23 21:18:03.84
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/19/23 21:18:03.845
    Jan 19 21:18:03.873: INFO: Waiting up to 5m0s for pod "projected-volume-58fa9042-25be-4979-af0e-244455102be6" in namespace "projected-4701" to be "Succeeded or Failed"
    Jan 19 21:18:03.879: INFO: Pod "projected-volume-58fa9042-25be-4979-af0e-244455102be6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.55745ms
    Jan 19 21:18:05.882: INFO: Pod "projected-volume-58fa9042-25be-4979-af0e-244455102be6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009492762s
    Jan 19 21:18:07.884: INFO: Pod "projected-volume-58fa9042-25be-4979-af0e-244455102be6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011450558s
    STEP: Saw pod success 01/19/23 21:18:07.884
    Jan 19 21:18:07.884: INFO: Pod "projected-volume-58fa9042-25be-4979-af0e-244455102be6" satisfied condition "Succeeded or Failed"
    Jan 19 21:18:07.887: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod projected-volume-58fa9042-25be-4979-af0e-244455102be6 container projected-all-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:18:07.895
    Jan 19 21:18:07.904: INFO: Waiting for pod projected-volume-58fa9042-25be-4979-af0e-244455102be6 to disappear
    Jan 19 21:18:07.908: INFO: Pod projected-volume-58fa9042-25be-4979-af0e-244455102be6 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jan 19 21:18:07.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4701" for this suite. 01/19/23 21:18:07.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:18:07.918
Jan 19 21:18:07.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename runtimeclass 01/19/23 21:18:07.918
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:07.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:07.946
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/19/23 21:18:07.948
STEP: getting /apis/node.k8s.io 01/19/23 21:18:07.95
STEP: getting /apis/node.k8s.io/v1 01/19/23 21:18:07.951
STEP: creating 01/19/23 21:18:07.951
STEP: watching 01/19/23 21:18:07.972
Jan 19 21:18:07.972: INFO: starting watch
STEP: getting 01/19/23 21:18:07.976
STEP: listing 01/19/23 21:18:07.978
STEP: patching 01/19/23 21:18:07.985
STEP: updating 01/19/23 21:18:07.997
Jan 19 21:18:08.003: INFO: waiting for watch events with expected annotations
STEP: deleting 01/19/23 21:18:08.003
STEP: deleting a collection 01/19/23 21:18:08.014
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 19 21:18:08.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-6328" for this suite. 01/19/23 21:18:08.031
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":114,"skipped":1940,"failed":0}
------------------------------
• [0.119 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:18:07.918
    Jan 19 21:18:07.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename runtimeclass 01/19/23 21:18:07.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:07.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:07.946
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/19/23 21:18:07.948
    STEP: getting /apis/node.k8s.io 01/19/23 21:18:07.95
    STEP: getting /apis/node.k8s.io/v1 01/19/23 21:18:07.951
    STEP: creating 01/19/23 21:18:07.951
    STEP: watching 01/19/23 21:18:07.972
    Jan 19 21:18:07.972: INFO: starting watch
    STEP: getting 01/19/23 21:18:07.976
    STEP: listing 01/19/23 21:18:07.978
    STEP: patching 01/19/23 21:18:07.985
    STEP: updating 01/19/23 21:18:07.997
    Jan 19 21:18:08.003: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/19/23 21:18:08.003
    STEP: deleting a collection 01/19/23 21:18:08.014
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 19 21:18:08.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-6328" for this suite. 01/19/23 21:18:08.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:18:08.038
Jan 19 21:18:08.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:18:08.039
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:08.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:08.05
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:18:08.105
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:18:08.462
STEP: Deploying the webhook pod 01/19/23 21:18:08.47
STEP: Wait for the deployment to be ready 01/19/23 21:18:08.481
Jan 19 21:18:08.489: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/19/23 21:18:10.517
STEP: Verifying the service has paired with the endpoint 01/19/23 21:18:10.536
Jan 19 21:18:11.536: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/19/23 21:18:11.539
STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 21:18:11.539
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/19/23 21:18:11.552
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/19/23 21:18:12.559
STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 21:18:12.559
STEP: Having no error when timeout is longer than webhook latency 01/19/23 21:18:13.582
STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 21:18:13.582
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/19/23 21:18:18.614
STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 21:18:18.615
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:18:23.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5963" for this suite. 01/19/23 21:18:23.644
STEP: Destroying namespace "webhook-5963-markers" for this suite. 01/19/23 21:18:23.652
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":115,"skipped":1965,"failed":0}
------------------------------
• [SLOW TEST] [15.661 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:18:08.038
    Jan 19 21:18:08.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:18:08.039
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:08.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:08.05
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:18:08.105
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:18:08.462
    STEP: Deploying the webhook pod 01/19/23 21:18:08.47
    STEP: Wait for the deployment to be ready 01/19/23 21:18:08.481
    Jan 19 21:18:08.489: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/19/23 21:18:10.517
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:18:10.536
    Jan 19 21:18:11.536: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/19/23 21:18:11.539
    STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 21:18:11.539
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/19/23 21:18:11.552
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/19/23 21:18:12.559
    STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 21:18:12.559
    STEP: Having no error when timeout is longer than webhook latency 01/19/23 21:18:13.582
    STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 21:18:13.582
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/19/23 21:18:18.614
    STEP: Registering slow webhook via the AdmissionRegistration API 01/19/23 21:18:18.615
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:18:23.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5963" for this suite. 01/19/23 21:18:23.644
    STEP: Destroying namespace "webhook-5963-markers" for this suite. 01/19/23 21:18:23.652
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:18:23.7
Jan 19 21:18:23.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename var-expansion 01/19/23 21:18:23.701
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:23.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:23.735
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 01/19/23 21:18:23.737
W0119 21:18:23.796779      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:18:23.796: INFO: Waiting up to 5m0s for pod "var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499" in namespace "var-expansion-7160" to be "Succeeded or Failed"
Jan 19 21:18:23.803: INFO: Pod "var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499": Phase="Pending", Reason="", readiness=false. Elapsed: 6.98317ms
Jan 19 21:18:25.806: INFO: Pod "var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010074556s
Jan 19 21:18:27.807: INFO: Pod "var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010554926s
STEP: Saw pod success 01/19/23 21:18:27.807
Jan 19 21:18:27.807: INFO: Pod "var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499" satisfied condition "Succeeded or Failed"
Jan 19 21:18:27.809: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499 container dapi-container: <nil>
STEP: delete the pod 01/19/23 21:18:27.814
Jan 19 21:18:27.823: INFO: Waiting for pod var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499 to disappear
Jan 19 21:18:27.826: INFO: Pod var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 21:18:27.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7160" for this suite. 01/19/23 21:18:27.83
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":116,"skipped":1973,"failed":0}
------------------------------
• [4.136 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:18:23.7
    Jan 19 21:18:23.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename var-expansion 01/19/23 21:18:23.701
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:23.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:23.735
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 01/19/23 21:18:23.737
    W0119 21:18:23.796779      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:18:23.796: INFO: Waiting up to 5m0s for pod "var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499" in namespace "var-expansion-7160" to be "Succeeded or Failed"
    Jan 19 21:18:23.803: INFO: Pod "var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499": Phase="Pending", Reason="", readiness=false. Elapsed: 6.98317ms
    Jan 19 21:18:25.806: INFO: Pod "var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010074556s
    Jan 19 21:18:27.807: INFO: Pod "var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010554926s
    STEP: Saw pod success 01/19/23 21:18:27.807
    Jan 19 21:18:27.807: INFO: Pod "var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499" satisfied condition "Succeeded or Failed"
    Jan 19 21:18:27.809: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499 container dapi-container: <nil>
    STEP: delete the pod 01/19/23 21:18:27.814
    Jan 19 21:18:27.823: INFO: Waiting for pod var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499 to disappear
    Jan 19 21:18:27.826: INFO: Pod var-expansion-8e8352fd-7a1d-4e63-ab68-60f6cfb4d499 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 21:18:27.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7160" for this suite. 01/19/23 21:18:27.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:18:27.837
Jan 19 21:18:27.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename var-expansion 01/19/23 21:18:27.837
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:27.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:27.85
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jan 19 21:18:27.997: INFO: Waiting up to 2m0s for pod "var-expansion-1339c4ee-d886-4b50-87c2-34a33fbb6cf7" in namespace "var-expansion-230" to be "container 0 failed with reason CreateContainerConfigError"
Jan 19 21:18:28.004: INFO: Pod "var-expansion-1339c4ee-d886-4b50-87c2-34a33fbb6cf7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.271309ms
Jan 19 21:18:30.008: INFO: Pod "var-expansion-1339c4ee-d886-4b50-87c2-34a33fbb6cf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010816794s
Jan 19 21:18:30.008: INFO: Pod "var-expansion-1339c4ee-d886-4b50-87c2-34a33fbb6cf7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 19 21:18:30.008: INFO: Deleting pod "var-expansion-1339c4ee-d886-4b50-87c2-34a33fbb6cf7" in namespace "var-expansion-230"
Jan 19 21:18:30.014: INFO: Wait up to 5m0s for pod "var-expansion-1339c4ee-d886-4b50-87c2-34a33fbb6cf7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 21:18:32.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-230" for this suite. 01/19/23 21:18:32.024
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":117,"skipped":1987,"failed":0}
------------------------------
• [4.194 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:18:27.837
    Jan 19 21:18:27.837: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename var-expansion 01/19/23 21:18:27.837
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:27.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:27.85
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jan 19 21:18:27.997: INFO: Waiting up to 2m0s for pod "var-expansion-1339c4ee-d886-4b50-87c2-34a33fbb6cf7" in namespace "var-expansion-230" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 19 21:18:28.004: INFO: Pod "var-expansion-1339c4ee-d886-4b50-87c2-34a33fbb6cf7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.271309ms
    Jan 19 21:18:30.008: INFO: Pod "var-expansion-1339c4ee-d886-4b50-87c2-34a33fbb6cf7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010816794s
    Jan 19 21:18:30.008: INFO: Pod "var-expansion-1339c4ee-d886-4b50-87c2-34a33fbb6cf7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 19 21:18:30.008: INFO: Deleting pod "var-expansion-1339c4ee-d886-4b50-87c2-34a33fbb6cf7" in namespace "var-expansion-230"
    Jan 19 21:18:30.014: INFO: Wait up to 5m0s for pod "var-expansion-1339c4ee-d886-4b50-87c2-34a33fbb6cf7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 21:18:32.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-230" for this suite. 01/19/23 21:18:32.024
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:18:32.031
Jan 19 21:18:32.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename containers 01/19/23 21:18:32.032
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:32.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:32.045
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 01/19/23 21:18:32.051
Jan 19 21:18:32.105: INFO: Waiting up to 5m0s for pod "client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e" in namespace "containers-535" to be "Succeeded or Failed"
Jan 19 21:18:32.115: INFO: Pod "client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.790017ms
Jan 19 21:18:34.118: INFO: Pod "client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012926785s
Jan 19 21:18:36.118: INFO: Pod "client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012896204s
STEP: Saw pod success 01/19/23 21:18:36.118
Jan 19 21:18:36.118: INFO: Pod "client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e" satisfied condition "Succeeded or Failed"
Jan 19 21:18:36.120: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:18:36.125
Jan 19 21:18:36.134: INFO: Waiting for pod client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e to disappear
Jan 19 21:18:36.136: INFO: Pod client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 19 21:18:36.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-535" for this suite. 01/19/23 21:18:36.14
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":118,"skipped":2013,"failed":0}
------------------------------
• [4.114 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:18:32.031
    Jan 19 21:18:32.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename containers 01/19/23 21:18:32.032
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:32.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:32.045
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 01/19/23 21:18:32.051
    Jan 19 21:18:32.105: INFO: Waiting up to 5m0s for pod "client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e" in namespace "containers-535" to be "Succeeded or Failed"
    Jan 19 21:18:32.115: INFO: Pod "client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.790017ms
    Jan 19 21:18:34.118: INFO: Pod "client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012926785s
    Jan 19 21:18:36.118: INFO: Pod "client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012896204s
    STEP: Saw pod success 01/19/23 21:18:36.118
    Jan 19 21:18:36.118: INFO: Pod "client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e" satisfied condition "Succeeded or Failed"
    Jan 19 21:18:36.120: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:18:36.125
    Jan 19 21:18:36.134: INFO: Waiting for pod client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e to disappear
    Jan 19 21:18:36.136: INFO: Pod client-containers-8b8be1a0-ad63-4970-91fe-07e25dbba08e no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 19 21:18:36.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-535" for this suite. 01/19/23 21:18:36.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:18:36.147
Jan 19 21:18:36.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename statefulset 01/19/23 21:18:36.147
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:36.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:36.165
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7891 01/19/23 21:18:36.171
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-7891 01/19/23 21:18:36.205
Jan 19 21:18:36.224: INFO: Found 0 stateful pods, waiting for 1
Jan 19 21:18:46.228: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/19/23 21:18:46.233
STEP: Getting /status 01/19/23 21:18:46.244
Jan 19 21:18:46.247: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/19/23 21:18:46.247
Jan 19 21:18:46.256: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/19/23 21:18:46.256
Jan 19 21:18:46.258: INFO: Observed &StatefulSet event: ADDED
Jan 19 21:18:46.258: INFO: Found Statefulset ss in namespace statefulset-7891 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 21:18:46.258: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/19/23 21:18:46.258
Jan 19 21:18:46.258: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 19 21:18:46.264: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/19/23 21:18:46.264
Jan 19 21:18:46.265: INFO: Observed &StatefulSet event: ADDED
Jan 19 21:18:46.265: INFO: Observed Statefulset ss in namespace statefulset-7891 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 21:18:46.266: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 21:18:46.266: INFO: Deleting all statefulset in ns statefulset-7891
Jan 19 21:18:46.268: INFO: Scaling statefulset ss to 0
Jan 19 21:18:56.284: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 21:18:56.287: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 21:18:56.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7891" for this suite. 01/19/23 21:18:56.307
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":119,"skipped":2062,"failed":0}
------------------------------
• [SLOW TEST] [20.166 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:18:36.147
    Jan 19 21:18:36.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename statefulset 01/19/23 21:18:36.147
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:36.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:36.165
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7891 01/19/23 21:18:36.171
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-7891 01/19/23 21:18:36.205
    Jan 19 21:18:36.224: INFO: Found 0 stateful pods, waiting for 1
    Jan 19 21:18:46.228: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/19/23 21:18:46.233
    STEP: Getting /status 01/19/23 21:18:46.244
    Jan 19 21:18:46.247: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/19/23 21:18:46.247
    Jan 19 21:18:46.256: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/19/23 21:18:46.256
    Jan 19 21:18:46.258: INFO: Observed &StatefulSet event: ADDED
    Jan 19 21:18:46.258: INFO: Found Statefulset ss in namespace statefulset-7891 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 19 21:18:46.258: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/19/23 21:18:46.258
    Jan 19 21:18:46.258: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 19 21:18:46.264: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/19/23 21:18:46.264
    Jan 19 21:18:46.265: INFO: Observed &StatefulSet event: ADDED
    Jan 19 21:18:46.265: INFO: Observed Statefulset ss in namespace statefulset-7891 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 19 21:18:46.266: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 21:18:46.266: INFO: Deleting all statefulset in ns statefulset-7891
    Jan 19 21:18:46.268: INFO: Scaling statefulset ss to 0
    Jan 19 21:18:56.284: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 21:18:56.287: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 21:18:56.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7891" for this suite. 01/19/23 21:18:56.307
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:18:56.313
Jan 19 21:18:56.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename security-context 01/19/23 21:18:56.314
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:56.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:56.339
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/19/23 21:18:56.341
Jan 19 21:18:56.374: INFO: Waiting up to 5m0s for pod "security-context-145a1cf4-832b-4e51-96a8-ba10c336b597" in namespace "security-context-6318" to be "Succeeded or Failed"
Jan 19 21:18:56.376: INFO: Pod "security-context-145a1cf4-832b-4e51-96a8-ba10c336b597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.172206ms
Jan 19 21:18:58.380: INFO: Pod "security-context-145a1cf4-832b-4e51-96a8-ba10c336b597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005731275s
Jan 19 21:19:00.381: INFO: Pod "security-context-145a1cf4-832b-4e51-96a8-ba10c336b597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006707282s
STEP: Saw pod success 01/19/23 21:19:00.381
Jan 19 21:19:00.381: INFO: Pod "security-context-145a1cf4-832b-4e51-96a8-ba10c336b597" satisfied condition "Succeeded or Failed"
Jan 19 21:19:00.383: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod security-context-145a1cf4-832b-4e51-96a8-ba10c336b597 container test-container: <nil>
STEP: delete the pod 01/19/23 21:19:00.388
Jan 19 21:19:00.397: INFO: Waiting for pod security-context-145a1cf4-832b-4e51-96a8-ba10c336b597 to disappear
Jan 19 21:19:00.399: INFO: Pod security-context-145a1cf4-832b-4e51-96a8-ba10c336b597 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 19 21:19:00.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6318" for this suite. 01/19/23 21:19:00.403
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":120,"skipped":2065,"failed":0}
------------------------------
• [4.095 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:18:56.313
    Jan 19 21:18:56.313: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename security-context 01/19/23 21:18:56.314
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:18:56.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:18:56.339
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/19/23 21:18:56.341
    Jan 19 21:18:56.374: INFO: Waiting up to 5m0s for pod "security-context-145a1cf4-832b-4e51-96a8-ba10c336b597" in namespace "security-context-6318" to be "Succeeded or Failed"
    Jan 19 21:18:56.376: INFO: Pod "security-context-145a1cf4-832b-4e51-96a8-ba10c336b597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.172206ms
    Jan 19 21:18:58.380: INFO: Pod "security-context-145a1cf4-832b-4e51-96a8-ba10c336b597": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005731275s
    Jan 19 21:19:00.381: INFO: Pod "security-context-145a1cf4-832b-4e51-96a8-ba10c336b597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006707282s
    STEP: Saw pod success 01/19/23 21:19:00.381
    Jan 19 21:19:00.381: INFO: Pod "security-context-145a1cf4-832b-4e51-96a8-ba10c336b597" satisfied condition "Succeeded or Failed"
    Jan 19 21:19:00.383: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod security-context-145a1cf4-832b-4e51-96a8-ba10c336b597 container test-container: <nil>
    STEP: delete the pod 01/19/23 21:19:00.388
    Jan 19 21:19:00.397: INFO: Waiting for pod security-context-145a1cf4-832b-4e51-96a8-ba10c336b597 to disappear
    Jan 19 21:19:00.399: INFO: Pod security-context-145a1cf4-832b-4e51-96a8-ba10c336b597 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 19 21:19:00.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-6318" for this suite. 01/19/23 21:19:00.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:19:00.41
Jan 19 21:19:00.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubelet-test 01/19/23 21:19:00.41
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:00.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:00.424
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 19 21:19:00.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2745" for this suite. 01/19/23 21:19:00.5
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":121,"skipped":2152,"failed":0}
------------------------------
• [0.098 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:19:00.41
    Jan 19 21:19:00.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubelet-test 01/19/23 21:19:00.41
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:00.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:00.424
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 19 21:19:00.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2745" for this suite. 01/19/23 21:19:00.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:19:00.509
Jan 19 21:19:00.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 21:19:00.51
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:00.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:00.536
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-2123 01/19/23 21:19:00.538
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2123 to expose endpoints map[] 01/19/23 21:19:00.568
Jan 19 21:19:00.574: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan 19 21:19:01.582: INFO: successfully validated that service endpoint-test2 in namespace services-2123 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-2123 01/19/23 21:19:01.582
Jan 19 21:19:01.599: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2123" to be "running and ready"
Jan 19 21:19:01.607: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.762523ms
Jan 19 21:19:01.607: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:19:03.629: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.029709443s
Jan 19 21:19:03.629: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan 19 21:19:03.629: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2123 to expose endpoints map[pod1:[80]] 01/19/23 21:19:03.651
Jan 19 21:19:03.691: INFO: successfully validated that service endpoint-test2 in namespace services-2123 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/19/23 21:19:03.691
Jan 19 21:19:03.691: INFO: Creating new exec pod
Jan 19 21:19:03.710: INFO: Waiting up to 5m0s for pod "execpodjnmz9" in namespace "services-2123" to be "running"
Jan 19 21:19:03.712: INFO: Pod "execpodjnmz9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.330786ms
Jan 19 21:19:05.715: INFO: Pod "execpodjnmz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005198147s
Jan 19 21:19:05.715: INFO: Pod "execpodjnmz9" satisfied condition "running"
Jan 19 21:19:06.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-2123 exec execpodjnmz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 19 21:19:06.850: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 19 21:19:06.850: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:19:06.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-2123 exec execpodjnmz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.170.154 80'
Jan 19 21:19:06.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.170.154 80\nConnection to 172.30.170.154 80 port [tcp/http] succeeded!\n"
Jan 19 21:19:06.962: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-2123 01/19/23 21:19:06.962
Jan 19 21:19:06.975: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2123" to be "running and ready"
Jan 19 21:19:06.977: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.414795ms
Jan 19 21:19:06.977: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:19:08.980: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005479999s
Jan 19 21:19:08.980: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan 19 21:19:08.980: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2123 to expose endpoints map[pod1:[80] pod2:[80]] 01/19/23 21:19:08.982
Jan 19 21:19:08.991: INFO: successfully validated that service endpoint-test2 in namespace services-2123 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/19/23 21:19:08.991
Jan 19 21:19:09.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-2123 exec execpodjnmz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 19 21:19:10.123: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 19 21:19:10.123: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:19:10.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-2123 exec execpodjnmz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.170.154 80'
Jan 19 21:19:10.242: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.170.154 80\nConnection to 172.30.170.154 80 port [tcp/http] succeeded!\n"
Jan 19 21:19:10.242: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-2123 01/19/23 21:19:10.242
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2123 to expose endpoints map[pod2:[80]] 01/19/23 21:19:10.252
Jan 19 21:19:10.265: INFO: successfully validated that service endpoint-test2 in namespace services-2123 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/19/23 21:19:10.265
Jan 19 21:19:11.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-2123 exec execpodjnmz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jan 19 21:19:11.400: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan 19 21:19:11.400: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:19:11.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-2123 exec execpodjnmz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.170.154 80'
Jan 19 21:19:11.526: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.170.154 80\nConnection to 172.30.170.154 80 port [tcp/http] succeeded!\n"
Jan 19 21:19:11.526: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-2123 01/19/23 21:19:11.526
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2123 to expose endpoints map[] 01/19/23 21:19:11.534
Jan 19 21:19:11.543: INFO: successfully validated that service endpoint-test2 in namespace services-2123 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 21:19:11.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2123" for this suite. 01/19/23 21:19:11.566
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":122,"skipped":2165,"failed":0}
------------------------------
• [SLOW TEST] [11.066 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:19:00.509
    Jan 19 21:19:00.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 21:19:00.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:00.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:00.536
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-2123 01/19/23 21:19:00.538
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2123 to expose endpoints map[] 01/19/23 21:19:00.568
    Jan 19 21:19:00.574: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan 19 21:19:01.582: INFO: successfully validated that service endpoint-test2 in namespace services-2123 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-2123 01/19/23 21:19:01.582
    Jan 19 21:19:01.599: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-2123" to be "running and ready"
    Jan 19 21:19:01.607: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.762523ms
    Jan 19 21:19:01.607: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:19:03.629: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.029709443s
    Jan 19 21:19:03.629: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan 19 21:19:03.629: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2123 to expose endpoints map[pod1:[80]] 01/19/23 21:19:03.651
    Jan 19 21:19:03.691: INFO: successfully validated that service endpoint-test2 in namespace services-2123 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/19/23 21:19:03.691
    Jan 19 21:19:03.691: INFO: Creating new exec pod
    Jan 19 21:19:03.710: INFO: Waiting up to 5m0s for pod "execpodjnmz9" in namespace "services-2123" to be "running"
    Jan 19 21:19:03.712: INFO: Pod "execpodjnmz9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.330786ms
    Jan 19 21:19:05.715: INFO: Pod "execpodjnmz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005198147s
    Jan 19 21:19:05.715: INFO: Pod "execpodjnmz9" satisfied condition "running"
    Jan 19 21:19:06.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-2123 exec execpodjnmz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 19 21:19:06.850: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 19 21:19:06.850: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:19:06.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-2123 exec execpodjnmz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.170.154 80'
    Jan 19 21:19:06.962: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.170.154 80\nConnection to 172.30.170.154 80 port [tcp/http] succeeded!\n"
    Jan 19 21:19:06.962: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-2123 01/19/23 21:19:06.962
    Jan 19 21:19:06.975: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-2123" to be "running and ready"
    Jan 19 21:19:06.977: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.414795ms
    Jan 19 21:19:06.977: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:19:08.980: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005479999s
    Jan 19 21:19:08.980: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan 19 21:19:08.980: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2123 to expose endpoints map[pod1:[80] pod2:[80]] 01/19/23 21:19:08.982
    Jan 19 21:19:08.991: INFO: successfully validated that service endpoint-test2 in namespace services-2123 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/19/23 21:19:08.991
    Jan 19 21:19:09.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-2123 exec execpodjnmz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 19 21:19:10.123: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 19 21:19:10.123: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:19:10.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-2123 exec execpodjnmz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.170.154 80'
    Jan 19 21:19:10.242: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.170.154 80\nConnection to 172.30.170.154 80 port [tcp/http] succeeded!\n"
    Jan 19 21:19:10.242: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-2123 01/19/23 21:19:10.242
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2123 to expose endpoints map[pod2:[80]] 01/19/23 21:19:10.252
    Jan 19 21:19:10.265: INFO: successfully validated that service endpoint-test2 in namespace services-2123 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/19/23 21:19:10.265
    Jan 19 21:19:11.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-2123 exec execpodjnmz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jan 19 21:19:11.400: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan 19 21:19:11.400: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:19:11.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-2123 exec execpodjnmz9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.170.154 80'
    Jan 19 21:19:11.526: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.170.154 80\nConnection to 172.30.170.154 80 port [tcp/http] succeeded!\n"
    Jan 19 21:19:11.526: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-2123 01/19/23 21:19:11.526
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2123 to expose endpoints map[] 01/19/23 21:19:11.534
    Jan 19 21:19:11.543: INFO: successfully validated that service endpoint-test2 in namespace services-2123 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 21:19:11.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2123" for this suite. 01/19/23 21:19:11.566
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:19:11.575
Jan 19 21:19:11.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename init-container 01/19/23 21:19:11.576
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:11.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:11.605
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 01/19/23 21:19:11.608
Jan 19 21:19:11.608: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 21:19:16.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7733" for this suite. 01/19/23 21:19:16.459
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":123,"skipped":2166,"failed":0}
------------------------------
• [4.889 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:19:11.575
    Jan 19 21:19:11.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename init-container 01/19/23 21:19:11.576
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:11.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:11.605
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 01/19/23 21:19:11.608
    Jan 19 21:19:11.608: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 21:19:16.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-7733" for this suite. 01/19/23 21:19:16.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:19:16.465
Jan 19 21:19:16.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:19:16.466
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:16.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:16.482
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jan 19 21:19:16.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/19/23 21:19:26.012
Jan 19 21:19:26.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-3255 --namespace=crd-publish-openapi-3255 create -f -'
Jan 19 21:19:27.576: INFO: stderr: ""
Jan 19 21:19:27.576: INFO: stdout: "e2e-test-crd-publish-openapi-9324-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 19 21:19:27.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-3255 --namespace=crd-publish-openapi-3255 delete e2e-test-crd-publish-openapi-9324-crds test-cr'
Jan 19 21:19:27.640: INFO: stderr: ""
Jan 19 21:19:27.640: INFO: stdout: "e2e-test-crd-publish-openapi-9324-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan 19 21:19:27.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-3255 --namespace=crd-publish-openapi-3255 apply -f -'
Jan 19 21:19:29.110: INFO: stderr: ""
Jan 19 21:19:29.110: INFO: stdout: "e2e-test-crd-publish-openapi-9324-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan 19 21:19:29.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-3255 --namespace=crd-publish-openapi-3255 delete e2e-test-crd-publish-openapi-9324-crds test-cr'
Jan 19 21:19:29.171: INFO: stderr: ""
Jan 19 21:19:29.171: INFO: stdout: "e2e-test-crd-publish-openapi-9324-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/19/23 21:19:29.171
Jan 19 21:19:29.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-3255 explain e2e-test-crd-publish-openapi-9324-crds'
Jan 19 21:19:29.524: INFO: stderr: ""
Jan 19 21:19:29.524: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9324-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:19:38.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3255" for this suite. 01/19/23 21:19:38.381
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":124,"skipped":2186,"failed":0}
------------------------------
• [SLOW TEST] [21.923 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:19:16.465
    Jan 19 21:19:16.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:19:16.466
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:16.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:16.482
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jan 19 21:19:16.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/19/23 21:19:26.012
    Jan 19 21:19:26.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-3255 --namespace=crd-publish-openapi-3255 create -f -'
    Jan 19 21:19:27.576: INFO: stderr: ""
    Jan 19 21:19:27.576: INFO: stdout: "e2e-test-crd-publish-openapi-9324-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 19 21:19:27.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-3255 --namespace=crd-publish-openapi-3255 delete e2e-test-crd-publish-openapi-9324-crds test-cr'
    Jan 19 21:19:27.640: INFO: stderr: ""
    Jan 19 21:19:27.640: INFO: stdout: "e2e-test-crd-publish-openapi-9324-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan 19 21:19:27.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-3255 --namespace=crd-publish-openapi-3255 apply -f -'
    Jan 19 21:19:29.110: INFO: stderr: ""
    Jan 19 21:19:29.110: INFO: stdout: "e2e-test-crd-publish-openapi-9324-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan 19 21:19:29.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-3255 --namespace=crd-publish-openapi-3255 delete e2e-test-crd-publish-openapi-9324-crds test-cr'
    Jan 19 21:19:29.171: INFO: stderr: ""
    Jan 19 21:19:29.171: INFO: stdout: "e2e-test-crd-publish-openapi-9324-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/19/23 21:19:29.171
    Jan 19 21:19:29.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-3255 explain e2e-test-crd-publish-openapi-9324-crds'
    Jan 19 21:19:29.524: INFO: stderr: ""
    Jan 19 21:19:29.524: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9324-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:19:38.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3255" for this suite. 01/19/23 21:19:38.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:19:38.389
Jan 19 21:19:38.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 21:19:38.389
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:38.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:38.443
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-4540/secret-test-4a9de3d8-5cf1-468c-862f-77d5e64d1e2b 01/19/23 21:19:38.462
STEP: Creating a pod to test consume secrets 01/19/23 21:19:38.505
Jan 19 21:19:38.672: INFO: Waiting up to 5m0s for pod "pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6" in namespace "secrets-4540" to be "Succeeded or Failed"
Jan 19 21:19:38.685: INFO: Pod "pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.304589ms
Jan 19 21:19:40.689: INFO: Pod "pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016271292s
Jan 19 21:19:42.687: INFO: Pod "pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014781333s
STEP: Saw pod success 01/19/23 21:19:42.687
Jan 19 21:19:42.687: INFO: Pod "pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6" satisfied condition "Succeeded or Failed"
Jan 19 21:19:42.689: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6 container env-test: <nil>
STEP: delete the pod 01/19/23 21:19:42.699
Jan 19 21:19:42.710: INFO: Waiting for pod pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6 to disappear
Jan 19 21:19:42.712: INFO: Pod pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 19 21:19:42.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4540" for this suite. 01/19/23 21:19:42.717
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":125,"skipped":2206,"failed":0}
------------------------------
• [4.335 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:19:38.389
    Jan 19 21:19:38.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 21:19:38.389
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:38.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:38.443
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-4540/secret-test-4a9de3d8-5cf1-468c-862f-77d5e64d1e2b 01/19/23 21:19:38.462
    STEP: Creating a pod to test consume secrets 01/19/23 21:19:38.505
    Jan 19 21:19:38.672: INFO: Waiting up to 5m0s for pod "pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6" in namespace "secrets-4540" to be "Succeeded or Failed"
    Jan 19 21:19:38.685: INFO: Pod "pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.304589ms
    Jan 19 21:19:40.689: INFO: Pod "pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016271292s
    Jan 19 21:19:42.687: INFO: Pod "pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014781333s
    STEP: Saw pod success 01/19/23 21:19:42.687
    Jan 19 21:19:42.687: INFO: Pod "pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6" satisfied condition "Succeeded or Failed"
    Jan 19 21:19:42.689: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6 container env-test: <nil>
    STEP: delete the pod 01/19/23 21:19:42.699
    Jan 19 21:19:42.710: INFO: Waiting for pod pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6 to disappear
    Jan 19 21:19:42.712: INFO: Pod pod-configmaps-15eb8c41-1e0f-44a3-a6cc-f4f756fd53b6 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 21:19:42.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4540" for this suite. 01/19/23 21:19:42.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:19:42.724
Jan 19 21:19:42.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:19:42.725
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:42.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:42.756
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-08f33499-91d4-4609-93f1-134739d4f6f0 01/19/23 21:19:42.759
STEP: Creating a pod to test consume secrets 01/19/23 21:19:42.782
Jan 19 21:19:42.826: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41" in namespace "projected-6310" to be "Succeeded or Failed"
Jan 19 21:19:42.833: INFO: Pod "pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41": Phase="Pending", Reason="", readiness=false. Elapsed: 6.90141ms
Jan 19 21:19:44.837: INFO: Pod "pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01073003s
Jan 19 21:19:46.836: INFO: Pod "pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009699466s
STEP: Saw pod success 01/19/23 21:19:46.836
Jan 19 21:19:46.836: INFO: Pod "pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41" satisfied condition "Succeeded or Failed"
Jan 19 21:19:46.838: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 21:19:46.843
Jan 19 21:19:46.853: INFO: Waiting for pod pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41 to disappear
Jan 19 21:19:46.856: INFO: Pod pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 21:19:46.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6310" for this suite. 01/19/23 21:19:46.86
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":126,"skipped":2234,"failed":0}
------------------------------
• [4.142 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:19:42.724
    Jan 19 21:19:42.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:19:42.725
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:42.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:42.756
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-08f33499-91d4-4609-93f1-134739d4f6f0 01/19/23 21:19:42.759
    STEP: Creating a pod to test consume secrets 01/19/23 21:19:42.782
    Jan 19 21:19:42.826: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41" in namespace "projected-6310" to be "Succeeded or Failed"
    Jan 19 21:19:42.833: INFO: Pod "pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41": Phase="Pending", Reason="", readiness=false. Elapsed: 6.90141ms
    Jan 19 21:19:44.837: INFO: Pod "pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01073003s
    Jan 19 21:19:46.836: INFO: Pod "pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009699466s
    STEP: Saw pod success 01/19/23 21:19:46.836
    Jan 19 21:19:46.836: INFO: Pod "pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41" satisfied condition "Succeeded or Failed"
    Jan 19 21:19:46.838: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:19:46.843
    Jan 19 21:19:46.853: INFO: Waiting for pod pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41 to disappear
    Jan 19 21:19:46.856: INFO: Pod pod-projected-secrets-625d90ad-0945-413c-bab7-b25bd7eb6f41 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 21:19:46.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6310" for this suite. 01/19/23 21:19:46.86
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:19:46.866
Jan 19 21:19:46.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 21:19:46.867
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:46.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:46.914
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:19:46.918
Jan 19 21:19:46.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1" in namespace "downward-api-5887" to be "Succeeded or Failed"
Jan 19 21:19:46.961: INFO: Pod "downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1": Phase="Pending", Reason="", readiness=false. Elapsed: 26.806642ms
Jan 19 21:19:48.965: INFO: Pod "downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030625548s
Jan 19 21:19:50.965: INFO: Pod "downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030564255s
STEP: Saw pod success 01/19/23 21:19:50.965
Jan 19 21:19:50.965: INFO: Pod "downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1" satisfied condition "Succeeded or Failed"
Jan 19 21:19:50.967: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1 container client-container: <nil>
STEP: delete the pod 01/19/23 21:19:50.972
Jan 19 21:19:50.982: INFO: Waiting for pod downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1 to disappear
Jan 19 21:19:50.984: INFO: Pod downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 21:19:50.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5887" for this suite. 01/19/23 21:19:50.988
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":127,"skipped":2235,"failed":0}
------------------------------
• [4.127 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:19:46.866
    Jan 19 21:19:46.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 21:19:46.867
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:46.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:46.914
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:19:46.918
    Jan 19 21:19:46.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1" in namespace "downward-api-5887" to be "Succeeded or Failed"
    Jan 19 21:19:46.961: INFO: Pod "downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1": Phase="Pending", Reason="", readiness=false. Elapsed: 26.806642ms
    Jan 19 21:19:48.965: INFO: Pod "downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030625548s
    Jan 19 21:19:50.965: INFO: Pod "downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030564255s
    STEP: Saw pod success 01/19/23 21:19:50.965
    Jan 19 21:19:50.965: INFO: Pod "downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1" satisfied condition "Succeeded or Failed"
    Jan 19 21:19:50.967: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1 container client-container: <nil>
    STEP: delete the pod 01/19/23 21:19:50.972
    Jan 19 21:19:50.982: INFO: Waiting for pod downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1 to disappear
    Jan 19 21:19:50.984: INFO: Pod downwardapi-volume-04d64897-25e5-4a09-979d-df9ad0f847c1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 21:19:50.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5887" for this suite. 01/19/23 21:19:50.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:19:50.995
Jan 19 21:19:50.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:19:50.996
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:51.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:51.03
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:19:51.072
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:19:51.206
STEP: Deploying the webhook pod 01/19/23 21:19:51.216
STEP: Wait for the deployment to be ready 01/19/23 21:19:51.233
Jan 19 21:19:51.250: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/19/23 21:19:53.258
STEP: Verifying the service has paired with the endpoint 01/19/23 21:19:53.268
Jan 19 21:19:54.269: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jan 19 21:19:54.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2100-crds.webhook.example.com via the AdmissionRegistration API 01/19/23 21:19:54.781
STEP: Creating a custom resource that should be mutated by the webhook 01/19/23 21:19:54.795
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:19:57.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4064" for this suite. 01/19/23 21:19:57.364
STEP: Destroying namespace "webhook-4064-markers" for this suite. 01/19/23 21:19:57.371
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":128,"skipped":2302,"failed":0}
------------------------------
• [SLOW TEST] [6.424 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:19:50.995
    Jan 19 21:19:50.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:19:50.996
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:51.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:51.03
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:19:51.072
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:19:51.206
    STEP: Deploying the webhook pod 01/19/23 21:19:51.216
    STEP: Wait for the deployment to be ready 01/19/23 21:19:51.233
    Jan 19 21:19:51.250: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/19/23 21:19:53.258
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:19:53.268
    Jan 19 21:19:54.269: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jan 19 21:19:54.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2100-crds.webhook.example.com via the AdmissionRegistration API 01/19/23 21:19:54.781
    STEP: Creating a custom resource that should be mutated by the webhook 01/19/23 21:19:54.795
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:19:57.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4064" for this suite. 01/19/23 21:19:57.364
    STEP: Destroying namespace "webhook-4064-markers" for this suite. 01/19/23 21:19:57.371
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:19:57.42
Jan 19 21:19:57.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename resourcequota 01/19/23 21:19:57.421
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:57.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:57.465
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 01/19/23 21:19:57.473
STEP: Creating a ResourceQuota 01/19/23 21:20:02.475
STEP: Ensuring resource quota status is calculated 01/19/23 21:20:02.479
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 21:20:04.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5885" for this suite. 01/19/23 21:20:04.487
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":129,"skipped":2315,"failed":0}
------------------------------
• [SLOW TEST] [7.072 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:19:57.42
    Jan 19 21:19:57.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename resourcequota 01/19/23 21:19:57.421
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:19:57.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:19:57.465
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 01/19/23 21:19:57.473
    STEP: Creating a ResourceQuota 01/19/23 21:20:02.475
    STEP: Ensuring resource quota status is calculated 01/19/23 21:20:02.479
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 21:20:04.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5885" for this suite. 01/19/23 21:20:04.487
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:20:04.492
Jan 19 21:20:04.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:20:04.493
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:04.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:04.525
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:20:04.552
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:20:04.955
STEP: Deploying the webhook pod 01/19/23 21:20:04.96
STEP: Wait for the deployment to be ready 01/19/23 21:20:04.971
Jan 19 21:20:05.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 20, 4, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 20, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 21:20:07.004
STEP: Verifying the service has paired with the endpoint 01/19/23 21:20:07.015
Jan 19 21:20:08.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jan 19 21:20:08.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/19/23 21:20:08.526
STEP: Creating a custom resource that should be denied by the webhook 01/19/23 21:20:08.54
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/19/23 21:20:10.575
STEP: Updating the custom resource with disallowed data should be denied 01/19/23 21:20:10.586
STEP: Deleting the custom resource should be denied 01/19/23 21:20:10.593
STEP: Remove the offending key and value from the custom resource data 01/19/23 21:20:10.599
STEP: Deleting the updated custom resource should be successful 01/19/23 21:20:10.606
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:20:11.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3500" for this suite. 01/19/23 21:20:11.132
STEP: Destroying namespace "webhook-3500-markers" for this suite. 01/19/23 21:20:11.141
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":130,"skipped":2325,"failed":0}
------------------------------
• [SLOW TEST] [6.715 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:20:04.492
    Jan 19 21:20:04.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:20:04.493
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:04.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:04.525
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:20:04.552
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:20:04.955
    STEP: Deploying the webhook pod 01/19/23 21:20:04.96
    STEP: Wait for the deployment to be ready 01/19/23 21:20:04.971
    Jan 19 21:20:05.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 20, 4, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 20, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 20, 4, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 21:20:07.004
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:20:07.015
    Jan 19 21:20:08.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jan 19 21:20:08.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/19/23 21:20:08.526
    STEP: Creating a custom resource that should be denied by the webhook 01/19/23 21:20:08.54
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/19/23 21:20:10.575
    STEP: Updating the custom resource with disallowed data should be denied 01/19/23 21:20:10.586
    STEP: Deleting the custom resource should be denied 01/19/23 21:20:10.593
    STEP: Remove the offending key and value from the custom resource data 01/19/23 21:20:10.599
    STEP: Deleting the updated custom resource should be successful 01/19/23 21:20:10.606
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:20:11.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3500" for this suite. 01/19/23 21:20:11.132
    STEP: Destroying namespace "webhook-3500-markers" for this suite. 01/19/23 21:20:11.141
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:20:11.21
Jan 19 21:20:11.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir-wrapper 01/19/23 21:20:11.21
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:11.261
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:11.264
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/19/23 21:20:11.286
STEP: Creating RC which spawns configmap-volume pods 01/19/23 21:20:11.634
Jan 19 21:20:11.647: INFO: Pod name wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2: Found 0 pods out of 5
Jan 19 21:20:16.651: INFO: Pod name wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/19/23 21:20:16.651
Jan 19 21:20:16.651: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-fxq9q" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:16.654: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-fxq9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.457641ms
Jan 19 21:20:16.654: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-fxq9q" satisfied condition "running"
Jan 19 21:20:16.654: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-j5x4g" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:16.656: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-j5x4g": Phase="Running", Reason="", readiness=true. Elapsed: 2.376963ms
Jan 19 21:20:16.656: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-j5x4g" satisfied condition "running"
Jan 19 21:20:16.656: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-mbxrz" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:16.659: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-mbxrz": Phase="Running", Reason="", readiness=true. Elapsed: 2.525483ms
Jan 19 21:20:16.659: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-mbxrz" satisfied condition "running"
Jan 19 21:20:16.659: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-qcjdv" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:16.663: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-qcjdv": Phase="Running", Reason="", readiness=true. Elapsed: 3.682728ms
Jan 19 21:20:16.663: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-qcjdv" satisfied condition "running"
Jan 19 21:20:16.663: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-qhjwx" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:16.665: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-qhjwx": Phase="Running", Reason="", readiness=true. Elapsed: 2.263885ms
Jan 19 21:20:16.665: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-qhjwx" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2 in namespace emptydir-wrapper-4095, will wait for the garbage collector to delete the pods 01/19/23 21:20:16.665
Jan 19 21:20:16.724: INFO: Deleting ReplicationController wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2 took: 5.903689ms
Jan 19 21:20:16.824: INFO: Terminating ReplicationController wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2 pods took: 100.489666ms
STEP: Creating RC which spawns configmap-volume pods 01/19/23 21:20:18.53
Jan 19 21:20:18.541: INFO: Pod name wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460: Found 0 pods out of 5
Jan 19 21:20:23.548: INFO: Pod name wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/19/23 21:20:23.548
Jan 19 21:20:23.548: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-5nfc4" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:23.551: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-5nfc4": Phase="Running", Reason="", readiness=true. Elapsed: 2.696833ms
Jan 19 21:20:23.551: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-5nfc4" satisfied condition "running"
Jan 19 21:20:23.551: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-l8w6f" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:23.554: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-l8w6f": Phase="Running", Reason="", readiness=true. Elapsed: 2.426062ms
Jan 19 21:20:23.554: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-l8w6f" satisfied condition "running"
Jan 19 21:20:23.554: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-npvz9" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:23.556: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-npvz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.279907ms
Jan 19 21:20:23.556: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-npvz9" satisfied condition "running"
Jan 19 21:20:23.556: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-vvdhw" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:23.558: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-vvdhw": Phase="Running", Reason="", readiness=true. Elapsed: 2.476067ms
Jan 19 21:20:23.558: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-vvdhw" satisfied condition "running"
Jan 19 21:20:23.558: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-w94cl" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:23.561: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-w94cl": Phase="Running", Reason="", readiness=true. Elapsed: 2.516081ms
Jan 19 21:20:23.561: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-w94cl" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460 in namespace emptydir-wrapper-4095, will wait for the garbage collector to delete the pods 01/19/23 21:20:23.561
Jan 19 21:20:23.621: INFO: Deleting ReplicationController wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460 took: 7.308214ms
Jan 19 21:20:23.722: INFO: Terminating ReplicationController wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460 pods took: 100.400882ms
STEP: Creating RC which spawns configmap-volume pods 01/19/23 21:20:25.827
Jan 19 21:20:25.840: INFO: Pod name wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1: Found 0 pods out of 5
Jan 19 21:20:30.844: INFO: Pod name wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/19/23 21:20:30.844
Jan 19 21:20:30.844: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-kr5f6" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:30.847: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-kr5f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.94566ms
Jan 19 21:20:30.847: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-kr5f6" satisfied condition "running"
Jan 19 21:20:30.847: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-lb855" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:30.850: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-lb855": Phase="Running", Reason="", readiness=true. Elapsed: 2.741575ms
Jan 19 21:20:30.850: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-lb855" satisfied condition "running"
Jan 19 21:20:30.850: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-sh9gt" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:30.852: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-sh9gt": Phase="Running", Reason="", readiness=true. Elapsed: 2.100215ms
Jan 19 21:20:30.852: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-sh9gt" satisfied condition "running"
Jan 19 21:20:30.852: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-vvq5k" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:30.854: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-vvq5k": Phase="Running", Reason="", readiness=true. Elapsed: 2.040465ms
Jan 19 21:20:30.854: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-vvq5k" satisfied condition "running"
Jan 19 21:20:30.854: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-wmsvl" in namespace "emptydir-wrapper-4095" to be "running"
Jan 19 21:20:30.857: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-wmsvl": Phase="Running", Reason="", readiness=true. Elapsed: 2.504462ms
Jan 19 21:20:30.857: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-wmsvl" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1 in namespace emptydir-wrapper-4095, will wait for the garbage collector to delete the pods 01/19/23 21:20:30.857
Jan 19 21:20:30.917: INFO: Deleting ReplicationController wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1 took: 6.783677ms
Jan 19 21:20:31.017: INFO: Terminating ReplicationController wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1 pods took: 100.106186ms
STEP: Cleaning up the configMaps 01/19/23 21:20:32.917
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 19 21:20:33.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4095" for this suite. 01/19/23 21:20:33.227
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":131,"skipped":2354,"failed":0}
------------------------------
• [SLOW TEST] [22.022 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:20:11.21
    Jan 19 21:20:11.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir-wrapper 01/19/23 21:20:11.21
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:11.261
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:11.264
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/19/23 21:20:11.286
    STEP: Creating RC which spawns configmap-volume pods 01/19/23 21:20:11.634
    Jan 19 21:20:11.647: INFO: Pod name wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2: Found 0 pods out of 5
    Jan 19 21:20:16.651: INFO: Pod name wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/19/23 21:20:16.651
    Jan 19 21:20:16.651: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-fxq9q" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:16.654: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-fxq9q": Phase="Running", Reason="", readiness=true. Elapsed: 2.457641ms
    Jan 19 21:20:16.654: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-fxq9q" satisfied condition "running"
    Jan 19 21:20:16.654: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-j5x4g" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:16.656: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-j5x4g": Phase="Running", Reason="", readiness=true. Elapsed: 2.376963ms
    Jan 19 21:20:16.656: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-j5x4g" satisfied condition "running"
    Jan 19 21:20:16.656: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-mbxrz" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:16.659: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-mbxrz": Phase="Running", Reason="", readiness=true. Elapsed: 2.525483ms
    Jan 19 21:20:16.659: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-mbxrz" satisfied condition "running"
    Jan 19 21:20:16.659: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-qcjdv" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:16.663: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-qcjdv": Phase="Running", Reason="", readiness=true. Elapsed: 3.682728ms
    Jan 19 21:20:16.663: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-qcjdv" satisfied condition "running"
    Jan 19 21:20:16.663: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-qhjwx" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:16.665: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-qhjwx": Phase="Running", Reason="", readiness=true. Elapsed: 2.263885ms
    Jan 19 21:20:16.665: INFO: Pod "wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2-qhjwx" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2 in namespace emptydir-wrapper-4095, will wait for the garbage collector to delete the pods 01/19/23 21:20:16.665
    Jan 19 21:20:16.724: INFO: Deleting ReplicationController wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2 took: 5.903689ms
    Jan 19 21:20:16.824: INFO: Terminating ReplicationController wrapped-volume-race-32a3f80d-2307-444b-b9af-a986c47013c2 pods took: 100.489666ms
    STEP: Creating RC which spawns configmap-volume pods 01/19/23 21:20:18.53
    Jan 19 21:20:18.541: INFO: Pod name wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460: Found 0 pods out of 5
    Jan 19 21:20:23.548: INFO: Pod name wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/19/23 21:20:23.548
    Jan 19 21:20:23.548: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-5nfc4" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:23.551: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-5nfc4": Phase="Running", Reason="", readiness=true. Elapsed: 2.696833ms
    Jan 19 21:20:23.551: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-5nfc4" satisfied condition "running"
    Jan 19 21:20:23.551: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-l8w6f" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:23.554: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-l8w6f": Phase="Running", Reason="", readiness=true. Elapsed: 2.426062ms
    Jan 19 21:20:23.554: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-l8w6f" satisfied condition "running"
    Jan 19 21:20:23.554: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-npvz9" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:23.556: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-npvz9": Phase="Running", Reason="", readiness=true. Elapsed: 2.279907ms
    Jan 19 21:20:23.556: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-npvz9" satisfied condition "running"
    Jan 19 21:20:23.556: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-vvdhw" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:23.558: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-vvdhw": Phase="Running", Reason="", readiness=true. Elapsed: 2.476067ms
    Jan 19 21:20:23.558: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-vvdhw" satisfied condition "running"
    Jan 19 21:20:23.558: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-w94cl" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:23.561: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-w94cl": Phase="Running", Reason="", readiness=true. Elapsed: 2.516081ms
    Jan 19 21:20:23.561: INFO: Pod "wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460-w94cl" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460 in namespace emptydir-wrapper-4095, will wait for the garbage collector to delete the pods 01/19/23 21:20:23.561
    Jan 19 21:20:23.621: INFO: Deleting ReplicationController wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460 took: 7.308214ms
    Jan 19 21:20:23.722: INFO: Terminating ReplicationController wrapped-volume-race-068eb971-ea1b-48d6-9d94-394d93844460 pods took: 100.400882ms
    STEP: Creating RC which spawns configmap-volume pods 01/19/23 21:20:25.827
    Jan 19 21:20:25.840: INFO: Pod name wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1: Found 0 pods out of 5
    Jan 19 21:20:30.844: INFO: Pod name wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/19/23 21:20:30.844
    Jan 19 21:20:30.844: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-kr5f6" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:30.847: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-kr5f6": Phase="Running", Reason="", readiness=true. Elapsed: 2.94566ms
    Jan 19 21:20:30.847: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-kr5f6" satisfied condition "running"
    Jan 19 21:20:30.847: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-lb855" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:30.850: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-lb855": Phase="Running", Reason="", readiness=true. Elapsed: 2.741575ms
    Jan 19 21:20:30.850: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-lb855" satisfied condition "running"
    Jan 19 21:20:30.850: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-sh9gt" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:30.852: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-sh9gt": Phase="Running", Reason="", readiness=true. Elapsed: 2.100215ms
    Jan 19 21:20:30.852: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-sh9gt" satisfied condition "running"
    Jan 19 21:20:30.852: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-vvq5k" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:30.854: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-vvq5k": Phase="Running", Reason="", readiness=true. Elapsed: 2.040465ms
    Jan 19 21:20:30.854: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-vvq5k" satisfied condition "running"
    Jan 19 21:20:30.854: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-wmsvl" in namespace "emptydir-wrapper-4095" to be "running"
    Jan 19 21:20:30.857: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-wmsvl": Phase="Running", Reason="", readiness=true. Elapsed: 2.504462ms
    Jan 19 21:20:30.857: INFO: Pod "wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1-wmsvl" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1 in namespace emptydir-wrapper-4095, will wait for the garbage collector to delete the pods 01/19/23 21:20:30.857
    Jan 19 21:20:30.917: INFO: Deleting ReplicationController wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1 took: 6.783677ms
    Jan 19 21:20:31.017: INFO: Terminating ReplicationController wrapped-volume-race-89a48247-4a85-482f-a8a3-ff81a2832af1 pods took: 100.106186ms
    STEP: Cleaning up the configMaps 01/19/23 21:20:32.917
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:20:33.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-4095" for this suite. 01/19/23 21:20:33.227
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:20:33.233
Jan 19 21:20:33.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename var-expansion 01/19/23 21:20:33.234
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:33.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:33.281
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 01/19/23 21:20:33.283
Jan 19 21:20:33.336: INFO: Waiting up to 5m0s for pod "var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40" in namespace "var-expansion-6726" to be "Succeeded or Failed"
Jan 19 21:20:33.340: INFO: Pod "var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.376079ms
Jan 19 21:20:35.345: INFO: Pod "var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00887443s
Jan 19 21:20:37.343: INFO: Pod "var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006926937s
STEP: Saw pod success 01/19/23 21:20:37.343
Jan 19 21:20:37.343: INFO: Pod "var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40" satisfied condition "Succeeded or Failed"
Jan 19 21:20:37.345: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40 container dapi-container: <nil>
STEP: delete the pod 01/19/23 21:20:37.355
Jan 19 21:20:37.364: INFO: Waiting for pod var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40 to disappear
Jan 19 21:20:37.367: INFO: Pod var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 21:20:37.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6726" for this suite. 01/19/23 21:20:37.371
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":132,"skipped":2375,"failed":0}
------------------------------
• [4.143 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:20:33.233
    Jan 19 21:20:33.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename var-expansion 01/19/23 21:20:33.234
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:33.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:33.281
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 01/19/23 21:20:33.283
    Jan 19 21:20:33.336: INFO: Waiting up to 5m0s for pod "var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40" in namespace "var-expansion-6726" to be "Succeeded or Failed"
    Jan 19 21:20:33.340: INFO: Pod "var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40": Phase="Pending", Reason="", readiness=false. Elapsed: 4.376079ms
    Jan 19 21:20:35.345: INFO: Pod "var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00887443s
    Jan 19 21:20:37.343: INFO: Pod "var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006926937s
    STEP: Saw pod success 01/19/23 21:20:37.343
    Jan 19 21:20:37.343: INFO: Pod "var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40" satisfied condition "Succeeded or Failed"
    Jan 19 21:20:37.345: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40 container dapi-container: <nil>
    STEP: delete the pod 01/19/23 21:20:37.355
    Jan 19 21:20:37.364: INFO: Waiting for pod var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40 to disappear
    Jan 19 21:20:37.367: INFO: Pod var-expansion-e1d462ba-4943-4ce1-9ba1-db0723836e40 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 21:20:37.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6726" for this suite. 01/19/23 21:20:37.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:20:37.378
Jan 19 21:20:37.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename ingressclass 01/19/23 21:20:37.379
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:37.423
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:37.426
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/19/23 21:20:37.428
STEP: getting /apis/networking.k8s.io 01/19/23 21:20:37.432
STEP: getting /apis/networking.k8s.iov1 01/19/23 21:20:37.432
STEP: creating 01/19/23 21:20:37.433
STEP: getting 01/19/23 21:20:37.483
STEP: listing 01/19/23 21:20:37.49
STEP: watching 01/19/23 21:20:37.492
Jan 19 21:20:37.492: INFO: starting watch
STEP: patching 01/19/23 21:20:37.493
STEP: updating 01/19/23 21:20:37.498
Jan 19 21:20:37.508: INFO: waiting for watch events with expected annotations
Jan 19 21:20:37.508: INFO: saw patched and updated annotations
STEP: deleting 01/19/23 21:20:37.508
STEP: deleting a collection 01/19/23 21:20:37.544
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jan 19 21:20:37.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-436" for this suite. 01/19/23 21:20:37.565
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":133,"skipped":2437,"failed":0}
------------------------------
• [0.206 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:20:37.378
    Jan 19 21:20:37.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename ingressclass 01/19/23 21:20:37.379
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:37.423
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:37.426
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/19/23 21:20:37.428
    STEP: getting /apis/networking.k8s.io 01/19/23 21:20:37.432
    STEP: getting /apis/networking.k8s.iov1 01/19/23 21:20:37.432
    STEP: creating 01/19/23 21:20:37.433
    STEP: getting 01/19/23 21:20:37.483
    STEP: listing 01/19/23 21:20:37.49
    STEP: watching 01/19/23 21:20:37.492
    Jan 19 21:20:37.492: INFO: starting watch
    STEP: patching 01/19/23 21:20:37.493
    STEP: updating 01/19/23 21:20:37.498
    Jan 19 21:20:37.508: INFO: waiting for watch events with expected annotations
    Jan 19 21:20:37.508: INFO: saw patched and updated annotations
    STEP: deleting 01/19/23 21:20:37.508
    STEP: deleting a collection 01/19/23 21:20:37.544
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jan 19 21:20:37.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-436" for this suite. 01/19/23 21:20:37.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:20:37.585
Jan 19 21:20:37.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:20:37.586
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:37.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:37.624
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-66971177-526d-4006-ad07-277f950c926d 01/19/23 21:20:37.627
STEP: Creating a pod to test consume configMaps 01/19/23 21:20:37.636
Jan 19 21:20:37.686: INFO: Waiting up to 5m0s for pod "pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280" in namespace "configmap-367" to be "Succeeded or Failed"
Jan 19 21:20:37.691: INFO: Pod "pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280": Phase="Pending", Reason="", readiness=false. Elapsed: 4.935772ms
Jan 19 21:20:39.695: INFO: Pod "pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008853908s
Jan 19 21:20:41.694: INFO: Pod "pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007561815s
STEP: Saw pod success 01/19/23 21:20:41.694
Jan 19 21:20:41.694: INFO: Pod "pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280" satisfied condition "Succeeded or Failed"
Jan 19 21:20:41.696: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:20:41.701
Jan 19 21:20:41.712: INFO: Waiting for pod pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280 to disappear
Jan 19 21:20:41.715: INFO: Pod pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:20:41.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-367" for this suite. 01/19/23 21:20:41.718
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":134,"skipped":2456,"failed":0}
------------------------------
• [4.138 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:20:37.585
    Jan 19 21:20:37.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:20:37.586
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:37.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:37.624
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-66971177-526d-4006-ad07-277f950c926d 01/19/23 21:20:37.627
    STEP: Creating a pod to test consume configMaps 01/19/23 21:20:37.636
    Jan 19 21:20:37.686: INFO: Waiting up to 5m0s for pod "pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280" in namespace "configmap-367" to be "Succeeded or Failed"
    Jan 19 21:20:37.691: INFO: Pod "pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280": Phase="Pending", Reason="", readiness=false. Elapsed: 4.935772ms
    Jan 19 21:20:39.695: INFO: Pod "pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008853908s
    Jan 19 21:20:41.694: INFO: Pod "pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007561815s
    STEP: Saw pod success 01/19/23 21:20:41.694
    Jan 19 21:20:41.694: INFO: Pod "pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280" satisfied condition "Succeeded or Failed"
    Jan 19 21:20:41.696: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:20:41.701
    Jan 19 21:20:41.712: INFO: Waiting for pod pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280 to disappear
    Jan 19 21:20:41.715: INFO: Pod pod-configmaps-94cd545a-b9ee-427a-b5cc-10e7a6d39280 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:20:41.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-367" for this suite. 01/19/23 21:20:41.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:20:41.725
Jan 19 21:20:41.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 21:20:41.726
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:41.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:41.749
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/19/23 21:20:41.755
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/19/23 21:20:41.757
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/19/23 21:20:41.757
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/19/23 21:20:41.757
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/19/23 21:20:41.757
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/19/23 21:20:41.758
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/19/23 21:20:41.763
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:20:41.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8593" for this suite. 01/19/23 21:20:41.77
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":135,"skipped":2502,"failed":0}
------------------------------
• [0.052 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:20:41.725
    Jan 19 21:20:41.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 21:20:41.726
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:41.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:41.749
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/19/23 21:20:41.755
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/19/23 21:20:41.757
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/19/23 21:20:41.757
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/19/23 21:20:41.757
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/19/23 21:20:41.757
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/19/23 21:20:41.758
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/19/23 21:20:41.763
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:20:41.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8593" for this suite. 01/19/23 21:20:41.77
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:20:41.777
Jan 19 21:20:41.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 21:20:41.778
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:41.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:41.846
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 01/19/23 21:20:41.903
Jan 19 21:20:41.925: INFO: Waiting up to 5m0s for pod "pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb" in namespace "emptydir-5853" to be "Succeeded or Failed"
Jan 19 21:20:41.927: INFO: Pod "pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.125648ms
Jan 19 21:20:43.931: INFO: Pod "pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005791259s
Jan 19 21:20:45.931: INFO: Pod "pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006497846s
Jan 19 21:20:47.999: INFO: Pod "pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073563116s
STEP: Saw pod success 01/19/23 21:20:47.999
Jan 19 21:20:47.999: INFO: Pod "pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb" satisfied condition "Succeeded or Failed"
Jan 19 21:20:48.014: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb container test-container: <nil>
STEP: delete the pod 01/19/23 21:20:48.053
Jan 19 21:20:48.080: INFO: Waiting for pod pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb to disappear
Jan 19 21:20:48.082: INFO: Pod pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 21:20:48.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5853" for this suite. 01/19/23 21:20:48.087
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":136,"skipped":2503,"failed":0}
------------------------------
• [SLOW TEST] [6.315 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:20:41.777
    Jan 19 21:20:41.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 21:20:41.778
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:41.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:41.846
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/19/23 21:20:41.903
    Jan 19 21:20:41.925: INFO: Waiting up to 5m0s for pod "pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb" in namespace "emptydir-5853" to be "Succeeded or Failed"
    Jan 19 21:20:41.927: INFO: Pod "pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.125648ms
    Jan 19 21:20:43.931: INFO: Pod "pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005791259s
    Jan 19 21:20:45.931: INFO: Pod "pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006497846s
    Jan 19 21:20:47.999: INFO: Pod "pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.073563116s
    STEP: Saw pod success 01/19/23 21:20:47.999
    Jan 19 21:20:47.999: INFO: Pod "pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb" satisfied condition "Succeeded or Failed"
    Jan 19 21:20:48.014: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb container test-container: <nil>
    STEP: delete the pod 01/19/23 21:20:48.053
    Jan 19 21:20:48.080: INFO: Waiting for pod pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb to disappear
    Jan 19 21:20:48.082: INFO: Pod pod-3dd2bcf8-3427-4ddb-9adc-442a106ff3eb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:20:48.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5853" for this suite. 01/19/23 21:20:48.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:20:48.093
Jan 19 21:20:48.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename svcaccounts 01/19/23 21:20:48.094
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:48.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:48.116
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jan 19 21:20:48.151: INFO: Waiting up to 5m0s for pod "pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f" in namespace "svcaccounts-5019" to be "running"
Jan 19 21:20:48.160: INFO: Pod "pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.24115ms
Jan 19 21:20:50.211: INFO: Pod "pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f": Phase="Running", Reason="", readiness=true. Elapsed: 2.05974834s
Jan 19 21:20:50.211: INFO: Pod "pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f" satisfied condition "running"
STEP: reading a file in the container 01/19/23 21:20:50.211
Jan 19 21:20:50.211: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5019 pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/19/23 21:20:50.356
Jan 19 21:20:50.356: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5019 pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/19/23 21:20:50.476
Jan 19 21:20:50.476: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5019 pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan 19 21:20:50.598: INFO: Got root ca configmap in namespace "svcaccounts-5019"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 19 21:20:50.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5019" for this suite. 01/19/23 21:20:50.604
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":137,"skipped":2514,"failed":0}
------------------------------
• [2.515 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:20:48.093
    Jan 19 21:20:48.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename svcaccounts 01/19/23 21:20:48.094
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:48.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:48.116
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jan 19 21:20:48.151: INFO: Waiting up to 5m0s for pod "pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f" in namespace "svcaccounts-5019" to be "running"
    Jan 19 21:20:48.160: INFO: Pod "pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.24115ms
    Jan 19 21:20:50.211: INFO: Pod "pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f": Phase="Running", Reason="", readiness=true. Elapsed: 2.05974834s
    Jan 19 21:20:50.211: INFO: Pod "pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f" satisfied condition "running"
    STEP: reading a file in the container 01/19/23 21:20:50.211
    Jan 19 21:20:50.211: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5019 pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/19/23 21:20:50.356
    Jan 19 21:20:50.356: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5019 pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/19/23 21:20:50.476
    Jan 19 21:20:50.476: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5019 pod-service-account-05622958-64be-44d7-9c13-75e3889ac03f -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan 19 21:20:50.598: INFO: Got root ca configmap in namespace "svcaccounts-5019"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 19 21:20:50.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5019" for this suite. 01/19/23 21:20:50.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:20:50.61
Jan 19 21:20:50.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename disruption 01/19/23 21:20:50.61
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:50.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:50.639
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 01/19/23 21:20:50.642
STEP: Waiting for the pdb to be processed 01/19/23 21:20:50.658
STEP: updating the pdb 01/19/23 21:20:52.675
STEP: Waiting for the pdb to be processed 01/19/23 21:20:52.682
STEP: patching the pdb 01/19/23 21:20:54.689
STEP: Waiting for the pdb to be processed 01/19/23 21:20:54.698
STEP: Waiting for the pdb to be deleted 01/19/23 21:20:56.713
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 19 21:20:56.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8306" for this suite. 01/19/23 21:20:56.719
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":138,"skipped":2549,"failed":0}
------------------------------
• [SLOW TEST] [6.117 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:20:50.61
    Jan 19 21:20:50.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename disruption 01/19/23 21:20:50.61
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:50.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:50.639
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 01/19/23 21:20:50.642
    STEP: Waiting for the pdb to be processed 01/19/23 21:20:50.658
    STEP: updating the pdb 01/19/23 21:20:52.675
    STEP: Waiting for the pdb to be processed 01/19/23 21:20:52.682
    STEP: patching the pdb 01/19/23 21:20:54.689
    STEP: Waiting for the pdb to be processed 01/19/23 21:20:54.698
    STEP: Waiting for the pdb to be deleted 01/19/23 21:20:56.713
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 19 21:20:56.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8306" for this suite. 01/19/23 21:20:56.719
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:20:56.727
Jan 19 21:20:56.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename disruption 01/19/23 21:20:56.727
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:56.778
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:56.78
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 01/19/23 21:20:56.802
STEP: Updating PodDisruptionBudget status 01/19/23 21:20:58.844
STEP: Waiting for all pods to be running 01/19/23 21:20:58.858
Jan 19 21:20:58.860: INFO: running pods: 0 < 1
STEP: locating a running pod 01/19/23 21:21:00.864
STEP: Waiting for the pdb to be processed 01/19/23 21:21:00.887
STEP: Patching PodDisruptionBudget status 01/19/23 21:21:00.896
STEP: Waiting for the pdb to be processed 01/19/23 21:21:00.903
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 19 21:21:00.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9467" for this suite. 01/19/23 21:21:00.911
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":139,"skipped":2552,"failed":0}
------------------------------
• [4.189 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:20:56.727
    Jan 19 21:20:56.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename disruption 01/19/23 21:20:56.727
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:20:56.778
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:20:56.78
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 01/19/23 21:20:56.802
    STEP: Updating PodDisruptionBudget status 01/19/23 21:20:58.844
    STEP: Waiting for all pods to be running 01/19/23 21:20:58.858
    Jan 19 21:20:58.860: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/19/23 21:21:00.864
    STEP: Waiting for the pdb to be processed 01/19/23 21:21:00.887
    STEP: Patching PodDisruptionBudget status 01/19/23 21:21:00.896
    STEP: Waiting for the pdb to be processed 01/19/23 21:21:00.903
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 19 21:21:00.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9467" for this suite. 01/19/23 21:21:00.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:21:00.928
Jan 19 21:21:00.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir-wrapper 01/19/23 21:21:00.929
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:21:00.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:21:00.964
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan 19 21:21:01.022: INFO: Waiting up to 5m0s for pod "pod-secrets-28036d69-bb93-4208-9026-dd91f766b770" in namespace "emptydir-wrapper-2715" to be "running and ready"
Jan 19 21:21:01.028: INFO: Pod "pod-secrets-28036d69-bb93-4208-9026-dd91f766b770": Phase="Pending", Reason="", readiness=false. Elapsed: 5.837644ms
Jan 19 21:21:01.028: INFO: The phase of Pod pod-secrets-28036d69-bb93-4208-9026-dd91f766b770 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:21:03.032: INFO: Pod "pod-secrets-28036d69-bb93-4208-9026-dd91f766b770": Phase="Running", Reason="", readiness=true. Elapsed: 2.00940228s
Jan 19 21:21:03.032: INFO: The phase of Pod pod-secrets-28036d69-bb93-4208-9026-dd91f766b770 is Running (Ready = true)
Jan 19 21:21:03.032: INFO: Pod "pod-secrets-28036d69-bb93-4208-9026-dd91f766b770" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/19/23 21:21:03.034
STEP: Cleaning up the configmap 01/19/23 21:21:03.039
STEP: Cleaning up the pod 01/19/23 21:21:03.048
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jan 19 21:21:03.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2715" for this suite. 01/19/23 21:21:03.066
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":140,"skipped":2571,"failed":0}
------------------------------
• [2.144 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:21:00.928
    Jan 19 21:21:00.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir-wrapper 01/19/23 21:21:00.929
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:21:00.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:21:00.964
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan 19 21:21:01.022: INFO: Waiting up to 5m0s for pod "pod-secrets-28036d69-bb93-4208-9026-dd91f766b770" in namespace "emptydir-wrapper-2715" to be "running and ready"
    Jan 19 21:21:01.028: INFO: Pod "pod-secrets-28036d69-bb93-4208-9026-dd91f766b770": Phase="Pending", Reason="", readiness=false. Elapsed: 5.837644ms
    Jan 19 21:21:01.028: INFO: The phase of Pod pod-secrets-28036d69-bb93-4208-9026-dd91f766b770 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:21:03.032: INFO: Pod "pod-secrets-28036d69-bb93-4208-9026-dd91f766b770": Phase="Running", Reason="", readiness=true. Elapsed: 2.00940228s
    Jan 19 21:21:03.032: INFO: The phase of Pod pod-secrets-28036d69-bb93-4208-9026-dd91f766b770 is Running (Ready = true)
    Jan 19 21:21:03.032: INFO: Pod "pod-secrets-28036d69-bb93-4208-9026-dd91f766b770" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/19/23 21:21:03.034
    STEP: Cleaning up the configmap 01/19/23 21:21:03.039
    STEP: Cleaning up the pod 01/19/23 21:21:03.048
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:21:03.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-2715" for this suite. 01/19/23 21:21:03.066
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:21:03.073
Jan 19 21:21:03.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename security-context-test 01/19/23 21:21:03.074
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:21:03.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:21:03.102
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jan 19 21:21:03.130: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3518f0cb-4d19-4c81-93a1-3aff80e98075" in namespace "security-context-test-429" to be "Succeeded or Failed"
Jan 19 21:21:03.133: INFO: Pod "busybox-privileged-false-3518f0cb-4d19-4c81-93a1-3aff80e98075": Phase="Pending", Reason="", readiness=false. Elapsed: 2.267514ms
Jan 19 21:21:05.136: INFO: Pod "busybox-privileged-false-3518f0cb-4d19-4c81-93a1-3aff80e98075": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005991099s
Jan 19 21:21:07.136: INFO: Pod "busybox-privileged-false-3518f0cb-4d19-4c81-93a1-3aff80e98075": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005883705s
Jan 19 21:21:07.136: INFO: Pod "busybox-privileged-false-3518f0cb-4d19-4c81-93a1-3aff80e98075" satisfied condition "Succeeded or Failed"
Jan 19 21:21:07.142: INFO: Got logs for pod "busybox-privileged-false-3518f0cb-4d19-4c81-93a1-3aff80e98075": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 19 21:21:07.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-429" for this suite. 01/19/23 21:21:07.146
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":141,"skipped":2606,"failed":0}
------------------------------
• [4.078 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:21:03.073
    Jan 19 21:21:03.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename security-context-test 01/19/23 21:21:03.074
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:21:03.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:21:03.102
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jan 19 21:21:03.130: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3518f0cb-4d19-4c81-93a1-3aff80e98075" in namespace "security-context-test-429" to be "Succeeded or Failed"
    Jan 19 21:21:03.133: INFO: Pod "busybox-privileged-false-3518f0cb-4d19-4c81-93a1-3aff80e98075": Phase="Pending", Reason="", readiness=false. Elapsed: 2.267514ms
    Jan 19 21:21:05.136: INFO: Pod "busybox-privileged-false-3518f0cb-4d19-4c81-93a1-3aff80e98075": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005991099s
    Jan 19 21:21:07.136: INFO: Pod "busybox-privileged-false-3518f0cb-4d19-4c81-93a1-3aff80e98075": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005883705s
    Jan 19 21:21:07.136: INFO: Pod "busybox-privileged-false-3518f0cb-4d19-4c81-93a1-3aff80e98075" satisfied condition "Succeeded or Failed"
    Jan 19 21:21:07.142: INFO: Got logs for pod "busybox-privileged-false-3518f0cb-4d19-4c81-93a1-3aff80e98075": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 19 21:21:07.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-429" for this suite. 01/19/23 21:21:07.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:21:07.152
Jan 19 21:21:07.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename resourcequota 01/19/23 21:21:07.153
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:21:07.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:21:07.175
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 01/19/23 21:21:07.178
STEP: Getting a ResourceQuota 01/19/23 21:21:07.183
STEP: Listing all ResourceQuotas with LabelSelector 01/19/23 21:21:07.193
STEP: Patching the ResourceQuota 01/19/23 21:21:07.198
STEP: Deleting a Collection of ResourceQuotas 01/19/23 21:21:07.204
STEP: Verifying the deleted ResourceQuota 01/19/23 21:21:07.217
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 21:21:07.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3932" for this suite. 01/19/23 21:21:07.227
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":142,"skipped":2643,"failed":0}
------------------------------
• [0.082 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:21:07.152
    Jan 19 21:21:07.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename resourcequota 01/19/23 21:21:07.153
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:21:07.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:21:07.175
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 01/19/23 21:21:07.178
    STEP: Getting a ResourceQuota 01/19/23 21:21:07.183
    STEP: Listing all ResourceQuotas with LabelSelector 01/19/23 21:21:07.193
    STEP: Patching the ResourceQuota 01/19/23 21:21:07.198
    STEP: Deleting a Collection of ResourceQuotas 01/19/23 21:21:07.204
    STEP: Verifying the deleted ResourceQuota 01/19/23 21:21:07.217
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 21:21:07.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3932" for this suite. 01/19/23 21:21:07.227
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:21:07.234
Jan 19 21:21:07.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 21:21:07.235
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:21:07.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:21:07.256
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-697d417c-18a8-472e-9ed2-edcbde16a2bf 01/19/23 21:21:07.259
STEP: Creating a pod to test consume secrets 01/19/23 21:21:07.267
Jan 19 21:21:07.301: INFO: Waiting up to 5m0s for pod "pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519" in namespace "secrets-1641" to be "Succeeded or Failed"
Jan 19 21:21:07.307: INFO: Pod "pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519": Phase="Pending", Reason="", readiness=false. Elapsed: 5.189994ms
Jan 19 21:21:09.309: INFO: Pod "pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007527252s
Jan 19 21:21:11.310: INFO: Pod "pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008577484s
STEP: Saw pod success 01/19/23 21:21:11.31
Jan 19 21:21:11.310: INFO: Pod "pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519" satisfied condition "Succeeded or Failed"
Jan 19 21:21:11.313: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 21:21:11.317
Jan 19 21:21:11.326: INFO: Waiting for pod pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519 to disappear
Jan 19 21:21:11.328: INFO: Pod pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 21:21:11.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1641" for this suite. 01/19/23 21:21:11.332
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":143,"skipped":2643,"failed":0}
------------------------------
• [4.103 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:21:07.234
    Jan 19 21:21:07.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 21:21:07.235
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:21:07.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:21:07.256
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-697d417c-18a8-472e-9ed2-edcbde16a2bf 01/19/23 21:21:07.259
    STEP: Creating a pod to test consume secrets 01/19/23 21:21:07.267
    Jan 19 21:21:07.301: INFO: Waiting up to 5m0s for pod "pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519" in namespace "secrets-1641" to be "Succeeded or Failed"
    Jan 19 21:21:07.307: INFO: Pod "pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519": Phase="Pending", Reason="", readiness=false. Elapsed: 5.189994ms
    Jan 19 21:21:09.309: INFO: Pod "pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007527252s
    Jan 19 21:21:11.310: INFO: Pod "pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008577484s
    STEP: Saw pod success 01/19/23 21:21:11.31
    Jan 19 21:21:11.310: INFO: Pod "pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519" satisfied condition "Succeeded or Failed"
    Jan 19 21:21:11.313: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:21:11.317
    Jan 19 21:21:11.326: INFO: Waiting for pod pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519 to disappear
    Jan 19 21:21:11.328: INFO: Pod pod-secrets-6a69a9f8-13da-4f48-9be7-14170774e519 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 21:21:11.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1641" for this suite. 01/19/23 21:21:11.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:21:11.338
Jan 19 21:21:11.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:21:11.339
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:21:11.364
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:21:11.366
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:21:11.369
Jan 19 21:21:11.409: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0" in namespace "projected-3825" to be "Succeeded or Failed"
Jan 19 21:21:11.412: INFO: Pod "downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.877378ms
Jan 19 21:21:13.415: INFO: Pod "downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0062333s
Jan 19 21:21:15.416: INFO: Pod "downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006610768s
STEP: Saw pod success 01/19/23 21:21:15.416
Jan 19 21:21:15.416: INFO: Pod "downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0" satisfied condition "Succeeded or Failed"
Jan 19 21:21:15.418: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0 container client-container: <nil>
STEP: delete the pod 01/19/23 21:21:15.423
Jan 19 21:21:15.436: INFO: Waiting for pod downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0 to disappear
Jan 19 21:21:15.438: INFO: Pod downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 21:21:15.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3825" for this suite. 01/19/23 21:21:15.444
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":144,"skipped":2660,"failed":0}
------------------------------
• [4.111 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:21:11.338
    Jan 19 21:21:11.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:21:11.339
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:21:11.364
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:21:11.366
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:21:11.369
    Jan 19 21:21:11.409: INFO: Waiting up to 5m0s for pod "downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0" in namespace "projected-3825" to be "Succeeded or Failed"
    Jan 19 21:21:11.412: INFO: Pod "downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.877378ms
    Jan 19 21:21:13.415: INFO: Pod "downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0062333s
    Jan 19 21:21:15.416: INFO: Pod "downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006610768s
    STEP: Saw pod success 01/19/23 21:21:15.416
    Jan 19 21:21:15.416: INFO: Pod "downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0" satisfied condition "Succeeded or Failed"
    Jan 19 21:21:15.418: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0 container client-container: <nil>
    STEP: delete the pod 01/19/23 21:21:15.423
    Jan 19 21:21:15.436: INFO: Waiting for pod downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0 to disappear
    Jan 19 21:21:15.438: INFO: Pod downwardapi-volume-15d8785d-d3a8-4219-9854-aba3a8c687a0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 21:21:15.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3825" for this suite. 01/19/23 21:21:15.444
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:21:15.45
Jan 19 21:21:15.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename taint-multiple-pods 01/19/23 21:21:15.451
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:21:15.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:21:15.486
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jan 19 21:21:15.494: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 21:22:15.693: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jan 19 21:22:15.699: INFO: Starting informer...
STEP: Starting pods... 01/19/23 21:22:15.699
Jan 19 21:22:15.918: INFO: Pod1 is running on ip-10-0-172-44.ec2.internal. Tainting Node
Jan 19 21:22:16.132: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-102" to be "running"
Jan 19 21:22:16.134: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.640374ms
Jan 19 21:22:18.137: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005662202s
Jan 19 21:22:18.137: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan 19 21:22:18.137: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-102" to be "running"
Jan 19 21:22:18.140: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.074349ms
Jan 19 21:22:18.140: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan 19 21:22:18.140: INFO: Pod2 is running on ip-10-0-172-44.ec2.internal. Tainting Node
STEP: Trying to apply a taint on the Node 01/19/23 21:22:18.14
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 21:22:18.155
STEP: Waiting for Pod1 and Pod2 to be deleted 01/19/23 21:22:18.17
Jan 19 21:22:24.414: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan 19 21:22:44.455: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 21:22:44.471
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:22:44.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-102" for this suite. 01/19/23 21:22:44.487
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":145,"skipped":2676,"failed":0}
------------------------------
• [SLOW TEST] [89.053 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:21:15.45
    Jan 19 21:21:15.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename taint-multiple-pods 01/19/23 21:21:15.451
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:21:15.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:21:15.486
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jan 19 21:21:15.494: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 19 21:22:15.693: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jan 19 21:22:15.699: INFO: Starting informer...
    STEP: Starting pods... 01/19/23 21:22:15.699
    Jan 19 21:22:15.918: INFO: Pod1 is running on ip-10-0-172-44.ec2.internal. Tainting Node
    Jan 19 21:22:16.132: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-102" to be "running"
    Jan 19 21:22:16.134: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.640374ms
    Jan 19 21:22:18.137: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005662202s
    Jan 19 21:22:18.137: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan 19 21:22:18.137: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-102" to be "running"
    Jan 19 21:22:18.140: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.074349ms
    Jan 19 21:22:18.140: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan 19 21:22:18.140: INFO: Pod2 is running on ip-10-0-172-44.ec2.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 01/19/23 21:22:18.14
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 21:22:18.155
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/19/23 21:22:18.17
    Jan 19 21:22:24.414: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan 19 21:22:44.455: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/19/23 21:22:44.471
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:22:44.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-102" for this suite. 01/19/23 21:22:44.487
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:22:44.503
Jan 19 21:22:44.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename deployment 01/19/23 21:22:44.504
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:22:44.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:22:44.573
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan 19 21:22:44.576: INFO: Creating simple deployment test-new-deployment
Jan 19 21:22:44.693: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 01/19/23 21:22:46.703
STEP: updating a scale subresource 01/19/23 21:22:46.706
STEP: verifying the deployment Spec.Replicas was modified 01/19/23 21:22:46.711
STEP: Patch a scale subresource 01/19/23 21:22:46.714
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 21:22:46.733: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-9924  7e4ae3d1-05ba-486b-9941-fd53f0980c05 181657 3 2023-01-19 21:22:44 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-19 21:22:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b71b278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 21:22:46 +0000 UTC,LastTransitionTime:2023-01-19 21:22:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-19 21:22:46 +0000 UTC,LastTransitionTime:2023-01-19 21:22:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 21:22:46.737: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-9924  4745aaf0-b1af-487d-ab63-db30bde231ad 181659 3 2023-01-19 21:22:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 7e4ae3d1-05ba-486b-9941-fd53f0980c05 0xc004289ec7 0xc004289ec8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 21:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e4ae3d1-05ba-486b-9941-fd53f0980c05\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:22:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004289f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 21:22:46.744: INFO: Pod "test-new-deployment-845c8977d9-d8w52" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-d8w52 test-new-deployment-845c8977d9- deployment-9924  493aadfb-afca-4419-9aac-5857b23cd2a9 181660 0 2023-01-19 21:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 4745aaf0-b1af-487d-ab63-db30bde231ad 0xc00b464a07 0xc00b464a08}] [] [{kube-controller-manager Update v1 2023-01-19 21:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4745aaf0-b1af-487d-ab63-db30bde231ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8l9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8l9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c51,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kt2f7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan 19 21:22:46.744: INFO: Pod "test-new-deployment-845c8977d9-fbjmh" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-fbjmh test-new-deployment-845c8977d9- deployment-9924  97b68f3b-869d-4f54-b6f0-86aec3475b4c 181649 0 2023-01-19 21:22:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.140/23"],"mac_address":"0a:58:0a:80:08:8c","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.140/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.140"
    ],
    "mac": "0a:58:0a:80:08:8c",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.140"
    ],
    "mac": "0a:58:0a:80:08:8c",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 4745aaf0-b1af-487d-ab63-db30bde231ad 0xc00b464b97 0xc00b464b98}] [] [{ip-10-0-194-246 Update v1 2023-01-19 21:22:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 21:22:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4745aaf0-b1af-487d-ab63-db30bde231ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 21:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 21:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mr8mq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mr8mq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c51,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kt2f7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:22:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:22:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.140,StartTime:2023-01-19 21:22:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 21:22:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://db4115a18389555d1d2df50352785a38f76624620f5cbcf43f1b051f8d6bbda1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 21:22:46.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9924" for this suite. 01/19/23 21:22:46.751
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":146,"skipped":2678,"failed":0}
------------------------------
• [2.257 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:22:44.503
    Jan 19 21:22:44.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename deployment 01/19/23 21:22:44.504
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:22:44.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:22:44.573
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan 19 21:22:44.576: INFO: Creating simple deployment test-new-deployment
    Jan 19 21:22:44.693: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 01/19/23 21:22:46.703
    STEP: updating a scale subresource 01/19/23 21:22:46.706
    STEP: verifying the deployment Spec.Replicas was modified 01/19/23 21:22:46.711
    STEP: Patch a scale subresource 01/19/23 21:22:46.714
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 21:22:46.733: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-9924  7e4ae3d1-05ba-486b-9941-fd53f0980c05 181657 3 2023-01-19 21:22:44 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-19 21:22:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b71b278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 21:22:46 +0000 UTC,LastTransitionTime:2023-01-19 21:22:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-01-19 21:22:46 +0000 UTC,LastTransitionTime:2023-01-19 21:22:44 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 19 21:22:46.737: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-9924  4745aaf0-b1af-487d-ab63-db30bde231ad 181659 3 2023-01-19 21:22:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 7e4ae3d1-05ba-486b-9941-fd53f0980c05 0xc004289ec7 0xc004289ec8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 21:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e4ae3d1-05ba-486b-9941-fd53f0980c05\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:22:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004289f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 21:22:46.744: INFO: Pod "test-new-deployment-845c8977d9-d8w52" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-d8w52 test-new-deployment-845c8977d9- deployment-9924  493aadfb-afca-4419-9aac-5857b23cd2a9 181660 0 2023-01-19 21:22:46 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 4745aaf0-b1af-487d-ab63-db30bde231ad 0xc00b464a07 0xc00b464a08}] [] [{kube-controller-manager Update v1 2023-01-19 21:22:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4745aaf0-b1af-487d-ab63-db30bde231ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8l9b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8l9b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c51,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kt2f7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan 19 21:22:46.744: INFO: Pod "test-new-deployment-845c8977d9-fbjmh" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-fbjmh test-new-deployment-845c8977d9- deployment-9924  97b68f3b-869d-4f54-b6f0-86aec3475b4c 181649 0 2023-01-19 21:22:44 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.140/23"],"mac_address":"0a:58:0a:80:08:8c","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.140/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.140"
        ],
        "mac": "0a:58:0a:80:08:8c",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.140"
        ],
        "mac": "0a:58:0a:80:08:8c",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 4745aaf0-b1af-487d-ab63-db30bde231ad 0xc00b464b97 0xc00b464b98}] [] [{ip-10-0-194-246 Update v1 2023-01-19 21:22:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 21:22:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4745aaf0-b1af-487d-ab63-db30bde231ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 21:22:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 21:22:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mr8mq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mr8mq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c51,c35,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kt2f7,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:22:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:22:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:22:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.140,StartTime:2023-01-19 21:22:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 21:22:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://db4115a18389555d1d2df50352785a38f76624620f5cbcf43f1b051f8d6bbda1,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 21:22:46.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9924" for this suite. 01/19/23 21:22:46.751
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:22:46.761
Jan 19 21:22:46.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename replication-controller 01/19/23 21:22:46.762
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:22:46.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:22:46.792
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jan 19 21:22:46.808: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/19/23 21:22:46.854
STEP: Checking rc "condition-test" has the desired failure condition set 01/19/23 21:22:46.864
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/19/23 21:22:47.887
Jan 19 21:22:47.895: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/19/23 21:22:47.895
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 19 21:22:48.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6518" for this suite. 01/19/23 21:22:48.905
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":147,"skipped":2679,"failed":0}
------------------------------
• [2.151 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:22:46.761
    Jan 19 21:22:46.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename replication-controller 01/19/23 21:22:46.762
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:22:46.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:22:46.792
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jan 19 21:22:46.808: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/19/23 21:22:46.854
    STEP: Checking rc "condition-test" has the desired failure condition set 01/19/23 21:22:46.864
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/19/23 21:22:47.887
    Jan 19 21:22:47.895: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/19/23 21:22:47.895
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 19 21:22:48.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6518" for this suite. 01/19/23 21:22:48.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:22:48.912
Jan 19 21:22:48.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename watch 01/19/23 21:22:48.913
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:22:48.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:22:48.943
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/19/23 21:22:48.945
STEP: starting a background goroutine to produce watch events 01/19/23 21:22:48.954
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/19/23 21:22:48.954
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 19 21:22:51.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9518" for this suite. 01/19/23 21:22:51.768
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":148,"skipped":2689,"failed":0}
------------------------------
• [2.907 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:22:48.912
    Jan 19 21:22:48.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename watch 01/19/23 21:22:48.913
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:22:48.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:22:48.943
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/19/23 21:22:48.945
    STEP: starting a background goroutine to produce watch events 01/19/23 21:22:48.954
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/19/23 21:22:48.954
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 19 21:22:51.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-9518" for this suite. 01/19/23 21:22:51.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:22:51.821
Jan 19 21:22:51.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename replicaset 01/19/23 21:22:51.821
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:22:51.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:22:51.866
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/19/23 21:22:51.868
W0119 21:22:51.892385      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:22:51.892: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4704" to be "running and ready"
Jan 19 21:22:51.895: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.07119ms
Jan 19 21:22:51.895: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:22:53.899: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.006916499s
Jan 19 21:22:53.899: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan 19 21:22:53.899: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/19/23 21:22:53.901
STEP: Then the orphan pod is adopted 01/19/23 21:22:53.908
STEP: When the matched label of one of its pods change 01/19/23 21:22:54.915
Jan 19 21:22:54.918: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/19/23 21:22:54.939
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 19 21:22:55.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4704" for this suite. 01/19/23 21:22:55.955
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":149,"skipped":2719,"failed":0}
------------------------------
• [4.141 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:22:51.821
    Jan 19 21:22:51.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename replicaset 01/19/23 21:22:51.821
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:22:51.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:22:51.866
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/19/23 21:22:51.868
    W0119 21:22:51.892385      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-adoption-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-adoption-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-adoption-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-adoption-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:22:51.892: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-4704" to be "running and ready"
    Jan 19 21:22:51.895: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 3.07119ms
    Jan 19 21:22:51.895: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:22:53.899: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.006916499s
    Jan 19 21:22:53.899: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan 19 21:22:53.899: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/19/23 21:22:53.901
    STEP: Then the orphan pod is adopted 01/19/23 21:22:53.908
    STEP: When the matched label of one of its pods change 01/19/23 21:22:54.915
    Jan 19 21:22:54.918: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/19/23 21:22:54.939
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 19 21:22:55.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-4704" for this suite. 01/19/23 21:22:55.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:22:55.962
Jan 19 21:22:55.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename podtemplate 01/19/23 21:22:55.963
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:22:56.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:22:56.029
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 19 21:22:56.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6783" for this suite. 01/19/23 21:22:56.134
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":150,"skipped":2725,"failed":0}
------------------------------
• [0.177 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:22:55.962
    Jan 19 21:22:55.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename podtemplate 01/19/23 21:22:55.963
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:22:56.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:22:56.029
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 19 21:22:56.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-6783" for this suite. 01/19/23 21:22:56.134
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:22:56.139
Jan 19 21:22:56.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename svcaccounts 01/19/23 21:22:56.141
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:22:56.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:22:56.173
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  01/19/23 21:22:56.176
Jan 19 21:22:56.229: INFO: Waiting up to 5m0s for pod "test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0" in namespace "svcaccounts-9052" to be "Succeeded or Failed"
Jan 19 21:22:56.234: INFO: Pod "test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.147692ms
Jan 19 21:22:58.237: INFO: Pod "test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008052956s
Jan 19 21:23:00.236: INFO: Pod "test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006948436s
STEP: Saw pod success 01/19/23 21:23:00.236
Jan 19 21:23:00.236: INFO: Pod "test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0" satisfied condition "Succeeded or Failed"
Jan 19 21:23:00.239: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:23:00.247
Jan 19 21:23:00.260: INFO: Waiting for pod test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0 to disappear
Jan 19 21:23:00.261: INFO: Pod test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 19 21:23:00.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9052" for this suite. 01/19/23 21:23:00.265
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":151,"skipped":2729,"failed":0}
------------------------------
• [4.131 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:22:56.139
    Jan 19 21:22:56.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename svcaccounts 01/19/23 21:22:56.141
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:22:56.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:22:56.173
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  01/19/23 21:22:56.176
    Jan 19 21:22:56.229: INFO: Waiting up to 5m0s for pod "test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0" in namespace "svcaccounts-9052" to be "Succeeded or Failed"
    Jan 19 21:22:56.234: INFO: Pod "test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.147692ms
    Jan 19 21:22:58.237: INFO: Pod "test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008052956s
    Jan 19 21:23:00.236: INFO: Pod "test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006948436s
    STEP: Saw pod success 01/19/23 21:23:00.236
    Jan 19 21:23:00.236: INFO: Pod "test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0" satisfied condition "Succeeded or Failed"
    Jan 19 21:23:00.239: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:23:00.247
    Jan 19 21:23:00.260: INFO: Waiting for pod test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0 to disappear
    Jan 19 21:23:00.261: INFO: Pod test-pod-a9adbb8a-280b-4c86-b458-f7b40f5ae0b0 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 19 21:23:00.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9052" for this suite. 01/19/23 21:23:00.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:23:00.272
Jan 19 21:23:00.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:23:00.273
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:23:00.286
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:23:00.288
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:23:00.321
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:23:00.867
STEP: Deploying the webhook pod 01/19/23 21:23:00.876
STEP: Wait for the deployment to be ready 01/19/23 21:23:00.886
Jan 19 21:23:00.894: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/19/23 21:23:02.908
STEP: Verifying the service has paired with the endpoint 01/19/23 21:23:02.918
Jan 19 21:23:03.918: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 01/19/23 21:23:03.994
STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 21:23:04.025
STEP: Deleting the collection of validation webhooks 01/19/23 21:23:04.051
STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 21:23:04.1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:23:04.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5584" for this suite. 01/19/23 21:23:04.118
STEP: Destroying namespace "webhook-5584-markers" for this suite. 01/19/23 21:23:04.141
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":152,"skipped":2759,"failed":0}
------------------------------
• [3.937 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:23:00.272
    Jan 19 21:23:00.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:23:00.273
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:23:00.286
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:23:00.288
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:23:00.321
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:23:00.867
    STEP: Deploying the webhook pod 01/19/23 21:23:00.876
    STEP: Wait for the deployment to be ready 01/19/23 21:23:00.886
    Jan 19 21:23:00.894: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/19/23 21:23:02.908
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:23:02.918
    Jan 19 21:23:03.918: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 01/19/23 21:23:03.994
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 21:23:04.025
    STEP: Deleting the collection of validation webhooks 01/19/23 21:23:04.051
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 21:23:04.1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:23:04.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5584" for this suite. 01/19/23 21:23:04.118
    STEP: Destroying namespace "webhook-5584-markers" for this suite. 01/19/23 21:23:04.141
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:23:04.209
Jan 19 21:23:04.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename subpath 01/19/23 21:23:04.21
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:23:04.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:23:04.269
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/19/23 21:23:04.272
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-7c55 01/19/23 21:23:04.298
STEP: Creating a pod to test atomic-volume-subpath 01/19/23 21:23:04.298
Jan 19 21:23:04.339: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7c55" in namespace "subpath-2021" to be "Succeeded or Failed"
Jan 19 21:23:04.343: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Pending", Reason="", readiness=false. Elapsed: 3.807153ms
Jan 19 21:23:06.346: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 2.007234714s
Jan 19 21:23:08.347: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 4.007273591s
Jan 19 21:23:10.346: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 6.007177134s
Jan 19 21:23:12.347: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 8.007504702s
Jan 19 21:23:14.347: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 10.007769766s
Jan 19 21:23:16.346: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 12.007052797s
Jan 19 21:23:18.348: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 14.008864662s
Jan 19 21:23:20.348: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 16.008482895s
Jan 19 21:23:22.347: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 18.007882895s
Jan 19 21:23:24.347: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 20.008045661s
Jan 19 21:23:26.348: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=false. Elapsed: 22.009172467s
Jan 19 21:23:28.346: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006914956s
STEP: Saw pod success 01/19/23 21:23:28.346
Jan 19 21:23:28.346: INFO: Pod "pod-subpath-test-secret-7c55" satisfied condition "Succeeded or Failed"
Jan 19 21:23:28.349: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-subpath-test-secret-7c55 container test-container-subpath-secret-7c55: <nil>
STEP: delete the pod 01/19/23 21:23:28.357
Jan 19 21:23:28.367: INFO: Waiting for pod pod-subpath-test-secret-7c55 to disappear
Jan 19 21:23:28.369: INFO: Pod pod-subpath-test-secret-7c55 no longer exists
STEP: Deleting pod pod-subpath-test-secret-7c55 01/19/23 21:23:28.369
Jan 19 21:23:28.369: INFO: Deleting pod "pod-subpath-test-secret-7c55" in namespace "subpath-2021"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 19 21:23:28.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2021" for this suite. 01/19/23 21:23:28.375
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":153,"skipped":2770,"failed":0}
------------------------------
• [SLOW TEST] [24.170 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:23:04.209
    Jan 19 21:23:04.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename subpath 01/19/23 21:23:04.21
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:23:04.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:23:04.269
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/19/23 21:23:04.272
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-7c55 01/19/23 21:23:04.298
    STEP: Creating a pod to test atomic-volume-subpath 01/19/23 21:23:04.298
    Jan 19 21:23:04.339: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-7c55" in namespace "subpath-2021" to be "Succeeded or Failed"
    Jan 19 21:23:04.343: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Pending", Reason="", readiness=false. Elapsed: 3.807153ms
    Jan 19 21:23:06.346: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 2.007234714s
    Jan 19 21:23:08.347: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 4.007273591s
    Jan 19 21:23:10.346: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 6.007177134s
    Jan 19 21:23:12.347: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 8.007504702s
    Jan 19 21:23:14.347: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 10.007769766s
    Jan 19 21:23:16.346: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 12.007052797s
    Jan 19 21:23:18.348: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 14.008864662s
    Jan 19 21:23:20.348: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 16.008482895s
    Jan 19 21:23:22.347: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 18.007882895s
    Jan 19 21:23:24.347: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=true. Elapsed: 20.008045661s
    Jan 19 21:23:26.348: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Running", Reason="", readiness=false. Elapsed: 22.009172467s
    Jan 19 21:23:28.346: INFO: Pod "pod-subpath-test-secret-7c55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006914956s
    STEP: Saw pod success 01/19/23 21:23:28.346
    Jan 19 21:23:28.346: INFO: Pod "pod-subpath-test-secret-7c55" satisfied condition "Succeeded or Failed"
    Jan 19 21:23:28.349: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-subpath-test-secret-7c55 container test-container-subpath-secret-7c55: <nil>
    STEP: delete the pod 01/19/23 21:23:28.357
    Jan 19 21:23:28.367: INFO: Waiting for pod pod-subpath-test-secret-7c55 to disappear
    Jan 19 21:23:28.369: INFO: Pod pod-subpath-test-secret-7c55 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-7c55 01/19/23 21:23:28.369
    Jan 19 21:23:28.369: INFO: Deleting pod "pod-subpath-test-secret-7c55" in namespace "subpath-2021"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 19 21:23:28.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-2021" for this suite. 01/19/23 21:23:28.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:23:28.38
Jan 19 21:23:28.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 21:23:28.381
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:23:28.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:23:28.4
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:23:28.403
Jan 19 21:23:28.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f" in namespace "downward-api-9916" to be "Succeeded or Failed"
Jan 19 21:23:28.452: INFO: Pod "downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.148272ms
Jan 19 21:23:30.456: INFO: Pod "downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010646416s
Jan 19 21:23:32.456: INFO: Pod "downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011302863s
STEP: Saw pod success 01/19/23 21:23:32.456
Jan 19 21:23:32.456: INFO: Pod "downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f" satisfied condition "Succeeded or Failed"
Jan 19 21:23:32.458: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f container client-container: <nil>
STEP: delete the pod 01/19/23 21:23:32.464
Jan 19 21:23:32.476: INFO: Waiting for pod downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f to disappear
Jan 19 21:23:32.479: INFO: Pod downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 21:23:32.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9916" for this suite. 01/19/23 21:23:32.483
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":154,"skipped":2786,"failed":0}
------------------------------
• [4.133 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:23:28.38
    Jan 19 21:23:28.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 21:23:28.381
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:23:28.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:23:28.4
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:23:28.403
    Jan 19 21:23:28.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f" in namespace "downward-api-9916" to be "Succeeded or Failed"
    Jan 19 21:23:28.452: INFO: Pod "downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.148272ms
    Jan 19 21:23:30.456: INFO: Pod "downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010646416s
    Jan 19 21:23:32.456: INFO: Pod "downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011302863s
    STEP: Saw pod success 01/19/23 21:23:32.456
    Jan 19 21:23:32.456: INFO: Pod "downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f" satisfied condition "Succeeded or Failed"
    Jan 19 21:23:32.458: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f container client-container: <nil>
    STEP: delete the pod 01/19/23 21:23:32.464
    Jan 19 21:23:32.476: INFO: Waiting for pod downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f to disappear
    Jan 19 21:23:32.479: INFO: Pod downwardapi-volume-26ae21ca-0de6-495a-b7ac-84537d42817f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 21:23:32.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9916" for this suite. 01/19/23 21:23:32.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:23:32.514
Jan 19 21:23:32.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename resourcequota 01/19/23 21:23:32.515
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:23:32.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:23:32.538
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 01/19/23 21:23:32.54
STEP: Ensuring ResourceQuota status is calculated 01/19/23 21:23:32.552
STEP: Creating a ResourceQuota with not terminating scope 01/19/23 21:23:34.555
STEP: Ensuring ResourceQuota status is calculated 01/19/23 21:23:34.56
STEP: Creating a long running pod 01/19/23 21:23:36.563
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/19/23 21:23:36.582
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/19/23 21:23:38.586
STEP: Deleting the pod 01/19/23 21:23:40.59
STEP: Ensuring resource quota status released the pod usage 01/19/23 21:23:40.604
STEP: Creating a terminating pod 01/19/23 21:23:42.607
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/19/23 21:23:42.623
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/19/23 21:23:44.626
STEP: Deleting the pod 01/19/23 21:23:46.629
STEP: Ensuring resource quota status released the pod usage 01/19/23 21:23:46.639
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 21:23:48.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5530" for this suite. 01/19/23 21:23:48.648
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":155,"skipped":2815,"failed":0}
------------------------------
• [SLOW TEST] [16.138 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:23:32.514
    Jan 19 21:23:32.514: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename resourcequota 01/19/23 21:23:32.515
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:23:32.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:23:32.538
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 01/19/23 21:23:32.54
    STEP: Ensuring ResourceQuota status is calculated 01/19/23 21:23:32.552
    STEP: Creating a ResourceQuota with not terminating scope 01/19/23 21:23:34.555
    STEP: Ensuring ResourceQuota status is calculated 01/19/23 21:23:34.56
    STEP: Creating a long running pod 01/19/23 21:23:36.563
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/19/23 21:23:36.582
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/19/23 21:23:38.586
    STEP: Deleting the pod 01/19/23 21:23:40.59
    STEP: Ensuring resource quota status released the pod usage 01/19/23 21:23:40.604
    STEP: Creating a terminating pod 01/19/23 21:23:42.607
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/19/23 21:23:42.623
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/19/23 21:23:44.626
    STEP: Deleting the pod 01/19/23 21:23:46.629
    STEP: Ensuring resource quota status released the pod usage 01/19/23 21:23:46.639
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 21:23:48.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-5530" for this suite. 01/19/23 21:23:48.648
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:23:48.653
Jan 19 21:23:48.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 21:23:48.654
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:23:48.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:23:48.68
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 01/19/23 21:23:48.682
Jan 19 21:23:48.736: INFO: Waiting up to 5m0s for pod "downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08" in namespace "downward-api-8092" to be "Succeeded or Failed"
Jan 19 21:23:48.742: INFO: Pod "downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08": Phase="Pending", Reason="", readiness=false. Elapsed: 5.743568ms
Jan 19 21:23:50.744: INFO: Pod "downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008401559s
Jan 19 21:23:52.748: INFO: Pod "downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012490629s
STEP: Saw pod success 01/19/23 21:23:52.749
Jan 19 21:23:52.749: INFO: Pod "downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08" satisfied condition "Succeeded or Failed"
Jan 19 21:23:52.752: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08 container dapi-container: <nil>
STEP: delete the pod 01/19/23 21:23:52.766
Jan 19 21:23:52.786: INFO: Waiting for pod downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08 to disappear
Jan 19 21:23:52.789: INFO: Pod downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 19 21:23:52.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8092" for this suite. 01/19/23 21:23:52.795
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":156,"skipped":2817,"failed":0}
------------------------------
• [4.149 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:23:48.653
    Jan 19 21:23:48.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 21:23:48.654
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:23:48.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:23:48.68
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 01/19/23 21:23:48.682
    Jan 19 21:23:48.736: INFO: Waiting up to 5m0s for pod "downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08" in namespace "downward-api-8092" to be "Succeeded or Failed"
    Jan 19 21:23:48.742: INFO: Pod "downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08": Phase="Pending", Reason="", readiness=false. Elapsed: 5.743568ms
    Jan 19 21:23:50.744: INFO: Pod "downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008401559s
    Jan 19 21:23:52.748: INFO: Pod "downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012490629s
    STEP: Saw pod success 01/19/23 21:23:52.749
    Jan 19 21:23:52.749: INFO: Pod "downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08" satisfied condition "Succeeded or Failed"
    Jan 19 21:23:52.752: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08 container dapi-container: <nil>
    STEP: delete the pod 01/19/23 21:23:52.766
    Jan 19 21:23:52.786: INFO: Waiting for pod downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08 to disappear
    Jan 19 21:23:52.789: INFO: Pod downward-api-0d588e00-f9f7-48c6-8149-c05da650ae08 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 19 21:23:52.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8092" for this suite. 01/19/23 21:23:52.795
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:23:52.802
Jan 19 21:23:52.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 21:23:52.803
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:23:52.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:23:52.845
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/19/23 21:23:52.891
Jan 19 21:23:52.945: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4001" to be "running and ready"
Jan 19 21:23:52.965: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 20.595054ms
Jan 19 21:23:52.965: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:23:54.969: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.024070399s
Jan 19 21:23:54.969: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 19 21:23:54.969: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 01/19/23 21:23:54.971
Jan 19 21:23:54.986: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4001" to be "running and ready"
Jan 19 21:23:54.992: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.242896ms
Jan 19 21:23:54.992: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:23:56.996: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009857741s
Jan 19 21:23:56.996: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan 19 21:23:56.996: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/19/23 21:23:56.998
Jan 19 21:23:57.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 21:23:57.006: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 19 21:23:59.007: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 21:23:59.011: INFO: Pod pod-with-prestop-exec-hook still exists
Jan 19 21:24:01.007: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan 19 21:24:01.010: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/19/23 21:24:01.01
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 19 21:24:01.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4001" for this suite. 01/19/23 21:24:01.027
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":157,"skipped":2825,"failed":0}
------------------------------
• [SLOW TEST] [8.229 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:23:52.802
    Jan 19 21:23:52.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 21:23:52.803
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:23:52.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:23:52.845
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/19/23 21:23:52.891
    Jan 19 21:23:52.945: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4001" to be "running and ready"
    Jan 19 21:23:52.965: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 20.595054ms
    Jan 19 21:23:52.965: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:23:54.969: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.024070399s
    Jan 19 21:23:54.969: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 19 21:23:54.969: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 01/19/23 21:23:54.971
    Jan 19 21:23:54.986: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4001" to be "running and ready"
    Jan 19 21:23:54.992: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.242896ms
    Jan 19 21:23:54.992: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:23:56.996: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009857741s
    Jan 19 21:23:56.996: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan 19 21:23:56.996: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/19/23 21:23:56.998
    Jan 19 21:23:57.004: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 19 21:23:57.006: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 19 21:23:59.007: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 19 21:23:59.011: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan 19 21:24:01.007: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan 19 21:24:01.010: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/19/23 21:24:01.01
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 19 21:24:01.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4001" for this suite. 01/19/23 21:24:01.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:01.032
Jan 19 21:24:01.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename disruption 01/19/23 21:24:01.033
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:01.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:01.068
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 01/19/23 21:24:01.071
STEP: Waiting for the pdb to be processed 01/19/23 21:24:01.081
STEP: First trying to evict a pod which shouldn't be evictable 01/19/23 21:24:03.1
STEP: Waiting for all pods to be running 01/19/23 21:24:03.1
Jan 19 21:24:03.102: INFO: pods: 0 < 3
STEP: locating a running pod 01/19/23 21:24:05.105
STEP: Updating the pdb to allow a pod to be evicted 01/19/23 21:24:05.112
STEP: Waiting for the pdb to be processed 01/19/23 21:24:05.118
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/19/23 21:24:07.124
STEP: Waiting for all pods to be running 01/19/23 21:24:07.124
STEP: Waiting for the pdb to observed all healthy pods 01/19/23 21:24:07.127
STEP: Patching the pdb to disallow a pod to be evicted 01/19/23 21:24:07.154
STEP: Waiting for the pdb to be processed 01/19/23 21:24:07.165
STEP: Waiting for all pods to be running 01/19/23 21:24:09.174
STEP: locating a running pod 01/19/23 21:24:09.176
STEP: Deleting the pdb to allow a pod to be evicted 01/19/23 21:24:09.183
STEP: Waiting for the pdb to be deleted 01/19/23 21:24:09.19
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/19/23 21:24:09.192
STEP: Waiting for all pods to be running 01/19/23 21:24:09.192
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 19 21:24:09.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-799" for this suite. 01/19/23 21:24:09.208
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":158,"skipped":2831,"failed":0}
------------------------------
• [SLOW TEST] [8.183 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:01.032
    Jan 19 21:24:01.032: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename disruption 01/19/23 21:24:01.033
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:01.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:01.068
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 01/19/23 21:24:01.071
    STEP: Waiting for the pdb to be processed 01/19/23 21:24:01.081
    STEP: First trying to evict a pod which shouldn't be evictable 01/19/23 21:24:03.1
    STEP: Waiting for all pods to be running 01/19/23 21:24:03.1
    Jan 19 21:24:03.102: INFO: pods: 0 < 3
    STEP: locating a running pod 01/19/23 21:24:05.105
    STEP: Updating the pdb to allow a pod to be evicted 01/19/23 21:24:05.112
    STEP: Waiting for the pdb to be processed 01/19/23 21:24:05.118
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/19/23 21:24:07.124
    STEP: Waiting for all pods to be running 01/19/23 21:24:07.124
    STEP: Waiting for the pdb to observed all healthy pods 01/19/23 21:24:07.127
    STEP: Patching the pdb to disallow a pod to be evicted 01/19/23 21:24:07.154
    STEP: Waiting for the pdb to be processed 01/19/23 21:24:07.165
    STEP: Waiting for all pods to be running 01/19/23 21:24:09.174
    STEP: locating a running pod 01/19/23 21:24:09.176
    STEP: Deleting the pdb to allow a pod to be evicted 01/19/23 21:24:09.183
    STEP: Waiting for the pdb to be deleted 01/19/23 21:24:09.19
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/19/23 21:24:09.192
    STEP: Waiting for all pods to be running 01/19/23 21:24:09.192
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 19 21:24:09.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-799" for this suite. 01/19/23 21:24:09.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:09.216
Jan 19 21:24:09.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename conformance-tests 01/19/23 21:24:09.217
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:09.253
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:09.255
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/19/23 21:24:09.259
Jan 19 21:24:09.259: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jan 19 21:24:09.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-1898" for this suite. 01/19/23 21:24:09.279
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":159,"skipped":2844,"failed":0}
------------------------------
• [0.075 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:09.216
    Jan 19 21:24:09.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename conformance-tests 01/19/23 21:24:09.217
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:09.253
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:09.255
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/19/23 21:24:09.259
    Jan 19 21:24:09.259: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jan 19 21:24:09.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-1898" for this suite. 01/19/23 21:24:09.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:09.292
Jan 19 21:24:09.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 21:24:09.292
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:09.331
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:09.333
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan 19 21:24:09.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:24:09.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8672" for this suite. 01/19/23 21:24:09.943
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":160,"skipped":2852,"failed":0}
------------------------------
• [0.662 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:09.292
    Jan 19 21:24:09.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 21:24:09.292
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:09.331
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:09.333
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan 19 21:24:09.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:24:09.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-8672" for this suite. 01/19/23 21:24:09.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:09.954
Jan 19 21:24:09.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename replicaset 01/19/23 21:24:09.955
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:09.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:09.98
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/19/23 21:24:10.012
STEP: Verify that the required pods have come up. 01/19/23 21:24:10.021
Jan 19 21:24:10.028: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 19 21:24:15.035: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/19/23 21:24:15.035
STEP: Getting /status 01/19/23 21:24:15.035
Jan 19 21:24:15.041: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/19/23 21:24:15.041
Jan 19 21:24:15.070: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/19/23 21:24:15.07
Jan 19 21:24:15.087: INFO: Observed &ReplicaSet event: ADDED
Jan 19 21:24:15.087: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 21:24:15.087: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 21:24:15.087: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 21:24:15.087: INFO: Found replicaset test-rs in namespace replicaset-1639 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 19 21:24:15.087: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/19/23 21:24:15.087
Jan 19 21:24:15.087: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan 19 21:24:15.096: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/19/23 21:24:15.096
Jan 19 21:24:15.097: INFO: Observed &ReplicaSet event: ADDED
Jan 19 21:24:15.097: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 21:24:15.097: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 21:24:15.098: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 21:24:15.098: INFO: Observed replicaset test-rs in namespace replicaset-1639 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan 19 21:24:15.098: INFO: Observed &ReplicaSet event: MODIFIED
Jan 19 21:24:15.098: INFO: Found replicaset test-rs in namespace replicaset-1639 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan 19 21:24:15.098: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 19 21:24:15.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1639" for this suite. 01/19/23 21:24:15.104
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":161,"skipped":2860,"failed":0}
------------------------------
• [SLOW TEST] [5.155 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:09.954
    Jan 19 21:24:09.955: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename replicaset 01/19/23 21:24:09.955
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:09.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:09.98
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/19/23 21:24:10.012
    STEP: Verify that the required pods have come up. 01/19/23 21:24:10.021
    Jan 19 21:24:10.028: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 19 21:24:15.035: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/19/23 21:24:15.035
    STEP: Getting /status 01/19/23 21:24:15.035
    Jan 19 21:24:15.041: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/19/23 21:24:15.041
    Jan 19 21:24:15.070: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/19/23 21:24:15.07
    Jan 19 21:24:15.087: INFO: Observed &ReplicaSet event: ADDED
    Jan 19 21:24:15.087: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 21:24:15.087: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 21:24:15.087: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 21:24:15.087: INFO: Found replicaset test-rs in namespace replicaset-1639 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 19 21:24:15.087: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/19/23 21:24:15.087
    Jan 19 21:24:15.087: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan 19 21:24:15.096: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/19/23 21:24:15.096
    Jan 19 21:24:15.097: INFO: Observed &ReplicaSet event: ADDED
    Jan 19 21:24:15.097: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 21:24:15.097: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 21:24:15.098: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 21:24:15.098: INFO: Observed replicaset test-rs in namespace replicaset-1639 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan 19 21:24:15.098: INFO: Observed &ReplicaSet event: MODIFIED
    Jan 19 21:24:15.098: INFO: Found replicaset test-rs in namespace replicaset-1639 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan 19 21:24:15.098: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 19 21:24:15.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1639" for this suite. 01/19/23 21:24:15.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:15.111
Jan 19 21:24:15.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:24:15.112
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:15.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:15.158
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 01/19/23 21:24:15.16
Jan 19 21:24:15.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-6948 cluster-info'
Jan 19 21:24:15.251: INFO: stderr: ""
Jan 19 21:24:15.251: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:24:15.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6948" for this suite. 01/19/23 21:24:15.258
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":162,"skipped":2896,"failed":0}
------------------------------
• [0.155 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:15.111
    Jan 19 21:24:15.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:24:15.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:15.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:15.158
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 01/19/23 21:24:15.16
    Jan 19 21:24:15.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-6948 cluster-info'
    Jan 19 21:24:15.251: INFO: stderr: ""
    Jan 19 21:24:15.251: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.30.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:24:15.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6948" for this suite. 01/19/23 21:24:15.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:15.267
Jan 19 21:24:15.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 21:24:15.268
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:15.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:15.298
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-5795 01/19/23 21:24:15.301
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/19/23 21:24:15.341
STEP: creating service externalsvc in namespace services-5795 01/19/23 21:24:15.341
STEP: creating replication controller externalsvc in namespace services-5795 01/19/23 21:24:15.379
I0119 21:24:15.386162      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5795, replica count: 2
I0119 21:24:18.436416      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/19/23 21:24:18.439
Jan 19 21:24:18.453: INFO: Creating new exec pod
Jan 19 21:24:18.469: INFO: Waiting up to 5m0s for pod "execpod267gr" in namespace "services-5795" to be "running"
Jan 19 21:24:18.473: INFO: Pod "execpod267gr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.509233ms
Jan 19 21:24:20.478: INFO: Pod "execpod267gr": Phase="Running", Reason="", readiness=true. Elapsed: 2.009371072s
Jan 19 21:24:20.478: INFO: Pod "execpod267gr" satisfied condition "running"
Jan 19 21:24:20.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5795 exec execpod267gr -- /bin/sh -x -c nslookup nodeport-service.services-5795.svc.cluster.local'
Jan 19 21:24:20.637: INFO: stderr: "+ nslookup nodeport-service.services-5795.svc.cluster.local\n"
Jan 19 21:24:20.638: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-5795.svc.cluster.local\tcanonical name = externalsvc.services-5795.svc.cluster.local.\nName:\texternalsvc.services-5795.svc.cluster.local\nAddress: 172.30.66.62\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-5795, will wait for the garbage collector to delete the pods 01/19/23 21:24:20.638
Jan 19 21:24:20.696: INFO: Deleting ReplicationController externalsvc took: 4.34745ms
Jan 19 21:24:20.797: INFO: Terminating ReplicationController externalsvc pods took: 100.489896ms
Jan 19 21:24:23.321: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 21:24:23.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5795" for this suite. 01/19/23 21:24:23.338
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":163,"skipped":2921,"failed":0}
------------------------------
• [SLOW TEST] [8.092 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:15.267
    Jan 19 21:24:15.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 21:24:15.268
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:15.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:15.298
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-5795 01/19/23 21:24:15.301
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/19/23 21:24:15.341
    STEP: creating service externalsvc in namespace services-5795 01/19/23 21:24:15.341
    STEP: creating replication controller externalsvc in namespace services-5795 01/19/23 21:24:15.379
    I0119 21:24:15.386162      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-5795, replica count: 2
    I0119 21:24:18.436416      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/19/23 21:24:18.439
    Jan 19 21:24:18.453: INFO: Creating new exec pod
    Jan 19 21:24:18.469: INFO: Waiting up to 5m0s for pod "execpod267gr" in namespace "services-5795" to be "running"
    Jan 19 21:24:18.473: INFO: Pod "execpod267gr": Phase="Pending", Reason="", readiness=false. Elapsed: 4.509233ms
    Jan 19 21:24:20.478: INFO: Pod "execpod267gr": Phase="Running", Reason="", readiness=true. Elapsed: 2.009371072s
    Jan 19 21:24:20.478: INFO: Pod "execpod267gr" satisfied condition "running"
    Jan 19 21:24:20.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-5795 exec execpod267gr -- /bin/sh -x -c nslookup nodeport-service.services-5795.svc.cluster.local'
    Jan 19 21:24:20.637: INFO: stderr: "+ nslookup nodeport-service.services-5795.svc.cluster.local\n"
    Jan 19 21:24:20.638: INFO: stdout: "Server:\t\t172.30.0.10\nAddress:\t172.30.0.10#53\n\nnodeport-service.services-5795.svc.cluster.local\tcanonical name = externalsvc.services-5795.svc.cluster.local.\nName:\texternalsvc.services-5795.svc.cluster.local\nAddress: 172.30.66.62\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-5795, will wait for the garbage collector to delete the pods 01/19/23 21:24:20.638
    Jan 19 21:24:20.696: INFO: Deleting ReplicationController externalsvc took: 4.34745ms
    Jan 19 21:24:20.797: INFO: Terminating ReplicationController externalsvc pods took: 100.489896ms
    Jan 19 21:24:23.321: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 21:24:23.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5795" for this suite. 01/19/23 21:24:23.338
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:23.36
Jan 19 21:24:23.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename namespaces 01/19/23 21:24:23.361
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:23.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:23.385
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 01/19/23 21:24:23.397
STEP: patching the Namespace 01/19/23 21:24:23.471
STEP: get the Namespace and ensuring it has the label 01/19/23 21:24:23.496
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:24:23.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1100" for this suite. 01/19/23 21:24:23.511
STEP: Destroying namespace "nspatchtest-7baaebf6-2528-44ea-9db9-d9f77d1c916e-4817" for this suite. 01/19/23 21:24:23.521
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":164,"skipped":2936,"failed":0}
------------------------------
• [0.178 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:23.36
    Jan 19 21:24:23.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename namespaces 01/19/23 21:24:23.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:23.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:23.385
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 01/19/23 21:24:23.397
    STEP: patching the Namespace 01/19/23 21:24:23.471
    STEP: get the Namespace and ensuring it has the label 01/19/23 21:24:23.496
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:24:23.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-1100" for this suite. 01/19/23 21:24:23.511
    STEP: Destroying namespace "nspatchtest-7baaebf6-2528-44ea-9db9-d9f77d1c916e-4817" for this suite. 01/19/23 21:24:23.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:23.54
Jan 19 21:24:23.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:24:23.541
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:23.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:23.567
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 01/19/23 21:24:23.571
Jan 19 21:24:23.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 create -f -'
Jan 19 21:24:26.001: INFO: stderr: ""
Jan 19 21:24:26.001: INFO: stdout: "pod/pause created\n"
Jan 19 21:24:26.001: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan 19 21:24:26.001: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-342" to be "running and ready"
Jan 19 21:24:26.004: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.244169ms
Jan 19 21:24:26.004: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-0-172-44.ec2.internal' to be 'Running' but was 'Pending'
Jan 19 21:24:28.007: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006237304s
Jan 19 21:24:28.007: INFO: Pod "pause" satisfied condition "running and ready"
Jan 19 21:24:28.007: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 01/19/23 21:24:28.007
Jan 19 21:24:28.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 label pods pause testing-label=testing-label-value'
Jan 19 21:24:28.078: INFO: stderr: ""
Jan 19 21:24:28.078: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/19/23 21:24:28.078
Jan 19 21:24:28.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 get pod pause -L testing-label'
Jan 19 21:24:28.134: INFO: stderr: ""
Jan 19 21:24:28.134: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/19/23 21:24:28.134
Jan 19 21:24:28.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 label pods pause testing-label-'
Jan 19 21:24:28.202: INFO: stderr: ""
Jan 19 21:24:28.202: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/19/23 21:24:28.202
Jan 19 21:24:28.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 get pod pause -L testing-label'
Jan 19 21:24:28.258: INFO: stderr: ""
Jan 19 21:24:28.258: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 01/19/23 21:24:28.259
Jan 19 21:24:28.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 delete --grace-period=0 --force -f -'
Jan 19 21:24:28.320: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 21:24:28.320: INFO: stdout: "pod \"pause\" force deleted\n"
Jan 19 21:24:28.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 get rc,svc -l name=pause --no-headers'
Jan 19 21:24:28.388: INFO: stderr: "No resources found in kubectl-342 namespace.\n"
Jan 19 21:24:28.388: INFO: stdout: ""
Jan 19 21:24:28.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 19 21:24:28.442: INFO: stderr: ""
Jan 19 21:24:28.443: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:24:28.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-342" for this suite. 01/19/23 21:24:28.446
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":165,"skipped":2984,"failed":0}
------------------------------
• [4.912 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:23.54
    Jan 19 21:24:23.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:24:23.541
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:23.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:23.567
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 01/19/23 21:24:23.571
    Jan 19 21:24:23.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 create -f -'
    Jan 19 21:24:26.001: INFO: stderr: ""
    Jan 19 21:24:26.001: INFO: stdout: "pod/pause created\n"
    Jan 19 21:24:26.001: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan 19 21:24:26.001: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-342" to be "running and ready"
    Jan 19 21:24:26.004: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.244169ms
    Jan 19 21:24:26.004: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-0-172-44.ec2.internal' to be 'Running' but was 'Pending'
    Jan 19 21:24:28.007: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006237304s
    Jan 19 21:24:28.007: INFO: Pod "pause" satisfied condition "running and ready"
    Jan 19 21:24:28.007: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 01/19/23 21:24:28.007
    Jan 19 21:24:28.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 label pods pause testing-label=testing-label-value'
    Jan 19 21:24:28.078: INFO: stderr: ""
    Jan 19 21:24:28.078: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/19/23 21:24:28.078
    Jan 19 21:24:28.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 get pod pause -L testing-label'
    Jan 19 21:24:28.134: INFO: stderr: ""
    Jan 19 21:24:28.134: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/19/23 21:24:28.134
    Jan 19 21:24:28.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 label pods pause testing-label-'
    Jan 19 21:24:28.202: INFO: stderr: ""
    Jan 19 21:24:28.202: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/19/23 21:24:28.202
    Jan 19 21:24:28.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 get pod pause -L testing-label'
    Jan 19 21:24:28.258: INFO: stderr: ""
    Jan 19 21:24:28.258: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 01/19/23 21:24:28.259
    Jan 19 21:24:28.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 delete --grace-period=0 --force -f -'
    Jan 19 21:24:28.320: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 21:24:28.320: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan 19 21:24:28.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 get rc,svc -l name=pause --no-headers'
    Jan 19 21:24:28.388: INFO: stderr: "No resources found in kubectl-342 namespace.\n"
    Jan 19 21:24:28.388: INFO: stdout: ""
    Jan 19 21:24:28.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-342 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 19 21:24:28.442: INFO: stderr: ""
    Jan 19 21:24:28.443: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:24:28.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-342" for this suite. 01/19/23 21:24:28.446
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:28.453
Jan 19 21:24:28.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename gc 01/19/23 21:24:28.454
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:28.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:28.472
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
W0119 21:24:28.504620      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:24:28.558: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"19e1a989-c6bc-4e0e-93b4-b8144779f2b3", Controller:(*bool)(0xc000855172), BlockOwnerDeletion:(*bool)(0xc000855173)}}
Jan 19 21:24:28.566: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"97d58b23-8acd-42b9-9371-3e0a939472be", Controller:(*bool)(0xc002511586), BlockOwnerDeletion:(*bool)(0xc002511587)}}
Jan 19 21:24:28.590: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"fb1edc7e-4e4d-48bb-a0fb-99581f461353", Controller:(*bool)(0xc0039e40b2), BlockOwnerDeletion:(*bool)(0xc0039e40b3)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 21:24:33.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6762" for this suite. 01/19/23 21:24:33.61
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":166,"skipped":2995,"failed":0}
------------------------------
• [SLOW TEST] [5.163 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:28.453
    Jan 19 21:24:28.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename gc 01/19/23 21:24:28.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:28.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:28.472
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    W0119 21:24:28.504620      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:24:28.558: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"19e1a989-c6bc-4e0e-93b4-b8144779f2b3", Controller:(*bool)(0xc000855172), BlockOwnerDeletion:(*bool)(0xc000855173)}}
    Jan 19 21:24:28.566: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"97d58b23-8acd-42b9-9371-3e0a939472be", Controller:(*bool)(0xc002511586), BlockOwnerDeletion:(*bool)(0xc002511587)}}
    Jan 19 21:24:28.590: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"fb1edc7e-4e4d-48bb-a0fb-99581f461353", Controller:(*bool)(0xc0039e40b2), BlockOwnerDeletion:(*bool)(0xc0039e40b3)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 21:24:33.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6762" for this suite. 01/19/23 21:24:33.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:33.617
Jan 19 21:24:33.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename replication-controller 01/19/23 21:24:33.618
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:33.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:33.645
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 01/19/23 21:24:33.648
W0119 21:24:33.657800      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: When the matched label of one of its pods change 01/19/23 21:24:33.657
Jan 19 21:24:33.672: INFO: Pod name pod-release: Found 0 pods out of 1
Jan 19 21:24:38.675: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/19/23 21:24:38.691
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 19 21:24:39.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4993" for this suite. 01/19/23 21:24:39.7
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":167,"skipped":3042,"failed":0}
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:33.617
    Jan 19 21:24:33.618: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename replication-controller 01/19/23 21:24:33.618
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:33.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:33.645
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 01/19/23 21:24:33.648
    W0119 21:24:33.657800      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "pod-release" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "pod-release" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "pod-release" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "pod-release" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: When the matched label of one of its pods change 01/19/23 21:24:33.657
    Jan 19 21:24:33.672: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan 19 21:24:38.675: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/19/23 21:24:38.691
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 19 21:24:39.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4993" for this suite. 01/19/23 21:24:39.7
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:39.706
Jan 19 21:24:39.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:24:39.707
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:39.726
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:39.728
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:24:39.839
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:24:40.127
STEP: Deploying the webhook pod 01/19/23 21:24:40.174
STEP: Wait for the deployment to be ready 01/19/23 21:24:40.221
Jan 19 21:24:40.230: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/19/23 21:24:42.238
STEP: Verifying the service has paired with the endpoint 01/19/23 21:24:42.247
Jan 19 21:24:43.248: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jan 19 21:24:43.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7222-crds.webhook.example.com via the AdmissionRegistration API 01/19/23 21:24:43.762
STEP: Creating a custom resource while v1 is storage version 01/19/23 21:24:43.775
STEP: Patching Custom Resource Definition to set v2 as storage 01/19/23 21:24:45.829
STEP: Patching the custom resource while v2 is storage version 01/19/23 21:24:45.85
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:24:46.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3419" for this suite. 01/19/23 21:24:46.422
STEP: Destroying namespace "webhook-3419-markers" for this suite. 01/19/23 21:24:46.43
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":168,"skipped":3045,"failed":0}
------------------------------
• [SLOW TEST] [6.788 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:39.706
    Jan 19 21:24:39.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:24:39.707
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:39.726
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:39.728
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:24:39.839
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:24:40.127
    STEP: Deploying the webhook pod 01/19/23 21:24:40.174
    STEP: Wait for the deployment to be ready 01/19/23 21:24:40.221
    Jan 19 21:24:40.230: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/19/23 21:24:42.238
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:24:42.247
    Jan 19 21:24:43.248: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jan 19 21:24:43.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7222-crds.webhook.example.com via the AdmissionRegistration API 01/19/23 21:24:43.762
    STEP: Creating a custom resource while v1 is storage version 01/19/23 21:24:43.775
    STEP: Patching Custom Resource Definition to set v2 as storage 01/19/23 21:24:45.829
    STEP: Patching the custom resource while v2 is storage version 01/19/23 21:24:45.85
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:24:46.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3419" for this suite. 01/19/23 21:24:46.422
    STEP: Destroying namespace "webhook-3419-markers" for this suite. 01/19/23 21:24:46.43
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:46.497
Jan 19 21:24:46.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 21:24:46.498
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:46.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:46.535
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-6045 01/19/23 21:24:46.574
STEP: creating service affinity-clusterip in namespace services-6045 01/19/23 21:24:46.574
STEP: creating replication controller affinity-clusterip in namespace services-6045 01/19/23 21:24:46.612
I0119 21:24:46.633454      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6045, replica count: 3
I0119 21:24:49.684180      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 21:24:49.688: INFO: Creating new exec pod
Jan 19 21:24:49.703: INFO: Waiting up to 5m0s for pod "execpod-affinityvxtkk" in namespace "services-6045" to be "running"
Jan 19 21:24:49.705: INFO: Pod "execpod-affinityvxtkk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182204ms
Jan 19 21:24:51.707: INFO: Pod "execpod-affinityvxtkk": Phase="Running", Reason="", readiness=true. Elapsed: 2.004623584s
Jan 19 21:24:51.707: INFO: Pod "execpod-affinityvxtkk" satisfied condition "running"
Jan 19 21:24:52.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6045 exec execpod-affinityvxtkk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jan 19 21:24:53.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan 19 21:24:53.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:24:53.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6045 exec execpod-affinityvxtkk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.211.199 80'
Jan 19 21:24:54.009: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.211.199 80\nConnection to 172.30.211.199 80 port [tcp/http] succeeded!\n"
Jan 19 21:24:54.009: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:24:54.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6045 exec execpod-affinityvxtkk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.211.199:80/ ; done'
Jan 19 21:24:54.175: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n"
Jan 19 21:24:54.175: INFO: stdout: "\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf"
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
Jan 19 21:24:54.175: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6045, will wait for the garbage collector to delete the pods 01/19/23 21:24:54.184
Jan 19 21:24:54.242: INFO: Deleting ReplicationController affinity-clusterip took: 5.509856ms
Jan 19 21:24:54.342: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.379323ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 21:24:56.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6045" for this suite. 01/19/23 21:24:56.569
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":169,"skipped":3115,"failed":0}
------------------------------
• [SLOW TEST] [10.082 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:46.497
    Jan 19 21:24:46.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 21:24:46.498
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:46.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:46.535
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-6045 01/19/23 21:24:46.574
    STEP: creating service affinity-clusterip in namespace services-6045 01/19/23 21:24:46.574
    STEP: creating replication controller affinity-clusterip in namespace services-6045 01/19/23 21:24:46.612
    I0119 21:24:46.633454      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6045, replica count: 3
    I0119 21:24:49.684180      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 21:24:49.688: INFO: Creating new exec pod
    Jan 19 21:24:49.703: INFO: Waiting up to 5m0s for pod "execpod-affinityvxtkk" in namespace "services-6045" to be "running"
    Jan 19 21:24:49.705: INFO: Pod "execpod-affinityvxtkk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.182204ms
    Jan 19 21:24:51.707: INFO: Pod "execpod-affinityvxtkk": Phase="Running", Reason="", readiness=true. Elapsed: 2.004623584s
    Jan 19 21:24:51.707: INFO: Pod "execpod-affinityvxtkk" satisfied condition "running"
    Jan 19 21:24:52.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6045 exec execpod-affinityvxtkk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jan 19 21:24:53.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan 19 21:24:53.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:24:53.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6045 exec execpod-affinityvxtkk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.211.199 80'
    Jan 19 21:24:54.009: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.211.199 80\nConnection to 172.30.211.199 80 port [tcp/http] succeeded!\n"
    Jan 19 21:24:54.009: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:24:54.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6045 exec execpod-affinityvxtkk -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.211.199:80/ ; done'
    Jan 19 21:24:54.175: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.211.199:80/\n"
    Jan 19 21:24:54.175: INFO: stdout: "\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf\naffinity-clusterip-jzwjf"
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Received response from host: affinity-clusterip-jzwjf
    Jan 19 21:24:54.175: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-6045, will wait for the garbage collector to delete the pods 01/19/23 21:24:54.184
    Jan 19 21:24:54.242: INFO: Deleting ReplicationController affinity-clusterip took: 5.509856ms
    Jan 19 21:24:54.342: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.379323ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 21:24:56.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6045" for this suite. 01/19/23 21:24:56.569
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:56.582
Jan 19 21:24:56.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 21:24:56.583
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:56.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:56.619
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan 19 21:24:56.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:24:59.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2464" for this suite. 01/19/23 21:24:59.753
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":170,"skipped":3132,"failed":0}
------------------------------
• [3.176 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:56.582
    Jan 19 21:24:56.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename custom-resource-definition 01/19/23 21:24:56.583
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:56.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:56.619
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan 19 21:24:56.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:24:59.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2464" for this suite. 01/19/23 21:24:59.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:24:59.759
Jan 19 21:24:59.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename deployment 01/19/23 21:24:59.76
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:59.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:59.788
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan 19 21:24:59.790: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
W0119 21:24:59.800288      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:24:59.812: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 19 21:25:04.817: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/19/23 21:25:04.817
Jan 19 21:25:04.817: INFO: Creating deployment "test-rolling-update-deployment"
Jan 19 21:25:04.822: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan 19 21:25:04.828: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan 19 21:25:06.833: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan 19 21:25:06.836: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 21:25:06.842: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6565  098aaf6b-0b12-40b3-9fef-9908b367606e 186618 1 2023-01-19 21:25:04 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-19 21:25:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bb104a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 21:25:04 +0000 UTC,LastTransitionTime:2023-01-19 21:25:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-19 21:25:05 +0000 UTC,LastTransitionTime:2023-01-19 21:25:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 21:25:06.844: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-6565  a76cb515-e26a-4c7f-a28d-2c5375ebd5bc 186608 1 2023-01-19 21:25:04 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 098aaf6b-0b12-40b3-9fef-9908b367606e 0xc00bb109b7 0xc00bb109b8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 21:25:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098aaf6b-0b12-40b3-9fef-9908b367606e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bb10a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 21:25:06.844: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan 19 21:25:06.844: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6565  51e3b860-b752-45df-98c6-d48555dec691 186616 2 2023-01-19 21:24:59 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 098aaf6b-0b12-40b3-9fef-9908b367606e 0xc00bb10887 0xc00bb10888}] [] [{e2e.test Update apps/v1 2023-01-19 21:24:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098aaf6b-0b12-40b3-9fef-9908b367606e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00bb10948 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 21:25:06.847: INFO: Pod "test-rolling-update-deployment-78f575d8ff-nvmjf" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-nvmjf test-rolling-update-deployment-78f575d8ff- deployment-6565  7b132dd3-043d-48ab-a3bb-0c42a1f1e31e 186607 0 2023-01-19 21:25:04 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.160/23"],"mac_address":"0a:58:0a:80:08:a0","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.160/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.160"
    ],
    "mac": "0a:58:0a:80:08:a0",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.160"
    ],
    "mac": "0a:58:0a:80:08:a0",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff a76cb515-e26a-4c7f-a28d-2c5375ebd5bc 0xc00427f3b7 0xc00427f3b8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 21:25:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 21:25:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a76cb515-e26a-4c7f-a28d-2c5375ebd5bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 21:25:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-19 21:25:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gpjlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gpjlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ll4qk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.160,StartTime:2023-01-19 21:25:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 21:25:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://5eec8d334382cd14cdffa68c9ec9b5cae4eee3724628d3aa56ad261d8e2d72a5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 21:25:06.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6565" for this suite. 01/19/23 21:25:06.851
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":171,"skipped":3155,"failed":0}
------------------------------
• [SLOW TEST] [7.098 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:24:59.759
    Jan 19 21:24:59.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename deployment 01/19/23 21:24:59.76
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:24:59.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:24:59.788
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan 19 21:24:59.790: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    W0119 21:24:59.800288      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:24:59.812: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 19 21:25:04.817: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/19/23 21:25:04.817
    Jan 19 21:25:04.817: INFO: Creating deployment "test-rolling-update-deployment"
    Jan 19 21:25:04.822: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan 19 21:25:04.828: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan 19 21:25:06.833: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan 19 21:25:06.836: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 21:25:06.842: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-6565  098aaf6b-0b12-40b3-9fef-9908b367606e 186618 1 2023-01-19 21:25:04 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-19 21:25:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bb104a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 21:25:04 +0000 UTC,LastTransitionTime:2023-01-19 21:25:04 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-01-19 21:25:05 +0000 UTC,LastTransitionTime:2023-01-19 21:25:04 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 19 21:25:06.844: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-6565  a76cb515-e26a-4c7f-a28d-2c5375ebd5bc 186608 1 2023-01-19 21:25:04 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 098aaf6b-0b12-40b3-9fef-9908b367606e 0xc00bb109b7 0xc00bb109b8}] [] [{kube-controller-manager Update apps/v1 2023-01-19 21:25:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098aaf6b-0b12-40b3-9fef-9908b367606e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bb10a68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 21:25:06.844: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan 19 21:25:06.844: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-6565  51e3b860-b752-45df-98c6-d48555dec691 186616 2 2023-01-19 21:24:59 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 098aaf6b-0b12-40b3-9fef-9908b367606e 0xc00bb10887 0xc00bb10888}] [] [{e2e.test Update apps/v1 2023-01-19 21:24:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"098aaf6b-0b12-40b3-9fef-9908b367606e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00bb10948 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 21:25:06.847: INFO: Pod "test-rolling-update-deployment-78f575d8ff-nvmjf" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-nvmjf test-rolling-update-deployment-78f575d8ff- deployment-6565  7b132dd3-043d-48ab-a3bb-0c42a1f1e31e 186607 0 2023-01-19 21:25:04 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.160/23"],"mac_address":"0a:58:0a:80:08:a0","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.160/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.160"
        ],
        "mac": "0a:58:0a:80:08:a0",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.160"
        ],
        "mac": "0a:58:0a:80:08:a0",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff a76cb515-e26a-4c7f-a28d-2c5375ebd5bc 0xc00427f3b7 0xc00427f3b8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 21:25:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 21:25:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a76cb515-e26a-4c7f-a28d-2c5375ebd5bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 21:25:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-19 21:25:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gpjlc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gpjlc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-ll4qk,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.160,StartTime:2023-01-19 21:25:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 21:25:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://5eec8d334382cd14cdffa68c9ec9b5cae4eee3724628d3aa56ad261d8e2d72a5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 21:25:06.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6565" for this suite. 01/19/23 21:25:06.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:06.857
Jan 19 21:25:06.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename daemonsets 01/19/23 21:25:06.858
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:06.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:06.887
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 01/19/23 21:25:07.025
STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 21:25:07.031
Jan 19 21:25:07.055: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:25:07.055: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:25:07.055: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:25:07.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:25:07.057: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:25:08.062: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:25:08.062: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:25:08.062: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:25:08.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 21:25:08.065: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:25:09.062: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:25:09.062: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:25:09.062: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:25:09.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 19 21:25:09.065: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
STEP: listing all DeamonSets 01/19/23 21:25:09.067
STEP: DeleteCollection of the DaemonSets 01/19/23 21:25:09.071
STEP: Verify that ReplicaSets have been deleted 01/19/23 21:25:09.077
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jan 19 21:25:09.090: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"186803"},"items":null}

Jan 19 21:25:09.095: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"186809"},"items":[{"metadata":{"name":"daemon-set-7szzm","generateName":"daemon-set-","namespace":"daemonsets-1272","uid":"cc40b0a1-a776-4770-aeff-da26a72c175c","resourceVersion":"186804","creationTimestamp":"2023-01-19T21:25:07Z","deletionTimestamp":"2023-01-19T21:25:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.6.23/23\"],\"mac_address\":\"0a:58:0a:80:06:17\",\"gateway_ips\":[\"10.128.6.1\"],\"ip_address\":\"10.128.6.23/23\",\"gateway_ip\":\"10.128.6.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.6.23\"\n    ],\n    \"mac\": \"0a:58:0a:80:06:17\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.6.23\"\n    ],\n    \"mac\": \"0a:58:0a:80:06:17\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"aa90399d-2977-49e3-a1fa-f89c09afeae0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-194-246","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa90399d-2977-49e3-a1fa-f89c09afeae0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.6.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qf9jz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qf9jz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-188-71.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c54,c24"}},"imagePullSecrets":[{"name":"default-dockercfg-7dxtz"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-188-71.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"}],"hostIP":"10.0.188.71","podIP":"10.128.6.23","podIPs":[{"ip":"10.128.6.23"}],"startTime":"2023-01-19T21:25:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T21:25:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://d4f535519fa92bd562a367c3db7b4ac7c733e83a191598528490e8c269bb8f33","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-dcx9t","generateName":"daemon-set-","namespace":"daemonsets-1272","uid":"99c60cf6-e12c-4497-8d35-52bf55723ef4","resourceVersion":"186806","creationTimestamp":"2023-01-19T21:25:07Z","deletionTimestamp":"2023-01-19T21:25:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.8.161/23\"],\"mac_address\":\"0a:58:0a:80:08:a1\",\"gateway_ips\":[\"10.128.8.1\"],\"ip_address\":\"10.128.8.161/23\",\"gateway_ip\":\"10.128.8.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.8.161\"\n    ],\n    \"mac\": \"0a:58:0a:80:08:a1\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.8.161\"\n    ],\n    \"mac\": \"0a:58:0a:80:08:a1\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"aa90399d-2977-49e3-a1fa-f89c09afeae0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-194-246","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa90399d-2977-49e3-a1fa-f89c09afeae0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6n8q2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6n8q2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-172-44.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c54,c24"}},"imagePullSecrets":[{"name":"default-dockercfg-7dxtz"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-172-44.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"}],"hostIP":"10.0.172.44","podIP":"10.128.8.161","podIPs":[{"ip":"10.128.8.161"}],"startTime":"2023-01-19T21:25:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T21:25:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://caa4eb0e4150d497d35ca5dfacf320287e8abf03b7c2c4434876233760a83992","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qd5fj","generateName":"daemon-set-","namespace":"daemonsets-1272","uid":"fad53066-5bd1-44ee-b973-e53f8b3eb7b4","resourceVersion":"186805","creationTimestamp":"2023-01-19T21:25:07Z","deletionTimestamp":"2023-01-19T21:25:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.16.91/23\"],\"mac_address\":\"0a:58:0a:80:10:5b\",\"gateway_ips\":[\"10.128.16.1\"],\"ip_address\":\"10.128.16.91/23\",\"gateway_ip\":\"10.128.16.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.16.91\"\n    ],\n    \"mac\": \"0a:58:0a:80:10:5b\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.16.91\"\n    ],\n    \"mac\": \"0a:58:0a:80:10:5b\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"aa90399d-2977-49e3-a1fa-f89c09afeae0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-194-246","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa90399d-2977-49e3-a1fa-f89c09afeae0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.16.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9z2t5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9z2t5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-207-77.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c54,c24"}},"imagePullSecrets":[{"name":"default-dockercfg-7dxtz"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-207-77.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"}],"hostIP":"10.0.207.77","podIP":"10.128.16.91","podIPs":[{"ip":"10.128.16.91"}],"startTime":"2023-01-19T21:25:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T21:25:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://3ad6b952bfe406112bcfbce2b61c13d5b64f070d425f3529896e2b5b32cfb521","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rld4r","generateName":"daemon-set-","namespace":"daemonsets-1272","uid":"342beb22-2c4b-4753-abff-55d477499330","resourceVersion":"186809","creationTimestamp":"2023-01-19T21:25:07Z","deletionTimestamp":"2023-01-19T21:25:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.14.50/23\"],\"mac_address\":\"0a:58:0a:80:0e:32\",\"gateway_ips\":[\"10.128.14.1\"],\"ip_address\":\"10.128.14.50/23\",\"gateway_ip\":\"10.128.14.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.14.50\"\n    ],\n    \"mac\": \"0a:58:0a:80:0e:32\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.14.50\"\n    ],\n    \"mac\": \"0a:58:0a:80:0e:32\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"aa90399d-2977-49e3-a1fa-f89c09afeae0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-194-246","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa90399d-2977-49e3-a1fa-f89c09afeae0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.14.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zw8p2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zw8p2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-151-158.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c54,c24"}},"imagePullSecrets":[{"name":"default-dockercfg-7dxtz"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-151-158.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"}],"hostIP":"10.0.151.158","podIP":"10.128.14.50","podIPs":[{"ip":"10.128.14.50"}],"startTime":"2023-01-19T21:25:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T21:25:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://77e47fb9fd355b15a53d95cef9d7a5b76f168be0899789ca1f187f55ae989df4","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tx8dp","generateName":"daemon-set-","namespace":"daemonsets-1272","uid":"02e37846-e764-4169-ae48-5553a18d8700","resourceVersion":"186803","creationTimestamp":"2023-01-19T21:25:07Z","deletionTimestamp":"2023-01-19T21:25:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.12.84/23\"],\"mac_address\":\"0a:58:0a:80:0c:54\",\"gateway_ips\":[\"10.128.12.1\"],\"ip_address\":\"10.128.12.84/23\",\"gateway_ip\":\"10.128.12.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.12.84\"\n    ],\n    \"mac\": \"0a:58:0a:80:0c:54\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.12.84\"\n    ],\n    \"mac\": \"0a:58:0a:80:0c:54\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"aa90399d-2977-49e3-a1fa-f89c09afeae0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-194-246","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa90399d-2977-49e3-a1fa-f89c09afeae0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9fk5m","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9fk5m","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-171-213.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c54,c24"}},"imagePullSecrets":[{"name":"default-dockercfg-7dxtz"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-171-213.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"}],"hostIP":"10.0.171.213","podIP":"10.128.12.84","podIPs":[{"ip":"10.128.12.84"}],"startTime":"2023-01-19T21:25:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T21:25:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://42e8ec572f0347c634661b6dbf80f29e4826df820a8a74a01d57f7465b2258ee","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-xnjhp","generateName":"daemon-set-","namespace":"daemonsets-1272","uid":"04abd1a8-fd34-4ae0-8845-bae875ea950a","resourceVersion":"186807","creationTimestamp":"2023-01-19T21:25:07Z","deletionTimestamp":"2023-01-19T21:25:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.10.56/23\"],\"mac_address\":\"0a:58:0a:80:0a:38\",\"gateway_ips\":[\"10.128.10.1\"],\"ip_address\":\"10.128.10.56/23\",\"gateway_ip\":\"10.128.10.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.10.56\"\n    ],\n    \"mac\": \"0a:58:0a:80:0a:38\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.10.56\"\n    ],\n    \"mac\": \"0a:58:0a:80:0a:38\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"aa90399d-2977-49e3-a1fa-f89c09afeae0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-194-246","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa90399d-2977-49e3-a1fa-f89c09afeae0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.10.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5n6xm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5n6xm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-146-42.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c54,c24"}},"imagePullSecrets":[{"name":"default-dockercfg-7dxtz"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-146-42.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"}],"hostIP":"10.0.146.42","podIP":"10.128.10.56","podIPs":[{"ip":"10.128.10.56"}],"startTime":"2023-01-19T21:25:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T21:25:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://a307e1878b3e2e7ff2143a87a0db6fd97e37d7076ca7e392239302c12b687ea5","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:25:09.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1272" for this suite. 01/19/23 21:25:09.116
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":172,"skipped":3160,"failed":0}
------------------------------
• [2.263 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:06.857
    Jan 19 21:25:06.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename daemonsets 01/19/23 21:25:06.858
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:06.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:06.887
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 01/19/23 21:25:07.025
    STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 21:25:07.031
    Jan 19 21:25:07.055: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:25:07.055: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:25:07.055: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:25:07.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:25:07.057: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:25:08.062: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:25:08.062: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:25:08.062: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:25:08.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 21:25:08.065: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:25:09.062: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:25:09.062: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:25:09.062: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:25:09.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
    Jan 19 21:25:09.065: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
    STEP: listing all DeamonSets 01/19/23 21:25:09.067
    STEP: DeleteCollection of the DaemonSets 01/19/23 21:25:09.071
    STEP: Verify that ReplicaSets have been deleted 01/19/23 21:25:09.077
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jan 19 21:25:09.090: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"186803"},"items":null}

    Jan 19 21:25:09.095: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"186809"},"items":[{"metadata":{"name":"daemon-set-7szzm","generateName":"daemon-set-","namespace":"daemonsets-1272","uid":"cc40b0a1-a776-4770-aeff-da26a72c175c","resourceVersion":"186804","creationTimestamp":"2023-01-19T21:25:07Z","deletionTimestamp":"2023-01-19T21:25:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.6.23/23\"],\"mac_address\":\"0a:58:0a:80:06:17\",\"gateway_ips\":[\"10.128.6.1\"],\"ip_address\":\"10.128.6.23/23\",\"gateway_ip\":\"10.128.6.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.6.23\"\n    ],\n    \"mac\": \"0a:58:0a:80:06:17\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.6.23\"\n    ],\n    \"mac\": \"0a:58:0a:80:06:17\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"aa90399d-2977-49e3-a1fa-f89c09afeae0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-194-246","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa90399d-2977-49e3-a1fa-f89c09afeae0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.6.23\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-qf9jz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-qf9jz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-188-71.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c54,c24"}},"imagePullSecrets":[{"name":"default-dockercfg-7dxtz"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-188-71.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"}],"hostIP":"10.0.188.71","podIP":"10.128.6.23","podIPs":[{"ip":"10.128.6.23"}],"startTime":"2023-01-19T21:25:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T21:25:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://d4f535519fa92bd562a367c3db7b4ac7c733e83a191598528490e8c269bb8f33","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-dcx9t","generateName":"daemon-set-","namespace":"daemonsets-1272","uid":"99c60cf6-e12c-4497-8d35-52bf55723ef4","resourceVersion":"186806","creationTimestamp":"2023-01-19T21:25:07Z","deletionTimestamp":"2023-01-19T21:25:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.8.161/23\"],\"mac_address\":\"0a:58:0a:80:08:a1\",\"gateway_ips\":[\"10.128.8.1\"],\"ip_address\":\"10.128.8.161/23\",\"gateway_ip\":\"10.128.8.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.8.161\"\n    ],\n    \"mac\": \"0a:58:0a:80:08:a1\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.8.161\"\n    ],\n    \"mac\": \"0a:58:0a:80:08:a1\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"aa90399d-2977-49e3-a1fa-f89c09afeae0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-194-246","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa90399d-2977-49e3-a1fa-f89c09afeae0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6n8q2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6n8q2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-172-44.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c54,c24"}},"imagePullSecrets":[{"name":"default-dockercfg-7dxtz"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-172-44.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"}],"hostIP":"10.0.172.44","podIP":"10.128.8.161","podIPs":[{"ip":"10.128.8.161"}],"startTime":"2023-01-19T21:25:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T21:25:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://caa4eb0e4150d497d35ca5dfacf320287e8abf03b7c2c4434876233760a83992","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qd5fj","generateName":"daemon-set-","namespace":"daemonsets-1272","uid":"fad53066-5bd1-44ee-b973-e53f8b3eb7b4","resourceVersion":"186805","creationTimestamp":"2023-01-19T21:25:07Z","deletionTimestamp":"2023-01-19T21:25:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.16.91/23\"],\"mac_address\":\"0a:58:0a:80:10:5b\",\"gateway_ips\":[\"10.128.16.1\"],\"ip_address\":\"10.128.16.91/23\",\"gateway_ip\":\"10.128.16.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.16.91\"\n    ],\n    \"mac\": \"0a:58:0a:80:10:5b\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.16.91\"\n    ],\n    \"mac\": \"0a:58:0a:80:10:5b\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"aa90399d-2977-49e3-a1fa-f89c09afeae0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-194-246","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa90399d-2977-49e3-a1fa-f89c09afeae0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.16.91\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9z2t5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9z2t5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-207-77.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c54,c24"}},"imagePullSecrets":[{"name":"default-dockercfg-7dxtz"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-207-77.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"}],"hostIP":"10.0.207.77","podIP":"10.128.16.91","podIPs":[{"ip":"10.128.16.91"}],"startTime":"2023-01-19T21:25:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T21:25:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://3ad6b952bfe406112bcfbce2b61c13d5b64f070d425f3529896e2b5b32cfb521","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rld4r","generateName":"daemon-set-","namespace":"daemonsets-1272","uid":"342beb22-2c4b-4753-abff-55d477499330","resourceVersion":"186809","creationTimestamp":"2023-01-19T21:25:07Z","deletionTimestamp":"2023-01-19T21:25:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.14.50/23\"],\"mac_address\":\"0a:58:0a:80:0e:32\",\"gateway_ips\":[\"10.128.14.1\"],\"ip_address\":\"10.128.14.50/23\",\"gateway_ip\":\"10.128.14.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.14.50\"\n    ],\n    \"mac\": \"0a:58:0a:80:0e:32\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.14.50\"\n    ],\n    \"mac\": \"0a:58:0a:80:0e:32\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"aa90399d-2977-49e3-a1fa-f89c09afeae0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-194-246","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa90399d-2977-49e3-a1fa-f89c09afeae0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.14.50\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-zw8p2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-zw8p2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-151-158.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c54,c24"}},"imagePullSecrets":[{"name":"default-dockercfg-7dxtz"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-151-158.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"}],"hostIP":"10.0.151.158","podIP":"10.128.14.50","podIPs":[{"ip":"10.128.14.50"}],"startTime":"2023-01-19T21:25:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T21:25:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://77e47fb9fd355b15a53d95cef9d7a5b76f168be0899789ca1f187f55ae989df4","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tx8dp","generateName":"daemon-set-","namespace":"daemonsets-1272","uid":"02e37846-e764-4169-ae48-5553a18d8700","resourceVersion":"186803","creationTimestamp":"2023-01-19T21:25:07Z","deletionTimestamp":"2023-01-19T21:25:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.12.84/23\"],\"mac_address\":\"0a:58:0a:80:0c:54\",\"gateway_ips\":[\"10.128.12.1\"],\"ip_address\":\"10.128.12.84/23\",\"gateway_ip\":\"10.128.12.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.12.84\"\n    ],\n    \"mac\": \"0a:58:0a:80:0c:54\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.12.84\"\n    ],\n    \"mac\": \"0a:58:0a:80:0c:54\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"aa90399d-2977-49e3-a1fa-f89c09afeae0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-194-246","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa90399d-2977-49e3-a1fa-f89c09afeae0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9fk5m","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9fk5m","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-171-213.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c54,c24"}},"imagePullSecrets":[{"name":"default-dockercfg-7dxtz"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-171-213.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"}],"hostIP":"10.0.171.213","podIP":"10.128.12.84","podIPs":[{"ip":"10.128.12.84"}],"startTime":"2023-01-19T21:25:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T21:25:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://42e8ec572f0347c634661b6dbf80f29e4826df820a8a74a01d57f7465b2258ee","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-xnjhp","generateName":"daemon-set-","namespace":"daemonsets-1272","uid":"04abd1a8-fd34-4ae0-8845-bae875ea950a","resourceVersion":"186807","creationTimestamp":"2023-01-19T21:25:07Z","deletionTimestamp":"2023-01-19T21:25:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"k8s.ovn.org/pod-networks":"{\"default\":{\"ip_addresses\":[\"10.128.10.56/23\"],\"mac_address\":\"0a:58:0a:80:0a:38\",\"gateway_ips\":[\"10.128.10.1\"],\"ip_address\":\"10.128.10.56/23\",\"gateway_ip\":\"10.128.10.1\"}}","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.10.56\"\n    ],\n    \"mac\": \"0a:58:0a:80:0a:38\",\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"ovn-kubernetes\",\n    \"interface\": \"eth0\",\n    \"ips\": [\n        \"10.128.10.56\"\n    ],\n    \"mac\": \"0a:58:0a:80:0a:38\",\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"aa90399d-2977-49e3-a1fa-f89c09afeae0","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"ip-10-0-194-246","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}}},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa90399d-2977-49e3-a1fa-f89c09afeae0\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-19T21:25:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.10.56\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5n6xm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5n6xm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-146-42.ec2.internal","securityContext":{"seLinuxOptions":{"level":"s0:c54,c24"}},"imagePullSecrets":[{"name":"default-dockercfg-7dxtz"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-146-42.ec2.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-19T21:25:07Z"}],"hostIP":"10.0.146.42","podIP":"10.128.10.56","podIPs":[{"ip":"10.128.10.56"}],"startTime":"2023-01-19T21:25:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-19T21:25:07Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://a307e1878b3e2e7ff2143a87a0db6fd97e37d7076ca7e392239302c12b687ea5","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:25:09.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1272" for this suite. 01/19/23 21:25:09.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:09.123
Jan 19 21:25:09.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename dns 01/19/23 21:25:09.123
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:09.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:09.14
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5657.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5657.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/19/23 21:25:09.147
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5657.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5657.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/19/23 21:25:09.148
STEP: creating a pod to probe /etc/hosts 01/19/23 21:25:09.148
STEP: submitting the pod to kubernetes 01/19/23 21:25:09.148
Jan 19 21:25:09.173: INFO: Waiting up to 15m0s for pod "dns-test-37cc9712-e48b-49a7-a52b-8449c3f0b442" in namespace "dns-5657" to be "running"
Jan 19 21:25:09.179: INFO: Pod "dns-test-37cc9712-e48b-49a7-a52b-8449c3f0b442": Phase="Pending", Reason="", readiness=false. Elapsed: 5.652009ms
Jan 19 21:25:11.182: INFO: Pod "dns-test-37cc9712-e48b-49a7-a52b-8449c3f0b442": Phase="Running", Reason="", readiness=true. Elapsed: 2.008888756s
Jan 19 21:25:11.182: INFO: Pod "dns-test-37cc9712-e48b-49a7-a52b-8449c3f0b442" satisfied condition "running"
STEP: retrieving the pod 01/19/23 21:25:11.182
STEP: looking for the results for each expected name from probers 01/19/23 21:25:11.185
Jan 19 21:25:11.198: INFO: DNS probes using dns-5657/dns-test-37cc9712-e48b-49a7-a52b-8449c3f0b442 succeeded

STEP: deleting the pod 01/19/23 21:25:11.198
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 21:25:11.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5657" for this suite. 01/19/23 21:25:11.224
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":173,"skipped":3225,"failed":0}
------------------------------
• [2.107 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:09.123
    Jan 19 21:25:09.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename dns 01/19/23 21:25:09.123
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:09.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:09.14
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5657.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5657.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/19/23 21:25:09.147
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5657.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5657.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/19/23 21:25:09.148
    STEP: creating a pod to probe /etc/hosts 01/19/23 21:25:09.148
    STEP: submitting the pod to kubernetes 01/19/23 21:25:09.148
    Jan 19 21:25:09.173: INFO: Waiting up to 15m0s for pod "dns-test-37cc9712-e48b-49a7-a52b-8449c3f0b442" in namespace "dns-5657" to be "running"
    Jan 19 21:25:09.179: INFO: Pod "dns-test-37cc9712-e48b-49a7-a52b-8449c3f0b442": Phase="Pending", Reason="", readiness=false. Elapsed: 5.652009ms
    Jan 19 21:25:11.182: INFO: Pod "dns-test-37cc9712-e48b-49a7-a52b-8449c3f0b442": Phase="Running", Reason="", readiness=true. Elapsed: 2.008888756s
    Jan 19 21:25:11.182: INFO: Pod "dns-test-37cc9712-e48b-49a7-a52b-8449c3f0b442" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 21:25:11.182
    STEP: looking for the results for each expected name from probers 01/19/23 21:25:11.185
    Jan 19 21:25:11.198: INFO: DNS probes using dns-5657/dns-test-37cc9712-e48b-49a7-a52b-8449c3f0b442 succeeded

    STEP: deleting the pod 01/19/23 21:25:11.198
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 21:25:11.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5657" for this suite. 01/19/23 21:25:11.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:11.231
Jan 19 21:25:11.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename replicaset 01/19/23 21:25:11.231
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:11.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:11.274
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/19/23 21:25:11.276
Jan 19 21:25:11.297: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan 19 21:25:16.302: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/19/23 21:25:16.302
STEP: getting scale subresource 01/19/23 21:25:16.302
STEP: updating a scale subresource 01/19/23 21:25:16.307
STEP: verifying the replicaset Spec.Replicas was modified 01/19/23 21:25:16.311
STEP: Patch a scale subresource 01/19/23 21:25:16.313
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 19 21:25:16.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8002" for this suite. 01/19/23 21:25:16.328
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":174,"skipped":3243,"failed":0}
------------------------------
• [SLOW TEST] [5.107 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:11.231
    Jan 19 21:25:11.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename replicaset 01/19/23 21:25:11.231
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:11.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:11.274
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/19/23 21:25:11.276
    Jan 19 21:25:11.297: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan 19 21:25:16.302: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/19/23 21:25:16.302
    STEP: getting scale subresource 01/19/23 21:25:16.302
    STEP: updating a scale subresource 01/19/23 21:25:16.307
    STEP: verifying the replicaset Spec.Replicas was modified 01/19/23 21:25:16.311
    STEP: Patch a scale subresource 01/19/23 21:25:16.313
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 19 21:25:16.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-8002" for this suite. 01/19/23 21:25:16.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:16.338
Jan 19 21:25:16.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pods 01/19/23 21:25:16.34
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:16.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:16.39
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 01/19/23 21:25:16.475
STEP: watching for Pod to be ready 01/19/23 21:25:16.494
Jan 19 21:25:16.495: INFO: observed Pod pod-test in namespace pods-1444 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan 19 21:25:16.499: INFO: observed Pod pod-test in namespace pods-1444 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  }]
Jan 19 21:25:16.510: INFO: observed Pod pod-test in namespace pods-1444 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  }]
Jan 19 21:25:16.525: INFO: observed Pod pod-test in namespace pods-1444 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  }]
Jan 19 21:25:17.082: INFO: observed Pod pod-test in namespace pods-1444 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  }]
Jan 19 21:25:17.927: INFO: Found Pod pod-test in namespace pods-1444 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/19/23 21:25:17.942
STEP: getting the Pod and ensuring that it's patched 01/19/23 21:25:17.96
STEP: replacing the Pod's status Ready condition to False 01/19/23 21:25:17.962
STEP: check the Pod again to ensure its Ready conditions are False 01/19/23 21:25:17.973
STEP: deleting the Pod via a Collection with a LabelSelector 01/19/23 21:25:17.973
STEP: watching for the Pod to be deleted 01/19/23 21:25:17.983
Jan 19 21:25:17.984: INFO: observed event type MODIFIED
Jan 19 21:25:19.932: INFO: observed event type MODIFIED
Jan 19 21:25:20.935: INFO: observed event type MODIFIED
Jan 19 21:25:20.941: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 21:25:20.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1444" for this suite. 01/19/23 21:25:20.951
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":175,"skipped":3248,"failed":0}
------------------------------
• [4.619 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:16.338
    Jan 19 21:25:16.339: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pods 01/19/23 21:25:16.34
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:16.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:16.39
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 01/19/23 21:25:16.475
    STEP: watching for Pod to be ready 01/19/23 21:25:16.494
    Jan 19 21:25:16.495: INFO: observed Pod pod-test in namespace pods-1444 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan 19 21:25:16.499: INFO: observed Pod pod-test in namespace pods-1444 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  }]
    Jan 19 21:25:16.510: INFO: observed Pod pod-test in namespace pods-1444 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  }]
    Jan 19 21:25:16.525: INFO: observed Pod pod-test in namespace pods-1444 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  }]
    Jan 19 21:25:17.082: INFO: observed Pod pod-test in namespace pods-1444 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  }]
    Jan 19 21:25:17.927: INFO: Found Pod pod-test in namespace pods-1444 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:25:16 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/19/23 21:25:17.942
    STEP: getting the Pod and ensuring that it's patched 01/19/23 21:25:17.96
    STEP: replacing the Pod's status Ready condition to False 01/19/23 21:25:17.962
    STEP: check the Pod again to ensure its Ready conditions are False 01/19/23 21:25:17.973
    STEP: deleting the Pod via a Collection with a LabelSelector 01/19/23 21:25:17.973
    STEP: watching for the Pod to be deleted 01/19/23 21:25:17.983
    Jan 19 21:25:17.984: INFO: observed event type MODIFIED
    Jan 19 21:25:19.932: INFO: observed event type MODIFIED
    Jan 19 21:25:20.935: INFO: observed event type MODIFIED
    Jan 19 21:25:20.941: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 21:25:20.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1444" for this suite. 01/19/23 21:25:20.951
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:20.957
Jan 19 21:25:20.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:25:20.958
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:20.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:20.984
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
Jan 19 21:25:21.006: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-3dfa6bf6-c770-4557-afa2-24492ee91de8 01/19/23 21:25:21.006
STEP: Creating the pod 01/19/23 21:25:21.018
Jan 19 21:25:21.050: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7c556a8-3a60-4ad7-bb60-ee2fa181f960" in namespace "configmap-1311" to be "running and ready"
Jan 19 21:25:21.055: INFO: Pod "pod-configmaps-b7c556a8-3a60-4ad7-bb60-ee2fa181f960": Phase="Pending", Reason="", readiness=false. Elapsed: 5.07266ms
Jan 19 21:25:21.055: INFO: The phase of Pod pod-configmaps-b7c556a8-3a60-4ad7-bb60-ee2fa181f960 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:25:23.058: INFO: Pod "pod-configmaps-b7c556a8-3a60-4ad7-bb60-ee2fa181f960": Phase="Running", Reason="", readiness=true. Elapsed: 2.008084244s
Jan 19 21:25:23.058: INFO: The phase of Pod pod-configmaps-b7c556a8-3a60-4ad7-bb60-ee2fa181f960 is Running (Ready = true)
Jan 19 21:25:23.058: INFO: Pod "pod-configmaps-b7c556a8-3a60-4ad7-bb60-ee2fa181f960" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-3dfa6bf6-c770-4557-afa2-24492ee91de8 01/19/23 21:25:23.069
STEP: waiting to observe update in volume 01/19/23 21:25:23.078
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:25:25.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1311" for this suite. 01/19/23 21:25:25.093
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":176,"skipped":3251,"failed":0}
------------------------------
• [4.152 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:20.957
    Jan 19 21:25:20.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:25:20.958
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:20.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:20.984
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    Jan 19 21:25:21.006: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-3dfa6bf6-c770-4557-afa2-24492ee91de8 01/19/23 21:25:21.006
    STEP: Creating the pod 01/19/23 21:25:21.018
    Jan 19 21:25:21.050: INFO: Waiting up to 5m0s for pod "pod-configmaps-b7c556a8-3a60-4ad7-bb60-ee2fa181f960" in namespace "configmap-1311" to be "running and ready"
    Jan 19 21:25:21.055: INFO: Pod "pod-configmaps-b7c556a8-3a60-4ad7-bb60-ee2fa181f960": Phase="Pending", Reason="", readiness=false. Elapsed: 5.07266ms
    Jan 19 21:25:21.055: INFO: The phase of Pod pod-configmaps-b7c556a8-3a60-4ad7-bb60-ee2fa181f960 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:25:23.058: INFO: Pod "pod-configmaps-b7c556a8-3a60-4ad7-bb60-ee2fa181f960": Phase="Running", Reason="", readiness=true. Elapsed: 2.008084244s
    Jan 19 21:25:23.058: INFO: The phase of Pod pod-configmaps-b7c556a8-3a60-4ad7-bb60-ee2fa181f960 is Running (Ready = true)
    Jan 19 21:25:23.058: INFO: Pod "pod-configmaps-b7c556a8-3a60-4ad7-bb60-ee2fa181f960" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-3dfa6bf6-c770-4557-afa2-24492ee91de8 01/19/23 21:25:23.069
    STEP: waiting to observe update in volume 01/19/23 21:25:23.078
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:25:25.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1311" for this suite. 01/19/23 21:25:25.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:25.11
Jan 19 21:25:25.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:25:25.111
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:25.126
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:25.129
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:25:25.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6558" for this suite. 01/19/23 21:25:25.198
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":177,"skipped":3266,"failed":0}
------------------------------
• [0.095 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:25.11
    Jan 19 21:25:25.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:25:25.111
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:25.126
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:25.129
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:25:25.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6558" for this suite. 01/19/23 21:25:25.198
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:25.206
Jan 19 21:25:25.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename deployment 01/19/23 21:25:25.207
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:25.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:25.238
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan 19 21:25:25.241: INFO: Creating deployment "test-recreate-deployment"
W0119 21:25:25.255974      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:25:25.256: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan 19 21:25:25.260: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan 19 21:25:27.266: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan 19 21:25:27.268: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan 19 21:25:27.291: INFO: Updating deployment test-recreate-deployment
Jan 19 21:25:27.291: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 21:25:27.384: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8208  8f2a4bea-054f-4342-b244-cb39b1127ae5 187688 2 2023-01-19 21:25:25 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038c4bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-19 21:25:27 +0000 UTC,LastTransitionTime:2023-01-19 21:25:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-19 21:25:27 +0000 UTC,LastTransitionTime:2023-01-19 21:25:25 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan 19 21:25:27.388: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-8208  fd43d052-9660-4b8f-9386-0254ce752822 187685 1 2023-01-19 21:25:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8f2a4bea-054f-4342-b244-cb39b1127ae5 0xc0038c50b0 0xc0038c50b1}] [] [{kube-controller-manager Update apps/v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f2a4bea-054f-4342-b244-cb39b1127ae5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038c5148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 21:25:27.388: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan 19 21:25:27.388: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-8208  5eb0d0f0-2ae5-4c17-8680-eaff6c5ccc80 187675 2 2023-01-19 21:25:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8f2a4bea-054f-4342-b244-cb39b1127ae5 0xc0038c4f97 0xc0038c4f98}] [] [{kube-controller-manager Update apps/v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f2a4bea-054f-4342-b244-cb39b1127ae5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038c5048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 21:25:27.391: INFO: Pod "test-recreate-deployment-9d58999df-vcxxr" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-vcxxr test-recreate-deployment-9d58999df- deployment-8208  f15ff048-d597-456e-8252-b49f184e8c08 187687 0 2023-01-19 21:25:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.86/23"],"mac_address":"0a:58:0a:80:0c:56","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.86/23","gateway_ip":"10.128.12.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df fd43d052-9660-4b8f-9386-0254ce752822 0xc0038c55d7 0xc0038c55d8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd43d052-9660-4b8f-9386-0254ce752822\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lz6nt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lz6nt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-171-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c55,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-l994z,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.171.213,PodIP:,StartTime:2023-01-19 21:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 21:25:27.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8208" for this suite. 01/19/23 21:25:27.395
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":178,"skipped":3302,"failed":0}
------------------------------
• [2.195 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:25.206
    Jan 19 21:25:25.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename deployment 01/19/23 21:25:25.207
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:25.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:25.238
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan 19 21:25:25.241: INFO: Creating deployment "test-recreate-deployment"
    W0119 21:25:25.255974      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:25:25.256: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan 19 21:25:25.260: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    Jan 19 21:25:27.266: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan 19 21:25:27.268: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan 19 21:25:27.291: INFO: Updating deployment test-recreate-deployment
    Jan 19 21:25:27.291: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 21:25:27.384: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-8208  8f2a4bea-054f-4342-b244-cb39b1127ae5 187688 2 2023-01-19 21:25:25 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038c4bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-19 21:25:27 +0000 UTC,LastTransitionTime:2023-01-19 21:25:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-01-19 21:25:27 +0000 UTC,LastTransitionTime:2023-01-19 21:25:25 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 19 21:25:27.388: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-8208  fd43d052-9660-4b8f-9386-0254ce752822 187685 1 2023-01-19 21:25:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 8f2a4bea-054f-4342-b244-cb39b1127ae5 0xc0038c50b0 0xc0038c50b1}] [] [{kube-controller-manager Update apps/v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f2a4bea-054f-4342-b244-cb39b1127ae5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038c5148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 21:25:27.388: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan 19 21:25:27.388: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-8208  5eb0d0f0-2ae5-4c17-8680-eaff6c5ccc80 187675 2 2023-01-19 21:25:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 8f2a4bea-054f-4342-b244-cb39b1127ae5 0xc0038c4f97 0xc0038c4f98}] [] [{kube-controller-manager Update apps/v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f2a4bea-054f-4342-b244-cb39b1127ae5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038c5048 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 21:25:27.391: INFO: Pod "test-recreate-deployment-9d58999df-vcxxr" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-vcxxr test-recreate-deployment-9d58999df- deployment-8208  f15ff048-d597-456e-8252-b49f184e8c08 187687 0 2023-01-19 21:25:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.86/23"],"mac_address":"0a:58:0a:80:0c:56","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.86/23","gateway_ip":"10.128.12.1"}} openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df fd43d052-9660-4b8f-9386-0254ce752822 0xc0038c55d7 0xc0038c55d8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fd43d052-9660-4b8f-9386-0254ce752822\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 21:25:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lz6nt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lz6nt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-171-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c55,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-l994z,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:25:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.171.213,PodIP:,StartTime:2023-01-19 21:25:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 21:25:27.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8208" for this suite. 01/19/23 21:25:27.395
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:27.401
Jan 19 21:25:27.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 21:25:27.402
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:27.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:27.435
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 01/19/23 21:25:27.438
Jan 19 21:25:27.461: INFO: Waiting up to 5m0s for pod "downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad" in namespace "downward-api-241" to be "Succeeded or Failed"
Jan 19 21:25:27.465: INFO: Pod "downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.769957ms
Jan 19 21:25:29.468: INFO: Pod "downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006277398s
Jan 19 21:25:31.469: INFO: Pod "downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007238013s
STEP: Saw pod success 01/19/23 21:25:31.469
Jan 19 21:25:31.469: INFO: Pod "downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad" satisfied condition "Succeeded or Failed"
Jan 19 21:25:31.471: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad container dapi-container: <nil>
STEP: delete the pod 01/19/23 21:25:31.479
Jan 19 21:25:31.490: INFO: Waiting for pod downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad to disappear
Jan 19 21:25:31.493: INFO: Pod downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 19 21:25:31.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-241" for this suite. 01/19/23 21:25:31.5
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":179,"skipped":3304,"failed":0}
------------------------------
• [4.104 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:27.401
    Jan 19 21:25:27.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 21:25:27.402
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:27.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:27.435
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 01/19/23 21:25:27.438
    Jan 19 21:25:27.461: INFO: Waiting up to 5m0s for pod "downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad" in namespace "downward-api-241" to be "Succeeded or Failed"
    Jan 19 21:25:27.465: INFO: Pod "downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad": Phase="Pending", Reason="", readiness=false. Elapsed: 3.769957ms
    Jan 19 21:25:29.468: INFO: Pod "downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006277398s
    Jan 19 21:25:31.469: INFO: Pod "downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007238013s
    STEP: Saw pod success 01/19/23 21:25:31.469
    Jan 19 21:25:31.469: INFO: Pod "downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad" satisfied condition "Succeeded or Failed"
    Jan 19 21:25:31.471: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad container dapi-container: <nil>
    STEP: delete the pod 01/19/23 21:25:31.479
    Jan 19 21:25:31.490: INFO: Waiting for pod downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad to disappear
    Jan 19 21:25:31.493: INFO: Pod downward-api-c3696792-760d-4a0d-92f8-cb6e35e683ad no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 19 21:25:31.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-241" for this suite. 01/19/23 21:25:31.5
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:31.506
Jan 19 21:25:31.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename disruption 01/19/23 21:25:31.507
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:31.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:31.588
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 01/19/23 21:25:31.609
STEP: Waiting for all pods to be running 01/19/23 21:25:31.708
Jan 19 21:25:31.713: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 19 21:25:33.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1949" for this suite. 01/19/23 21:25:33.722
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":180,"skipped":3306,"failed":0}
------------------------------
• [2.224 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:31.506
    Jan 19 21:25:31.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename disruption 01/19/23 21:25:31.507
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:31.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:31.588
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 01/19/23 21:25:31.609
    STEP: Waiting for all pods to be running 01/19/23 21:25:31.708
    Jan 19 21:25:31.713: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 19 21:25:33.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1949" for this suite. 01/19/23 21:25:33.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:33.731
Jan 19 21:25:33.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename runtimeclass 01/19/23 21:25:33.731
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:33.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:33.758
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan 19 21:25:33.795: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-925 to be scheduled
Jan 19 21:25:33.804: INFO: 1 pods are not scheduled: [runtimeclass-925/test-runtimeclass-runtimeclass-925-preconfigured-handler-kcdq7(b5310b7a-3849-4032-a0a4-58f6058a85ce)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 19 21:25:35.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-925" for this suite. 01/19/23 21:25:35.816
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":181,"skipped":3314,"failed":0}
------------------------------
• [2.091 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:33.731
    Jan 19 21:25:33.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename runtimeclass 01/19/23 21:25:33.731
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:33.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:33.758
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan 19 21:25:33.795: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-925 to be scheduled
    Jan 19 21:25:33.804: INFO: 1 pods are not scheduled: [runtimeclass-925/test-runtimeclass-runtimeclass-925-preconfigured-handler-kcdq7(b5310b7a-3849-4032-a0a4-58f6058a85ce)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 19 21:25:35.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-925" for this suite. 01/19/23 21:25:35.816
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:35.822
Jan 19 21:25:35.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename certificates 01/19/23 21:25:35.823
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:35.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:35.862
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/19/23 21:25:36.405
STEP: getting /apis/certificates.k8s.io 01/19/23 21:25:36.407
STEP: getting /apis/certificates.k8s.io/v1 01/19/23 21:25:36.407
STEP: creating 01/19/23 21:25:36.408
STEP: getting 01/19/23 21:25:36.422
STEP: listing 01/19/23 21:25:36.425
STEP: watching 01/19/23 21:25:36.427
Jan 19 21:25:36.427: INFO: starting watch
STEP: patching 01/19/23 21:25:36.428
STEP: updating 01/19/23 21:25:36.434
Jan 19 21:25:36.440: INFO: waiting for watch events with expected annotations
Jan 19 21:25:36.440: INFO: saw patched and updated annotations
STEP: getting /approval 01/19/23 21:25:36.44
STEP: patching /approval 01/19/23 21:25:36.442
STEP: updating /approval 01/19/23 21:25:36.447
STEP: getting /status 01/19/23 21:25:36.452
STEP: patching /status 01/19/23 21:25:36.454
STEP: updating /status 01/19/23 21:25:36.46
STEP: deleting 01/19/23 21:25:36.466
STEP: deleting a collection 01/19/23 21:25:36.475
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:25:36.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-6340" for this suite. 01/19/23 21:25:36.494
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":182,"skipped":3314,"failed":0}
------------------------------
• [0.678 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:35.822
    Jan 19 21:25:35.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename certificates 01/19/23 21:25:35.823
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:35.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:35.862
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/19/23 21:25:36.405
    STEP: getting /apis/certificates.k8s.io 01/19/23 21:25:36.407
    STEP: getting /apis/certificates.k8s.io/v1 01/19/23 21:25:36.407
    STEP: creating 01/19/23 21:25:36.408
    STEP: getting 01/19/23 21:25:36.422
    STEP: listing 01/19/23 21:25:36.425
    STEP: watching 01/19/23 21:25:36.427
    Jan 19 21:25:36.427: INFO: starting watch
    STEP: patching 01/19/23 21:25:36.428
    STEP: updating 01/19/23 21:25:36.434
    Jan 19 21:25:36.440: INFO: waiting for watch events with expected annotations
    Jan 19 21:25:36.440: INFO: saw patched and updated annotations
    STEP: getting /approval 01/19/23 21:25:36.44
    STEP: patching /approval 01/19/23 21:25:36.442
    STEP: updating /approval 01/19/23 21:25:36.447
    STEP: getting /status 01/19/23 21:25:36.452
    STEP: patching /status 01/19/23 21:25:36.454
    STEP: updating /status 01/19/23 21:25:36.46
    STEP: deleting 01/19/23 21:25:36.466
    STEP: deleting a collection 01/19/23 21:25:36.475
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:25:36.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-6340" for this suite. 01/19/23 21:25:36.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:36.501
Jan 19 21:25:36.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename dns 01/19/23 21:25:36.502
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:36.53
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:36.532
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/19/23 21:25:36.534
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7909 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7909;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7909 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7909;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7909.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7909.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7909.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7909.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7909.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7909.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7909.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7909.svc;check="$$(dig +notcp +noall +answer +search 145.50.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.50.145_udp@PTR;check="$$(dig +tcp +noall +answer +search 145.50.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.50.145_tcp@PTR;sleep 1; done
 01/19/23 21:25:36.581
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7909 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7909;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7909 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7909;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7909.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7909.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7909.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7909.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7909.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7909.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7909.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7909.svc;check="$$(dig +notcp +noall +answer +search 145.50.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.50.145_udp@PTR;check="$$(dig +tcp +noall +answer +search 145.50.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.50.145_tcp@PTR;sleep 1; done
 01/19/23 21:25:36.581
STEP: creating a pod to probe DNS 01/19/23 21:25:36.581
STEP: submitting the pod to kubernetes 01/19/23 21:25:36.581
Jan 19 21:25:36.622: INFO: Waiting up to 15m0s for pod "dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3" in namespace "dns-7909" to be "running"
Jan 19 21:25:36.628: INFO: Pod "dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.513547ms
Jan 19 21:25:38.631: INFO: Pod "dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.009042795s
Jan 19 21:25:38.631: INFO: Pod "dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3" satisfied condition "running"
STEP: retrieving the pod 01/19/23 21:25:38.631
STEP: looking for the results for each expected name from probers 01/19/23 21:25:38.634
Jan 19 21:25:38.647: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.653: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.656: INFO: Unable to read wheezy_udp@dns-test-service.dns-7909 from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.659: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7909 from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.661: INFO: Unable to read wheezy_udp@dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.664: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.666: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.669: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.683: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.685: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.688: INFO: Unable to read jessie_udp@dns-test-service.dns-7909 from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.690: INFO: Unable to read jessie_tcp@dns-test-service.dns-7909 from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.693: INFO: Unable to read jessie_udp@dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.696: INFO: Unable to read jessie_tcp@dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.698: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.703: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:38.714: INFO: Lookups using dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7909 wheezy_tcp@dns-test-service.dns-7909 wheezy_udp@dns-test-service.dns-7909.svc wheezy_tcp@dns-test-service.dns-7909.svc wheezy_udp@_http._tcp.dns-test-service.dns-7909.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7909.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7909 jessie_tcp@dns-test-service.dns-7909 jessie_udp@dns-test-service.dns-7909.svc jessie_tcp@dns-test-service.dns-7909.svc jessie_udp@_http._tcp.dns-test-service.dns-7909.svc jessie_tcp@_http._tcp.dns-test-service.dns-7909.svc]

Jan 19 21:25:43.737: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:43.740: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
Jan 19 21:25:43.782: INFO: Lookups using dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-7909.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7909.svc]

Jan 19 21:25:48.783: INFO: DNS probes using dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3 succeeded

STEP: deleting the pod 01/19/23 21:25:48.783
STEP: deleting the test service 01/19/23 21:25:48.807
STEP: deleting the test headless service 01/19/23 21:25:48.83
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 21:25:48.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7909" for this suite. 01/19/23 21:25:48.848
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":183,"skipped":3355,"failed":0}
------------------------------
• [SLOW TEST] [12.364 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:36.501
    Jan 19 21:25:36.501: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename dns 01/19/23 21:25:36.502
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:36.53
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:36.532
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/19/23 21:25:36.534
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7909 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7909;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7909 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7909;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7909.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7909.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7909.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7909.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7909.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7909.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7909.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7909.svc;check="$$(dig +notcp +noall +answer +search 145.50.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.50.145_udp@PTR;check="$$(dig +tcp +noall +answer +search 145.50.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.50.145_tcp@PTR;sleep 1; done
     01/19/23 21:25:36.581
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7909 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7909;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7909 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7909;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7909.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7909.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7909.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7909.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7909.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7909.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7909.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7909.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7909.svc;check="$$(dig +notcp +noall +answer +search 145.50.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.50.145_udp@PTR;check="$$(dig +tcp +noall +answer +search 145.50.30.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.30.50.145_tcp@PTR;sleep 1; done
     01/19/23 21:25:36.581
    STEP: creating a pod to probe DNS 01/19/23 21:25:36.581
    STEP: submitting the pod to kubernetes 01/19/23 21:25:36.581
    Jan 19 21:25:36.622: INFO: Waiting up to 15m0s for pod "dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3" in namespace "dns-7909" to be "running"
    Jan 19 21:25:36.628: INFO: Pod "dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.513547ms
    Jan 19 21:25:38.631: INFO: Pod "dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.009042795s
    Jan 19 21:25:38.631: INFO: Pod "dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 21:25:38.631
    STEP: looking for the results for each expected name from probers 01/19/23 21:25:38.634
    Jan 19 21:25:38.647: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.653: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.656: INFO: Unable to read wheezy_udp@dns-test-service.dns-7909 from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.659: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7909 from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.661: INFO: Unable to read wheezy_udp@dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.664: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.666: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.669: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.683: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.685: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.688: INFO: Unable to read jessie_udp@dns-test-service.dns-7909 from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.690: INFO: Unable to read jessie_tcp@dns-test-service.dns-7909 from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.693: INFO: Unable to read jessie_udp@dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.696: INFO: Unable to read jessie_tcp@dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.698: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.703: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:38.714: INFO: Lookups using dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7909 wheezy_tcp@dns-test-service.dns-7909 wheezy_udp@dns-test-service.dns-7909.svc wheezy_tcp@dns-test-service.dns-7909.svc wheezy_udp@_http._tcp.dns-test-service.dns-7909.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7909.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7909 jessie_tcp@dns-test-service.dns-7909 jessie_udp@dns-test-service.dns-7909.svc jessie_tcp@dns-test-service.dns-7909.svc jessie_udp@_http._tcp.dns-test-service.dns-7909.svc jessie_tcp@_http._tcp.dns-test-service.dns-7909.svc]

    Jan 19 21:25:43.737: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:43.740: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7909.svc from pod dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3: the server could not find the requested resource (get pods dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3)
    Jan 19 21:25:43.782: INFO: Lookups using dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-7909.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7909.svc]

    Jan 19 21:25:48.783: INFO: DNS probes using dns-7909/dns-test-07f31add-e776-4ba9-ae5d-b85b6e2d46d3 succeeded

    STEP: deleting the pod 01/19/23 21:25:48.783
    STEP: deleting the test service 01/19/23 21:25:48.807
    STEP: deleting the test headless service 01/19/23 21:25:48.83
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 21:25:48.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-7909" for this suite. 01/19/23 21:25:48.848
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:48.865
Jan 19 21:25:48.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubelet-test 01/19/23 21:25:48.866
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:48.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:48.886
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
W0119 21:25:48.931819      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:25:48.931: INFO: Waiting up to 5m0s for pod "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394" in namespace "kubelet-test-8840" to be "running and ready"
Jan 19 21:25:48.934: INFO: Pod "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.581506ms
Jan 19 21:25:48.934: INFO: The phase of Pod busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:25:50.937: INFO: Pod "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394": Phase="Running", Reason="", readiness=true. Elapsed: 2.005483956s
Jan 19 21:25:50.937: INFO: The phase of Pod busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394 is Running (Ready = true)
Jan 19 21:25:50.937: INFO: Pod "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 19 21:25:50.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8840" for this suite. 01/19/23 21:25:50.953
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":184,"skipped":3358,"failed":0}
------------------------------
• [2.095 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:48.865
    Jan 19 21:25:48.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubelet-test 01/19/23 21:25:48.866
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:48.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:48.886
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    W0119 21:25:48.931819      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:25:48.931: INFO: Waiting up to 5m0s for pod "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394" in namespace "kubelet-test-8840" to be "running and ready"
    Jan 19 21:25:48.934: INFO: Pod "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394": Phase="Pending", Reason="", readiness=false. Elapsed: 2.581506ms
    Jan 19 21:25:48.934: INFO: The phase of Pod busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:25:50.937: INFO: Pod "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394": Phase="Running", Reason="", readiness=true. Elapsed: 2.005483956s
    Jan 19 21:25:50.937: INFO: The phase of Pod busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394 is Running (Ready = true)
    Jan 19 21:25:50.937: INFO: Pod "busybox-scheduling-2810f08c-cea7-454b-97ab-bcf0cc731394" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 19 21:25:50.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8840" for this suite. 01/19/23 21:25:50.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:50.961
Jan 19 21:25:50.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pods 01/19/23 21:25:50.962
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:50.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:50.983
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 01/19/23 21:25:50.985
STEP: submitting the pod to kubernetes 01/19/23 21:25:50.985
Jan 19 21:25:51.016: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d" in namespace "pods-6838" to be "running and ready"
Jan 19 21:25:51.018: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.443599ms
Jan 19 21:25:51.018: INFO: The phase of Pod pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:25:53.023: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007109068s
Jan 19 21:25:53.023: INFO: The phase of Pod pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d is Running (Ready = true)
Jan 19 21:25:53.023: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/19/23 21:25:53.026
STEP: updating the pod 01/19/23 21:25:53.028
Jan 19 21:25:53.545: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d"
Jan 19 21:25:53.545: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d" in namespace "pods-6838" to be "terminated with reason DeadlineExceeded"
Jan 19 21:25:53.548: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d": Phase="Running", Reason="", readiness=true. Elapsed: 3.217429ms
Jan 19 21:25:55.566: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.021183086s
Jan 19 21:25:57.552: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d": Phase="Running", Reason="", readiness=false. Elapsed: 4.007346991s
Jan 19 21:25:59.552: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.0068253s
Jan 19 21:25:59.552: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 21:25:59.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6838" for this suite. 01/19/23 21:25:59.556
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":185,"skipped":3364,"failed":0}
------------------------------
• [SLOW TEST] [8.602 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:50.961
    Jan 19 21:25:50.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pods 01/19/23 21:25:50.962
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:50.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:50.983
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 01/19/23 21:25:50.985
    STEP: submitting the pod to kubernetes 01/19/23 21:25:50.985
    Jan 19 21:25:51.016: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d" in namespace "pods-6838" to be "running and ready"
    Jan 19 21:25:51.018: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.443599ms
    Jan 19 21:25:51.018: INFO: The phase of Pod pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:25:53.023: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.007109068s
    Jan 19 21:25:53.023: INFO: The phase of Pod pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d is Running (Ready = true)
    Jan 19 21:25:53.023: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/19/23 21:25:53.026
    STEP: updating the pod 01/19/23 21:25:53.028
    Jan 19 21:25:53.545: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d"
    Jan 19 21:25:53.545: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d" in namespace "pods-6838" to be "terminated with reason DeadlineExceeded"
    Jan 19 21:25:53.548: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d": Phase="Running", Reason="", readiness=true. Elapsed: 3.217429ms
    Jan 19 21:25:55.566: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d": Phase="Running", Reason="", readiness=true. Elapsed: 2.021183086s
    Jan 19 21:25:57.552: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d": Phase="Running", Reason="", readiness=false. Elapsed: 4.007346991s
    Jan 19 21:25:59.552: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.0068253s
    Jan 19 21:25:59.552: INFO: Pod "pod-update-activedeadlineseconds-9205c74c-8549-40c6-af5d-a986f7ce2d0d" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 21:25:59.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6838" for this suite. 01/19/23 21:25:59.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:25:59.563
Jan 19 21:25:59.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:25:59.564
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:59.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:59.59
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
Jan 19 21:25:59.610: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-cf916194-f53c-4af7-945d-ee26e862ea8d 01/19/23 21:25:59.61
STEP: Creating secret with name s-test-opt-upd-c4613cfe-c668-43ed-8809-a48dfb6ca4cd 01/19/23 21:25:59.615
STEP: Creating the pod 01/19/23 21:25:59.634
Jan 19 21:25:59.653: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-484b1221-5668-4c09-a3fa-e3134623169d" in namespace "projected-3068" to be "running and ready"
Jan 19 21:25:59.661: INFO: Pod "pod-projected-secrets-484b1221-5668-4c09-a3fa-e3134623169d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.609369ms
Jan 19 21:25:59.661: INFO: The phase of Pod pod-projected-secrets-484b1221-5668-4c09-a3fa-e3134623169d is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:26:01.668: INFO: Pod "pod-projected-secrets-484b1221-5668-4c09-a3fa-e3134623169d": Phase="Running", Reason="", readiness=true. Elapsed: 2.014799225s
Jan 19 21:26:01.668: INFO: The phase of Pod pod-projected-secrets-484b1221-5668-4c09-a3fa-e3134623169d is Running (Ready = true)
Jan 19 21:26:01.668: INFO: Pod "pod-projected-secrets-484b1221-5668-4c09-a3fa-e3134623169d" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-cf916194-f53c-4af7-945d-ee26e862ea8d 01/19/23 21:26:01.693
STEP: Updating secret s-test-opt-upd-c4613cfe-c668-43ed-8809-a48dfb6ca4cd 01/19/23 21:26:01.699
STEP: Creating secret with name s-test-opt-create-2e0383c9-9a1e-4ed0-8ef4-074b28ced7fa 01/19/23 21:26:01.708
STEP: waiting to observe update in volume 01/19/23 21:26:01.713
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 21:26:05.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3068" for this suite. 01/19/23 21:26:05.76
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":186,"skipped":3374,"failed":0}
------------------------------
• [SLOW TEST] [6.204 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:25:59.563
    Jan 19 21:25:59.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:25:59.564
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:25:59.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:25:59.59
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    Jan 19 21:25:59.610: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-cf916194-f53c-4af7-945d-ee26e862ea8d 01/19/23 21:25:59.61
    STEP: Creating secret with name s-test-opt-upd-c4613cfe-c668-43ed-8809-a48dfb6ca4cd 01/19/23 21:25:59.615
    STEP: Creating the pod 01/19/23 21:25:59.634
    Jan 19 21:25:59.653: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-484b1221-5668-4c09-a3fa-e3134623169d" in namespace "projected-3068" to be "running and ready"
    Jan 19 21:25:59.661: INFO: Pod "pod-projected-secrets-484b1221-5668-4c09-a3fa-e3134623169d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.609369ms
    Jan 19 21:25:59.661: INFO: The phase of Pod pod-projected-secrets-484b1221-5668-4c09-a3fa-e3134623169d is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:26:01.668: INFO: Pod "pod-projected-secrets-484b1221-5668-4c09-a3fa-e3134623169d": Phase="Running", Reason="", readiness=true. Elapsed: 2.014799225s
    Jan 19 21:26:01.668: INFO: The phase of Pod pod-projected-secrets-484b1221-5668-4c09-a3fa-e3134623169d is Running (Ready = true)
    Jan 19 21:26:01.668: INFO: Pod "pod-projected-secrets-484b1221-5668-4c09-a3fa-e3134623169d" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-cf916194-f53c-4af7-945d-ee26e862ea8d 01/19/23 21:26:01.693
    STEP: Updating secret s-test-opt-upd-c4613cfe-c668-43ed-8809-a48dfb6ca4cd 01/19/23 21:26:01.699
    STEP: Creating secret with name s-test-opt-create-2e0383c9-9a1e-4ed0-8ef4-074b28ced7fa 01/19/23 21:26:01.708
    STEP: waiting to observe update in volume 01/19/23 21:26:01.713
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 21:26:05.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3068" for this suite. 01/19/23 21:26:05.76
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:26:05.767
Jan 19 21:26:05.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:26:05.768
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:05.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:05.813
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-ee1c974c-6fa2-4f8e-acf7-f55095468c38 01/19/23 21:26:05.822
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:26:05.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2526" for this suite. 01/19/23 21:26:05.85
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":187,"skipped":3374,"failed":0}
------------------------------
• [0.100 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:26:05.767
    Jan 19 21:26:05.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:26:05.768
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:05.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:05.813
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-ee1c974c-6fa2-4f8e-acf7-f55095468c38 01/19/23 21:26:05.822
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:26:05.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2526" for this suite. 01/19/23 21:26:05.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:26:05.868
Jan 19 21:26:05.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 21:26:05.869
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:05.894
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:05.897
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-89c2f1e0-bcbd-4948-8b61-2683a7cb1bc1 01/19/23 21:26:05.905
STEP: Creating a pod to test consume secrets 01/19/23 21:26:05.924
Jan 19 21:26:05.956: INFO: Waiting up to 5m0s for pod "pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1" in namespace "secrets-6350" to be "Succeeded or Failed"
Jan 19 21:26:05.962: INFO: Pod "pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.102117ms
Jan 19 21:26:07.966: INFO: Pod "pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010096215s
Jan 19 21:26:09.966: INFO: Pod "pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01001466s
STEP: Saw pod success 01/19/23 21:26:09.966
Jan 19 21:26:09.966: INFO: Pod "pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1" satisfied condition "Succeeded or Failed"
Jan 19 21:26:09.969: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 21:26:09.973
Jan 19 21:26:09.983: INFO: Waiting for pod pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1 to disappear
Jan 19 21:26:09.985: INFO: Pod pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 21:26:09.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6350" for this suite. 01/19/23 21:26:09.988
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":188,"skipped":3392,"failed":0}
------------------------------
• [4.127 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:26:05.868
    Jan 19 21:26:05.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 21:26:05.869
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:05.894
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:05.897
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-89c2f1e0-bcbd-4948-8b61-2683a7cb1bc1 01/19/23 21:26:05.905
    STEP: Creating a pod to test consume secrets 01/19/23 21:26:05.924
    Jan 19 21:26:05.956: INFO: Waiting up to 5m0s for pod "pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1" in namespace "secrets-6350" to be "Succeeded or Failed"
    Jan 19 21:26:05.962: INFO: Pod "pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.102117ms
    Jan 19 21:26:07.966: INFO: Pod "pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010096215s
    Jan 19 21:26:09.966: INFO: Pod "pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01001466s
    STEP: Saw pod success 01/19/23 21:26:09.966
    Jan 19 21:26:09.966: INFO: Pod "pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1" satisfied condition "Succeeded or Failed"
    Jan 19 21:26:09.969: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:26:09.973
    Jan 19 21:26:09.983: INFO: Waiting for pod pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1 to disappear
    Jan 19 21:26:09.985: INFO: Pod pod-secrets-5a6131c2-8d7e-427c-985c-c2576561b7c1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 21:26:09.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6350" for this suite. 01/19/23 21:26:09.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:26:09.996
Jan 19 21:26:09.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename namespaces 01/19/23 21:26:09.997
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:10.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:10.017
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 01/19/23 21:26:10.02
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:10.058
STEP: Creating a pod in the namespace 01/19/23 21:26:10.069
STEP: Waiting for the pod to have running status 01/19/23 21:26:10.115
Jan 19 21:26:10.115: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9570" to be "running"
Jan 19 21:26:10.125: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.902249ms
Jan 19 21:26:12.129: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013538498s
Jan 19 21:26:12.129: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/19/23 21:26:12.129
STEP: Waiting for the namespace to be removed. 01/19/23 21:26:12.138
STEP: Recreating the namespace 01/19/23 21:26:24.14
STEP: Verifying there are no pods in the namespace 01/19/23 21:26:24.167
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:26:24.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2294" for this suite. 01/19/23 21:26:24.333
STEP: Destroying namespace "nsdeletetest-9570" for this suite. 01/19/23 21:26:24.431
Jan 19 21:26:24.456: INFO: Namespace nsdeletetest-9570 was already deleted
STEP: Destroying namespace "nsdeletetest-105" for this suite. 01/19/23 21:26:24.456
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":189,"skipped":3424,"failed":0}
------------------------------
• [SLOW TEST] [14.477 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:26:09.996
    Jan 19 21:26:09.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename namespaces 01/19/23 21:26:09.997
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:10.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:10.017
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 01/19/23 21:26:10.02
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:10.058
    STEP: Creating a pod in the namespace 01/19/23 21:26:10.069
    STEP: Waiting for the pod to have running status 01/19/23 21:26:10.115
    Jan 19 21:26:10.115: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9570" to be "running"
    Jan 19 21:26:10.125: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.902249ms
    Jan 19 21:26:12.129: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.013538498s
    Jan 19 21:26:12.129: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/19/23 21:26:12.129
    STEP: Waiting for the namespace to be removed. 01/19/23 21:26:12.138
    STEP: Recreating the namespace 01/19/23 21:26:24.14
    STEP: Verifying there are no pods in the namespace 01/19/23 21:26:24.167
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:26:24.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2294" for this suite. 01/19/23 21:26:24.333
    STEP: Destroying namespace "nsdeletetest-9570" for this suite. 01/19/23 21:26:24.431
    Jan 19 21:26:24.456: INFO: Namespace nsdeletetest-9570 was already deleted
    STEP: Destroying namespace "nsdeletetest-105" for this suite. 01/19/23 21:26:24.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:26:24.474
Jan 19 21:26:24.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename daemonsets 01/19/23 21:26:24.475
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:24.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:24.509
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 01/19/23 21:26:24.628
STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 21:26:24.635
Jan 19 21:26:24.642: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:26:24.642: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:26:24.642: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:26:24.644: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:26:24.644: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:26:25.649: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:26:25.650: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:26:25.650: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:26:25.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 21:26:25.653: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:26:26.649: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:26:26.649: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:26:26.649: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:26:26.654: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 19 21:26:26.654: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
STEP: Getting /status 01/19/23 21:26:26.656
Jan 19 21:26:26.658: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/19/23 21:26:26.658
Jan 19 21:26:26.665: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/19/23 21:26:26.665
Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: ADDED
Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.667: INFO: Found daemon set daemon-set in namespace daemonsets-803 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 19 21:26:26.667: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/19/23 21:26:26.667
STEP: watching for the daemon set status to be patched 01/19/23 21:26:26.673
Jan 19 21:26:26.674: INFO: Observed &DaemonSet event: ADDED
Jan 19 21:26:26.674: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.674: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.675: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.675: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.675: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.675: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.675: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.675: INFO: Observed daemon set daemon-set in namespace daemonsets-803 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan 19 21:26:26.675: INFO: Observed &DaemonSet event: MODIFIED
Jan 19 21:26:26.686: INFO: Found daemon set daemon-set in namespace daemonsets-803 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan 19 21:26:26.686: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/19/23 21:26:26.688
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-803, will wait for the garbage collector to delete the pods 01/19/23 21:26:26.688
Jan 19 21:26:26.747: INFO: Deleting DaemonSet.extensions daemon-set took: 4.863853ms
Jan 19 21:26:26.847: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.081448ms
Jan 19 21:26:29.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:26:29.249: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 21:26:29.252: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"189955"},"items":null}

Jan 19 21:26:29.254: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"189955"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:26:29.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-803" for this suite. 01/19/23 21:26:29.275
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":190,"skipped":3439,"failed":0}
------------------------------
• [4.806 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:26:24.474
    Jan 19 21:26:24.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename daemonsets 01/19/23 21:26:24.475
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:24.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:24.509
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 01/19/23 21:26:24.628
    STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 21:26:24.635
    Jan 19 21:26:24.642: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:26:24.642: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:26:24.642: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:26:24.644: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:26:24.644: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:26:25.649: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:26:25.650: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:26:25.650: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:26:25.653: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 21:26:25.653: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:26:26.649: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:26:26.649: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:26:26.649: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:26:26.654: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
    Jan 19 21:26:26.654: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
    STEP: Getting /status 01/19/23 21:26:26.656
    Jan 19 21:26:26.658: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/19/23 21:26:26.658
    Jan 19 21:26:26.665: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/19/23 21:26:26.665
    Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: ADDED
    Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.667: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.667: INFO: Found daemon set daemon-set in namespace daemonsets-803 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 19 21:26:26.667: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/19/23 21:26:26.667
    STEP: watching for the daemon set status to be patched 01/19/23 21:26:26.673
    Jan 19 21:26:26.674: INFO: Observed &DaemonSet event: ADDED
    Jan 19 21:26:26.674: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.674: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.675: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.675: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.675: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.675: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.675: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.675: INFO: Observed daemon set daemon-set in namespace daemonsets-803 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan 19 21:26:26.675: INFO: Observed &DaemonSet event: MODIFIED
    Jan 19 21:26:26.686: INFO: Found daemon set daemon-set in namespace daemonsets-803 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan 19 21:26:26.686: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/19/23 21:26:26.688
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-803, will wait for the garbage collector to delete the pods 01/19/23 21:26:26.688
    Jan 19 21:26:26.747: INFO: Deleting DaemonSet.extensions daemon-set took: 4.863853ms
    Jan 19 21:26:26.847: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.081448ms
    Jan 19 21:26:29.249: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:26:29.249: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 19 21:26:29.252: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"189955"},"items":null}

    Jan 19 21:26:29.254: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"189955"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:26:29.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-803" for this suite. 01/19/23 21:26:29.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:26:29.282
Jan 19 21:26:29.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename watch 01/19/23 21:26:29.282
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:29.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:29.298
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/19/23 21:26:29.301
STEP: creating a new configmap 01/19/23 21:26:29.303
STEP: modifying the configmap once 01/19/23 21:26:29.316
STEP: closing the watch once it receives two notifications 01/19/23 21:26:29.331
Jan 19 21:26:29.332: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7067  f310c2bd-8f12-473a-b111-4ec99a2fcac3 189969 0 2023-01-19 21:26:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 21:26:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 21:26:29.332: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7067  f310c2bd-8f12-473a-b111-4ec99a2fcac3 189977 0 2023-01-19 21:26:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 21:26:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/19/23 21:26:29.332
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/19/23 21:26:29.34
STEP: deleting the configmap 01/19/23 21:26:29.341
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/19/23 21:26:29.357
Jan 19 21:26:29.357: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7067  f310c2bd-8f12-473a-b111-4ec99a2fcac3 189978 0 2023-01-19 21:26:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 21:26:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 21:26:29.357: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7067  f310c2bd-8f12-473a-b111-4ec99a2fcac3 189983 0 2023-01-19 21:26:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 21:26:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 19 21:26:29.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7067" for this suite. 01/19/23 21:26:29.365
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":191,"skipped":3461,"failed":0}
------------------------------
• [0.090 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:26:29.282
    Jan 19 21:26:29.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename watch 01/19/23 21:26:29.282
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:29.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:29.298
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/19/23 21:26:29.301
    STEP: creating a new configmap 01/19/23 21:26:29.303
    STEP: modifying the configmap once 01/19/23 21:26:29.316
    STEP: closing the watch once it receives two notifications 01/19/23 21:26:29.331
    Jan 19 21:26:29.332: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7067  f310c2bd-8f12-473a-b111-4ec99a2fcac3 189969 0 2023-01-19 21:26:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 21:26:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 21:26:29.332: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7067  f310c2bd-8f12-473a-b111-4ec99a2fcac3 189977 0 2023-01-19 21:26:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 21:26:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/19/23 21:26:29.332
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/19/23 21:26:29.34
    STEP: deleting the configmap 01/19/23 21:26:29.341
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/19/23 21:26:29.357
    Jan 19 21:26:29.357: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7067  f310c2bd-8f12-473a-b111-4ec99a2fcac3 189978 0 2023-01-19 21:26:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 21:26:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 21:26:29.357: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-7067  f310c2bd-8f12-473a-b111-4ec99a2fcac3 189983 0 2023-01-19 21:26:29 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-19 21:26:29 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 19 21:26:29.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-7067" for this suite. 01/19/23 21:26:29.365
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:26:29.372
Jan 19 21:26:29.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename subpath 01/19/23 21:26:29.373
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:29.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:29.392
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/19/23 21:26:29.399
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-h6pv 01/19/23 21:26:29.419
STEP: Creating a pod to test atomic-volume-subpath 01/19/23 21:26:29.419
Jan 19 21:26:29.445: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-h6pv" in namespace "subpath-1933" to be "Succeeded or Failed"
Jan 19 21:26:29.451: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.30068ms
Jan 19 21:26:31.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 2.010125206s
Jan 19 21:26:33.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 4.010145841s
Jan 19 21:26:35.459: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 6.01388312s
Jan 19 21:26:37.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 8.010637959s
Jan 19 21:26:39.454: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 10.00922145s
Jan 19 21:26:41.454: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 12.00960897s
Jan 19 21:26:43.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 14.010405737s
Jan 19 21:26:45.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 16.010201304s
Jan 19 21:26:47.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 18.01012202s
Jan 19 21:26:49.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 20.010326659s
Jan 19 21:26:51.453: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=false. Elapsed: 22.008613989s
Jan 19 21:26:53.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010534268s
STEP: Saw pod success 01/19/23 21:26:53.455
Jan 19 21:26:53.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv" satisfied condition "Succeeded or Failed"
Jan 19 21:26:53.458: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-subpath-test-downwardapi-h6pv container test-container-subpath-downwardapi-h6pv: <nil>
STEP: delete the pod 01/19/23 21:26:53.463
Jan 19 21:26:53.474: INFO: Waiting for pod pod-subpath-test-downwardapi-h6pv to disappear
Jan 19 21:26:53.477: INFO: Pod pod-subpath-test-downwardapi-h6pv no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-h6pv 01/19/23 21:26:53.477
Jan 19 21:26:53.477: INFO: Deleting pod "pod-subpath-test-downwardapi-h6pv" in namespace "subpath-1933"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 19 21:26:53.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1933" for this suite. 01/19/23 21:26:53.483
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":192,"skipped":3478,"failed":0}
------------------------------
• [SLOW TEST] [24.115 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:26:29.372
    Jan 19 21:26:29.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename subpath 01/19/23 21:26:29.373
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:29.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:29.392
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/19/23 21:26:29.399
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-h6pv 01/19/23 21:26:29.419
    STEP: Creating a pod to test atomic-volume-subpath 01/19/23 21:26:29.419
    Jan 19 21:26:29.445: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-h6pv" in namespace "subpath-1933" to be "Succeeded or Failed"
    Jan 19 21:26:29.451: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.30068ms
    Jan 19 21:26:31.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 2.010125206s
    Jan 19 21:26:33.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 4.010145841s
    Jan 19 21:26:35.459: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 6.01388312s
    Jan 19 21:26:37.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 8.010637959s
    Jan 19 21:26:39.454: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 10.00922145s
    Jan 19 21:26:41.454: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 12.00960897s
    Jan 19 21:26:43.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 14.010405737s
    Jan 19 21:26:45.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 16.010201304s
    Jan 19 21:26:47.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 18.01012202s
    Jan 19 21:26:49.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=true. Elapsed: 20.010326659s
    Jan 19 21:26:51.453: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Running", Reason="", readiness=false. Elapsed: 22.008613989s
    Jan 19 21:26:53.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010534268s
    STEP: Saw pod success 01/19/23 21:26:53.455
    Jan 19 21:26:53.455: INFO: Pod "pod-subpath-test-downwardapi-h6pv" satisfied condition "Succeeded or Failed"
    Jan 19 21:26:53.458: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-subpath-test-downwardapi-h6pv container test-container-subpath-downwardapi-h6pv: <nil>
    STEP: delete the pod 01/19/23 21:26:53.463
    Jan 19 21:26:53.474: INFO: Waiting for pod pod-subpath-test-downwardapi-h6pv to disappear
    Jan 19 21:26:53.477: INFO: Pod pod-subpath-test-downwardapi-h6pv no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-h6pv 01/19/23 21:26:53.477
    Jan 19 21:26:53.477: INFO: Deleting pod "pod-subpath-test-downwardapi-h6pv" in namespace "subpath-1933"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 19 21:26:53.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-1933" for this suite. 01/19/23 21:26:53.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:26:53.488
Jan 19 21:26:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 21:26:53.488
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:53.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:53.524
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 01/19/23 21:26:53.527
Jan 19 21:26:53.567: INFO: Waiting up to 5m0s for pod "pod-68d62302-023e-4582-90c0-74490876a5d7" in namespace "emptydir-21" to be "Succeeded or Failed"
Jan 19 21:26:53.570: INFO: Pod "pod-68d62302-023e-4582-90c0-74490876a5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.955595ms
Jan 19 21:26:55.575: INFO: Pod "pod-68d62302-023e-4582-90c0-74490876a5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00722186s
Jan 19 21:26:57.574: INFO: Pod "pod-68d62302-023e-4582-90c0-74490876a5d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006258757s
STEP: Saw pod success 01/19/23 21:26:57.574
Jan 19 21:26:57.574: INFO: Pod "pod-68d62302-023e-4582-90c0-74490876a5d7" satisfied condition "Succeeded or Failed"
Jan 19 21:26:57.576: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-68d62302-023e-4582-90c0-74490876a5d7 container test-container: <nil>
STEP: delete the pod 01/19/23 21:26:57.581
Jan 19 21:26:57.592: INFO: Waiting for pod pod-68d62302-023e-4582-90c0-74490876a5d7 to disappear
Jan 19 21:26:57.594: INFO: Pod pod-68d62302-023e-4582-90c0-74490876a5d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 21:26:57.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-21" for this suite. 01/19/23 21:26:57.606
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":193,"skipped":3484,"failed":0}
------------------------------
• [4.123 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:26:53.488
    Jan 19 21:26:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 21:26:53.488
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:53.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:53.524
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/19/23 21:26:53.527
    Jan 19 21:26:53.567: INFO: Waiting up to 5m0s for pod "pod-68d62302-023e-4582-90c0-74490876a5d7" in namespace "emptydir-21" to be "Succeeded or Failed"
    Jan 19 21:26:53.570: INFO: Pod "pod-68d62302-023e-4582-90c0-74490876a5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.955595ms
    Jan 19 21:26:55.575: INFO: Pod "pod-68d62302-023e-4582-90c0-74490876a5d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00722186s
    Jan 19 21:26:57.574: INFO: Pod "pod-68d62302-023e-4582-90c0-74490876a5d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006258757s
    STEP: Saw pod success 01/19/23 21:26:57.574
    Jan 19 21:26:57.574: INFO: Pod "pod-68d62302-023e-4582-90c0-74490876a5d7" satisfied condition "Succeeded or Failed"
    Jan 19 21:26:57.576: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-68d62302-023e-4582-90c0-74490876a5d7 container test-container: <nil>
    STEP: delete the pod 01/19/23 21:26:57.581
    Jan 19 21:26:57.592: INFO: Waiting for pod pod-68d62302-023e-4582-90c0-74490876a5d7 to disappear
    Jan 19 21:26:57.594: INFO: Pod pod-68d62302-023e-4582-90c0-74490876a5d7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:26:57.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-21" for this suite. 01/19/23 21:26:57.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:26:57.612
Jan 19 21:26:57.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename resourcequota 01/19/23 21:26:57.613
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:57.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:57.632
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 01/19/23 21:26:57.636
STEP: Creating a ResourceQuota 01/19/23 21:27:02.639
STEP: Ensuring resource quota status is calculated 01/19/23 21:27:02.643
STEP: Creating a Pod that fits quota 01/19/23 21:27:04.647
STEP: Ensuring ResourceQuota status captures the pod usage 01/19/23 21:27:04.672
STEP: Not allowing a pod to be created that exceeds remaining quota 01/19/23 21:27:06.675
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/19/23 21:27:06.682
STEP: Ensuring a pod cannot update its resource requirements 01/19/23 21:27:06.69
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/19/23 21:27:06.695
STEP: Deleting the pod 01/19/23 21:27:08.697
STEP: Ensuring resource quota status released the pod usage 01/19/23 21:27:08.705
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 21:27:10.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9948" for this suite. 01/19/23 21:27:10.713
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":194,"skipped":3515,"failed":0}
------------------------------
• [SLOW TEST] [13.107 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:26:57.612
    Jan 19 21:26:57.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename resourcequota 01/19/23 21:26:57.613
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:26:57.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:26:57.632
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 01/19/23 21:26:57.636
    STEP: Creating a ResourceQuota 01/19/23 21:27:02.639
    STEP: Ensuring resource quota status is calculated 01/19/23 21:27:02.643
    STEP: Creating a Pod that fits quota 01/19/23 21:27:04.647
    STEP: Ensuring ResourceQuota status captures the pod usage 01/19/23 21:27:04.672
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/19/23 21:27:06.675
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/19/23 21:27:06.682
    STEP: Ensuring a pod cannot update its resource requirements 01/19/23 21:27:06.69
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/19/23 21:27:06.695
    STEP: Deleting the pod 01/19/23 21:27:08.697
    STEP: Ensuring resource quota status released the pod usage 01/19/23 21:27:08.705
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 21:27:10.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9948" for this suite. 01/19/23 21:27:10.713
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:27:10.72
Jan 19 21:27:10.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename disruption 01/19/23 21:27:10.72
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:10.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:10.753
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:27:10.758
Jan 19 21:27:10.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename disruption-2 01/19/23 21:27:10.758
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:10.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:10.819
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 01/19/23 21:27:10.843
STEP: Waiting for the pdb to be processed 01/19/23 21:27:12.867
STEP: Waiting for the pdb to be processed 01/19/23 21:27:14.877
STEP: listing a collection of PDBs across all namespaces 01/19/23 21:27:16.886
STEP: listing a collection of PDBs in namespace disruption-1993 01/19/23 21:27:16.889
STEP: deleting a collection of PDBs 01/19/23 21:27:16.891
STEP: Waiting for the PDB collection to be deleted 01/19/23 21:27:16.901
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jan 19 21:27:16.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-4482" for this suite. 01/19/23 21:27:16.907
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jan 19 21:27:16.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1993" for this suite. 01/19/23 21:27:16.917
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":195,"skipped":3518,"failed":0}
------------------------------
• [SLOW TEST] [6.203 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:27:10.72
    Jan 19 21:27:10.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename disruption 01/19/23 21:27:10.72
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:10.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:10.753
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:27:10.758
    Jan 19 21:27:10.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename disruption-2 01/19/23 21:27:10.758
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:10.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:10.819
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 01/19/23 21:27:10.843
    STEP: Waiting for the pdb to be processed 01/19/23 21:27:12.867
    STEP: Waiting for the pdb to be processed 01/19/23 21:27:14.877
    STEP: listing a collection of PDBs across all namespaces 01/19/23 21:27:16.886
    STEP: listing a collection of PDBs in namespace disruption-1993 01/19/23 21:27:16.889
    STEP: deleting a collection of PDBs 01/19/23 21:27:16.891
    STEP: Waiting for the PDB collection to be deleted 01/19/23 21:27:16.901
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jan 19 21:27:16.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-4482" for this suite. 01/19/23 21:27:16.907
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jan 19 21:27:16.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1993" for this suite. 01/19/23 21:27:16.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:27:16.924
Jan 19 21:27:16.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:27:16.924
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:16.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:16.944
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-49d49d5d-88b2-4f97-91e5-3e8620580c66 01/19/23 21:27:16.949
STEP: Creating a pod to test consume secrets 01/19/23 21:27:16.961
Jan 19 21:27:17.018: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f" in namespace "projected-4430" to be "Succeeded or Failed"
Jan 19 21:27:17.033: INFO: Pod "pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.935766ms
Jan 19 21:27:19.037: INFO: Pod "pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018165603s
Jan 19 21:27:21.037: INFO: Pod "pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018550348s
STEP: Saw pod success 01/19/23 21:27:21.037
Jan 19 21:27:21.037: INFO: Pod "pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f" satisfied condition "Succeeded or Failed"
Jan 19 21:27:21.039: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f container projected-secret-volume-test: <nil>
STEP: delete the pod 01/19/23 21:27:21.043
Jan 19 21:27:21.052: INFO: Waiting for pod pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f to disappear
Jan 19 21:27:21.054: INFO: Pod pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 21:27:21.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4430" for this suite. 01/19/23 21:27:21.059
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":196,"skipped":3540,"failed":0}
------------------------------
• [4.140 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:27:16.924
    Jan 19 21:27:16.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:27:16.924
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:16.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:16.944
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-49d49d5d-88b2-4f97-91e5-3e8620580c66 01/19/23 21:27:16.949
    STEP: Creating a pod to test consume secrets 01/19/23 21:27:16.961
    Jan 19 21:27:17.018: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f" in namespace "projected-4430" to be "Succeeded or Failed"
    Jan 19 21:27:17.033: INFO: Pod "pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 14.935766ms
    Jan 19 21:27:19.037: INFO: Pod "pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018165603s
    Jan 19 21:27:21.037: INFO: Pod "pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018550348s
    STEP: Saw pod success 01/19/23 21:27:21.037
    Jan 19 21:27:21.037: INFO: Pod "pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f" satisfied condition "Succeeded or Failed"
    Jan 19 21:27:21.039: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:27:21.043
    Jan 19 21:27:21.052: INFO: Waiting for pod pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f to disappear
    Jan 19 21:27:21.054: INFO: Pod pod-projected-secrets-1584ae8f-d8be-4fb6-9c57-911065d41d2f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 21:27:21.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4430" for this suite. 01/19/23 21:27:21.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:27:21.066
Jan 19 21:27:21.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 21:27:21.066
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:21.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:21.092
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:27:21.094
Jan 19 21:27:21.129: INFO: Waiting up to 5m0s for pod "downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d" in namespace "downward-api-9543" to be "Succeeded or Failed"
Jan 19 21:27:21.136: INFO: Pod "downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.543522ms
Jan 19 21:27:23.139: INFO: Pod "downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010814407s
Jan 19 21:27:25.141: INFO: Pod "downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012087555s
Jan 19 21:27:27.140: INFO: Pod "downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011029329s
STEP: Saw pod success 01/19/23 21:27:27.14
Jan 19 21:27:27.140: INFO: Pod "downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d" satisfied condition "Succeeded or Failed"
Jan 19 21:27:27.142: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d container client-container: <nil>
STEP: delete the pod 01/19/23 21:27:27.146
Jan 19 21:27:27.155: INFO: Waiting for pod downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d to disappear
Jan 19 21:27:27.157: INFO: Pod downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 21:27:27.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9543" for this suite. 01/19/23 21:27:27.161
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":197,"skipped":3601,"failed":0}
------------------------------
• [SLOW TEST] [6.101 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:27:21.066
    Jan 19 21:27:21.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 21:27:21.066
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:21.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:21.092
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:27:21.094
    Jan 19 21:27:21.129: INFO: Waiting up to 5m0s for pod "downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d" in namespace "downward-api-9543" to be "Succeeded or Failed"
    Jan 19 21:27:21.136: INFO: Pod "downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.543522ms
    Jan 19 21:27:23.139: INFO: Pod "downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010814407s
    Jan 19 21:27:25.141: INFO: Pod "downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012087555s
    Jan 19 21:27:27.140: INFO: Pod "downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011029329s
    STEP: Saw pod success 01/19/23 21:27:27.14
    Jan 19 21:27:27.140: INFO: Pod "downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d" satisfied condition "Succeeded or Failed"
    Jan 19 21:27:27.142: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d container client-container: <nil>
    STEP: delete the pod 01/19/23 21:27:27.146
    Jan 19 21:27:27.155: INFO: Waiting for pod downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d to disappear
    Jan 19 21:27:27.157: INFO: Pod downwardapi-volume-daa6bf70-606d-4bbe-8335-affe1a23c09d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 21:27:27.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9543" for this suite. 01/19/23 21:27:27.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:27:27.168
Jan 19 21:27:27.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename security-context-test 01/19/23 21:27:27.169
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:27.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:27.198
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jan 19 21:27:27.227: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-d68a6120-3f1e-412e-9305-ea3dcbb94524" in namespace "security-context-test-5215" to be "Succeeded or Failed"
Jan 19 21:27:27.232: INFO: Pod "busybox-readonly-false-d68a6120-3f1e-412e-9305-ea3dcbb94524": Phase="Pending", Reason="", readiness=false. Elapsed: 5.024021ms
Jan 19 21:27:29.234: INFO: Pod "busybox-readonly-false-d68a6120-3f1e-412e-9305-ea3dcbb94524": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007465817s
Jan 19 21:27:31.236: INFO: Pod "busybox-readonly-false-d68a6120-3f1e-412e-9305-ea3dcbb94524": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008536153s
Jan 19 21:27:31.236: INFO: Pod "busybox-readonly-false-d68a6120-3f1e-412e-9305-ea3dcbb94524" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 19 21:27:31.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-5215" for this suite. 01/19/23 21:27:31.24
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":198,"skipped":3649,"failed":0}
------------------------------
• [4.078 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:27:27.168
    Jan 19 21:27:27.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename security-context-test 01/19/23 21:27:27.169
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:27.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:27.198
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jan 19 21:27:27.227: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-d68a6120-3f1e-412e-9305-ea3dcbb94524" in namespace "security-context-test-5215" to be "Succeeded or Failed"
    Jan 19 21:27:27.232: INFO: Pod "busybox-readonly-false-d68a6120-3f1e-412e-9305-ea3dcbb94524": Phase="Pending", Reason="", readiness=false. Elapsed: 5.024021ms
    Jan 19 21:27:29.234: INFO: Pod "busybox-readonly-false-d68a6120-3f1e-412e-9305-ea3dcbb94524": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007465817s
    Jan 19 21:27:31.236: INFO: Pod "busybox-readonly-false-d68a6120-3f1e-412e-9305-ea3dcbb94524": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008536153s
    Jan 19 21:27:31.236: INFO: Pod "busybox-readonly-false-d68a6120-3f1e-412e-9305-ea3dcbb94524" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 19 21:27:31.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-5215" for this suite. 01/19/23 21:27:31.24
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:27:31.246
Jan 19 21:27:31.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:27:31.247
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:31.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:31.271
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-95f2cc6d-0e60-4ca5-af3b-bafc28eb6796 01/19/23 21:27:31.273
STEP: Creating a pod to test consume configMaps 01/19/23 21:27:31.292
Jan 19 21:27:31.311: INFO: Waiting up to 5m0s for pod "pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638" in namespace "configmap-9970" to be "Succeeded or Failed"
Jan 19 21:27:31.315: INFO: Pod "pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638": Phase="Pending", Reason="", readiness=false. Elapsed: 3.967885ms
Jan 19 21:27:33.318: INFO: Pod "pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007548445s
Jan 19 21:27:35.318: INFO: Pod "pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007279929s
STEP: Saw pod success 01/19/23 21:27:35.318
Jan 19 21:27:35.318: INFO: Pod "pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638" satisfied condition "Succeeded or Failed"
Jan 19 21:27:35.320: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:27:35.326
Jan 19 21:27:35.335: INFO: Waiting for pod pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638 to disappear
Jan 19 21:27:35.338: INFO: Pod pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:27:35.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9970" for this suite. 01/19/23 21:27:35.342
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":199,"skipped":3649,"failed":0}
------------------------------
• [4.102 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:27:31.246
    Jan 19 21:27:31.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:27:31.247
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:31.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:31.271
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-95f2cc6d-0e60-4ca5-af3b-bafc28eb6796 01/19/23 21:27:31.273
    STEP: Creating a pod to test consume configMaps 01/19/23 21:27:31.292
    Jan 19 21:27:31.311: INFO: Waiting up to 5m0s for pod "pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638" in namespace "configmap-9970" to be "Succeeded or Failed"
    Jan 19 21:27:31.315: INFO: Pod "pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638": Phase="Pending", Reason="", readiness=false. Elapsed: 3.967885ms
    Jan 19 21:27:33.318: INFO: Pod "pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007548445s
    Jan 19 21:27:35.318: INFO: Pod "pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007279929s
    STEP: Saw pod success 01/19/23 21:27:35.318
    Jan 19 21:27:35.318: INFO: Pod "pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638" satisfied condition "Succeeded or Failed"
    Jan 19 21:27:35.320: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:27:35.326
    Jan 19 21:27:35.335: INFO: Waiting for pod pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638 to disappear
    Jan 19 21:27:35.338: INFO: Pod pod-configmaps-12ea49c7-afc2-4cea-b308-822e1fbac638 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:27:35.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9970" for this suite. 01/19/23 21:27:35.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:27:35.351
Jan 19 21:27:35.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 21:27:35.352
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:35.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:35.379
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-2c0e86c0-a236-4557-9f7b-74d4f5098433 01/19/23 21:27:35.384
STEP: Creating a pod to test consume secrets 01/19/23 21:27:35.392
Jan 19 21:27:35.428: INFO: Waiting up to 5m0s for pod "pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38" in namespace "secrets-2888" to be "Succeeded or Failed"
Jan 19 21:27:35.442: INFO: Pod "pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38": Phase="Pending", Reason="", readiness=false. Elapsed: 14.186342ms
Jan 19 21:27:37.444: INFO: Pod "pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016561706s
Jan 19 21:27:39.446: INFO: Pod "pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018285963s
STEP: Saw pod success 01/19/23 21:27:39.446
Jan 19 21:27:39.446: INFO: Pod "pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38" satisfied condition "Succeeded or Failed"
Jan 19 21:27:39.448: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 21:27:39.453
Jan 19 21:27:39.462: INFO: Waiting for pod pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38 to disappear
Jan 19 21:27:39.465: INFO: Pod pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 21:27:39.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2888" for this suite. 01/19/23 21:27:39.469
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":200,"skipped":3777,"failed":0}
------------------------------
• [4.125 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:27:35.351
    Jan 19 21:27:35.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 21:27:35.352
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:35.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:35.379
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-2c0e86c0-a236-4557-9f7b-74d4f5098433 01/19/23 21:27:35.384
    STEP: Creating a pod to test consume secrets 01/19/23 21:27:35.392
    Jan 19 21:27:35.428: INFO: Waiting up to 5m0s for pod "pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38" in namespace "secrets-2888" to be "Succeeded or Failed"
    Jan 19 21:27:35.442: INFO: Pod "pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38": Phase="Pending", Reason="", readiness=false. Elapsed: 14.186342ms
    Jan 19 21:27:37.444: INFO: Pod "pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016561706s
    Jan 19 21:27:39.446: INFO: Pod "pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018285963s
    STEP: Saw pod success 01/19/23 21:27:39.446
    Jan 19 21:27:39.446: INFO: Pod "pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38" satisfied condition "Succeeded or Failed"
    Jan 19 21:27:39.448: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:27:39.453
    Jan 19 21:27:39.462: INFO: Waiting for pod pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38 to disappear
    Jan 19 21:27:39.465: INFO: Pod pod-secrets-0b4d991d-6b46-4636-98ba-dcde9acffb38 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 21:27:39.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2888" for this suite. 01/19/23 21:27:39.469
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:27:39.476
Jan 19 21:27:39.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:27:39.477
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:39.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:39.496
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-77543337-21c8-48ff-8420-eb4f59b90a51 01/19/23 21:27:39.498
STEP: Creating a pod to test consume configMaps 01/19/23 21:27:39.51
Jan 19 21:27:39.535: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658" in namespace "projected-9316" to be "Succeeded or Failed"
Jan 19 21:27:39.537: INFO: Pod "pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.435075ms
Jan 19 21:27:41.540: INFO: Pod "pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005288863s
Jan 19 21:27:43.541: INFO: Pod "pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006193241s
STEP: Saw pod success 01/19/23 21:27:43.541
Jan 19 21:27:43.541: INFO: Pod "pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658" satisfied condition "Succeeded or Failed"
Jan 19 21:27:43.543: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:27:43.548
Jan 19 21:27:43.560: INFO: Waiting for pod pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658 to disappear
Jan 19 21:27:43.563: INFO: Pod pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 21:27:43.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9316" for this suite. 01/19/23 21:27:43.568
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":201,"skipped":3777,"failed":0}
------------------------------
• [4.098 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:27:39.476
    Jan 19 21:27:39.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:27:39.477
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:39.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:39.496
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-77543337-21c8-48ff-8420-eb4f59b90a51 01/19/23 21:27:39.498
    STEP: Creating a pod to test consume configMaps 01/19/23 21:27:39.51
    Jan 19 21:27:39.535: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658" in namespace "projected-9316" to be "Succeeded or Failed"
    Jan 19 21:27:39.537: INFO: Pod "pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.435075ms
    Jan 19 21:27:41.540: INFO: Pod "pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005288863s
    Jan 19 21:27:43.541: INFO: Pod "pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006193241s
    STEP: Saw pod success 01/19/23 21:27:43.541
    Jan 19 21:27:43.541: INFO: Pod "pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658" satisfied condition "Succeeded or Failed"
    Jan 19 21:27:43.543: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:27:43.548
    Jan 19 21:27:43.560: INFO: Waiting for pod pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658 to disappear
    Jan 19 21:27:43.563: INFO: Pod pod-projected-configmaps-e7d29c74-22f4-44fa-adba-698215888658 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 21:27:43.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9316" for this suite. 01/19/23 21:27:43.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:27:43.574
Jan 19 21:27:43.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename svcaccounts 01/19/23 21:27:43.575
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:43.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:43.598
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 01/19/23 21:27:43.606
STEP: watching for the ServiceAccount to be added 01/19/23 21:27:43.637
STEP: patching the ServiceAccount 01/19/23 21:27:43.639
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/19/23 21:27:43.649
STEP: deleting the ServiceAccount 01/19/23 21:27:43.676
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 19 21:27:43.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9501" for this suite. 01/19/23 21:27:43.73
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":202,"skipped":3783,"failed":0}
------------------------------
• [0.167 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:27:43.574
    Jan 19 21:27:43.574: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename svcaccounts 01/19/23 21:27:43.575
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:43.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:43.598
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 01/19/23 21:27:43.606
    STEP: watching for the ServiceAccount to be added 01/19/23 21:27:43.637
    STEP: patching the ServiceAccount 01/19/23 21:27:43.639
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/19/23 21:27:43.649
    STEP: deleting the ServiceAccount 01/19/23 21:27:43.676
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 19 21:27:43.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9501" for this suite. 01/19/23 21:27:43.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:27:43.742
Jan 19 21:27:43.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:27:43.743
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:43.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:43.791
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 01/19/23 21:27:43.796
Jan 19 21:27:43.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan 19 21:27:43.870: INFO: stderr: ""
Jan 19 21:27:43.870: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 01/19/23 21:27:43.87
Jan 19 21:27:43.870: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan 19 21:27:43.870: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4103" to be "running and ready, or succeeded"
Jan 19 21:27:43.876: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071186ms
Jan 19 21:27:43.876: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-10-0-172-44.ec2.internal' to be 'Running' but was 'Pending'
Jan 19 21:27:45.880: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.00980492s
Jan 19 21:27:45.880: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan 19 21:27:45.880: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/19/23 21:27:45.88
Jan 19 21:27:45.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 logs logs-generator logs-generator'
Jan 19 21:27:45.961: INFO: stderr: ""
Jan 19 21:27:45.961: INFO: stdout: "I0119 21:27:44.629586       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/wr9 517\nI0119 21:27:44.829611       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/rlvn 213\nI0119 21:27:45.030178       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/thq2 444\nI0119 21:27:45.230475       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/5d4 312\nI0119 21:27:45.429708       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/6mhj 392\nI0119 21:27:45.630032       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/6z2p 269\nI0119 21:27:45.830156       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/tzq 571\n"
STEP: limiting log lines 01/19/23 21:27:45.961
Jan 19 21:27:45.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 logs logs-generator logs-generator --tail=1'
Jan 19 21:27:46.022: INFO: stderr: ""
Jan 19 21:27:46.022: INFO: stdout: "I0119 21:27:45.830156       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/tzq 571\n"
Jan 19 21:27:46.022: INFO: got output "I0119 21:27:45.830156       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/tzq 571\n"
STEP: limiting log bytes 01/19/23 21:27:46.022
Jan 19 21:27:46.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 logs logs-generator logs-generator --limit-bytes=1'
Jan 19 21:27:46.090: INFO: stderr: ""
Jan 19 21:27:46.090: INFO: stdout: "I"
Jan 19 21:27:46.090: INFO: got output "I"
STEP: exposing timestamps 01/19/23 21:27:46.09
Jan 19 21:27:46.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 logs logs-generator logs-generator --tail=1 --timestamps'
Jan 19 21:27:46.149: INFO: stderr: ""
Jan 19 21:27:46.149: INFO: stdout: "2023-01-19T21:27:46.030509444Z I0119 21:27:46.030467       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jszm 262\n"
Jan 19 21:27:46.149: INFO: got output "2023-01-19T21:27:46.030509444Z I0119 21:27:46.030467       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jszm 262\n"
STEP: restricting to a time range 01/19/23 21:27:46.149
Jan 19 21:27:48.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 logs logs-generator logs-generator --since=1s'
Jan 19 21:27:48.712: INFO: stderr: ""
Jan 19 21:27:48.712: INFO: stdout: "I0119 21:27:47.829956       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/5xx 302\nI0119 21:27:48.030174       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/cv8 250\nI0119 21:27:48.230488       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/5644 314\nI0119 21:27:48.429606       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/9bb9 532\nI0119 21:27:48.629912       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/zfq 581\n"
Jan 19 21:27:48.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 logs logs-generator logs-generator --since=24h'
Jan 19 21:27:48.773: INFO: stderr: ""
Jan 19 21:27:48.773: INFO: stdout: "I0119 21:27:44.629586       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/wr9 517\nI0119 21:27:44.829611       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/rlvn 213\nI0119 21:27:45.030178       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/thq2 444\nI0119 21:27:45.230475       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/5d4 312\nI0119 21:27:45.429708       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/6mhj 392\nI0119 21:27:45.630032       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/6z2p 269\nI0119 21:27:45.830156       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/tzq 571\nI0119 21:27:46.030467       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jszm 262\nI0119 21:27:46.229718       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/nql 288\nI0119 21:27:46.430026       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/68qm 533\nI0119 21:27:46.630331       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/tncg 295\nI0119 21:27:46.829580       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/qnwq 349\nI0119 21:27:47.029884       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/hg4q 319\nI0119 21:27:47.230192       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/xx7 331\nI0119 21:27:47.430491       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/zks 256\nI0119 21:27:47.629606       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/t4d8 422\nI0119 21:27:47.829956       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/5xx 302\nI0119 21:27:48.030174       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/cv8 250\nI0119 21:27:48.230488       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/5644 314\nI0119 21:27:48.429606       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/9bb9 532\nI0119 21:27:48.629912       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/zfq 581\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jan 19 21:27:48.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 delete pod logs-generator'
Jan 19 21:27:50.369: INFO: stderr: ""
Jan 19 21:27:50.369: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:27:50.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4103" for this suite. 01/19/23 21:27:50.374
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":203,"skipped":3796,"failed":0}
------------------------------
• [SLOW TEST] [6.638 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:27:43.742
    Jan 19 21:27:43.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:27:43.743
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:43.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:43.791
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 01/19/23 21:27:43.796
    Jan 19 21:27:43.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan 19 21:27:43.870: INFO: stderr: ""
    Jan 19 21:27:43.870: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 01/19/23 21:27:43.87
    Jan 19 21:27:43.870: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan 19 21:27:43.870: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-4103" to be "running and ready, or succeeded"
    Jan 19 21:27:43.876: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071186ms
    Jan 19 21:27:43.876: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-10-0-172-44.ec2.internal' to be 'Running' but was 'Pending'
    Jan 19 21:27:45.880: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.00980492s
    Jan 19 21:27:45.880: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan 19 21:27:45.880: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/19/23 21:27:45.88
    Jan 19 21:27:45.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 logs logs-generator logs-generator'
    Jan 19 21:27:45.961: INFO: stderr: ""
    Jan 19 21:27:45.961: INFO: stdout: "I0119 21:27:44.629586       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/wr9 517\nI0119 21:27:44.829611       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/rlvn 213\nI0119 21:27:45.030178       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/thq2 444\nI0119 21:27:45.230475       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/5d4 312\nI0119 21:27:45.429708       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/6mhj 392\nI0119 21:27:45.630032       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/6z2p 269\nI0119 21:27:45.830156       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/tzq 571\n"
    STEP: limiting log lines 01/19/23 21:27:45.961
    Jan 19 21:27:45.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 logs logs-generator logs-generator --tail=1'
    Jan 19 21:27:46.022: INFO: stderr: ""
    Jan 19 21:27:46.022: INFO: stdout: "I0119 21:27:45.830156       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/tzq 571\n"
    Jan 19 21:27:46.022: INFO: got output "I0119 21:27:45.830156       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/tzq 571\n"
    STEP: limiting log bytes 01/19/23 21:27:46.022
    Jan 19 21:27:46.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 logs logs-generator logs-generator --limit-bytes=1'
    Jan 19 21:27:46.090: INFO: stderr: ""
    Jan 19 21:27:46.090: INFO: stdout: "I"
    Jan 19 21:27:46.090: INFO: got output "I"
    STEP: exposing timestamps 01/19/23 21:27:46.09
    Jan 19 21:27:46.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan 19 21:27:46.149: INFO: stderr: ""
    Jan 19 21:27:46.149: INFO: stdout: "2023-01-19T21:27:46.030509444Z I0119 21:27:46.030467       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jszm 262\n"
    Jan 19 21:27:46.149: INFO: got output "2023-01-19T21:27:46.030509444Z I0119 21:27:46.030467       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jszm 262\n"
    STEP: restricting to a time range 01/19/23 21:27:46.149
    Jan 19 21:27:48.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 logs logs-generator logs-generator --since=1s'
    Jan 19 21:27:48.712: INFO: stderr: ""
    Jan 19 21:27:48.712: INFO: stdout: "I0119 21:27:47.829956       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/5xx 302\nI0119 21:27:48.030174       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/cv8 250\nI0119 21:27:48.230488       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/5644 314\nI0119 21:27:48.429606       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/9bb9 532\nI0119 21:27:48.629912       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/zfq 581\n"
    Jan 19 21:27:48.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 logs logs-generator logs-generator --since=24h'
    Jan 19 21:27:48.773: INFO: stderr: ""
    Jan 19 21:27:48.773: INFO: stdout: "I0119 21:27:44.629586       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/ns/pods/wr9 517\nI0119 21:27:44.829611       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/rlvn 213\nI0119 21:27:45.030178       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/thq2 444\nI0119 21:27:45.230475       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/5d4 312\nI0119 21:27:45.429708       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/6mhj 392\nI0119 21:27:45.630032       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/6z2p 269\nI0119 21:27:45.830156       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/tzq 571\nI0119 21:27:46.030467       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/jszm 262\nI0119 21:27:46.229718       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/nql 288\nI0119 21:27:46.430026       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/68qm 533\nI0119 21:27:46.630331       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/tncg 295\nI0119 21:27:46.829580       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/qnwq 349\nI0119 21:27:47.029884       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/hg4q 319\nI0119 21:27:47.230192       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/xx7 331\nI0119 21:27:47.430491       1 logs_generator.go:76] 14 POST /api/v1/namespaces/ns/pods/zks 256\nI0119 21:27:47.629606       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/t4d8 422\nI0119 21:27:47.829956       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/ns/pods/5xx 302\nI0119 21:27:48.030174       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/cv8 250\nI0119 21:27:48.230488       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/5644 314\nI0119 21:27:48.429606       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/9bb9 532\nI0119 21:27:48.629912       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/zfq 581\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jan 19 21:27:48.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4103 delete pod logs-generator'
    Jan 19 21:27:50.369: INFO: stderr: ""
    Jan 19 21:27:50.369: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:27:50.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4103" for this suite. 01/19/23 21:27:50.374
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:27:50.381
Jan 19 21:27:50.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-probe 01/19/23 21:27:50.382
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:50.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:50.425
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 in namespace container-probe-9218 01/19/23 21:27:50.427
W0119 21:27:50.482515      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:27:50.482: INFO: Waiting up to 5m0s for pod "liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44" in namespace "container-probe-9218" to be "not pending"
Jan 19 21:27:50.490: INFO: Pod "liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44": Phase="Pending", Reason="", readiness=false. Elapsed: 7.398241ms
Jan 19 21:27:52.492: INFO: Pod "liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44": Phase="Running", Reason="", readiness=true. Elapsed: 2.010253639s
Jan 19 21:27:52.492: INFO: Pod "liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44" satisfied condition "not pending"
Jan 19 21:27:52.492: INFO: Started pod liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 in namespace container-probe-9218
STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 21:27:52.492
Jan 19 21:27:52.496: INFO: Initial restart count of pod liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 is 0
Jan 19 21:28:12.548: INFO: Restart count of pod container-probe-9218/liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 is now 1 (20.05198249s elapsed)
Jan 19 21:28:32.585: INFO: Restart count of pod container-probe-9218/liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 is now 2 (40.089387266s elapsed)
Jan 19 21:28:52.625: INFO: Restart count of pod container-probe-9218/liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 is now 3 (1m0.129013556s elapsed)
Jan 19 21:29:12.660: INFO: Restart count of pod container-probe-9218/liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 is now 4 (1m20.164281713s elapsed)
Jan 19 21:30:12.775: INFO: Restart count of pod container-probe-9218/liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 is now 5 (2m20.278861374s elapsed)
STEP: deleting the pod 01/19/23 21:30:12.775
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 21:30:12.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9218" for this suite. 01/19/23 21:30:12.789
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":204,"skipped":3798,"failed":0}
------------------------------
• [SLOW TEST] [142.413 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:27:50.381
    Jan 19 21:27:50.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-probe 01/19/23 21:27:50.382
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:27:50.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:27:50.425
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 in namespace container-probe-9218 01/19/23 21:27:50.427
    W0119 21:27:50.482515      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "agnhost-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:27:50.482: INFO: Waiting up to 5m0s for pod "liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44" in namespace "container-probe-9218" to be "not pending"
    Jan 19 21:27:50.490: INFO: Pod "liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44": Phase="Pending", Reason="", readiness=false. Elapsed: 7.398241ms
    Jan 19 21:27:52.492: INFO: Pod "liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44": Phase="Running", Reason="", readiness=true. Elapsed: 2.010253639s
    Jan 19 21:27:52.492: INFO: Pod "liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44" satisfied condition "not pending"
    Jan 19 21:27:52.492: INFO: Started pod liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 in namespace container-probe-9218
    STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 21:27:52.492
    Jan 19 21:27:52.496: INFO: Initial restart count of pod liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 is 0
    Jan 19 21:28:12.548: INFO: Restart count of pod container-probe-9218/liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 is now 1 (20.05198249s elapsed)
    Jan 19 21:28:32.585: INFO: Restart count of pod container-probe-9218/liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 is now 2 (40.089387266s elapsed)
    Jan 19 21:28:52.625: INFO: Restart count of pod container-probe-9218/liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 is now 3 (1m0.129013556s elapsed)
    Jan 19 21:29:12.660: INFO: Restart count of pod container-probe-9218/liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 is now 4 (1m20.164281713s elapsed)
    Jan 19 21:30:12.775: INFO: Restart count of pod container-probe-9218/liveness-c839a7fc-665d-4094-a0fc-5f7a35890b44 is now 5 (2m20.278861374s elapsed)
    STEP: deleting the pod 01/19/23 21:30:12.775
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 21:30:12.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9218" for this suite. 01/19/23 21:30:12.789
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:30:12.796
Jan 19 21:30:12.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-probe 01/19/23 21:30:12.797
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:30:12.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:30:12.831
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-47ff7b51-70a2-428c-a34a-60806dd311bc in namespace container-probe-8392 01/19/23 21:30:12.833
Jan 19 21:30:12.879: INFO: Waiting up to 5m0s for pod "liveness-47ff7b51-70a2-428c-a34a-60806dd311bc" in namespace "container-probe-8392" to be "not pending"
Jan 19 21:30:12.889: INFO: Pod "liveness-47ff7b51-70a2-428c-a34a-60806dd311bc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.3701ms
Jan 19 21:30:14.892: INFO: Pod "liveness-47ff7b51-70a2-428c-a34a-60806dd311bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.013663902s
Jan 19 21:30:14.892: INFO: Pod "liveness-47ff7b51-70a2-428c-a34a-60806dd311bc" satisfied condition "not pending"
Jan 19 21:30:14.892: INFO: Started pod liveness-47ff7b51-70a2-428c-a34a-60806dd311bc in namespace container-probe-8392
STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 21:30:14.892
Jan 19 21:30:14.895: INFO: Initial restart count of pod liveness-47ff7b51-70a2-428c-a34a-60806dd311bc is 0
STEP: deleting the pod 01/19/23 21:34:15.338
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 21:34:15.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8392" for this suite. 01/19/23 21:34:15.358
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":205,"skipped":3870,"failed":0}
------------------------------
• [SLOW TEST] [242.568 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:30:12.796
    Jan 19 21:30:12.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-probe 01/19/23 21:30:12.797
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:30:12.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:30:12.831
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-47ff7b51-70a2-428c-a34a-60806dd311bc in namespace container-probe-8392 01/19/23 21:30:12.833
    Jan 19 21:30:12.879: INFO: Waiting up to 5m0s for pod "liveness-47ff7b51-70a2-428c-a34a-60806dd311bc" in namespace "container-probe-8392" to be "not pending"
    Jan 19 21:30:12.889: INFO: Pod "liveness-47ff7b51-70a2-428c-a34a-60806dd311bc": Phase="Pending", Reason="", readiness=false. Elapsed: 10.3701ms
    Jan 19 21:30:14.892: INFO: Pod "liveness-47ff7b51-70a2-428c-a34a-60806dd311bc": Phase="Running", Reason="", readiness=true. Elapsed: 2.013663902s
    Jan 19 21:30:14.892: INFO: Pod "liveness-47ff7b51-70a2-428c-a34a-60806dd311bc" satisfied condition "not pending"
    Jan 19 21:30:14.892: INFO: Started pod liveness-47ff7b51-70a2-428c-a34a-60806dd311bc in namespace container-probe-8392
    STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 21:30:14.892
    Jan 19 21:30:14.895: INFO: Initial restart count of pod liveness-47ff7b51-70a2-428c-a34a-60806dd311bc is 0
    STEP: deleting the pod 01/19/23 21:34:15.338
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 21:34:15.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8392" for this suite. 01/19/23 21:34:15.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:34:15.366
Jan 19 21:34:15.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename gc 01/19/23 21:34:15.367
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:34:15.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:34:15.397
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/19/23 21:34:15.418
W0119 21:34:15.427061      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc 01/19/23 21:34:20.431
STEP: wait for the rc to be deleted 01/19/23 21:34:20.436
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/19/23 21:34:25.439
STEP: Gathering metrics 01/19/23 21:34:55.454
W0119 21:34:55.456919      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0119 21:34:55.456937      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 21:34:55.456: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 19 21:34:55.456: INFO: Deleting pod "simpletest.rc-2dc76" in namespace "gc-2213"
Jan 19 21:34:55.464: INFO: Deleting pod "simpletest.rc-2ldm5" in namespace "gc-2213"
Jan 19 21:34:55.474: INFO: Deleting pod "simpletest.rc-2stwg" in namespace "gc-2213"
Jan 19 21:34:55.484: INFO: Deleting pod "simpletest.rc-4d8v4" in namespace "gc-2213"
Jan 19 21:34:55.496: INFO: Deleting pod "simpletest.rc-4mhcc" in namespace "gc-2213"
Jan 19 21:34:55.511: INFO: Deleting pod "simpletest.rc-4msrs" in namespace "gc-2213"
Jan 19 21:34:55.523: INFO: Deleting pod "simpletest.rc-4tb6n" in namespace "gc-2213"
Jan 19 21:34:55.532: INFO: Deleting pod "simpletest.rc-5ml69" in namespace "gc-2213"
Jan 19 21:34:55.549: INFO: Deleting pod "simpletest.rc-5nqt7" in namespace "gc-2213"
Jan 19 21:34:55.565: INFO: Deleting pod "simpletest.rc-5q4jh" in namespace "gc-2213"
Jan 19 21:34:55.574: INFO: Deleting pod "simpletest.rc-6flz7" in namespace "gc-2213"
Jan 19 21:34:55.587: INFO: Deleting pod "simpletest.rc-6wdvc" in namespace "gc-2213"
Jan 19 21:34:55.597: INFO: Deleting pod "simpletest.rc-6wpzs" in namespace "gc-2213"
Jan 19 21:34:55.611: INFO: Deleting pod "simpletest.rc-76gn7" in namespace "gc-2213"
Jan 19 21:34:55.620: INFO: Deleting pod "simpletest.rc-77stg" in namespace "gc-2213"
Jan 19 21:34:55.635: INFO: Deleting pod "simpletest.rc-78k9v" in namespace "gc-2213"
Jan 19 21:34:55.644: INFO: Deleting pod "simpletest.rc-7d5pc" in namespace "gc-2213"
Jan 19 21:34:55.655: INFO: Deleting pod "simpletest.rc-7plwv" in namespace "gc-2213"
Jan 19 21:34:55.667: INFO: Deleting pod "simpletest.rc-7wbxg" in namespace "gc-2213"
Jan 19 21:34:55.685: INFO: Deleting pod "simpletest.rc-8bkkz" in namespace "gc-2213"
Jan 19 21:34:55.710: INFO: Deleting pod "simpletest.rc-8cvqk" in namespace "gc-2213"
Jan 19 21:34:55.724: INFO: Deleting pod "simpletest.rc-8xrhg" in namespace "gc-2213"
Jan 19 21:34:55.778: INFO: Deleting pod "simpletest.rc-96d28" in namespace "gc-2213"
Jan 19 21:34:55.789: INFO: Deleting pod "simpletest.rc-96s4l" in namespace "gc-2213"
Jan 19 21:34:55.822: INFO: Deleting pod "simpletest.rc-99mzh" in namespace "gc-2213"
Jan 19 21:34:55.837: INFO: Deleting pod "simpletest.rc-bc49c" in namespace "gc-2213"
Jan 19 21:34:55.850: INFO: Deleting pod "simpletest.rc-bf8j8" in namespace "gc-2213"
Jan 19 21:34:55.860: INFO: Deleting pod "simpletest.rc-bh47c" in namespace "gc-2213"
Jan 19 21:34:55.875: INFO: Deleting pod "simpletest.rc-bxj7p" in namespace "gc-2213"
Jan 19 21:34:55.887: INFO: Deleting pod "simpletest.rc-c2vw5" in namespace "gc-2213"
Jan 19 21:34:55.902: INFO: Deleting pod "simpletest.rc-c528f" in namespace "gc-2213"
Jan 19 21:34:55.910: INFO: Deleting pod "simpletest.rc-cfbw8" in namespace "gc-2213"
Jan 19 21:34:55.921: INFO: Deleting pod "simpletest.rc-ckswx" in namespace "gc-2213"
Jan 19 21:34:55.941: INFO: Deleting pod "simpletest.rc-cps8m" in namespace "gc-2213"
Jan 19 21:34:55.950: INFO: Deleting pod "simpletest.rc-crpmm" in namespace "gc-2213"
Jan 19 21:34:55.978: INFO: Deleting pod "simpletest.rc-cwwrn" in namespace "gc-2213"
Jan 19 21:34:55.990: INFO: Deleting pod "simpletest.rc-dd5k4" in namespace "gc-2213"
Jan 19 21:34:56.008: INFO: Deleting pod "simpletest.rc-dkpdn" in namespace "gc-2213"
Jan 19 21:34:56.017: INFO: Deleting pod "simpletest.rc-ds9xm" in namespace "gc-2213"
Jan 19 21:34:56.032: INFO: Deleting pod "simpletest.rc-g79hb" in namespace "gc-2213"
Jan 19 21:34:56.044: INFO: Deleting pod "simpletest.rc-gj6h8" in namespace "gc-2213"
Jan 19 21:34:56.061: INFO: Deleting pod "simpletest.rc-gnmgp" in namespace "gc-2213"
Jan 19 21:34:56.072: INFO: Deleting pod "simpletest.rc-h5b5s" in namespace "gc-2213"
Jan 19 21:34:56.087: INFO: Deleting pod "simpletest.rc-h8db4" in namespace "gc-2213"
Jan 19 21:34:56.101: INFO: Deleting pod "simpletest.rc-h8zcv" in namespace "gc-2213"
Jan 19 21:34:56.119: INFO: Deleting pod "simpletest.rc-hb2rc" in namespace "gc-2213"
Jan 19 21:34:56.140: INFO: Deleting pod "simpletest.rc-hcqt7" in namespace "gc-2213"
Jan 19 21:34:56.152: INFO: Deleting pod "simpletest.rc-hfvsp" in namespace "gc-2213"
Jan 19 21:34:56.167: INFO: Deleting pod "simpletest.rc-hx44h" in namespace "gc-2213"
Jan 19 21:34:56.180: INFO: Deleting pod "simpletest.rc-j4d8l" in namespace "gc-2213"
Jan 19 21:34:56.197: INFO: Deleting pod "simpletest.rc-j69cp" in namespace "gc-2213"
Jan 19 21:34:56.218: INFO: Deleting pod "simpletest.rc-krgcp" in namespace "gc-2213"
Jan 19 21:34:56.231: INFO: Deleting pod "simpletest.rc-l6d9z" in namespace "gc-2213"
Jan 19 21:34:56.252: INFO: Deleting pod "simpletest.rc-ljp87" in namespace "gc-2213"
Jan 19 21:34:56.275: INFO: Deleting pod "simpletest.rc-lkmzx" in namespace "gc-2213"
Jan 19 21:34:56.294: INFO: Deleting pod "simpletest.rc-lmhcm" in namespace "gc-2213"
Jan 19 21:34:56.317: INFO: Deleting pod "simpletest.rc-m2h69" in namespace "gc-2213"
Jan 19 21:34:56.335: INFO: Deleting pod "simpletest.rc-md6jg" in namespace "gc-2213"
Jan 19 21:34:56.350: INFO: Deleting pod "simpletest.rc-mh59b" in namespace "gc-2213"
Jan 19 21:34:56.366: INFO: Deleting pod "simpletest.rc-mnfdm" in namespace "gc-2213"
Jan 19 21:34:56.377: INFO: Deleting pod "simpletest.rc-n59rx" in namespace "gc-2213"
Jan 19 21:34:56.395: INFO: Deleting pod "simpletest.rc-nntkt" in namespace "gc-2213"
Jan 19 21:34:56.409: INFO: Deleting pod "simpletest.rc-nnzsx" in namespace "gc-2213"
Jan 19 21:34:56.428: INFO: Deleting pod "simpletest.rc-nwsrh" in namespace "gc-2213"
Jan 19 21:34:56.447: INFO: Deleting pod "simpletest.rc-nz4sw" in namespace "gc-2213"
Jan 19 21:34:56.461: INFO: Deleting pod "simpletest.rc-p55zs" in namespace "gc-2213"
Jan 19 21:34:56.487: INFO: Deleting pod "simpletest.rc-pvrlv" in namespace "gc-2213"
Jan 19 21:34:56.524: INFO: Deleting pod "simpletest.rc-qqpj9" in namespace "gc-2213"
Jan 19 21:34:56.538: INFO: Deleting pod "simpletest.rc-qrbgl" in namespace "gc-2213"
Jan 19 21:34:56.551: INFO: Deleting pod "simpletest.rc-qrhwh" in namespace "gc-2213"
Jan 19 21:34:56.564: INFO: Deleting pod "simpletest.rc-rp8x4" in namespace "gc-2213"
Jan 19 21:34:56.672: INFO: Deleting pod "simpletest.rc-rpc76" in namespace "gc-2213"
Jan 19 21:34:56.680: INFO: Deleting pod "simpletest.rc-rtmrq" in namespace "gc-2213"
Jan 19 21:34:56.718: INFO: Deleting pod "simpletest.rc-rvc8m" in namespace "gc-2213"
Jan 19 21:34:56.755: INFO: Deleting pod "simpletest.rc-rwxs7" in namespace "gc-2213"
Jan 19 21:34:56.817: INFO: Deleting pod "simpletest.rc-s4bcx" in namespace "gc-2213"
Jan 19 21:34:56.860: INFO: Deleting pod "simpletest.rc-smhr4" in namespace "gc-2213"
Jan 19 21:34:56.910: INFO: Deleting pod "simpletest.rc-sssxb" in namespace "gc-2213"
Jan 19 21:34:56.955: INFO: Deleting pod "simpletest.rc-svs49" in namespace "gc-2213"
Jan 19 21:34:57.008: INFO: Deleting pod "simpletest.rc-t2hr8" in namespace "gc-2213"
Jan 19 21:34:57.060: INFO: Deleting pod "simpletest.rc-t7xn8" in namespace "gc-2213"
Jan 19 21:34:57.109: INFO: Deleting pod "simpletest.rc-tgxf9" in namespace "gc-2213"
Jan 19 21:34:57.174: INFO: Deleting pod "simpletest.rc-trc9w" in namespace "gc-2213"
Jan 19 21:34:57.216: INFO: Deleting pod "simpletest.rc-vtxmg" in namespace "gc-2213"
Jan 19 21:34:57.255: INFO: Deleting pod "simpletest.rc-vzzhz" in namespace "gc-2213"
Jan 19 21:34:57.311: INFO: Deleting pod "simpletest.rc-wl95g" in namespace "gc-2213"
Jan 19 21:34:57.359: INFO: Deleting pod "simpletest.rc-wrn69" in namespace "gc-2213"
Jan 19 21:34:57.406: INFO: Deleting pod "simpletest.rc-x2w8m" in namespace "gc-2213"
Jan 19 21:34:57.466: INFO: Deleting pod "simpletest.rc-xbkx6" in namespace "gc-2213"
Jan 19 21:34:57.507: INFO: Deleting pod "simpletest.rc-xdqsd" in namespace "gc-2213"
Jan 19 21:34:57.553: INFO: Deleting pod "simpletest.rc-xf5jb" in namespace "gc-2213"
Jan 19 21:34:57.604: INFO: Deleting pod "simpletest.rc-xg772" in namespace "gc-2213"
Jan 19 21:34:57.665: INFO: Deleting pod "simpletest.rc-xsft7" in namespace "gc-2213"
Jan 19 21:34:57.704: INFO: Deleting pod "simpletest.rc-xvz82" in namespace "gc-2213"
Jan 19 21:34:57.807: INFO: Deleting pod "simpletest.rc-xxwt6" in namespace "gc-2213"
Jan 19 21:34:57.822: INFO: Deleting pod "simpletest.rc-z9s68" in namespace "gc-2213"
Jan 19 21:34:57.857: INFO: Deleting pod "simpletest.rc-zcmqs" in namespace "gc-2213"
Jan 19 21:34:57.906: INFO: Deleting pod "simpletest.rc-zdcdr" in namespace "gc-2213"
Jan 19 21:34:57.976: INFO: Deleting pod "simpletest.rc-zth9l" in namespace "gc-2213"
Jan 19 21:34:58.007: INFO: Deleting pod "simpletest.rc-zv9b5" in namespace "gc-2213"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 21:34:58.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2213" for this suite. 01/19/23 21:34:58.109
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":206,"skipped":3941,"failed":0}
------------------------------
• [SLOW TEST] [42.787 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:34:15.366
    Jan 19 21:34:15.367: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename gc 01/19/23 21:34:15.367
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:34:15.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:34:15.397
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/19/23 21:34:15.418
    W0119 21:34:15.427061      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: delete the rc 01/19/23 21:34:20.431
    STEP: wait for the rc to be deleted 01/19/23 21:34:20.436
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/19/23 21:34:25.439
    STEP: Gathering metrics 01/19/23 21:34:55.454
    W0119 21:34:55.456919      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0119 21:34:55.456937      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 19 21:34:55.456: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 19 21:34:55.456: INFO: Deleting pod "simpletest.rc-2dc76" in namespace "gc-2213"
    Jan 19 21:34:55.464: INFO: Deleting pod "simpletest.rc-2ldm5" in namespace "gc-2213"
    Jan 19 21:34:55.474: INFO: Deleting pod "simpletest.rc-2stwg" in namespace "gc-2213"
    Jan 19 21:34:55.484: INFO: Deleting pod "simpletest.rc-4d8v4" in namespace "gc-2213"
    Jan 19 21:34:55.496: INFO: Deleting pod "simpletest.rc-4mhcc" in namespace "gc-2213"
    Jan 19 21:34:55.511: INFO: Deleting pod "simpletest.rc-4msrs" in namespace "gc-2213"
    Jan 19 21:34:55.523: INFO: Deleting pod "simpletest.rc-4tb6n" in namespace "gc-2213"
    Jan 19 21:34:55.532: INFO: Deleting pod "simpletest.rc-5ml69" in namespace "gc-2213"
    Jan 19 21:34:55.549: INFO: Deleting pod "simpletest.rc-5nqt7" in namespace "gc-2213"
    Jan 19 21:34:55.565: INFO: Deleting pod "simpletest.rc-5q4jh" in namespace "gc-2213"
    Jan 19 21:34:55.574: INFO: Deleting pod "simpletest.rc-6flz7" in namespace "gc-2213"
    Jan 19 21:34:55.587: INFO: Deleting pod "simpletest.rc-6wdvc" in namespace "gc-2213"
    Jan 19 21:34:55.597: INFO: Deleting pod "simpletest.rc-6wpzs" in namespace "gc-2213"
    Jan 19 21:34:55.611: INFO: Deleting pod "simpletest.rc-76gn7" in namespace "gc-2213"
    Jan 19 21:34:55.620: INFO: Deleting pod "simpletest.rc-77stg" in namespace "gc-2213"
    Jan 19 21:34:55.635: INFO: Deleting pod "simpletest.rc-78k9v" in namespace "gc-2213"
    Jan 19 21:34:55.644: INFO: Deleting pod "simpletest.rc-7d5pc" in namespace "gc-2213"
    Jan 19 21:34:55.655: INFO: Deleting pod "simpletest.rc-7plwv" in namespace "gc-2213"
    Jan 19 21:34:55.667: INFO: Deleting pod "simpletest.rc-7wbxg" in namespace "gc-2213"
    Jan 19 21:34:55.685: INFO: Deleting pod "simpletest.rc-8bkkz" in namespace "gc-2213"
    Jan 19 21:34:55.710: INFO: Deleting pod "simpletest.rc-8cvqk" in namespace "gc-2213"
    Jan 19 21:34:55.724: INFO: Deleting pod "simpletest.rc-8xrhg" in namespace "gc-2213"
    Jan 19 21:34:55.778: INFO: Deleting pod "simpletest.rc-96d28" in namespace "gc-2213"
    Jan 19 21:34:55.789: INFO: Deleting pod "simpletest.rc-96s4l" in namespace "gc-2213"
    Jan 19 21:34:55.822: INFO: Deleting pod "simpletest.rc-99mzh" in namespace "gc-2213"
    Jan 19 21:34:55.837: INFO: Deleting pod "simpletest.rc-bc49c" in namespace "gc-2213"
    Jan 19 21:34:55.850: INFO: Deleting pod "simpletest.rc-bf8j8" in namespace "gc-2213"
    Jan 19 21:34:55.860: INFO: Deleting pod "simpletest.rc-bh47c" in namespace "gc-2213"
    Jan 19 21:34:55.875: INFO: Deleting pod "simpletest.rc-bxj7p" in namespace "gc-2213"
    Jan 19 21:34:55.887: INFO: Deleting pod "simpletest.rc-c2vw5" in namespace "gc-2213"
    Jan 19 21:34:55.902: INFO: Deleting pod "simpletest.rc-c528f" in namespace "gc-2213"
    Jan 19 21:34:55.910: INFO: Deleting pod "simpletest.rc-cfbw8" in namespace "gc-2213"
    Jan 19 21:34:55.921: INFO: Deleting pod "simpletest.rc-ckswx" in namespace "gc-2213"
    Jan 19 21:34:55.941: INFO: Deleting pod "simpletest.rc-cps8m" in namespace "gc-2213"
    Jan 19 21:34:55.950: INFO: Deleting pod "simpletest.rc-crpmm" in namespace "gc-2213"
    Jan 19 21:34:55.978: INFO: Deleting pod "simpletest.rc-cwwrn" in namespace "gc-2213"
    Jan 19 21:34:55.990: INFO: Deleting pod "simpletest.rc-dd5k4" in namespace "gc-2213"
    Jan 19 21:34:56.008: INFO: Deleting pod "simpletest.rc-dkpdn" in namespace "gc-2213"
    Jan 19 21:34:56.017: INFO: Deleting pod "simpletest.rc-ds9xm" in namespace "gc-2213"
    Jan 19 21:34:56.032: INFO: Deleting pod "simpletest.rc-g79hb" in namespace "gc-2213"
    Jan 19 21:34:56.044: INFO: Deleting pod "simpletest.rc-gj6h8" in namespace "gc-2213"
    Jan 19 21:34:56.061: INFO: Deleting pod "simpletest.rc-gnmgp" in namespace "gc-2213"
    Jan 19 21:34:56.072: INFO: Deleting pod "simpletest.rc-h5b5s" in namespace "gc-2213"
    Jan 19 21:34:56.087: INFO: Deleting pod "simpletest.rc-h8db4" in namespace "gc-2213"
    Jan 19 21:34:56.101: INFO: Deleting pod "simpletest.rc-h8zcv" in namespace "gc-2213"
    Jan 19 21:34:56.119: INFO: Deleting pod "simpletest.rc-hb2rc" in namespace "gc-2213"
    Jan 19 21:34:56.140: INFO: Deleting pod "simpletest.rc-hcqt7" in namespace "gc-2213"
    Jan 19 21:34:56.152: INFO: Deleting pod "simpletest.rc-hfvsp" in namespace "gc-2213"
    Jan 19 21:34:56.167: INFO: Deleting pod "simpletest.rc-hx44h" in namespace "gc-2213"
    Jan 19 21:34:56.180: INFO: Deleting pod "simpletest.rc-j4d8l" in namespace "gc-2213"
    Jan 19 21:34:56.197: INFO: Deleting pod "simpletest.rc-j69cp" in namespace "gc-2213"
    Jan 19 21:34:56.218: INFO: Deleting pod "simpletest.rc-krgcp" in namespace "gc-2213"
    Jan 19 21:34:56.231: INFO: Deleting pod "simpletest.rc-l6d9z" in namespace "gc-2213"
    Jan 19 21:34:56.252: INFO: Deleting pod "simpletest.rc-ljp87" in namespace "gc-2213"
    Jan 19 21:34:56.275: INFO: Deleting pod "simpletest.rc-lkmzx" in namespace "gc-2213"
    Jan 19 21:34:56.294: INFO: Deleting pod "simpletest.rc-lmhcm" in namespace "gc-2213"
    Jan 19 21:34:56.317: INFO: Deleting pod "simpletest.rc-m2h69" in namespace "gc-2213"
    Jan 19 21:34:56.335: INFO: Deleting pod "simpletest.rc-md6jg" in namespace "gc-2213"
    Jan 19 21:34:56.350: INFO: Deleting pod "simpletest.rc-mh59b" in namespace "gc-2213"
    Jan 19 21:34:56.366: INFO: Deleting pod "simpletest.rc-mnfdm" in namespace "gc-2213"
    Jan 19 21:34:56.377: INFO: Deleting pod "simpletest.rc-n59rx" in namespace "gc-2213"
    Jan 19 21:34:56.395: INFO: Deleting pod "simpletest.rc-nntkt" in namespace "gc-2213"
    Jan 19 21:34:56.409: INFO: Deleting pod "simpletest.rc-nnzsx" in namespace "gc-2213"
    Jan 19 21:34:56.428: INFO: Deleting pod "simpletest.rc-nwsrh" in namespace "gc-2213"
    Jan 19 21:34:56.447: INFO: Deleting pod "simpletest.rc-nz4sw" in namespace "gc-2213"
    Jan 19 21:34:56.461: INFO: Deleting pod "simpletest.rc-p55zs" in namespace "gc-2213"
    Jan 19 21:34:56.487: INFO: Deleting pod "simpletest.rc-pvrlv" in namespace "gc-2213"
    Jan 19 21:34:56.524: INFO: Deleting pod "simpletest.rc-qqpj9" in namespace "gc-2213"
    Jan 19 21:34:56.538: INFO: Deleting pod "simpletest.rc-qrbgl" in namespace "gc-2213"
    Jan 19 21:34:56.551: INFO: Deleting pod "simpletest.rc-qrhwh" in namespace "gc-2213"
    Jan 19 21:34:56.564: INFO: Deleting pod "simpletest.rc-rp8x4" in namespace "gc-2213"
    Jan 19 21:34:56.672: INFO: Deleting pod "simpletest.rc-rpc76" in namespace "gc-2213"
    Jan 19 21:34:56.680: INFO: Deleting pod "simpletest.rc-rtmrq" in namespace "gc-2213"
    Jan 19 21:34:56.718: INFO: Deleting pod "simpletest.rc-rvc8m" in namespace "gc-2213"
    Jan 19 21:34:56.755: INFO: Deleting pod "simpletest.rc-rwxs7" in namespace "gc-2213"
    Jan 19 21:34:56.817: INFO: Deleting pod "simpletest.rc-s4bcx" in namespace "gc-2213"
    Jan 19 21:34:56.860: INFO: Deleting pod "simpletest.rc-smhr4" in namespace "gc-2213"
    Jan 19 21:34:56.910: INFO: Deleting pod "simpletest.rc-sssxb" in namespace "gc-2213"
    Jan 19 21:34:56.955: INFO: Deleting pod "simpletest.rc-svs49" in namespace "gc-2213"
    Jan 19 21:34:57.008: INFO: Deleting pod "simpletest.rc-t2hr8" in namespace "gc-2213"
    Jan 19 21:34:57.060: INFO: Deleting pod "simpletest.rc-t7xn8" in namespace "gc-2213"
    Jan 19 21:34:57.109: INFO: Deleting pod "simpletest.rc-tgxf9" in namespace "gc-2213"
    Jan 19 21:34:57.174: INFO: Deleting pod "simpletest.rc-trc9w" in namespace "gc-2213"
    Jan 19 21:34:57.216: INFO: Deleting pod "simpletest.rc-vtxmg" in namespace "gc-2213"
    Jan 19 21:34:57.255: INFO: Deleting pod "simpletest.rc-vzzhz" in namespace "gc-2213"
    Jan 19 21:34:57.311: INFO: Deleting pod "simpletest.rc-wl95g" in namespace "gc-2213"
    Jan 19 21:34:57.359: INFO: Deleting pod "simpletest.rc-wrn69" in namespace "gc-2213"
    Jan 19 21:34:57.406: INFO: Deleting pod "simpletest.rc-x2w8m" in namespace "gc-2213"
    Jan 19 21:34:57.466: INFO: Deleting pod "simpletest.rc-xbkx6" in namespace "gc-2213"
    Jan 19 21:34:57.507: INFO: Deleting pod "simpletest.rc-xdqsd" in namespace "gc-2213"
    Jan 19 21:34:57.553: INFO: Deleting pod "simpletest.rc-xf5jb" in namespace "gc-2213"
    Jan 19 21:34:57.604: INFO: Deleting pod "simpletest.rc-xg772" in namespace "gc-2213"
    Jan 19 21:34:57.665: INFO: Deleting pod "simpletest.rc-xsft7" in namespace "gc-2213"
    Jan 19 21:34:57.704: INFO: Deleting pod "simpletest.rc-xvz82" in namespace "gc-2213"
    Jan 19 21:34:57.807: INFO: Deleting pod "simpletest.rc-xxwt6" in namespace "gc-2213"
    Jan 19 21:34:57.822: INFO: Deleting pod "simpletest.rc-z9s68" in namespace "gc-2213"
    Jan 19 21:34:57.857: INFO: Deleting pod "simpletest.rc-zcmqs" in namespace "gc-2213"
    Jan 19 21:34:57.906: INFO: Deleting pod "simpletest.rc-zdcdr" in namespace "gc-2213"
    Jan 19 21:34:57.976: INFO: Deleting pod "simpletest.rc-zth9l" in namespace "gc-2213"
    Jan 19 21:34:58.007: INFO: Deleting pod "simpletest.rc-zv9b5" in namespace "gc-2213"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 21:34:58.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2213" for this suite. 01/19/23 21:34:58.109
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:34:58.154
Jan 19 21:34:58.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-runtime 01/19/23 21:34:58.155
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:34:58.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:34:58.199
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 01/19/23 21:34:58.211
STEP: wait for the container to reach Succeeded 01/19/23 21:34:58.295
STEP: get the container status 01/19/23 21:35:03.334
STEP: the container should be terminated 01/19/23 21:35:03.339
STEP: the termination message should be set 01/19/23 21:35:03.339
Jan 19 21:35:03.339: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/19/23 21:35:03.339
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 19 21:35:03.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9784" for this suite. 01/19/23 21:35:03.356
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":207,"skipped":3941,"failed":0}
------------------------------
• [SLOW TEST] [5.209 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:34:58.154
    Jan 19 21:34:58.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-runtime 01/19/23 21:34:58.155
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:34:58.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:34:58.199
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 01/19/23 21:34:58.211
    STEP: wait for the container to reach Succeeded 01/19/23 21:34:58.295
    STEP: get the container status 01/19/23 21:35:03.334
    STEP: the container should be terminated 01/19/23 21:35:03.339
    STEP: the termination message should be set 01/19/23 21:35:03.339
    Jan 19 21:35:03.339: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/19/23 21:35:03.339
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 19 21:35:03.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9784" for this suite. 01/19/23 21:35:03.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:35:03.364
Jan 19 21:35:03.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 21:35:03.365
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:35:03.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:35:03.39
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/19/23 21:35:03.405
Jan 19 21:35:03.457: INFO: Waiting up to 5m0s for pod "pod-04103d34-3a2a-4b5f-8d83-e93e97128006" in namespace "emptydir-5841" to be "Succeeded or Failed"
Jan 19 21:35:03.463: INFO: Pod "pod-04103d34-3a2a-4b5f-8d83-e93e97128006": Phase="Pending", Reason="", readiness=false. Elapsed: 5.900152ms
Jan 19 21:35:05.467: INFO: Pod "pod-04103d34-3a2a-4b5f-8d83-e93e97128006": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009578882s
Jan 19 21:35:07.467: INFO: Pod "pod-04103d34-3a2a-4b5f-8d83-e93e97128006": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009801931s
STEP: Saw pod success 01/19/23 21:35:07.467
Jan 19 21:35:07.467: INFO: Pod "pod-04103d34-3a2a-4b5f-8d83-e93e97128006" satisfied condition "Succeeded or Failed"
Jan 19 21:35:07.469: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-04103d34-3a2a-4b5f-8d83-e93e97128006 container test-container: <nil>
STEP: delete the pod 01/19/23 21:35:07.486
Jan 19 21:35:07.502: INFO: Waiting for pod pod-04103d34-3a2a-4b5f-8d83-e93e97128006 to disappear
Jan 19 21:35:07.506: INFO: Pod pod-04103d34-3a2a-4b5f-8d83-e93e97128006 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 21:35:07.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5841" for this suite. 01/19/23 21:35:07.512
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":208,"skipped":3960,"failed":0}
------------------------------
• [4.153 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:35:03.364
    Jan 19 21:35:03.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 21:35:03.365
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:35:03.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:35:03.39
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/19/23 21:35:03.405
    Jan 19 21:35:03.457: INFO: Waiting up to 5m0s for pod "pod-04103d34-3a2a-4b5f-8d83-e93e97128006" in namespace "emptydir-5841" to be "Succeeded or Failed"
    Jan 19 21:35:03.463: INFO: Pod "pod-04103d34-3a2a-4b5f-8d83-e93e97128006": Phase="Pending", Reason="", readiness=false. Elapsed: 5.900152ms
    Jan 19 21:35:05.467: INFO: Pod "pod-04103d34-3a2a-4b5f-8d83-e93e97128006": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009578882s
    Jan 19 21:35:07.467: INFO: Pod "pod-04103d34-3a2a-4b5f-8d83-e93e97128006": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009801931s
    STEP: Saw pod success 01/19/23 21:35:07.467
    Jan 19 21:35:07.467: INFO: Pod "pod-04103d34-3a2a-4b5f-8d83-e93e97128006" satisfied condition "Succeeded or Failed"
    Jan 19 21:35:07.469: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-04103d34-3a2a-4b5f-8d83-e93e97128006 container test-container: <nil>
    STEP: delete the pod 01/19/23 21:35:07.486
    Jan 19 21:35:07.502: INFO: Waiting for pod pod-04103d34-3a2a-4b5f-8d83-e93e97128006 to disappear
    Jan 19 21:35:07.506: INFO: Pod pod-04103d34-3a2a-4b5f-8d83-e93e97128006 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:35:07.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5841" for this suite. 01/19/23 21:35:07.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:35:07.518
Jan 19 21:35:07.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:35:07.519
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:35:07.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:35:07.549
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/19/23 21:35:07.551
Jan 19 21:35:07.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:35:17.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:35:50.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1513" for this suite. 01/19/23 21:35:50.81
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":209,"skipped":3966,"failed":0}
------------------------------
• [SLOW TEST] [43.298 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:35:07.518
    Jan 19 21:35:07.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:35:07.519
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:35:07.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:35:07.549
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/19/23 21:35:07.551
    Jan 19 21:35:07.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:35:17.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:35:50.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1513" for this suite. 01/19/23 21:35:50.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:35:50.817
Jan 19 21:35:50.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pods 01/19/23 21:35:50.817
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:35:50.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:35:50.83
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jan 19 21:35:51.871: INFO: Waiting up to 5m0s for pod "server-envvars-33d5eae2-59f8-4515-88b0-2c22f6b141d3" in namespace "pods-6954" to be "running and ready"
Jan 19 21:35:51.873: INFO: Pod "server-envvars-33d5eae2-59f8-4515-88b0-2c22f6b141d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.336497ms
Jan 19 21:35:51.873: INFO: The phase of Pod server-envvars-33d5eae2-59f8-4515-88b0-2c22f6b141d3 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:35:53.877: INFO: Pod "server-envvars-33d5eae2-59f8-4515-88b0-2c22f6b141d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006296647s
Jan 19 21:35:53.877: INFO: The phase of Pod server-envvars-33d5eae2-59f8-4515-88b0-2c22f6b141d3 is Running (Ready = true)
Jan 19 21:35:53.877: INFO: Pod "server-envvars-33d5eae2-59f8-4515-88b0-2c22f6b141d3" satisfied condition "running and ready"
Jan 19 21:35:53.906: INFO: Waiting up to 5m0s for pod "client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114" in namespace "pods-6954" to be "Succeeded or Failed"
Jan 19 21:35:53.912: INFO: Pod "client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114": Phase="Pending", Reason="", readiness=false. Elapsed: 5.446576ms
Jan 19 21:35:55.914: INFO: Pod "client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00809098s
Jan 19 21:35:57.915: INFO: Pod "client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008813693s
STEP: Saw pod success 01/19/23 21:35:57.915
Jan 19 21:35:57.915: INFO: Pod "client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114" satisfied condition "Succeeded or Failed"
Jan 19 21:35:57.917: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114 container env3cont: <nil>
STEP: delete the pod 01/19/23 21:35:57.927
Jan 19 21:35:57.937: INFO: Waiting for pod client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114 to disappear
Jan 19 21:35:57.939: INFO: Pod client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 21:35:57.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6954" for this suite. 01/19/23 21:35:57.943
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":210,"skipped":3973,"failed":0}
------------------------------
• [SLOW TEST] [7.132 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:35:50.817
    Jan 19 21:35:50.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pods 01/19/23 21:35:50.817
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:35:50.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:35:50.83
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jan 19 21:35:51.871: INFO: Waiting up to 5m0s for pod "server-envvars-33d5eae2-59f8-4515-88b0-2c22f6b141d3" in namespace "pods-6954" to be "running and ready"
    Jan 19 21:35:51.873: INFO: Pod "server-envvars-33d5eae2-59f8-4515-88b0-2c22f6b141d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.336497ms
    Jan 19 21:35:51.873: INFO: The phase of Pod server-envvars-33d5eae2-59f8-4515-88b0-2c22f6b141d3 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:35:53.877: INFO: Pod "server-envvars-33d5eae2-59f8-4515-88b0-2c22f6b141d3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006296647s
    Jan 19 21:35:53.877: INFO: The phase of Pod server-envvars-33d5eae2-59f8-4515-88b0-2c22f6b141d3 is Running (Ready = true)
    Jan 19 21:35:53.877: INFO: Pod "server-envvars-33d5eae2-59f8-4515-88b0-2c22f6b141d3" satisfied condition "running and ready"
    Jan 19 21:35:53.906: INFO: Waiting up to 5m0s for pod "client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114" in namespace "pods-6954" to be "Succeeded or Failed"
    Jan 19 21:35:53.912: INFO: Pod "client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114": Phase="Pending", Reason="", readiness=false. Elapsed: 5.446576ms
    Jan 19 21:35:55.914: INFO: Pod "client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00809098s
    Jan 19 21:35:57.915: INFO: Pod "client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008813693s
    STEP: Saw pod success 01/19/23 21:35:57.915
    Jan 19 21:35:57.915: INFO: Pod "client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114" satisfied condition "Succeeded or Failed"
    Jan 19 21:35:57.917: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114 container env3cont: <nil>
    STEP: delete the pod 01/19/23 21:35:57.927
    Jan 19 21:35:57.937: INFO: Waiting for pod client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114 to disappear
    Jan 19 21:35:57.939: INFO: Pod client-envvars-9a883c76-83ed-451d-8bff-80bbd5885114 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 21:35:57.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6954" for this suite. 01/19/23 21:35:57.943
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:35:57.949
Jan 19 21:35:57.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:35:57.949
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:35:57.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:35:57.968
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-050bd44c-3775-43d9-932d-d35231891e61 01/19/23 21:35:57.971
STEP: Creating a pod to test consume configMaps 01/19/23 21:35:57.979
W0119 21:35:58.011600      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:35:58.011: INFO: Waiting up to 5m0s for pod "pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9" in namespace "configmap-2023" to be "Succeeded or Failed"
Jan 19 21:35:58.016: INFO: Pod "pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.025293ms
Jan 19 21:36:00.020: INFO: Pod "pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008797052s
Jan 19 21:36:02.021: INFO: Pod "pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010165232s
STEP: Saw pod success 01/19/23 21:36:02.021
Jan 19 21:36:02.021: INFO: Pod "pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9" satisfied condition "Succeeded or Failed"
Jan 19 21:36:02.025: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:36:02.036
Jan 19 21:36:02.047: INFO: Waiting for pod pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9 to disappear
Jan 19 21:36:02.050: INFO: Pod pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:36:02.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2023" for this suite. 01/19/23 21:36:02.054
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":211,"skipped":3974,"failed":0}
------------------------------
• [4.110 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:35:57.949
    Jan 19 21:35:57.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:35:57.949
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:35:57.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:35:57.968
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-050bd44c-3775-43d9-932d-d35231891e61 01/19/23 21:35:57.971
    STEP: Creating a pod to test consume configMaps 01/19/23 21:35:57.979
    W0119 21:35:58.011600      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:35:58.011: INFO: Waiting up to 5m0s for pod "pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9" in namespace "configmap-2023" to be "Succeeded or Failed"
    Jan 19 21:35:58.016: INFO: Pod "pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.025293ms
    Jan 19 21:36:00.020: INFO: Pod "pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008797052s
    Jan 19 21:36:02.021: INFO: Pod "pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010165232s
    STEP: Saw pod success 01/19/23 21:36:02.021
    Jan 19 21:36:02.021: INFO: Pod "pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9" satisfied condition "Succeeded or Failed"
    Jan 19 21:36:02.025: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:36:02.036
    Jan 19 21:36:02.047: INFO: Waiting for pod pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9 to disappear
    Jan 19 21:36:02.050: INFO: Pod pod-configmaps-7e31b31c-28e4-4339-8a7c-a5cfa838c9c9 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:36:02.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2023" for this suite. 01/19/23 21:36:02.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:36:02.06
Jan 19 21:36:02.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename security-context 01/19/23 21:36:02.06
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:36:02.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:36:02.078
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/19/23 21:36:02.082
Jan 19 21:36:02.109: INFO: Waiting up to 5m0s for pod "security-context-e4cee34c-af81-4992-9daa-51ef02568957" in namespace "security-context-7501" to be "Succeeded or Failed"
Jan 19 21:36:02.118: INFO: Pod "security-context-e4cee34c-af81-4992-9daa-51ef02568957": Phase="Pending", Reason="", readiness=false. Elapsed: 8.36209ms
Jan 19 21:36:04.121: INFO: Pod "security-context-e4cee34c-af81-4992-9daa-51ef02568957": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011743754s
Jan 19 21:36:06.120: INFO: Pod "security-context-e4cee34c-af81-4992-9daa-51ef02568957": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010966795s
STEP: Saw pod success 01/19/23 21:36:06.12
Jan 19 21:36:06.120: INFO: Pod "security-context-e4cee34c-af81-4992-9daa-51ef02568957" satisfied condition "Succeeded or Failed"
Jan 19 21:36:06.124: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod security-context-e4cee34c-af81-4992-9daa-51ef02568957 container test-container: <nil>
STEP: delete the pod 01/19/23 21:36:06.129
Jan 19 21:36:06.138: INFO: Waiting for pod security-context-e4cee34c-af81-4992-9daa-51ef02568957 to disappear
Jan 19 21:36:06.140: INFO: Pod security-context-e4cee34c-af81-4992-9daa-51ef02568957 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 19 21:36:06.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-7501" for this suite. 01/19/23 21:36:06.144
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":212,"skipped":4009,"failed":0}
------------------------------
• [4.089 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:36:02.06
    Jan 19 21:36:02.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename security-context 01/19/23 21:36:02.06
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:36:02.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:36:02.078
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/19/23 21:36:02.082
    Jan 19 21:36:02.109: INFO: Waiting up to 5m0s for pod "security-context-e4cee34c-af81-4992-9daa-51ef02568957" in namespace "security-context-7501" to be "Succeeded or Failed"
    Jan 19 21:36:02.118: INFO: Pod "security-context-e4cee34c-af81-4992-9daa-51ef02568957": Phase="Pending", Reason="", readiness=false. Elapsed: 8.36209ms
    Jan 19 21:36:04.121: INFO: Pod "security-context-e4cee34c-af81-4992-9daa-51ef02568957": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011743754s
    Jan 19 21:36:06.120: INFO: Pod "security-context-e4cee34c-af81-4992-9daa-51ef02568957": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010966795s
    STEP: Saw pod success 01/19/23 21:36:06.12
    Jan 19 21:36:06.120: INFO: Pod "security-context-e4cee34c-af81-4992-9daa-51ef02568957" satisfied condition "Succeeded or Failed"
    Jan 19 21:36:06.124: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod security-context-e4cee34c-af81-4992-9daa-51ef02568957 container test-container: <nil>
    STEP: delete the pod 01/19/23 21:36:06.129
    Jan 19 21:36:06.138: INFO: Waiting for pod security-context-e4cee34c-af81-4992-9daa-51ef02568957 to disappear
    Jan 19 21:36:06.140: INFO: Pod security-context-e4cee34c-af81-4992-9daa-51ef02568957 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 19 21:36:06.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-7501" for this suite. 01/19/23 21:36:06.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:36:06.15
Jan 19 21:36:06.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename replication-controller 01/19/23 21:36:06.151
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:36:06.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:36:06.165
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 01/19/23 21:36:06.172
W0119 21:36:06.184006      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "rc-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "rc-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "rc-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "rc-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for RC to be added 01/19/23 21:36:06.184
STEP: waiting for available Replicas 01/19/23 21:36:06.184
STEP: patching ReplicationController 01/19/23 21:36:07.65
STEP: waiting for RC to be modified 01/19/23 21:36:07.66
STEP: patching ReplicationController status 01/19/23 21:36:07.66
STEP: waiting for RC to be modified 01/19/23 21:36:07.665
STEP: waiting for available Replicas 01/19/23 21:36:07.665
STEP: fetching ReplicationController status 01/19/23 21:36:07.671
STEP: patching ReplicationController scale 01/19/23 21:36:07.675
STEP: waiting for RC to be modified 01/19/23 21:36:07.68
STEP: waiting for ReplicationController's scale to be the max amount 01/19/23 21:36:07.68
STEP: fetching ReplicationController; ensuring that it's patched 01/19/23 21:36:09.319
STEP: updating ReplicationController status 01/19/23 21:36:09.325
STEP: waiting for RC to be modified 01/19/23 21:36:09.33
STEP: listing all ReplicationControllers 01/19/23 21:36:09.33
STEP: checking that ReplicationController has expected values 01/19/23 21:36:09.332
STEP: deleting ReplicationControllers by collection 01/19/23 21:36:09.333
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/19/23 21:36:09.34
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 19 21:36:09.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1415" for this suite. 01/19/23 21:36:09.392
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":213,"skipped":4043,"failed":0}
------------------------------
• [3.248 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:36:06.15
    Jan 19 21:36:06.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename replication-controller 01/19/23 21:36:06.151
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:36:06.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:36:06.165
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 01/19/23 21:36:06.172
    W0119 21:36:06.184006      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "rc-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "rc-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "rc-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "rc-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: waiting for RC to be added 01/19/23 21:36:06.184
    STEP: waiting for available Replicas 01/19/23 21:36:06.184
    STEP: patching ReplicationController 01/19/23 21:36:07.65
    STEP: waiting for RC to be modified 01/19/23 21:36:07.66
    STEP: patching ReplicationController status 01/19/23 21:36:07.66
    STEP: waiting for RC to be modified 01/19/23 21:36:07.665
    STEP: waiting for available Replicas 01/19/23 21:36:07.665
    STEP: fetching ReplicationController status 01/19/23 21:36:07.671
    STEP: patching ReplicationController scale 01/19/23 21:36:07.675
    STEP: waiting for RC to be modified 01/19/23 21:36:07.68
    STEP: waiting for ReplicationController's scale to be the max amount 01/19/23 21:36:07.68
    STEP: fetching ReplicationController; ensuring that it's patched 01/19/23 21:36:09.319
    STEP: updating ReplicationController status 01/19/23 21:36:09.325
    STEP: waiting for RC to be modified 01/19/23 21:36:09.33
    STEP: listing all ReplicationControllers 01/19/23 21:36:09.33
    STEP: checking that ReplicationController has expected values 01/19/23 21:36:09.332
    STEP: deleting ReplicationControllers by collection 01/19/23 21:36:09.333
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/19/23 21:36:09.34
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 19 21:36:09.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-1415" for this suite. 01/19/23 21:36:09.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:36:09.399
Jan 19 21:36:09.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:36:09.399
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:36:09.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:36:09.417
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/19/23 21:36:09.439
Jan 19 21:36:09.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:36:17.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:36:50.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5625" for this suite. 01/19/23 21:36:50.408
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":214,"skipped":4054,"failed":0}
------------------------------
• [SLOW TEST] [41.015 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:36:09.399
    Jan 19 21:36:09.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:36:09.399
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:36:09.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:36:09.417
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/19/23 21:36:09.439
    Jan 19 21:36:09.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:36:17.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:36:50.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5625" for this suite. 01/19/23 21:36:50.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:36:50.414
Jan 19 21:36:50.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename subpath 01/19/23 21:36:50.415
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:36:50.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:36:50.451
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/19/23 21:36:50.454
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-5r5z 01/19/23 21:36:50.489
STEP: Creating a pod to test atomic-volume-subpath 01/19/23 21:36:50.489
Jan 19 21:36:50.536: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5r5z" in namespace "subpath-3766" to be "Succeeded or Failed"
Jan 19 21:36:50.555: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Pending", Reason="", readiness=false. Elapsed: 18.36166ms
Jan 19 21:36:52.559: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 2.022441047s
Jan 19 21:36:54.559: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 4.023161752s
Jan 19 21:36:56.562: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 6.025480488s
Jan 19 21:36:58.560: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 8.023558956s
Jan 19 21:37:00.557: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 10.021202127s
Jan 19 21:37:02.558: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 12.022157169s
Jan 19 21:37:04.559: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 14.023131577s
Jan 19 21:37:06.558: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 16.021806899s
Jan 19 21:37:08.558: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 18.022189362s
Jan 19 21:37:10.559: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 20.022874708s
Jan 19 21:37:12.560: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=false. Elapsed: 22.023232309s
Jan 19 21:37:14.560: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.023608733s
STEP: Saw pod success 01/19/23 21:37:14.56
Jan 19 21:37:14.560: INFO: Pod "pod-subpath-test-projected-5r5z" satisfied condition "Succeeded or Failed"
Jan 19 21:37:14.563: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-subpath-test-projected-5r5z container test-container-subpath-projected-5r5z: <nil>
STEP: delete the pod 01/19/23 21:37:14.572
Jan 19 21:37:14.584: INFO: Waiting for pod pod-subpath-test-projected-5r5z to disappear
Jan 19 21:37:14.587: INFO: Pod pod-subpath-test-projected-5r5z no longer exists
STEP: Deleting pod pod-subpath-test-projected-5r5z 01/19/23 21:37:14.587
Jan 19 21:37:14.587: INFO: Deleting pod "pod-subpath-test-projected-5r5z" in namespace "subpath-3766"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jan 19 21:37:14.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3766" for this suite. 01/19/23 21:37:14.593
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":215,"skipped":4062,"failed":0}
------------------------------
• [SLOW TEST] [24.185 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:36:50.414
    Jan 19 21:36:50.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename subpath 01/19/23 21:36:50.415
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:36:50.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:36:50.451
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/19/23 21:36:50.454
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-5r5z 01/19/23 21:36:50.489
    STEP: Creating a pod to test atomic-volume-subpath 01/19/23 21:36:50.489
    Jan 19 21:36:50.536: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-5r5z" in namespace "subpath-3766" to be "Succeeded or Failed"
    Jan 19 21:36:50.555: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Pending", Reason="", readiness=false. Elapsed: 18.36166ms
    Jan 19 21:36:52.559: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 2.022441047s
    Jan 19 21:36:54.559: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 4.023161752s
    Jan 19 21:36:56.562: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 6.025480488s
    Jan 19 21:36:58.560: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 8.023558956s
    Jan 19 21:37:00.557: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 10.021202127s
    Jan 19 21:37:02.558: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 12.022157169s
    Jan 19 21:37:04.559: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 14.023131577s
    Jan 19 21:37:06.558: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 16.021806899s
    Jan 19 21:37:08.558: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 18.022189362s
    Jan 19 21:37:10.559: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=true. Elapsed: 20.022874708s
    Jan 19 21:37:12.560: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Running", Reason="", readiness=false. Elapsed: 22.023232309s
    Jan 19 21:37:14.560: INFO: Pod "pod-subpath-test-projected-5r5z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.023608733s
    STEP: Saw pod success 01/19/23 21:37:14.56
    Jan 19 21:37:14.560: INFO: Pod "pod-subpath-test-projected-5r5z" satisfied condition "Succeeded or Failed"
    Jan 19 21:37:14.563: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-subpath-test-projected-5r5z container test-container-subpath-projected-5r5z: <nil>
    STEP: delete the pod 01/19/23 21:37:14.572
    Jan 19 21:37:14.584: INFO: Waiting for pod pod-subpath-test-projected-5r5z to disappear
    Jan 19 21:37:14.587: INFO: Pod pod-subpath-test-projected-5r5z no longer exists
    STEP: Deleting pod pod-subpath-test-projected-5r5z 01/19/23 21:37:14.587
    Jan 19 21:37:14.587: INFO: Deleting pod "pod-subpath-test-projected-5r5z" in namespace "subpath-3766"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jan 19 21:37:14.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3766" for this suite. 01/19/23 21:37:14.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:37:14.6
Jan 19 21:37:14.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pods 01/19/23 21:37:14.601
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:37:14.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:37:14.622
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 01/19/23 21:37:14.628
STEP: submitting the pod to kubernetes 01/19/23 21:37:14.628
Jan 19 21:37:14.700: INFO: Waiting up to 5m0s for pod "pod-update-7db4c250-6e46-446b-836c-10942978d709" in namespace "pods-8613" to be "running and ready"
Jan 19 21:37:14.707: INFO: Pod "pod-update-7db4c250-6e46-446b-836c-10942978d709": Phase="Pending", Reason="", readiness=false. Elapsed: 6.352105ms
Jan 19 21:37:14.707: INFO: The phase of Pod pod-update-7db4c250-6e46-446b-836c-10942978d709 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:37:16.710: INFO: Pod "pod-update-7db4c250-6e46-446b-836c-10942978d709": Phase="Running", Reason="", readiness=true. Elapsed: 2.009382002s
Jan 19 21:37:16.710: INFO: The phase of Pod pod-update-7db4c250-6e46-446b-836c-10942978d709 is Running (Ready = true)
Jan 19 21:37:16.710: INFO: Pod "pod-update-7db4c250-6e46-446b-836c-10942978d709" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/19/23 21:37:16.712
STEP: updating the pod 01/19/23 21:37:16.715
Jan 19 21:37:17.230: INFO: Successfully updated pod "pod-update-7db4c250-6e46-446b-836c-10942978d709"
Jan 19 21:37:17.230: INFO: Waiting up to 5m0s for pod "pod-update-7db4c250-6e46-446b-836c-10942978d709" in namespace "pods-8613" to be "running"
Jan 19 21:37:17.233: INFO: Pod "pod-update-7db4c250-6e46-446b-836c-10942978d709": Phase="Running", Reason="", readiness=true. Elapsed: 2.356705ms
Jan 19 21:37:17.233: INFO: Pod "pod-update-7db4c250-6e46-446b-836c-10942978d709" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/19/23 21:37:17.233
Jan 19 21:37:17.235: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 21:37:17.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8613" for this suite. 01/19/23 21:37:17.239
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":216,"skipped":4092,"failed":0}
------------------------------
• [2.647 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:37:14.6
    Jan 19 21:37:14.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pods 01/19/23 21:37:14.601
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:37:14.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:37:14.622
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 01/19/23 21:37:14.628
    STEP: submitting the pod to kubernetes 01/19/23 21:37:14.628
    Jan 19 21:37:14.700: INFO: Waiting up to 5m0s for pod "pod-update-7db4c250-6e46-446b-836c-10942978d709" in namespace "pods-8613" to be "running and ready"
    Jan 19 21:37:14.707: INFO: Pod "pod-update-7db4c250-6e46-446b-836c-10942978d709": Phase="Pending", Reason="", readiness=false. Elapsed: 6.352105ms
    Jan 19 21:37:14.707: INFO: The phase of Pod pod-update-7db4c250-6e46-446b-836c-10942978d709 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:37:16.710: INFO: Pod "pod-update-7db4c250-6e46-446b-836c-10942978d709": Phase="Running", Reason="", readiness=true. Elapsed: 2.009382002s
    Jan 19 21:37:16.710: INFO: The phase of Pod pod-update-7db4c250-6e46-446b-836c-10942978d709 is Running (Ready = true)
    Jan 19 21:37:16.710: INFO: Pod "pod-update-7db4c250-6e46-446b-836c-10942978d709" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/19/23 21:37:16.712
    STEP: updating the pod 01/19/23 21:37:16.715
    Jan 19 21:37:17.230: INFO: Successfully updated pod "pod-update-7db4c250-6e46-446b-836c-10942978d709"
    Jan 19 21:37:17.230: INFO: Waiting up to 5m0s for pod "pod-update-7db4c250-6e46-446b-836c-10942978d709" in namespace "pods-8613" to be "running"
    Jan 19 21:37:17.233: INFO: Pod "pod-update-7db4c250-6e46-446b-836c-10942978d709": Phase="Running", Reason="", readiness=true. Elapsed: 2.356705ms
    Jan 19 21:37:17.233: INFO: Pod "pod-update-7db4c250-6e46-446b-836c-10942978d709" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/19/23 21:37:17.233
    Jan 19 21:37:17.235: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 21:37:17.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8613" for this suite. 01/19/23 21:37:17.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:37:17.249
Jan 19 21:37:17.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pod-network-test 01/19/23 21:37:17.25
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:37:17.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:37:17.272
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-423 01/19/23 21:37:17.274
STEP: creating a selector 01/19/23 21:37:17.274
STEP: Creating the service pods in kubernetes 01/19/23 21:37:17.274
Jan 19 21:37:17.274: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 21:37:17.505: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-423" to be "running and ready"
Jan 19 21:37:17.512: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.494852ms
Jan 19 21:37:17.512: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:37:19.515: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009517262s
Jan 19 21:37:19.515: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:37:21.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010536393s
Jan 19 21:37:21.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:37:23.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011561434s
Jan 19 21:37:23.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:37:25.515: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009665179s
Jan 19 21:37:25.515: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:37:27.515: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009906637s
Jan 19 21:37:27.515: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:37:29.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010420283s
Jan 19 21:37:29.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:37:31.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010396743s
Jan 19 21:37:31.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:37:33.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010620215s
Jan 19 21:37:33.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:37:35.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012284001s
Jan 19 21:37:35.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:37:37.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010982658s
Jan 19 21:37:37.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:37:39.515: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010348369s
Jan 19 21:37:39.515: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 19 21:37:39.516: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 19 21:37:39.518: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-423" to be "running and ready"
Jan 19 21:37:39.520: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.388512ms
Jan 19 21:37:39.520: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 19 21:37:39.520: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 19 21:37:39.523: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-423" to be "running and ready"
Jan 19 21:37:39.525: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.212212ms
Jan 19 21:37:39.525: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 19 21:37:39.525: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan 19 21:37:39.527: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-423" to be "running and ready"
Jan 19 21:37:39.529: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.056471ms
Jan 19 21:37:39.529: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan 19 21:37:39.529: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jan 19 21:37:39.531: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-423" to be "running and ready"
Jan 19 21:37:39.533: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 2.126699ms
Jan 19 21:37:39.533: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jan 19 21:37:39.533: INFO: Pod "netserver-4" satisfied condition "running and ready"
Jan 19 21:37:39.536: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-423" to be "running and ready"
Jan 19 21:37:39.538: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 2.02827ms
Jan 19 21:37:39.538: INFO: The phase of Pod netserver-5 is Running (Ready = true)
Jan 19 21:37:39.538: INFO: Pod "netserver-5" satisfied condition "running and ready"
STEP: Creating test pods 01/19/23 21:37:39.54
Jan 19 21:37:39.564: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-423" to be "running"
Jan 19 21:37:39.567: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.843268ms
Jan 19 21:37:41.570: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006111589s
Jan 19 21:37:41.570: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 19 21:37:41.572: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-423" to be "running"
Jan 19 21:37:41.575: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.230009ms
Jan 19 21:37:41.575: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan 19 21:37:41.577: INFO: Setting MaxTries for pod polling to 66 for networking test based on endpoint count 6
Jan 19 21:37:41.577: INFO: Going to poll 10.128.10.75 on port 8081 at least 0 times, with a maximum of 66 tries before failing
Jan 19 21:37:41.579: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.10.75 8081 | grep -v '^\s*$'] Namespace:pod-network-test-423 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:37:41.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:37:41.579: INFO: ExecWithOptions: Clientset creation
Jan 19 21:37:41.579: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-423/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.10.75+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 21:37:42.691: INFO: Found all 1 expected endpoints: [netserver-0]
Jan 19 21:37:42.691: INFO: Going to poll 10.128.14.68 on port 8081 at least 0 times, with a maximum of 66 tries before failing
Jan 19 21:37:42.694: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.14.68 8081 | grep -v '^\s*$'] Namespace:pod-network-test-423 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:37:42.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:37:42.695: INFO: ExecWithOptions: Clientset creation
Jan 19 21:37:42.695: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-423/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.14.68+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 21:37:43.798: INFO: Found all 1 expected endpoints: [netserver-1]
Jan 19 21:37:43.798: INFO: Going to poll 10.128.12.112 on port 8081 at least 0 times, with a maximum of 66 tries before failing
Jan 19 21:37:43.801: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.12.112 8081 | grep -v '^\s*$'] Namespace:pod-network-test-423 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:37:43.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:37:43.802: INFO: ExecWithOptions: Clientset creation
Jan 19 21:37:43.802: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-423/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.12.112+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 21:37:44.914: INFO: Found all 1 expected endpoints: [netserver-2]
Jan 19 21:37:44.914: INFO: Going to poll 10.128.8.212 on port 8081 at least 0 times, with a maximum of 66 tries before failing
Jan 19 21:37:44.916: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.8.212 8081 | grep -v '^\s*$'] Namespace:pod-network-test-423 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:37:44.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:37:44.917: INFO: ExecWithOptions: Clientset creation
Jan 19 21:37:44.917: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-423/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.8.212+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 21:37:46.013: INFO: Found all 1 expected endpoints: [netserver-3]
Jan 19 21:37:46.013: INFO: Going to poll 10.128.6.61 on port 8081 at least 0 times, with a maximum of 66 tries before failing
Jan 19 21:37:46.017: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.6.61 8081 | grep -v '^\s*$'] Namespace:pod-network-test-423 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:37:46.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:37:46.017: INFO: ExecWithOptions: Clientset creation
Jan 19 21:37:46.017: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-423/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.6.61+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 21:37:47.114: INFO: Found all 1 expected endpoints: [netserver-4]
Jan 19 21:37:47.114: INFO: Going to poll 10.128.16.114 on port 8081 at least 0 times, with a maximum of 66 tries before failing
Jan 19 21:37:47.118: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.16.114 8081 | grep -v '^\s*$'] Namespace:pod-network-test-423 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:37:47.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:37:47.119: INFO: ExecWithOptions: Clientset creation
Jan 19 21:37:47.119: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-423/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.16.114+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan 19 21:37:48.214: INFO: Found all 1 expected endpoints: [netserver-5]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 19 21:37:48.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-423" for this suite. 01/19/23 21:37:48.219
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":217,"skipped":4131,"failed":0}
------------------------------
• [SLOW TEST] [30.976 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:37:17.249
    Jan 19 21:37:17.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pod-network-test 01/19/23 21:37:17.25
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:37:17.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:37:17.272
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-423 01/19/23 21:37:17.274
    STEP: creating a selector 01/19/23 21:37:17.274
    STEP: Creating the service pods in kubernetes 01/19/23 21:37:17.274
    Jan 19 21:37:17.274: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 19 21:37:17.505: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-423" to be "running and ready"
    Jan 19 21:37:17.512: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.494852ms
    Jan 19 21:37:17.512: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:37:19.515: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009517262s
    Jan 19 21:37:19.515: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:37:21.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010536393s
    Jan 19 21:37:21.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:37:23.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011561434s
    Jan 19 21:37:23.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:37:25.515: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009665179s
    Jan 19 21:37:25.515: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:37:27.515: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009906637s
    Jan 19 21:37:27.515: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:37:29.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010420283s
    Jan 19 21:37:29.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:37:31.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010396743s
    Jan 19 21:37:31.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:37:33.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010620215s
    Jan 19 21:37:33.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:37:35.517: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012284001s
    Jan 19 21:37:35.517: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:37:37.516: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010982658s
    Jan 19 21:37:37.516: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:37:39.515: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010348369s
    Jan 19 21:37:39.515: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 19 21:37:39.516: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 19 21:37:39.518: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-423" to be "running and ready"
    Jan 19 21:37:39.520: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.388512ms
    Jan 19 21:37:39.520: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 19 21:37:39.520: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 19 21:37:39.523: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-423" to be "running and ready"
    Jan 19 21:37:39.525: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.212212ms
    Jan 19 21:37:39.525: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 19 21:37:39.525: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan 19 21:37:39.527: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-423" to be "running and ready"
    Jan 19 21:37:39.529: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.056471ms
    Jan 19 21:37:39.529: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan 19 21:37:39.529: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jan 19 21:37:39.531: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-423" to be "running and ready"
    Jan 19 21:37:39.533: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 2.126699ms
    Jan 19 21:37:39.533: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jan 19 21:37:39.533: INFO: Pod "netserver-4" satisfied condition "running and ready"
    Jan 19 21:37:39.536: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-423" to be "running and ready"
    Jan 19 21:37:39.538: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 2.02827ms
    Jan 19 21:37:39.538: INFO: The phase of Pod netserver-5 is Running (Ready = true)
    Jan 19 21:37:39.538: INFO: Pod "netserver-5" satisfied condition "running and ready"
    STEP: Creating test pods 01/19/23 21:37:39.54
    Jan 19 21:37:39.564: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-423" to be "running"
    Jan 19 21:37:39.567: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.843268ms
    Jan 19 21:37:41.570: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006111589s
    Jan 19 21:37:41.570: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 19 21:37:41.572: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-423" to be "running"
    Jan 19 21:37:41.575: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.230009ms
    Jan 19 21:37:41.575: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan 19 21:37:41.577: INFO: Setting MaxTries for pod polling to 66 for networking test based on endpoint count 6
    Jan 19 21:37:41.577: INFO: Going to poll 10.128.10.75 on port 8081 at least 0 times, with a maximum of 66 tries before failing
    Jan 19 21:37:41.579: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.10.75 8081 | grep -v '^\s*$'] Namespace:pod-network-test-423 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:37:41.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:37:41.579: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:37:41.579: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-423/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.10.75+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 21:37:42.691: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan 19 21:37:42.691: INFO: Going to poll 10.128.14.68 on port 8081 at least 0 times, with a maximum of 66 tries before failing
    Jan 19 21:37:42.694: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.14.68 8081 | grep -v '^\s*$'] Namespace:pod-network-test-423 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:37:42.694: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:37:42.695: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:37:42.695: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-423/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.14.68+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 21:37:43.798: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan 19 21:37:43.798: INFO: Going to poll 10.128.12.112 on port 8081 at least 0 times, with a maximum of 66 tries before failing
    Jan 19 21:37:43.801: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.12.112 8081 | grep -v '^\s*$'] Namespace:pod-network-test-423 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:37:43.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:37:43.802: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:37:43.802: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-423/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.12.112+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 21:37:44.914: INFO: Found all 1 expected endpoints: [netserver-2]
    Jan 19 21:37:44.914: INFO: Going to poll 10.128.8.212 on port 8081 at least 0 times, with a maximum of 66 tries before failing
    Jan 19 21:37:44.916: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.8.212 8081 | grep -v '^\s*$'] Namespace:pod-network-test-423 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:37:44.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:37:44.917: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:37:44.917: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-423/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.8.212+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 21:37:46.013: INFO: Found all 1 expected endpoints: [netserver-3]
    Jan 19 21:37:46.013: INFO: Going to poll 10.128.6.61 on port 8081 at least 0 times, with a maximum of 66 tries before failing
    Jan 19 21:37:46.017: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.6.61 8081 | grep -v '^\s*$'] Namespace:pod-network-test-423 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:37:46.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:37:46.017: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:37:46.017: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-423/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.6.61+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 21:37:47.114: INFO: Found all 1 expected endpoints: [netserver-4]
    Jan 19 21:37:47.114: INFO: Going to poll 10.128.16.114 on port 8081 at least 0 times, with a maximum of 66 tries before failing
    Jan 19 21:37:47.118: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.128.16.114 8081 | grep -v '^\s*$'] Namespace:pod-network-test-423 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:37:47.118: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:37:47.119: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:37:47.119: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-423/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.128.16.114+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan 19 21:37:48.214: INFO: Found all 1 expected endpoints: [netserver-5]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 19 21:37:48.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-423" for this suite. 01/19/23 21:37:48.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:37:48.226
Jan 19 21:37:48.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-probe 01/19/23 21:37:48.227
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:37:48.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:37:48.256
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35 in namespace container-probe-3217 01/19/23 21:37:48.259
Jan 19 21:37:48.280: INFO: Waiting up to 5m0s for pod "busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35" in namespace "container-probe-3217" to be "not pending"
Jan 19 21:37:48.282: INFO: Pod "busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.35601ms
Jan 19 21:37:50.287: INFO: Pod "busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35": Phase="Running", Reason="", readiness=true. Elapsed: 2.006368165s
Jan 19 21:37:50.287: INFO: Pod "busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35" satisfied condition "not pending"
Jan 19 21:37:50.287: INFO: Started pod busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35 in namespace container-probe-3217
STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 21:37:50.287
Jan 19 21:37:50.289: INFO: Initial restart count of pod busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35 is 0
Jan 19 21:38:40.395: INFO: Restart count of pod container-probe-3217/busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35 is now 1 (50.105845973s elapsed)
STEP: deleting the pod 01/19/23 21:38:40.395
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 21:38:40.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3217" for this suite. 01/19/23 21:38:40.412
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":218,"skipped":4136,"failed":0}
------------------------------
• [SLOW TEST] [52.192 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:37:48.226
    Jan 19 21:37:48.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-probe 01/19/23 21:37:48.227
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:37:48.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:37:48.256
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35 in namespace container-probe-3217 01/19/23 21:37:48.259
    Jan 19 21:37:48.280: INFO: Waiting up to 5m0s for pod "busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35" in namespace "container-probe-3217" to be "not pending"
    Jan 19 21:37:48.282: INFO: Pod "busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.35601ms
    Jan 19 21:37:50.287: INFO: Pod "busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35": Phase="Running", Reason="", readiness=true. Elapsed: 2.006368165s
    Jan 19 21:37:50.287: INFO: Pod "busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35" satisfied condition "not pending"
    Jan 19 21:37:50.287: INFO: Started pod busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35 in namespace container-probe-3217
    STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 21:37:50.287
    Jan 19 21:37:50.289: INFO: Initial restart count of pod busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35 is 0
    Jan 19 21:38:40.395: INFO: Restart count of pod container-probe-3217/busybox-8431ff94-d9a1-4adb-ac5a-eef36ed88d35 is now 1 (50.105845973s elapsed)
    STEP: deleting the pod 01/19/23 21:38:40.395
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 21:38:40.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3217" for this suite. 01/19/23 21:38:40.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:38:40.42
Jan 19 21:38:40.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-runtime 01/19/23 21:38:40.421
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:38:40.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:38:40.448
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 01/19/23 21:38:40.45
W0119 21:38:40.517087      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: wait for the container to reach Succeeded 01/19/23 21:38:40.517
STEP: get the container status 01/19/23 21:38:44.536
STEP: the container should be terminated 01/19/23 21:38:44.538
STEP: the termination message should be set 01/19/23 21:38:44.538
Jan 19 21:38:44.538: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/19/23 21:38:44.538
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jan 19 21:38:44.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9273" for this suite. 01/19/23 21:38:44.555
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":219,"skipped":4204,"failed":0}
------------------------------
• [4.141 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:38:40.42
    Jan 19 21:38:40.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-runtime 01/19/23 21:38:40.421
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:38:40.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:38:40.448
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 01/19/23 21:38:40.45
    W0119 21:38:40.517087      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "termination-message-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "termination-message-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "termination-message-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "termination-message-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: wait for the container to reach Succeeded 01/19/23 21:38:40.517
    STEP: get the container status 01/19/23 21:38:44.536
    STEP: the container should be terminated 01/19/23 21:38:44.538
    STEP: the termination message should be set 01/19/23 21:38:44.538
    Jan 19 21:38:44.538: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/19/23 21:38:44.538
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jan 19 21:38:44.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9273" for this suite. 01/19/23 21:38:44.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:38:44.562
Jan 19 21:38:44.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename deployment 01/19/23 21:38:44.563
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:38:44.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:38:44.597
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
W0119 21:38:44.612098      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:38:44.616: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan 19 21:38:49.620: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/19/23 21:38:49.62
Jan 19 21:38:49.620: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/19/23 21:38:49.628
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 21:38:49.636: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7773  66fc2ce0-bd6c-46be-ac8b-bea22ce8bc47 202970 1 2023-01-19 21:38:49 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-19 21:38:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f78d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan 19 21:38:49.639: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan 19 21:38:49.639: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan 19 21:38:49.639: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7773  3bea692b-b160-418c-a9e9-b3b0c41b4430 202971 1 2023-01-19 21:38:44 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 66fc2ce0-bd6c-46be-ac8b-bea22ce8bc47 0xc005dfa887 0xc005dfa888}] [] [{e2e.test Update apps/v1 2023-01-19 21:38:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:38:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-19 21:38:49 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"66fc2ce0-bd6c-46be-ac8b-bea22ce8bc47\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005dfaa08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 21:38:49.641: INFO: Pod "test-cleanup-controller-lk685" is available:
&Pod{ObjectMeta:{test-cleanup-controller-lk685 test-cleanup-controller- deployment-7773  55293ee2-3f11-486d-adf8-d924262f4c77 202914 0 2023-01-19 21:38:44 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.216/23"],"mac_address":"0a:58:0a:80:08:d8","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.216/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.216"
    ],
    "mac": "0a:58:0a:80:08:d8",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.216"
    ],
    "mac": "0a:58:0a:80:08:d8",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 3bea692b-b160-418c-a9e9-b3b0c41b4430 0xc005fc6117 0xc005fc6118}] [] [{ip-10-0-194-246 Update v1 2023-01-19 21:38:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 21:38:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bea692b-b160-418c-a9e9-b3b0c41b4430\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 21:38:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-19 21:38:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s46c5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s46c5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c57,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:38:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:38:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:38:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:38:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.216,StartTime:2023-01-19 21:38:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 21:38:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://e69d0444f377a5664469a606361f6eed87f654b3e50c64be3a4ce8893d1e0c15,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 21:38:49.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7773" for this suite. 01/19/23 21:38:49.648
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":220,"skipped":4237,"failed":0}
------------------------------
• [SLOW TEST] [5.091 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:38:44.562
    Jan 19 21:38:44.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename deployment 01/19/23 21:38:44.563
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:38:44.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:38:44.597
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    W0119 21:38:44.612098      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:38:44.616: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan 19 21:38:49.620: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/19/23 21:38:49.62
    Jan 19 21:38:49.620: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/19/23 21:38:49.628
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 21:38:49.636: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-7773  66fc2ce0-bd6c-46be-ac8b-bea22ce8bc47 202970 1 2023-01-19 21:38:49 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-19 21:38:49 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005f78d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan 19 21:38:49.639: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jan 19 21:38:49.639: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan 19 21:38:49.639: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-7773  3bea692b-b160-418c-a9e9-b3b0c41b4430 202971 1 2023-01-19 21:38:44 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 66fc2ce0-bd6c-46be-ac8b-bea22ce8bc47 0xc005dfa887 0xc005dfa888}] [] [{e2e.test Update apps/v1 2023-01-19 21:38:44 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:38:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-19 21:38:49 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"66fc2ce0-bd6c-46be-ac8b-bea22ce8bc47\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005dfaa08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 21:38:49.641: INFO: Pod "test-cleanup-controller-lk685" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-lk685 test-cleanup-controller- deployment-7773  55293ee2-3f11-486d-adf8-d924262f4c77 202914 0 2023-01-19 21:38:44 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.216/23"],"mac_address":"0a:58:0a:80:08:d8","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.216/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.216"
        ],
        "mac": "0a:58:0a:80:08:d8",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.216"
        ],
        "mac": "0a:58:0a:80:08:d8",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-controller 3bea692b-b160-418c-a9e9-b3b0c41b4430 0xc005fc6117 0xc005fc6118}] [] [{ip-10-0-194-246 Update v1 2023-01-19 21:38:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 21:38:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3bea692b-b160-418c-a9e9-b3b0c41b4430\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 21:38:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-19 21:38:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s46c5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s46c5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c57,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:38:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:38:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:38:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:38:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.216,StartTime:2023-01-19 21:38:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 21:38:45 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://e69d0444f377a5664469a606361f6eed87f654b3e50c64be3a4ce8893d1e0c15,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 21:38:49.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-7773" for this suite. 01/19/23 21:38:49.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:38:49.654
Jan 19 21:38:49.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename job 01/19/23 21:38:49.654
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:38:49.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:38:49.684
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 01/19/23 21:38:49.688
W0119 21:38:49.698324      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 01/19/23 21:38:49.698
STEP: Orphaning one of the Job's Pods 01/19/23 21:38:51.701
Jan 19 21:38:52.223: INFO: Successfully updated pod "adopt-release-52hrz"
STEP: Checking that the Job readopts the Pod 01/19/23 21:38:52.223
Jan 19 21:38:52.223: INFO: Waiting up to 15m0s for pod "adopt-release-52hrz" in namespace "job-8226" to be "adopted"
Jan 19 21:38:52.239: INFO: Pod "adopt-release-52hrz": Phase="Running", Reason="", readiness=true. Elapsed: 16.047279ms
Jan 19 21:38:54.245: INFO: Pod "adopt-release-52hrz": Phase="Running", Reason="", readiness=true. Elapsed: 2.0219072s
Jan 19 21:38:54.245: INFO: Pod "adopt-release-52hrz" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/19/23 21:38:54.245
Jan 19 21:38:54.761: INFO: Successfully updated pod "adopt-release-52hrz"
STEP: Checking that the Job releases the Pod 01/19/23 21:38:54.761
Jan 19 21:38:54.761: INFO: Waiting up to 15m0s for pod "adopt-release-52hrz" in namespace "job-8226" to be "released"
Jan 19 21:38:54.764: INFO: Pod "adopt-release-52hrz": Phase="Running", Reason="", readiness=true. Elapsed: 3.04496ms
Jan 19 21:38:56.767: INFO: Pod "adopt-release-52hrz": Phase="Running", Reason="", readiness=true. Elapsed: 2.006027525s
Jan 19 21:38:56.767: INFO: Pod "adopt-release-52hrz" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 19 21:38:56.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8226" for this suite. 01/19/23 21:38:56.773
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":221,"skipped":4245,"failed":0}
------------------------------
• [SLOW TEST] [7.124 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:38:49.654
    Jan 19 21:38:49.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename job 01/19/23 21:38:49.654
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:38:49.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:38:49.684
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 01/19/23 21:38:49.688
    W0119 21:38:49.698324      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 01/19/23 21:38:49.698
    STEP: Orphaning one of the Job's Pods 01/19/23 21:38:51.701
    Jan 19 21:38:52.223: INFO: Successfully updated pod "adopt-release-52hrz"
    STEP: Checking that the Job readopts the Pod 01/19/23 21:38:52.223
    Jan 19 21:38:52.223: INFO: Waiting up to 15m0s for pod "adopt-release-52hrz" in namespace "job-8226" to be "adopted"
    Jan 19 21:38:52.239: INFO: Pod "adopt-release-52hrz": Phase="Running", Reason="", readiness=true. Elapsed: 16.047279ms
    Jan 19 21:38:54.245: INFO: Pod "adopt-release-52hrz": Phase="Running", Reason="", readiness=true. Elapsed: 2.0219072s
    Jan 19 21:38:54.245: INFO: Pod "adopt-release-52hrz" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/19/23 21:38:54.245
    Jan 19 21:38:54.761: INFO: Successfully updated pod "adopt-release-52hrz"
    STEP: Checking that the Job releases the Pod 01/19/23 21:38:54.761
    Jan 19 21:38:54.761: INFO: Waiting up to 15m0s for pod "adopt-release-52hrz" in namespace "job-8226" to be "released"
    Jan 19 21:38:54.764: INFO: Pod "adopt-release-52hrz": Phase="Running", Reason="", readiness=true. Elapsed: 3.04496ms
    Jan 19 21:38:56.767: INFO: Pod "adopt-release-52hrz": Phase="Running", Reason="", readiness=true. Elapsed: 2.006027525s
    Jan 19 21:38:56.767: INFO: Pod "adopt-release-52hrz" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 19 21:38:56.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8226" for this suite. 01/19/23 21:38:56.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:38:56.779
Jan 19 21:38:56.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 21:38:56.78
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:38:56.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:38:56.81
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189
STEP: creating service in namespace services-7574 01/19/23 21:38:56.828
STEP: creating service affinity-clusterip-transition in namespace services-7574 01/19/23 21:38:56.828
STEP: creating replication controller affinity-clusterip-transition in namespace services-7574 01/19/23 21:38:56.869
I0119 21:38:56.902980      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7574, replica count: 3
I0119 21:38:59.954011      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 21:38:59.959: INFO: Creating new exec pod
Jan 19 21:38:59.974: INFO: Waiting up to 5m0s for pod "execpod-affinity88bzl" in namespace "services-7574" to be "running"
Jan 19 21:38:59.977: INFO: Pod "execpod-affinity88bzl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.321028ms
Jan 19 21:39:01.980: INFO: Pod "execpod-affinity88bzl": Phase="Running", Reason="", readiness=true. Elapsed: 2.005776718s
Jan 19 21:39:01.980: INFO: Pod "execpod-affinity88bzl" satisfied condition "running"
Jan 19 21:39:02.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7574 exec execpod-affinity88bzl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jan 19 21:39:04.179: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan 19 21:39:04.179: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:39:04.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7574 exec execpod-affinity88bzl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.61.41 80'
Jan 19 21:39:04.318: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.61.41 80\nConnection to 172.30.61.41 80 port [tcp/http] succeeded!\n"
Jan 19 21:39:04.318: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:39:04.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7574 exec execpod-affinity88bzl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.61.41:80/ ; done'
Jan 19 21:39:04.505: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n"
Jan 19 21:39:04.505: INFO: stdout: "\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-f4n9b\naffinity-clusterip-transition-f4n9b\naffinity-clusterip-transition-f4n9b\naffinity-clusterip-transition-xbv8k\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-f4n9b\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-xbv8k\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-xbv8k"
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-f4n9b
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-f4n9b
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-f4n9b
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-xbv8k
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-f4n9b
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-xbv8k
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-xbv8k
Jan 19 21:39:04.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7574 exec execpod-affinity88bzl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.61.41:80/ ; done'
Jan 19 21:39:04.691: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n"
Jan 19 21:39:04.691: INFO: stdout: "\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl"
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
Jan 19 21:39:04.691: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7574, will wait for the garbage collector to delete the pods 01/19/23 21:39:04.703
Jan 19 21:39:04.761: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.078836ms
Jan 19 21:39:04.861: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.16093ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 21:39:07.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7574" for this suite. 01/19/23 21:39:07.191
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":222,"skipped":4271,"failed":0}
------------------------------
• [SLOW TEST] [10.418 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:38:56.779
    Jan 19 21:38:56.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 21:38:56.78
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:38:56.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:38:56.81
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2189
    STEP: creating service in namespace services-7574 01/19/23 21:38:56.828
    STEP: creating service affinity-clusterip-transition in namespace services-7574 01/19/23 21:38:56.828
    STEP: creating replication controller affinity-clusterip-transition in namespace services-7574 01/19/23 21:38:56.869
    I0119 21:38:56.902980      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-7574, replica count: 3
    I0119 21:38:59.954011      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 21:38:59.959: INFO: Creating new exec pod
    Jan 19 21:38:59.974: INFO: Waiting up to 5m0s for pod "execpod-affinity88bzl" in namespace "services-7574" to be "running"
    Jan 19 21:38:59.977: INFO: Pod "execpod-affinity88bzl": Phase="Pending", Reason="", readiness=false. Elapsed: 2.321028ms
    Jan 19 21:39:01.980: INFO: Pod "execpod-affinity88bzl": Phase="Running", Reason="", readiness=true. Elapsed: 2.005776718s
    Jan 19 21:39:01.980: INFO: Pod "execpod-affinity88bzl" satisfied condition "running"
    Jan 19 21:39:02.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7574 exec execpod-affinity88bzl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jan 19 21:39:04.179: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan 19 21:39:04.179: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:39:04.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7574 exec execpod-affinity88bzl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.61.41 80'
    Jan 19 21:39:04.318: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.61.41 80\nConnection to 172.30.61.41 80 port [tcp/http] succeeded!\n"
    Jan 19 21:39:04.318: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:39:04.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7574 exec execpod-affinity88bzl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.61.41:80/ ; done'
    Jan 19 21:39:04.505: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n"
    Jan 19 21:39:04.505: INFO: stdout: "\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-f4n9b\naffinity-clusterip-transition-f4n9b\naffinity-clusterip-transition-f4n9b\naffinity-clusterip-transition-xbv8k\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-f4n9b\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-xbv8k\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-xbv8k"
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-f4n9b
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-f4n9b
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-f4n9b
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-xbv8k
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-f4n9b
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-xbv8k
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.505: INFO: Received response from host: affinity-clusterip-transition-xbv8k
    Jan 19 21:39:04.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7574 exec execpod-affinity88bzl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.30.61.41:80/ ; done'
    Jan 19 21:39:04.691: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.30.61.41:80/\n"
    Jan 19 21:39:04.691: INFO: stdout: "\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl\naffinity-clusterip-transition-jcjkl"
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Received response from host: affinity-clusterip-transition-jcjkl
    Jan 19 21:39:04.691: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-7574, will wait for the garbage collector to delete the pods 01/19/23 21:39:04.703
    Jan 19 21:39:04.761: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.078836ms
    Jan 19 21:39:04.861: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.16093ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 21:39:07.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7574" for this suite. 01/19/23 21:39:07.191
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:39:07.197
Jan 19 21:39:07.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:39:07.198
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:07.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:07.226
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:39:07.229
Jan 19 21:39:07.285: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8" in namespace "projected-5464" to be "Succeeded or Failed"
Jan 19 21:39:07.305: INFO: Pod "downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.359965ms
Jan 19 21:39:09.308: INFO: Pod "downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023433057s
Jan 19 21:39:11.311: INFO: Pod "downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02605554s
STEP: Saw pod success 01/19/23 21:39:11.311
Jan 19 21:39:11.311: INFO: Pod "downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8" satisfied condition "Succeeded or Failed"
Jan 19 21:39:11.314: INFO: Trying to get logs from node ip-10-0-146-42.ec2.internal pod downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8 container client-container: <nil>
STEP: delete the pod 01/19/23 21:39:11.325
Jan 19 21:39:11.337: INFO: Waiting for pod downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8 to disappear
Jan 19 21:39:11.340: INFO: Pod downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 21:39:11.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5464" for this suite. 01/19/23 21:39:11.345
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":223,"skipped":4277,"failed":0}
------------------------------
• [4.153 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:39:07.197
    Jan 19 21:39:07.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:39:07.198
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:07.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:07.226
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:39:07.229
    Jan 19 21:39:07.285: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8" in namespace "projected-5464" to be "Succeeded or Failed"
    Jan 19 21:39:07.305: INFO: Pod "downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.359965ms
    Jan 19 21:39:09.308: INFO: Pod "downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023433057s
    Jan 19 21:39:11.311: INFO: Pod "downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02605554s
    STEP: Saw pod success 01/19/23 21:39:11.311
    Jan 19 21:39:11.311: INFO: Pod "downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8" satisfied condition "Succeeded or Failed"
    Jan 19 21:39:11.314: INFO: Trying to get logs from node ip-10-0-146-42.ec2.internal pod downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8 container client-container: <nil>
    STEP: delete the pod 01/19/23 21:39:11.325
    Jan 19 21:39:11.337: INFO: Waiting for pod downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8 to disappear
    Jan 19 21:39:11.340: INFO: Pod downwardapi-volume-bd0a9e5c-d7f6-40e8-8584-4df92130d3b8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 21:39:11.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5464" for this suite. 01/19/23 21:39:11.345
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:39:11.351
Jan 19 21:39:11.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename var-expansion 01/19/23 21:39:11.352
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:11.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:11.378
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 01/19/23 21:39:11.393
Jan 19 21:39:11.443: INFO: Waiting up to 5m0s for pod "var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c" in namespace "var-expansion-8566" to be "Succeeded or Failed"
Jan 19 21:39:11.459: INFO: Pod "var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.045975ms
Jan 19 21:39:13.463: INFO: Pod "var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01975812s
Jan 19 21:39:15.463: INFO: Pod "var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019907463s
STEP: Saw pod success 01/19/23 21:39:15.463
Jan 19 21:39:15.463: INFO: Pod "var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c" satisfied condition "Succeeded or Failed"
Jan 19 21:39:15.465: INFO: Trying to get logs from node ip-10-0-146-42.ec2.internal pod var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c container dapi-container: <nil>
STEP: delete the pod 01/19/23 21:39:15.47
Jan 19 21:39:15.484: INFO: Waiting for pod var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c to disappear
Jan 19 21:39:15.488: INFO: Pod var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 21:39:15.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8566" for this suite. 01/19/23 21:39:15.494
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":224,"skipped":4281,"failed":0}
------------------------------
• [4.148 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:39:11.351
    Jan 19 21:39:11.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename var-expansion 01/19/23 21:39:11.352
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:11.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:11.378
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 01/19/23 21:39:11.393
    Jan 19 21:39:11.443: INFO: Waiting up to 5m0s for pod "var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c" in namespace "var-expansion-8566" to be "Succeeded or Failed"
    Jan 19 21:39:11.459: INFO: Pod "var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.045975ms
    Jan 19 21:39:13.463: INFO: Pod "var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01975812s
    Jan 19 21:39:15.463: INFO: Pod "var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019907463s
    STEP: Saw pod success 01/19/23 21:39:15.463
    Jan 19 21:39:15.463: INFO: Pod "var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c" satisfied condition "Succeeded or Failed"
    Jan 19 21:39:15.465: INFO: Trying to get logs from node ip-10-0-146-42.ec2.internal pod var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c container dapi-container: <nil>
    STEP: delete the pod 01/19/23 21:39:15.47
    Jan 19 21:39:15.484: INFO: Waiting for pod var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c to disappear
    Jan 19 21:39:15.488: INFO: Pod var-expansion-7373de86-cfc9-472e-bc79-4ce15ac96b6c no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 21:39:15.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8566" for this suite. 01/19/23 21:39:15.494
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:39:15.499
Jan 19 21:39:15.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 21:39:15.5
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:15.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:15.516
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:39:15.518
W0119 21:39:15.543886      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:39:15.543: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86" in namespace "downward-api-1289" to be "Succeeded or Failed"
Jan 19 21:39:15.559: INFO: Pod "downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86": Phase="Pending", Reason="", readiness=false. Elapsed: 15.531842ms
Jan 19 21:39:17.562: INFO: Pod "downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018803515s
Jan 19 21:39:19.563: INFO: Pod "downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019207759s
STEP: Saw pod success 01/19/23 21:39:19.563
Jan 19 21:39:19.563: INFO: Pod "downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86" satisfied condition "Succeeded or Failed"
Jan 19 21:39:19.565: INFO: Trying to get logs from node ip-10-0-146-42.ec2.internal pod downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86 container client-container: <nil>
STEP: delete the pod 01/19/23 21:39:19.57
Jan 19 21:39:19.581: INFO: Waiting for pod downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86 to disappear
Jan 19 21:39:19.584: INFO: Pod downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 21:39:19.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1289" for this suite. 01/19/23 21:39:19.588
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":225,"skipped":4281,"failed":0}
------------------------------
• [4.094 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:39:15.499
    Jan 19 21:39:15.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 21:39:15.5
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:15.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:15.516
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:39:15.518
    W0119 21:39:15.543886      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:39:15.543: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86" in namespace "downward-api-1289" to be "Succeeded or Failed"
    Jan 19 21:39:15.559: INFO: Pod "downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86": Phase="Pending", Reason="", readiness=false. Elapsed: 15.531842ms
    Jan 19 21:39:17.562: INFO: Pod "downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018803515s
    Jan 19 21:39:19.563: INFO: Pod "downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019207759s
    STEP: Saw pod success 01/19/23 21:39:19.563
    Jan 19 21:39:19.563: INFO: Pod "downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86" satisfied condition "Succeeded or Failed"
    Jan 19 21:39:19.565: INFO: Trying to get logs from node ip-10-0-146-42.ec2.internal pod downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86 container client-container: <nil>
    STEP: delete the pod 01/19/23 21:39:19.57
    Jan 19 21:39:19.581: INFO: Waiting for pod downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86 to disappear
    Jan 19 21:39:19.584: INFO: Pod downwardapi-volume-8d2df4a9-6aa0-487d-a9e3-63c345f83e86 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 21:39:19.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1289" for this suite. 01/19/23 21:39:19.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:39:19.594
Jan 19 21:39:19.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename job 01/19/23 21:39:19.594
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:19.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:19.625
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 01/19/23 21:39:19.627
W0119 21:39:19.638328      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensure pods equal to paralellism count is attached to the job 01/19/23 21:39:19.638
STEP: patching /status 01/19/23 21:39:21.642
STEP: updating /status 01/19/23 21:39:21.648
STEP: get /status 01/19/23 21:39:21.674
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 19 21:39:21.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7345" for this suite. 01/19/23 21:39:21.68
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":226,"skipped":4289,"failed":0}
------------------------------
• [2.092 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:39:19.594
    Jan 19 21:39:19.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename job 01/19/23 21:39:19.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:19.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:19.625
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 01/19/23 21:39:19.627
    W0119 21:39:19.638328      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensure pods equal to paralellism count is attached to the job 01/19/23 21:39:19.638
    STEP: patching /status 01/19/23 21:39:21.642
    STEP: updating /status 01/19/23 21:39:21.648
    STEP: get /status 01/19/23 21:39:21.674
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 19 21:39:21.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7345" for this suite. 01/19/23 21:39:21.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:39:21.686
Jan 19 21:39:21.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:39:21.687
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:21.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:21.702
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-1785/configmap-test-8eba67b9-2cf2-4374-a864-cd9e4be959bc 01/19/23 21:39:21.706
STEP: Creating a pod to test consume configMaps 01/19/23 21:39:21.721
Jan 19 21:39:21.766: INFO: Waiting up to 5m0s for pod "pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc" in namespace "configmap-1785" to be "Succeeded or Failed"
Jan 19 21:39:21.776: INFO: Pod "pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.49607ms
Jan 19 21:39:23.788: INFO: Pod "pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02187402s
Jan 19 21:39:25.784: INFO: Pod "pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017887434s
STEP: Saw pod success 01/19/23 21:39:25.784
Jan 19 21:39:25.784: INFO: Pod "pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc" satisfied condition "Succeeded or Failed"
Jan 19 21:39:25.793: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc container env-test: <nil>
STEP: delete the pod 01/19/23 21:39:25.806
Jan 19 21:39:25.819: INFO: Waiting for pod pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc to disappear
Jan 19 21:39:25.821: INFO: Pod pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:39:25.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1785" for this suite. 01/19/23 21:39:25.826
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":227,"skipped":4298,"failed":0}
------------------------------
• [4.145 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:39:21.686
    Jan 19 21:39:21.686: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:39:21.687
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:21.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:21.702
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-1785/configmap-test-8eba67b9-2cf2-4374-a864-cd9e4be959bc 01/19/23 21:39:21.706
    STEP: Creating a pod to test consume configMaps 01/19/23 21:39:21.721
    Jan 19 21:39:21.766: INFO: Waiting up to 5m0s for pod "pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc" in namespace "configmap-1785" to be "Succeeded or Failed"
    Jan 19 21:39:21.776: INFO: Pod "pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc": Phase="Pending", Reason="", readiness=false. Elapsed: 9.49607ms
    Jan 19 21:39:23.788: INFO: Pod "pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02187402s
    Jan 19 21:39:25.784: INFO: Pod "pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017887434s
    STEP: Saw pod success 01/19/23 21:39:25.784
    Jan 19 21:39:25.784: INFO: Pod "pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc" satisfied condition "Succeeded or Failed"
    Jan 19 21:39:25.793: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc container env-test: <nil>
    STEP: delete the pod 01/19/23 21:39:25.806
    Jan 19 21:39:25.819: INFO: Waiting for pod pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc to disappear
    Jan 19 21:39:25.821: INFO: Pod pod-configmaps-68fd9214-fe1d-41f8-b1c5-48b054e9d0bc no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:39:25.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1785" for this suite. 01/19/23 21:39:25.826
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:39:25.831
Jan 19 21:39:25.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename svcaccounts 01/19/23 21:39:25.832
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:25.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:25.852
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jan 19 21:39:25.900: INFO: created pod
Jan 19 21:39:25.900: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3374" to be "Succeeded or Failed"
Jan 19 21:39:25.905: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.767676ms
Jan 19 21:39:27.908: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008184886s
Jan 19 21:39:29.908: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008172557s
STEP: Saw pod success 01/19/23 21:39:29.908
Jan 19 21:39:29.908: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan 19 21:39:59.909: INFO: polling logs
Jan 19 21:39:59.916: INFO: Pod logs: 
I0119 21:39:26.658478       1 log.go:195] OK: Got token
I0119 21:39:26.658506       1 log.go:195] validating with in-cluster discovery
I0119 21:39:26.658941       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0119 21:39:26.658969       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-3374:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674164966, NotBefore:1674164366, IssuedAt:1674164366, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3374", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c21896e6-c2d5-413c-ae08-c21b356953ec"}}}
I0119 21:39:26.669126       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0119 21:39:26.678247       1 log.go:195] OK: Validated signature on JWT
I0119 21:39:26.678326       1 log.go:195] OK: Got valid claims from token!
I0119 21:39:26.678362       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-3374:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674164966, NotBefore:1674164366, IssuedAt:1674164366, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3374", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c21896e6-c2d5-413c-ae08-c21b356953ec"}}}

Jan 19 21:39:59.916: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 19 21:39:59.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3374" for this suite. 01/19/23 21:39:59.927
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":228,"skipped":4300,"failed":0}
------------------------------
• [SLOW TEST] [34.101 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:39:25.831
    Jan 19 21:39:25.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename svcaccounts 01/19/23 21:39:25.832
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:25.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:25.852
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jan 19 21:39:25.900: INFO: created pod
    Jan 19 21:39:25.900: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-3374" to be "Succeeded or Failed"
    Jan 19 21:39:25.905: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.767676ms
    Jan 19 21:39:27.908: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008184886s
    Jan 19 21:39:29.908: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008172557s
    STEP: Saw pod success 01/19/23 21:39:29.908
    Jan 19 21:39:29.908: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan 19 21:39:59.909: INFO: polling logs
    Jan 19 21:39:59.916: INFO: Pod logs: 
    I0119 21:39:26.658478       1 log.go:195] OK: Got token
    I0119 21:39:26.658506       1 log.go:195] validating with in-cluster discovery
    I0119 21:39:26.658941       1 log.go:195] OK: got issuer https://kubernetes.default.svc
    I0119 21:39:26.658969       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-3374:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674164966, NotBefore:1674164366, IssuedAt:1674164366, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3374", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c21896e6-c2d5-413c-ae08-c21b356953ec"}}}
    I0119 21:39:26.669126       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0119 21:39:26.678247       1 log.go:195] OK: Validated signature on JWT
    I0119 21:39:26.678326       1 log.go:195] OK: Got valid claims from token!
    I0119 21:39:26.678362       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-3374:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1674164966, NotBefore:1674164366, IssuedAt:1674164366, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-3374", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"c21896e6-c2d5-413c-ae08-c21b356953ec"}}}

    Jan 19 21:39:59.916: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 19 21:39:59.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3374" for this suite. 01/19/23 21:39:59.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:39:59.932
Jan 19 21:39:59.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename sched-preemption 01/19/23 21:39:59.933
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:59.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:59.97
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 19 21:40:00.036: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 21:41:00.242: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 01/19/23 21:41:00.248
Jan 19 21:41:00.302: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan 19 21:41:00.321: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan 19 21:41:00.350: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan 19 21:41:00.366: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan 19 21:41:00.396: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan 19 21:41:00.411: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Jan 19 21:41:00.447: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Jan 19 21:41:00.459: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Jan 19 21:41:00.488: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Jan 19 21:41:00.504: INFO: Created pod: pod4-1-sched-preemption-medium-priority
Jan 19 21:41:00.534: INFO: Created pod: pod5-0-sched-preemption-medium-priority
Jan 19 21:41:00.548: INFO: Created pod: pod5-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/19/23 21:41:00.548
Jan 19 21:41:00.548: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:00.554: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.786478ms
Jan 19 21:41:02.558: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010463343s
Jan 19 21:41:04.558: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009951213s
Jan 19 21:41:06.557: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0091075s
Jan 19 21:41:08.558: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.009704208s
Jan 19 21:41:08.558: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan 19 21:41:08.558: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:08.561: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.740699ms
Jan 19 21:41:08.561: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 21:41:08.561: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:08.563: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.267544ms
Jan 19 21:41:08.563: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 21:41:08.563: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:08.565: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.290551ms
Jan 19 21:41:08.565: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 21:41:08.565: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:08.567: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.300583ms
Jan 19 21:41:08.567: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 21:41:08.567: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:08.570: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.33504ms
Jan 19 21:41:08.570: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 21:41:08.570: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:08.572: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.238565ms
Jan 19 21:41:08.572: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 21:41:08.572: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:08.575: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.444721ms
Jan 19 21:41:08.575: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 21:41:08.575: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:08.577: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297191ms
Jan 19 21:41:10.581: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.006294397s
Jan 19 21:41:10.581: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 21:41:10.581: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:10.583: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.579353ms
Jan 19 21:41:10.583: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 21:41:10.583: INFO: Waiting up to 5m0s for pod "pod5-0-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:10.586: INFO: Pod "pod5-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.842026ms
Jan 19 21:41:12.590: INFO: Pod "pod5-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.006733816s
Jan 19 21:41:12.590: INFO: Pod "pod5-0-sched-preemption-medium-priority" satisfied condition "running"
Jan 19 21:41:12.590: INFO: Waiting up to 5m0s for pod "pod5-1-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:12.593: INFO: Pod "pod5-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.575602ms
Jan 19 21:41:12.593: INFO: Pod "pod5-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/19/23 21:41:12.593
Jan 19 21:41:12.607: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5945" to be "running"
Jan 19 21:41:12.610: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.627753ms
Jan 19 21:41:14.613: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006163184s
Jan 19 21:41:16.613: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006150233s
Jan 19 21:41:18.614: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.007120248s
Jan 19 21:41:18.614: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:41:18.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5945" for this suite. 01/19/23 21:41:18.647
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":229,"skipped":4305,"failed":0}
------------------------------
• [SLOW TEST] [78.810 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:39:59.932
    Jan 19 21:39:59.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename sched-preemption 01/19/23 21:39:59.933
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:39:59.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:39:59.97
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 19 21:40:00.036: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 19 21:41:00.242: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 01/19/23 21:41:00.248
    Jan 19 21:41:00.302: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan 19 21:41:00.321: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan 19 21:41:00.350: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan 19 21:41:00.366: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan 19 21:41:00.396: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan 19 21:41:00.411: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Jan 19 21:41:00.447: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Jan 19 21:41:00.459: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Jan 19 21:41:00.488: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Jan 19 21:41:00.504: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    Jan 19 21:41:00.534: INFO: Created pod: pod5-0-sched-preemption-medium-priority
    Jan 19 21:41:00.548: INFO: Created pod: pod5-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/19/23 21:41:00.548
    Jan 19 21:41:00.548: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:00.554: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.786478ms
    Jan 19 21:41:02.558: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010463343s
    Jan 19 21:41:04.558: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009951213s
    Jan 19 21:41:06.557: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0091075s
    Jan 19 21:41:08.558: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 8.009704208s
    Jan 19 21:41:08.558: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan 19 21:41:08.558: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:08.561: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.740699ms
    Jan 19 21:41:08.561: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 21:41:08.561: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:08.563: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.267544ms
    Jan 19 21:41:08.563: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 21:41:08.563: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:08.565: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.290551ms
    Jan 19 21:41:08.565: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 21:41:08.565: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:08.567: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.300583ms
    Jan 19 21:41:08.567: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 21:41:08.567: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:08.570: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.33504ms
    Jan 19 21:41:08.570: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 21:41:08.570: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:08.572: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.238565ms
    Jan 19 21:41:08.572: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 21:41:08.572: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:08.575: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.444721ms
    Jan 19 21:41:08.575: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 21:41:08.575: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:08.577: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297191ms
    Jan 19 21:41:10.581: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.006294397s
    Jan 19 21:41:10.581: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 21:41:10.581: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:10.583: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.579353ms
    Jan 19 21:41:10.583: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 21:41:10.583: INFO: Waiting up to 5m0s for pod "pod5-0-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:10.586: INFO: Pod "pod5-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.842026ms
    Jan 19 21:41:12.590: INFO: Pod "pod5-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.006733816s
    Jan 19 21:41:12.590: INFO: Pod "pod5-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan 19 21:41:12.590: INFO: Waiting up to 5m0s for pod "pod5-1-sched-preemption-medium-priority" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:12.593: INFO: Pod "pod5-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.575602ms
    Jan 19 21:41:12.593: INFO: Pod "pod5-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/19/23 21:41:12.593
    Jan 19 21:41:12.607: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-5945" to be "running"
    Jan 19 21:41:12.610: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.627753ms
    Jan 19 21:41:14.613: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006163184s
    Jan 19 21:41:16.613: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006150233s
    Jan 19 21:41:18.614: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.007120248s
    Jan 19 21:41:18.614: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:41:18.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5945" for this suite. 01/19/23 21:41:18.647
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:41:18.742
Jan 19 21:41:18.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:41:18.743
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:18.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:18.762
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 01/19/23 21:41:18.765
Jan 19 21:41:18.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9885 api-versions'
Jan 19 21:41:18.814: INFO: stderr: ""
Jan 19 21:41:18.814: INFO: stdout: "addons.managed.openshift.io/v1alpha1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloud.network.openshift.io/v1\ncloudcredential.openshift.io/v1\ncloudingress.managed.openshift.io/v1alpha1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nk8s.ovn.org/v1\nmachine.openshift.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmanaged.openshift.io/v1alpha1\nmanaged.openshift.io/v1alpha2\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nmonitoring.openshift.io/v1alpha1\nmonitoring.rhobs/v1\nmonitoring.rhobs/v1alpha1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\nocmagent.managed.openshift.io/v1alpha1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nsplunkforwarder.managed.openshift.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nupgrade.managed.openshift.io/v1alpha1\nuser.openshift.io/v1\nv1\nvelero.io/v1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:41:18.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9885" for this suite. 01/19/23 21:41:18.837
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":230,"skipped":4309,"failed":0}
------------------------------
• [0.115 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:41:18.742
    Jan 19 21:41:18.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:41:18.743
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:18.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:18.762
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 01/19/23 21:41:18.765
    Jan 19 21:41:18.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9885 api-versions'
    Jan 19 21:41:18.814: INFO: stderr: ""
    Jan 19 21:41:18.814: INFO: stdout: "addons.managed.openshift.io/v1alpha1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling.openshift.io/v1\nautoscaling.openshift.io/v1beta1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloud.network.openshift.io/v1\ncloudcredential.openshift.io/v1\ncloudingress.managed.openshift.io/v1alpha1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nk8s.ovn.org/v1\nmachine.openshift.io/v1\nmachine.openshift.io/v1beta1\nmachineconfiguration.openshift.io/v1\nmanaged.openshift.io/v1alpha1\nmanaged.openshift.io/v1alpha2\nmetal3.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nmonitoring.openshift.io/v1alpha1\nmonitoring.rhobs/v1\nmonitoring.rhobs/v1alpha1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\nocmagent.managed.openshift.io/v1alpha1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nsplunkforwarder.managed.openshift.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nupgrade.managed.openshift.io/v1alpha1\nuser.openshift.io/v1\nv1\nvelero.io/v1\nwhereabouts.cni.cncf.io/v1alpha1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:41:18.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9885" for this suite. 01/19/23 21:41:18.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:41:18.858
Jan 19 21:41:18.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 21:41:18.859
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:18.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:18.902
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-a3713d48-5fe7-4aef-bfed-34dc1bc660ec 01/19/23 21:41:18.913
STEP: Creating a pod to test consume secrets 01/19/23 21:41:18.921
Jan 19 21:41:18.961: INFO: Waiting up to 5m0s for pod "pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6" in namespace "secrets-7392" to be "Succeeded or Failed"
Jan 19 21:41:18.973: INFO: Pod "pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.918177ms
Jan 19 21:41:20.977: INFO: Pod "pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01649502s
Jan 19 21:41:22.979: INFO: Pod "pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018121554s
STEP: Saw pod success 01/19/23 21:41:22.979
Jan 19 21:41:22.979: INFO: Pod "pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6" satisfied condition "Succeeded or Failed"
Jan 19 21:41:22.981: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 21:41:22.988
Jan 19 21:41:23.000: INFO: Waiting for pod pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6 to disappear
Jan 19 21:41:23.002: INFO: Pod pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 21:41:23.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7392" for this suite. 01/19/23 21:41:23.007
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":231,"skipped":4322,"failed":0}
------------------------------
• [4.153 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:41:18.858
    Jan 19 21:41:18.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 21:41:18.859
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:18.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:18.902
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-a3713d48-5fe7-4aef-bfed-34dc1bc660ec 01/19/23 21:41:18.913
    STEP: Creating a pod to test consume secrets 01/19/23 21:41:18.921
    Jan 19 21:41:18.961: INFO: Waiting up to 5m0s for pod "pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6" in namespace "secrets-7392" to be "Succeeded or Failed"
    Jan 19 21:41:18.973: INFO: Pod "pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6": Phase="Pending", Reason="", readiness=false. Elapsed: 12.918177ms
    Jan 19 21:41:20.977: INFO: Pod "pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01649502s
    Jan 19 21:41:22.979: INFO: Pod "pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018121554s
    STEP: Saw pod success 01/19/23 21:41:22.979
    Jan 19 21:41:22.979: INFO: Pod "pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6" satisfied condition "Succeeded or Failed"
    Jan 19 21:41:22.981: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:41:22.988
    Jan 19 21:41:23.000: INFO: Waiting for pod pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6 to disappear
    Jan 19 21:41:23.002: INFO: Pod pod-secrets-3b6ed73c-d1da-4a7f-ac13-1bd671da90f6 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 21:41:23.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7392" for this suite. 01/19/23 21:41:23.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:41:23.013
Jan 19 21:41:23.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename gc 01/19/23 21:41:23.014
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:23.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:23.035
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/19/23 21:41:23.044
STEP: create the rc2 01/19/23 21:41:23.06
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/19/23 21:41:28.09
STEP: delete the rc simpletest-rc-to-be-deleted 01/19/23 21:41:28.684
STEP: wait for the rc to be deleted 01/19/23 21:41:28.69
STEP: Gathering metrics 01/19/23 21:41:33.7
W0119 21:41:33.702846      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0119 21:41:33.702861      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 21:41:33.702: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan 19 21:41:33.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-28k57" in namespace "gc-90"
Jan 19 21:41:33.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bftc" in namespace "gc-90"
Jan 19 21:41:33.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xz9w" in namespace "gc-90"
Jan 19 21:41:33.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-4752k" in namespace "gc-90"
Jan 19 21:41:33.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-4chq9" in namespace "gc-90"
Jan 19 21:41:33.756: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mh68" in namespace "gc-90"
Jan 19 21:41:33.768: INFO: Deleting pod "simpletest-rc-to-be-deleted-4s62c" in namespace "gc-90"
Jan 19 21:41:33.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vlhh" in namespace "gc-90"
Jan 19 21:41:33.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-52gzw" in namespace "gc-90"
Jan 19 21:41:33.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-54z6d" in namespace "gc-90"
Jan 19 21:41:33.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-5csh8" in namespace "gc-90"
Jan 19 21:41:33.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-64vx2" in namespace "gc-90"
Jan 19 21:41:33.845: INFO: Deleting pod "simpletest-rc-to-be-deleted-67ms9" in namespace "gc-90"
Jan 19 21:41:33.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fjwj" in namespace "gc-90"
Jan 19 21:41:33.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-77bb7" in namespace "gc-90"
Jan 19 21:41:33.886: INFO: Deleting pod "simpletest-rc-to-be-deleted-786r4" in namespace "gc-90"
Jan 19 21:41:33.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-7h5sl" in namespace "gc-90"
Jan 19 21:41:33.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rjzn" in namespace "gc-90"
Jan 19 21:41:33.925: INFO: Deleting pod "simpletest-rc-to-be-deleted-85jlk" in namespace "gc-90"
Jan 19 21:41:33.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dn2n" in namespace "gc-90"
Jan 19 21:41:33.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jmr5" in namespace "gc-90"
Jan 19 21:41:33.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-8r497" in namespace "gc-90"
Jan 19 21:41:33.974: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sc7w" in namespace "gc-90"
Jan 19 21:41:33.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-8w5hw" in namespace "gc-90"
Jan 19 21:41:33.998: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bvbg" in namespace "gc-90"
Jan 19 21:41:34.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-9k5pg" in namespace "gc-90"
Jan 19 21:41:34.020: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7j7x" in namespace "gc-90"
Jan 19 21:41:34.032: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9fsf" in namespace "gc-90"
Jan 19 21:41:34.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-brn7j" in namespace "gc-90"
Jan 19 21:41:34.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbfxd" in namespace "gc-90"
Jan 19 21:41:34.074: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckzgw" in namespace "gc-90"
Jan 19 21:41:34.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl76b" in namespace "gc-90"
Jan 19 21:41:34.098: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqgnm" in namespace "gc-90"
Jan 19 21:41:34.123: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctcvv" in namespace "gc-90"
Jan 19 21:41:34.135: INFO: Deleting pod "simpletest-rc-to-be-deleted-czxwr" in namespace "gc-90"
Jan 19 21:41:34.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5m75" in namespace "gc-90"
Jan 19 21:41:34.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6sh4" in namespace "gc-90"
Jan 19 21:41:34.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-ds8ns" in namespace "gc-90"
Jan 19 21:41:34.195: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg6ft" in namespace "gc-90"
Jan 19 21:41:34.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-frkqr" in namespace "gc-90"
Jan 19 21:41:34.226: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbdsd" in namespace "gc-90"
Jan 19 21:41:34.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjt9h" in namespace "gc-90"
Jan 19 21:41:34.287: INFO: Deleting pod "simpletest-rc-to-be-deleted-gl2qh" in namespace "gc-90"
Jan 19 21:41:34.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-gswbr" in namespace "gc-90"
Jan 19 21:41:34.363: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzjdt" in namespace "gc-90"
Jan 19 21:41:34.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6slm" in namespace "gc-90"
Jan 19 21:41:34.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnvdg" in namespace "gc-90"
Jan 19 21:41:34.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-hr4mw" in namespace "gc-90"
Jan 19 21:41:34.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-hs27d" in namespace "gc-90"
Jan 19 21:41:34.436: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7vzp" in namespace "gc-90"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 21:41:34.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-90" for this suite. 01/19/23 21:41:34.55
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":232,"skipped":4351,"failed":0}
------------------------------
• [SLOW TEST] [11.543 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:41:23.013
    Jan 19 21:41:23.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename gc 01/19/23 21:41:23.014
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:23.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:23.035
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/19/23 21:41:23.044
    STEP: create the rc2 01/19/23 21:41:23.06
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/19/23 21:41:28.09
    STEP: delete the rc simpletest-rc-to-be-deleted 01/19/23 21:41:28.684
    STEP: wait for the rc to be deleted 01/19/23 21:41:28.69
    STEP: Gathering metrics 01/19/23 21:41:33.7
    W0119 21:41:33.702846      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0119 21:41:33.702861      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 19 21:41:33.702: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan 19 21:41:33.702: INFO: Deleting pod "simpletest-rc-to-be-deleted-28k57" in namespace "gc-90"
    Jan 19 21:41:33.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-2bftc" in namespace "gc-90"
    Jan 19 21:41:33.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xz9w" in namespace "gc-90"
    Jan 19 21:41:33.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-4752k" in namespace "gc-90"
    Jan 19 21:41:33.747: INFO: Deleting pod "simpletest-rc-to-be-deleted-4chq9" in namespace "gc-90"
    Jan 19 21:41:33.756: INFO: Deleting pod "simpletest-rc-to-be-deleted-4mh68" in namespace "gc-90"
    Jan 19 21:41:33.768: INFO: Deleting pod "simpletest-rc-to-be-deleted-4s62c" in namespace "gc-90"
    Jan 19 21:41:33.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vlhh" in namespace "gc-90"
    Jan 19 21:41:33.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-52gzw" in namespace "gc-90"
    Jan 19 21:41:33.804: INFO: Deleting pod "simpletest-rc-to-be-deleted-54z6d" in namespace "gc-90"
    Jan 19 21:41:33.820: INFO: Deleting pod "simpletest-rc-to-be-deleted-5csh8" in namespace "gc-90"
    Jan 19 21:41:33.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-64vx2" in namespace "gc-90"
    Jan 19 21:41:33.845: INFO: Deleting pod "simpletest-rc-to-be-deleted-67ms9" in namespace "gc-90"
    Jan 19 21:41:33.860: INFO: Deleting pod "simpletest-rc-to-be-deleted-6fjwj" in namespace "gc-90"
    Jan 19 21:41:33.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-77bb7" in namespace "gc-90"
    Jan 19 21:41:33.886: INFO: Deleting pod "simpletest-rc-to-be-deleted-786r4" in namespace "gc-90"
    Jan 19 21:41:33.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-7h5sl" in namespace "gc-90"
    Jan 19 21:41:33.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-7rjzn" in namespace "gc-90"
    Jan 19 21:41:33.925: INFO: Deleting pod "simpletest-rc-to-be-deleted-85jlk" in namespace "gc-90"
    Jan 19 21:41:33.937: INFO: Deleting pod "simpletest-rc-to-be-deleted-8dn2n" in namespace "gc-90"
    Jan 19 21:41:33.948: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jmr5" in namespace "gc-90"
    Jan 19 21:41:33.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-8r497" in namespace "gc-90"
    Jan 19 21:41:33.974: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sc7w" in namespace "gc-90"
    Jan 19 21:41:33.987: INFO: Deleting pod "simpletest-rc-to-be-deleted-8w5hw" in namespace "gc-90"
    Jan 19 21:41:33.998: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bvbg" in namespace "gc-90"
    Jan 19 21:41:34.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-9k5pg" in namespace "gc-90"
    Jan 19 21:41:34.020: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7j7x" in namespace "gc-90"
    Jan 19 21:41:34.032: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9fsf" in namespace "gc-90"
    Jan 19 21:41:34.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-brn7j" in namespace "gc-90"
    Jan 19 21:41:34.063: INFO: Deleting pod "simpletest-rc-to-be-deleted-cbfxd" in namespace "gc-90"
    Jan 19 21:41:34.074: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckzgw" in namespace "gc-90"
    Jan 19 21:41:34.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-cl76b" in namespace "gc-90"
    Jan 19 21:41:34.098: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqgnm" in namespace "gc-90"
    Jan 19 21:41:34.123: INFO: Deleting pod "simpletest-rc-to-be-deleted-ctcvv" in namespace "gc-90"
    Jan 19 21:41:34.135: INFO: Deleting pod "simpletest-rc-to-be-deleted-czxwr" in namespace "gc-90"
    Jan 19 21:41:34.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5m75" in namespace "gc-90"
    Jan 19 21:41:34.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6sh4" in namespace "gc-90"
    Jan 19 21:41:34.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-ds8ns" in namespace "gc-90"
    Jan 19 21:41:34.195: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg6ft" in namespace "gc-90"
    Jan 19 21:41:34.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-frkqr" in namespace "gc-90"
    Jan 19 21:41:34.226: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbdsd" in namespace "gc-90"
    Jan 19 21:41:34.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjt9h" in namespace "gc-90"
    Jan 19 21:41:34.287: INFO: Deleting pod "simpletest-rc-to-be-deleted-gl2qh" in namespace "gc-90"
    Jan 19 21:41:34.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-gswbr" in namespace "gc-90"
    Jan 19 21:41:34.363: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzjdt" in namespace "gc-90"
    Jan 19 21:41:34.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6slm" in namespace "gc-90"
    Jan 19 21:41:34.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-hnvdg" in namespace "gc-90"
    Jan 19 21:41:34.402: INFO: Deleting pod "simpletest-rc-to-be-deleted-hr4mw" in namespace "gc-90"
    Jan 19 21:41:34.424: INFO: Deleting pod "simpletest-rc-to-be-deleted-hs27d" in namespace "gc-90"
    Jan 19 21:41:34.436: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7vzp" in namespace "gc-90"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 21:41:34.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-90" for this suite. 01/19/23 21:41:34.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:41:34.556
Jan 19 21:41:34.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename containers 01/19/23 21:41:34.557
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:34.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:34.59
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 01/19/23 21:41:34.6
Jan 19 21:41:34.658: INFO: Waiting up to 5m0s for pod "client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b" in namespace "containers-7144" to be "Succeeded or Failed"
Jan 19 21:41:34.664: INFO: Pod "client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.701165ms
Jan 19 21:41:36.667: INFO: Pod "client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00893296s
Jan 19 21:41:38.667: INFO: Pod "client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009337665s
Jan 19 21:41:40.668: INFO: Pod "client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009823447s
STEP: Saw pod success 01/19/23 21:41:40.668
Jan 19 21:41:40.668: INFO: Pod "client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b" satisfied condition "Succeeded or Failed"
Jan 19 21:41:40.670: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:41:40.675
Jan 19 21:41:40.686: INFO: Waiting for pod client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b to disappear
Jan 19 21:41:40.689: INFO: Pod client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jan 19 21:41:40.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7144" for this suite. 01/19/23 21:41:40.693
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":233,"skipped":4368,"failed":0}
------------------------------
• [SLOW TEST] [6.141 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:41:34.556
    Jan 19 21:41:34.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename containers 01/19/23 21:41:34.557
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:34.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:34.59
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 01/19/23 21:41:34.6
    Jan 19 21:41:34.658: INFO: Waiting up to 5m0s for pod "client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b" in namespace "containers-7144" to be "Succeeded or Failed"
    Jan 19 21:41:34.664: INFO: Pod "client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.701165ms
    Jan 19 21:41:36.667: INFO: Pod "client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00893296s
    Jan 19 21:41:38.667: INFO: Pod "client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009337665s
    Jan 19 21:41:40.668: INFO: Pod "client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009823447s
    STEP: Saw pod success 01/19/23 21:41:40.668
    Jan 19 21:41:40.668: INFO: Pod "client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b" satisfied condition "Succeeded or Failed"
    Jan 19 21:41:40.670: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:41:40.675
    Jan 19 21:41:40.686: INFO: Waiting for pod client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b to disappear
    Jan 19 21:41:40.689: INFO: Pod client-containers-dd40614b-072b-44cb-8065-9ef94d65de3b no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jan 19 21:41:40.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7144" for this suite. 01/19/23 21:41:40.693
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:41:40.7
Jan 19 21:41:40.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:41:40.7
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:40.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:40.723
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 21:41:40.726
Jan 19 21:41:40.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7110 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan 19 21:41:40.848: INFO: stderr: ""
Jan 19 21:41:40.848: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/19/23 21:41:40.848
Jan 19 21:41:40.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7110 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jan 19 21:41:42.941: INFO: stderr: ""
Jan 19 21:41:42.941: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 21:41:42.941
Jan 19 21:41:42.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7110 delete pods e2e-test-httpd-pod'
Jan 19 21:41:44.806: INFO: stderr: ""
Jan 19 21:41:44.806: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:41:44.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7110" for this suite. 01/19/23 21:41:44.811
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":234,"skipped":4423,"failed":0}
------------------------------
• [4.118 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:41:40.7
    Jan 19 21:41:40.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:41:40.7
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:40.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:40.723
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 21:41:40.726
    Jan 19 21:41:40.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7110 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan 19 21:41:40.848: INFO: stderr: ""
    Jan 19 21:41:40.848: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/19/23 21:41:40.848
    Jan 19 21:41:40.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7110 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jan 19 21:41:42.941: INFO: stderr: ""
    Jan 19 21:41:42.941: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 21:41:42.941
    Jan 19 21:41:42.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7110 delete pods e2e-test-httpd-pod'
    Jan 19 21:41:44.806: INFO: stderr: ""
    Jan 19 21:41:44.806: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:41:44.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7110" for this suite. 01/19/23 21:41:44.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:41:44.818
Jan 19 21:41:44.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename daemonsets 01/19/23 21:41:44.819
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:44.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:44.84
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 01/19/23 21:41:44.905
STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 21:41:44.912
Jan 19 21:41:44.918: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:41:44.918: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:41:44.918: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:41:44.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:41:44.922: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:41:45.928: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:41:45.928: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:41:45.928: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:41:45.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan 19 21:41:45.935: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:41:46.927: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:41:46.927: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:41:46.927: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:41:46.931: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 19 21:41:46.931: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/19/23 21:41:46.933
Jan 19 21:41:46.947: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:41:46.947: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:41:46.947: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:41:46.952: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 19 21:41:46.952: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/19/23 21:41:46.952
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/19/23 21:41:46.957
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6137, will wait for the garbage collector to delete the pods 01/19/23 21:41:46.958
Jan 19 21:41:47.019: INFO: Deleting DaemonSet.extensions daemon-set took: 5.955119ms
Jan 19 21:41:47.119: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.642113ms
Jan 19 21:41:49.722: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:41:49.722: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 21:41:49.724: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"209067"},"items":null}

Jan 19 21:41:49.726: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"209067"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:41:49.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6137" for this suite. 01/19/23 21:41:49.754
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":235,"skipped":4431,"failed":0}
------------------------------
• [4.942 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:41:44.818
    Jan 19 21:41:44.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename daemonsets 01/19/23 21:41:44.819
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:44.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:44.84
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 01/19/23 21:41:44.905
    STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 21:41:44.912
    Jan 19 21:41:44.918: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:41:44.918: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:41:44.918: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:41:44.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:41:44.922: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:41:45.928: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:41:45.928: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:41:45.928: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:41:45.935: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan 19 21:41:45.935: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:41:46.927: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:41:46.927: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:41:46.927: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:41:46.931: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
    Jan 19 21:41:46.931: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/19/23 21:41:46.933
    Jan 19 21:41:46.947: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:41:46.947: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:41:46.947: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:41:46.952: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
    Jan 19 21:41:46.952: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/19/23 21:41:46.952
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/19/23 21:41:46.957
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6137, will wait for the garbage collector to delete the pods 01/19/23 21:41:46.958
    Jan 19 21:41:47.019: INFO: Deleting DaemonSet.extensions daemon-set took: 5.955119ms
    Jan 19 21:41:47.119: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.642113ms
    Jan 19 21:41:49.722: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:41:49.722: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 19 21:41:49.724: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"209067"},"items":null}

    Jan 19 21:41:49.726: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"209067"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:41:49.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6137" for this suite. 01/19/23 21:41:49.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:41:49.762
Jan 19 21:41:49.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename podtemplate 01/19/23 21:41:49.762
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:49.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:49.781
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/19/23 21:41:49.783
W0119 21:41:49.794638      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Replace a pod template 01/19/23 21:41:49.794
Jan 19 21:41:49.803: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 19 21:41:49.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-6696" for this suite. 01/19/23 21:41:49.814
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":236,"skipped":4460,"failed":0}
------------------------------
• [0.057 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:41:49.762
    Jan 19 21:41:49.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename podtemplate 01/19/23 21:41:49.762
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:49.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:49.781
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/19/23 21:41:49.783
    W0119 21:41:49.794638      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "e2e-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "e2e-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "e2e-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "e2e-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Replace a pod template 01/19/23 21:41:49.794
    Jan 19 21:41:49.803: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 19 21:41:49.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-6696" for this suite. 01/19/23 21:41:49.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:41:49.819
Jan 19 21:41:49.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename statefulset 01/19/23 21:41:49.82
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:49.853
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:49.855
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7908 01/19/23 21:41:49.858
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-7908 01/19/23 21:41:49.874
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7908 01/19/23 21:41:49.884
Jan 19 21:41:49.890: INFO: Found 0 stateful pods, waiting for 1
Jan 19 21:41:59.895: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/19/23 21:41:59.895
Jan 19 21:41:59.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 21:42:00.021: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 21:42:00.021: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 21:42:00.021: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 21:42:00.024: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 19 21:42:10.031: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 21:42:10.031: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 21:42:10.044: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Jan 19 21:42:10.044: INFO: ss-0  ip-10-0-172-44.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:41:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:41:49 +0000 UTC  }]
Jan 19 21:42:10.044: INFO: 
Jan 19 21:42:10.044: INFO: StatefulSet ss has not reached scale 3, at 1
Jan 19 21:42:11.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996437383s
Jan 19 21:42:12.052: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992810702s
Jan 19 21:42:13.055: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989381589s
Jan 19 21:42:14.058: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986387411s
Jan 19 21:42:15.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982825686s
Jan 19 21:42:16.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977265448s
Jan 19 21:42:17.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973889758s
Jan 19 21:42:18.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969798263s
Jan 19 21:42:19.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.828005ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7908 01/19/23 21:42:20.079
Jan 19 21:42:20.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 21:42:20.190: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 21:42:20.190: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 21:42:20.190: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 21:42:20.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 21:42:20.318: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 19 21:42:20.318: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 21:42:20.318: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 21:42:20.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 21:42:20.464: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan 19 21:42:20.464: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 21:42:20.464: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 21:42:20.467: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 21:42:20.467: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 21:42:20.467: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/19/23 21:42:20.467
Jan 19 21:42:20.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 21:42:20.645: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 21:42:20.645: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 21:42:20.645: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 21:42:20.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 21:42:20.764: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 21:42:20.764: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 21:42:20.764: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 21:42:20.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 21:42:20.871: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 21:42:20.871: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 21:42:20.871: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 21:42:20.871: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 21:42:20.874: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan 19 21:42:30.881: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 21:42:30.881: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 21:42:30.881: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 21:42:30.897: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
Jan 19 21:42:30.897: INFO: ss-0  ip-10-0-172-44.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:41:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:41:49 +0000 UTC  }]
Jan 19 21:42:30.897: INFO: ss-1  ip-10-0-171-213.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:10 +0000 UTC  }]
Jan 19 21:42:30.897: INFO: ss-2  ip-10-0-146-42.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:10 +0000 UTC  }]
Jan 19 21:42:30.897: INFO: 
Jan 19 21:42:30.897: INFO: StatefulSet ss has not reached scale 0, at 3
Jan 19 21:42:31.902: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
Jan 19 21:42:31.902: INFO: ss-0  ip-10-0-172-44.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:41:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:41:49 +0000 UTC  }]
Jan 19 21:42:31.902: INFO: 
Jan 19 21:42:31.902: INFO: StatefulSet ss has not reached scale 0, at 1
Jan 19 21:42:32.905: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990687116s
Jan 19 21:42:33.908: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987298043s
Jan 19 21:42:34.912: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.984542188s
Jan 19 21:42:35.915: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.980102074s
Jan 19 21:42:36.918: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.977005508s
Jan 19 21:42:37.921: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.974094025s
Jan 19 21:42:38.925: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.97087446s
Jan 19 21:42:39.928: INFO: Verifying statefulset ss doesn't scale past 0 for another 967.835614ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7908 01/19/23 21:42:40.929
Jan 19 21:42:40.932: INFO: Scaling statefulset ss to 0
Jan 19 21:42:40.940: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 21:42:40.942: INFO: Deleting all statefulset in ns statefulset-7908
Jan 19 21:42:40.944: INFO: Scaling statefulset ss to 0
Jan 19 21:42:40.952: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 21:42:40.954: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 21:42:40.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7908" for this suite. 01/19/23 21:42:40.97
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":237,"skipped":4466,"failed":0}
------------------------------
• [SLOW TEST] [51.156 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:41:49.819
    Jan 19 21:41:49.819: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename statefulset 01/19/23 21:41:49.82
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:41:49.853
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:41:49.855
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7908 01/19/23 21:41:49.858
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-7908 01/19/23 21:41:49.874
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7908 01/19/23 21:41:49.884
    Jan 19 21:41:49.890: INFO: Found 0 stateful pods, waiting for 1
    Jan 19 21:41:59.895: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/19/23 21:41:59.895
    Jan 19 21:41:59.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 21:42:00.021: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 21:42:00.021: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 21:42:00.021: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 21:42:00.024: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 19 21:42:10.031: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 21:42:10.031: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 21:42:10.044: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
    Jan 19 21:42:10.044: INFO: ss-0  ip-10-0-172-44.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:41:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:41:49 +0000 UTC  }]
    Jan 19 21:42:10.044: INFO: 
    Jan 19 21:42:10.044: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan 19 21:42:11.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996437383s
    Jan 19 21:42:12.052: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992810702s
    Jan 19 21:42:13.055: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989381589s
    Jan 19 21:42:14.058: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986387411s
    Jan 19 21:42:15.064: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982825686s
    Jan 19 21:42:16.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.977265448s
    Jan 19 21:42:17.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973889758s
    Jan 19 21:42:18.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969798263s
    Jan 19 21:42:19.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.828005ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7908 01/19/23 21:42:20.079
    Jan 19 21:42:20.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 21:42:20.190: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 21:42:20.190: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 21:42:20.190: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 21:42:20.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 21:42:20.318: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 19 21:42:20.318: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 21:42:20.318: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 21:42:20.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 21:42:20.464: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan 19 21:42:20.464: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 21:42:20.464: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 21:42:20.467: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 21:42:20.467: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 21:42:20.467: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/19/23 21:42:20.467
    Jan 19 21:42:20.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 21:42:20.645: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 21:42:20.645: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 21:42:20.645: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 21:42:20.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 21:42:20.764: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 21:42:20.764: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 21:42:20.764: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 21:42:20.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-7908 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 21:42:20.871: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 21:42:20.871: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 21:42:20.871: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 21:42:20.871: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 21:42:20.874: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan 19 21:42:30.881: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 21:42:30.881: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 21:42:30.881: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 21:42:30.897: INFO: POD   NODE                          PHASE    GRACE  CONDITIONS
    Jan 19 21:42:30.897: INFO: ss-0  ip-10-0-172-44.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:41:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:41:49 +0000 UTC  }]
    Jan 19 21:42:30.897: INFO: ss-1  ip-10-0-171-213.ec2.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:10 +0000 UTC  }]
    Jan 19 21:42:30.897: INFO: ss-2  ip-10-0-146-42.ec2.internal   Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:10 +0000 UTC  }]
    Jan 19 21:42:30.897: INFO: 
    Jan 19 21:42:30.897: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan 19 21:42:31.902: INFO: POD   NODE                         PHASE    GRACE  CONDITIONS
    Jan 19 21:42:31.902: INFO: ss-0  ip-10-0-172-44.ec2.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:41:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:42:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:41:49 +0000 UTC  }]
    Jan 19 21:42:31.902: INFO: 
    Jan 19 21:42:31.902: INFO: StatefulSet ss has not reached scale 0, at 1
    Jan 19 21:42:32.905: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.990687116s
    Jan 19 21:42:33.908: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987298043s
    Jan 19 21:42:34.912: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.984542188s
    Jan 19 21:42:35.915: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.980102074s
    Jan 19 21:42:36.918: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.977005508s
    Jan 19 21:42:37.921: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.974094025s
    Jan 19 21:42:38.925: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.97087446s
    Jan 19 21:42:39.928: INFO: Verifying statefulset ss doesn't scale past 0 for another 967.835614ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7908 01/19/23 21:42:40.929
    Jan 19 21:42:40.932: INFO: Scaling statefulset ss to 0
    Jan 19 21:42:40.940: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 21:42:40.942: INFO: Deleting all statefulset in ns statefulset-7908
    Jan 19 21:42:40.944: INFO: Scaling statefulset ss to 0
    Jan 19 21:42:40.952: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 21:42:40.954: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 21:42:40.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7908" for this suite. 01/19/23 21:42:40.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:42:40.976
Jan 19 21:42:40.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename watch 01/19/23 21:42:40.977
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:42:40.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:42:40.994
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/19/23 21:42:40.997
STEP: creating a watch on configmaps with label B 01/19/23 21:42:40.998
STEP: creating a watch on configmaps with label A or B 01/19/23 21:42:41.002
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/19/23 21:42:41.003
Jan 19 21:42:41.018: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209929 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 21:42:41.018: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209929 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/19/23 21:42:41.018
Jan 19 21:42:41.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209932 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 21:42:41.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209932 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/19/23 21:42:41.032
Jan 19 21:42:41.049: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209938 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 21:42:41.049: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209938 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/19/23 21:42:41.049
Jan 19 21:42:41.068: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209942 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 21:42:41.069: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209942 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/19/23 21:42:41.069
Jan 19 21:42:41.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1960  b1776a39-05cb-4437-8f5f-160834cd7507 209943 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 21:42:41.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1960  b1776a39-05cb-4437-8f5f-160834cd7507 209943 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/19/23 21:42:51.076
Jan 19 21:42:51.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1960  b1776a39-05cb-4437-8f5f-160834cd7507 210130 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 21:42:51.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1960  b1776a39-05cb-4437-8f5f-160834cd7507 210130 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 19 21:43:01.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1960" for this suite. 01/19/23 21:43:01.09
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":238,"skipped":4473,"failed":0}
------------------------------
• [SLOW TEST] [20.119 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:42:40.976
    Jan 19 21:42:40.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename watch 01/19/23 21:42:40.977
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:42:40.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:42:40.994
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/19/23 21:42:40.997
    STEP: creating a watch on configmaps with label B 01/19/23 21:42:40.998
    STEP: creating a watch on configmaps with label A or B 01/19/23 21:42:41.002
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/19/23 21:42:41.003
    Jan 19 21:42:41.018: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209929 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 21:42:41.018: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209929 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/19/23 21:42:41.018
    Jan 19 21:42:41.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209932 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 21:42:41.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209932 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/19/23 21:42:41.032
    Jan 19 21:42:41.049: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209938 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 21:42:41.049: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209938 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/19/23 21:42:41.049
    Jan 19 21:42:41.068: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209942 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 21:42:41.069: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-1960  6464ea51-9513-4c48-864b-d6250d1dea66 209942 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/19/23 21:42:41.069
    Jan 19 21:42:41.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1960  b1776a39-05cb-4437-8f5f-160834cd7507 209943 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 21:42:41.076: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1960  b1776a39-05cb-4437-8f5f-160834cd7507 209943 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/19/23 21:42:51.076
    Jan 19 21:42:51.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1960  b1776a39-05cb-4437-8f5f-160834cd7507 210130 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 21:42:51.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-1960  b1776a39-05cb-4437-8f5f-160834cd7507 210130 0 2023-01-19 21:42:41 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-19 21:42:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 19 21:43:01.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1960" for this suite. 01/19/23 21:43:01.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:43:01.096
Jan 19 21:43:01.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename namespaces 01/19/23 21:43:01.097
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:01.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:01.132
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 01/19/23 21:43:01.134
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:01.264
STEP: Creating a service in the namespace 01/19/23 21:43:01.267
STEP: Deleting the namespace 01/19/23 21:43:01.36
STEP: Waiting for the namespace to be removed. 01/19/23 21:43:01.435
STEP: Recreating the namespace 01/19/23 21:43:07.438
STEP: Verifying there is no service in the namespace 01/19/23 21:43:07.453
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:43:07.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2733" for this suite. 01/19/23 21:43:07.479
STEP: Destroying namespace "nsdeletetest-9934" for this suite. 01/19/23 21:43:07.492
Jan 19 21:43:07.497: INFO: Namespace nsdeletetest-9934 was already deleted
STEP: Destroying namespace "nsdeletetest-1697" for this suite. 01/19/23 21:43:07.497
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":239,"skipped":4478,"failed":0}
------------------------------
• [SLOW TEST] [6.414 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:43:01.096
    Jan 19 21:43:01.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename namespaces 01/19/23 21:43:01.097
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:01.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:01.132
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 01/19/23 21:43:01.134
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:01.264
    STEP: Creating a service in the namespace 01/19/23 21:43:01.267
    STEP: Deleting the namespace 01/19/23 21:43:01.36
    STEP: Waiting for the namespace to be removed. 01/19/23 21:43:01.435
    STEP: Recreating the namespace 01/19/23 21:43:07.438
    STEP: Verifying there is no service in the namespace 01/19/23 21:43:07.453
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:43:07.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2733" for this suite. 01/19/23 21:43:07.479
    STEP: Destroying namespace "nsdeletetest-9934" for this suite. 01/19/23 21:43:07.492
    Jan 19 21:43:07.497: INFO: Namespace nsdeletetest-9934 was already deleted
    STEP: Destroying namespace "nsdeletetest-1697" for this suite. 01/19/23 21:43:07.497
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:43:07.51
Jan 19 21:43:07.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename security-context-test 01/19/23 21:43:07.511
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:07.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:07.535
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jan 19 21:43:07.579: INFO: Waiting up to 5m0s for pod "busybox-user-65534-1c5e42cc-4007-4181-a15f-025b0c8af7bc" in namespace "security-context-test-3500" to be "Succeeded or Failed"
Jan 19 21:43:07.585: INFO: Pod "busybox-user-65534-1c5e42cc-4007-4181-a15f-025b0c8af7bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.231003ms
Jan 19 21:43:09.589: INFO: Pod "busybox-user-65534-1c5e42cc-4007-4181-a15f-025b0c8af7bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010207511s
Jan 19 21:43:11.588: INFO: Pod "busybox-user-65534-1c5e42cc-4007-4181-a15f-025b0c8af7bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009472817s
Jan 19 21:43:11.588: INFO: Pod "busybox-user-65534-1c5e42cc-4007-4181-a15f-025b0c8af7bc" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 19 21:43:11.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3500" for this suite. 01/19/23 21:43:11.594
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":240,"skipped":4479,"failed":0}
------------------------------
• [4.089 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:43:07.51
    Jan 19 21:43:07.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename security-context-test 01/19/23 21:43:07.511
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:07.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:07.535
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jan 19 21:43:07.579: INFO: Waiting up to 5m0s for pod "busybox-user-65534-1c5e42cc-4007-4181-a15f-025b0c8af7bc" in namespace "security-context-test-3500" to be "Succeeded or Failed"
    Jan 19 21:43:07.585: INFO: Pod "busybox-user-65534-1c5e42cc-4007-4181-a15f-025b0c8af7bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.231003ms
    Jan 19 21:43:09.589: INFO: Pod "busybox-user-65534-1c5e42cc-4007-4181-a15f-025b0c8af7bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010207511s
    Jan 19 21:43:11.588: INFO: Pod "busybox-user-65534-1c5e42cc-4007-4181-a15f-025b0c8af7bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009472817s
    Jan 19 21:43:11.588: INFO: Pod "busybox-user-65534-1c5e42cc-4007-4181-a15f-025b0c8af7bc" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 19 21:43:11.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3500" for this suite. 01/19/23 21:43:11.594
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:43:11.599
Jan 19 21:43:11.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename deployment 01/19/23 21:43:11.6
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:11.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:11.621
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/19/23 21:43:11.642
W0119 21:43:11.653468      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-deployment" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-deployment" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-deployment" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-deployment" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: waiting for Deployment to be created 01/19/23 21:43:11.653
STEP: waiting for all Replicas to be Ready 01/19/23 21:43:11.654
Jan 19 21:43:11.655: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 21:43:11.655: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 21:43:11.669: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 21:43:11.669: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 21:43:11.688: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 21:43:11.688: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 21:43:11.723: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 21:43:11.723: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan 19 21:43:12.797: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 19 21:43:12.797: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan 19 21:43:12.995: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/19/23 21:43:12.995
W0119 21:43:13.005440      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 19 21:43:13.006: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/19/23 21:43:13.006
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:13.023: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:13.023: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:13.051: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:13.051: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:13.068: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
Jan 19 21:43:13.068: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
Jan 19 21:43:13.086: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
Jan 19 21:43:13.086: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
Jan 19 21:43:14.030: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:14.030: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:14.051: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
STEP: listing Deployments 01/19/23 21:43:14.051
Jan 19 21:43:14.060: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/19/23 21:43:14.06
Jan 19 21:43:14.071: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/19/23 21:43:14.071
Jan 19 21:43:14.078: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 21:43:14.084: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 21:43:14.097: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 21:43:14.116: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 21:43:14.140: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 21:43:15.773: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 21:43:15.788: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 21:43:15.804: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 21:43:15.816: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 21:43:15.824: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan 19 21:43:17.014: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/19/23 21:43:17.055
STEP: fetching the DeploymentStatus 01/19/23 21:43:17.061
Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 3
STEP: deleting the Deployment 01/19/23 21:43:17.065
Jan 19 21:43:17.074: INFO: observed event type MODIFIED
Jan 19 21:43:17.074: INFO: observed event type MODIFIED
Jan 19 21:43:17.074: INFO: observed event type MODIFIED
Jan 19 21:43:17.074: INFO: observed event type MODIFIED
Jan 19 21:43:17.074: INFO: observed event type MODIFIED
Jan 19 21:43:17.074: INFO: observed event type MODIFIED
Jan 19 21:43:17.074: INFO: observed event type MODIFIED
Jan 19 21:43:17.074: INFO: observed event type MODIFIED
Jan 19 21:43:17.074: INFO: observed event type MODIFIED
Jan 19 21:43:17.074: INFO: observed event type MODIFIED
Jan 19 21:43:17.074: INFO: observed event type MODIFIED
Jan 19 21:43:17.074: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 21:43:17.077: INFO: Log out all the ReplicaSets if there is no deployment created
Jan 19 21:43:17.080: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-6287  052b43b9-f0ba-4767-802f-1659d845cc4e 210923 2 2023-01-19 21:43:14 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment a32e9d5c-1182-460a-bc8b-5abadf8de950 0xc003886767 0xc003886768}] [] [{kube-controller-manager Update apps/v1 2023-01-19 21:43:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32e9d5c-1182-460a-bc8b-5abadf8de950\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:43:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038867f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan 19 21:43:17.085: INFO: pod: "test-deployment-7c7d8d58c8-dlqv8":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-dlqv8 test-deployment-7c7d8d58c8- deployment-6287  b98165b4-ad8c-44be-87a6-37b3775ad769 210922 0 2023-01-19 21:43:15 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.250/23"],"mac_address":"0a:58:0a:80:08:fa","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.250/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.250"
    ],
    "mac": "0a:58:0a:80:08:fa",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.8.250"
    ],
    "mac": "0a:58:0a:80:08:fa",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 052b43b9-f0ba-4767-802f-1659d845cc4e 0xc00b66abf7 0xc00b66abf8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 21:43:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 21:43:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"052b43b9-f0ba-4767-802f-1659d845cc4e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 21:43:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-19 21:43:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ztmvh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ztmvh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c55,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6h27v,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.250,StartTime:2023-01-19 21:43:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 21:43:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7450f2e67da3042e3894f2b5a987c43ec096d69c347bf0d9675d49959e9d692c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 19 21:43:17.085: INFO: pod: "test-deployment-7c7d8d58c8-rj26d":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-rj26d test-deployment-7c7d8d58c8- deployment-6287  946ef644-7f60-4d8d-8903-4e4cbb2db4f0 210846 0 2023-01-19 21:43:14 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.10.103/23"],"mac_address":"0a:58:0a:80:0a:67","gateway_ips":["10.128.10.1"],"ip_address":"10.128.10.103/23","gateway_ip":"10.128.10.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.10.103"
    ],
    "mac": "0a:58:0a:80:0a:67",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.10.103"
    ],
    "mac": "0a:58:0a:80:0a:67",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 052b43b9-f0ba-4767-802f-1659d845cc4e 0xc00b66aea7 0xc00b66aea8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 21:43:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 21:43:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"052b43b9-f0ba-4767-802f-1659d845cc4e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 21:43:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 21:43:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.10.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjxbp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjxbp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-146-42.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c55,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6h27v,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.146.42,PodIP:10.128.10.103,StartTime:2023-01-19 21:43:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 21:43:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://aad173027aa1163ecd344d3e980599bc72e6607c2e906dc8da726735da2960ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.10.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan 19 21:43:17.085: INFO: ReplicaSet "test-deployment-8594bb6fdd":
&ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-6287  b012a89e-66f4-441f-985b-d12966748f4c 210781 3 2023-01-19 21:43:11 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment a32e9d5c-1182-460a-bc8b-5abadf8de950 0xc003886857 0xc003886858}] [] [{kube-controller-manager Update apps/v1 2023-01-19 21:43:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32e9d5c-1182-460a-bc8b-5abadf8de950\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:43:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038868f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 21:43:17.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6287" for this suite. 01/19/23 21:43:17.095
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":241,"skipped":4481,"failed":0}
------------------------------
• [SLOW TEST] [5.502 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:43:11.599
    Jan 19 21:43:11.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename deployment 01/19/23 21:43:11.6
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:11.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:11.621
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/19/23 21:43:11.642
    W0119 21:43:11.653468      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-deployment" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-deployment" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-deployment" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-deployment" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: waiting for Deployment to be created 01/19/23 21:43:11.653
    STEP: waiting for all Replicas to be Ready 01/19/23 21:43:11.654
    Jan 19 21:43:11.655: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 21:43:11.655: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 21:43:11.669: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 21:43:11.669: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 21:43:11.688: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 21:43:11.688: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 21:43:11.723: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 21:43:11.723: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan 19 21:43:12.797: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 19 21:43:12.797: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan 19 21:43:12.995: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/19/23 21:43:12.995
    W0119 21:43:13.005440      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 19 21:43:13.006: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/19/23 21:43:13.006
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 0
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:13.008: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:13.023: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:13.023: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:13.051: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:13.051: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:13.068: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    Jan 19 21:43:13.068: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    Jan 19 21:43:13.086: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    Jan 19 21:43:13.086: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    Jan 19 21:43:14.030: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:14.030: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:14.051: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    STEP: listing Deployments 01/19/23 21:43:14.051
    Jan 19 21:43:14.060: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/19/23 21:43:14.06
    Jan 19 21:43:14.071: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/19/23 21:43:14.071
    Jan 19 21:43:14.078: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 21:43:14.084: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 21:43:14.097: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 21:43:14.116: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 21:43:14.140: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 21:43:15.773: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 21:43:15.788: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 21:43:15.804: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 21:43:15.816: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 21:43:15.824: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan 19 21:43:17.014: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/19/23 21:43:17.055
    STEP: fetching the DeploymentStatus 01/19/23 21:43:17.061
    Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 1
    Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 2
    Jan 19 21:43:17.065: INFO: observed Deployment test-deployment in namespace deployment-6287 with ReadyReplicas 3
    STEP: deleting the Deployment 01/19/23 21:43:17.065
    Jan 19 21:43:17.074: INFO: observed event type MODIFIED
    Jan 19 21:43:17.074: INFO: observed event type MODIFIED
    Jan 19 21:43:17.074: INFO: observed event type MODIFIED
    Jan 19 21:43:17.074: INFO: observed event type MODIFIED
    Jan 19 21:43:17.074: INFO: observed event type MODIFIED
    Jan 19 21:43:17.074: INFO: observed event type MODIFIED
    Jan 19 21:43:17.074: INFO: observed event type MODIFIED
    Jan 19 21:43:17.074: INFO: observed event type MODIFIED
    Jan 19 21:43:17.074: INFO: observed event type MODIFIED
    Jan 19 21:43:17.074: INFO: observed event type MODIFIED
    Jan 19 21:43:17.074: INFO: observed event type MODIFIED
    Jan 19 21:43:17.074: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 21:43:17.077: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan 19 21:43:17.080: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-6287  052b43b9-f0ba-4767-802f-1659d845cc4e 210923 2 2023-01-19 21:43:14 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment a32e9d5c-1182-460a-bc8b-5abadf8de950 0xc003886767 0xc003886768}] [] [{kube-controller-manager Update apps/v1 2023-01-19 21:43:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32e9d5c-1182-460a-bc8b-5abadf8de950\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:43:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038867f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan 19 21:43:17.085: INFO: pod: "test-deployment-7c7d8d58c8-dlqv8":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-dlqv8 test-deployment-7c7d8d58c8- deployment-6287  b98165b4-ad8c-44be-87a6-37b3775ad769 210922 0 2023-01-19 21:43:15 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.8.250/23"],"mac_address":"0a:58:0a:80:08:fa","gateway_ips":["10.128.8.1"],"ip_address":"10.128.8.250/23","gateway_ip":"10.128.8.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.250"
        ],
        "mac": "0a:58:0a:80:08:fa",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.8.250"
        ],
        "mac": "0a:58:0a:80:08:fa",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 052b43b9-f0ba-4767-802f-1659d845cc4e 0xc00b66abf7 0xc00b66abf8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 21:43:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 21:43:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"052b43b9-f0ba-4767-802f-1659d845cc4e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-19 21:43:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.8.250\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-01-19 21:43:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ztmvh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ztmvh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-172-44.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c55,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6h27v,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.172.44,PodIP:10.128.8.250,StartTime:2023-01-19 21:43:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 21:43:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://7450f2e67da3042e3894f2b5a987c43ec096d69c347bf0d9675d49959e9d692c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.8.250,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 19 21:43:17.085: INFO: pod: "test-deployment-7c7d8d58c8-rj26d":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-rj26d test-deployment-7c7d8d58c8- deployment-6287  946ef644-7f60-4d8d-8903-4e4cbb2db4f0 210846 0 2023-01-19 21:43:14 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.10.103/23"],"mac_address":"0a:58:0a:80:0a:67","gateway_ips":["10.128.10.1"],"ip_address":"10.128.10.103/23","gateway_ip":"10.128.10.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.10.103"
        ],
        "mac": "0a:58:0a:80:0a:67",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.10.103"
        ],
        "mac": "0a:58:0a:80:0a:67",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 052b43b9-f0ba-4767-802f-1659d845cc4e 0xc00b66aea7 0xc00b66aea8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 21:43:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 21:43:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"052b43b9-f0ba-4767-802f-1659d845cc4e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 21:43:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 21:43:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.10.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wjxbp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wjxbp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-146-42.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c55,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-6h27v,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 21:43:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.146.42,PodIP:10.128.10.103,StartTime:2023-01-19 21:43:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 21:43:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://aad173027aa1163ecd344d3e980599bc72e6607c2e906dc8da726735da2960ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.10.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan 19 21:43:17.085: INFO: ReplicaSet "test-deployment-8594bb6fdd":
    &ReplicaSet{ObjectMeta:{test-deployment-8594bb6fdd  deployment-6287  b012a89e-66f4-441f-985b-d12966748f4c 210781 3 2023-01-19 21:43:11 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment a32e9d5c-1182-460a-bc8b-5abadf8de950 0xc003886857 0xc003886858}] [] [{kube-controller-manager Update apps/v1 2023-01-19 21:43:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a32e9d5c-1182-460a-bc8b-5abadf8de950\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 21:43:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 8594bb6fdd,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:8594bb6fdd test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038868f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 21:43:17.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-6287" for this suite. 01/19/23 21:43:17.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:43:17.103
Jan 19 21:43:17.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:43:17.103
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:17.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:17.135
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
Jan 19 21:43:17.161: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-2718ffb3-c0aa-4545-b854-31878da2341e 01/19/23 21:43:17.161
STEP: Creating configMap with name cm-test-opt-upd-4cb43ccd-79e1-425e-aad6-cef64829ec2a 01/19/23 21:43:17.172
STEP: Creating the pod 01/19/23 21:43:17.184
Jan 19 21:43:17.205: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a336411e-85d7-458d-9c2d-f76d622c7dce" in namespace "projected-9566" to be "running and ready"
Jan 19 21:43:17.207: INFO: Pod "pod-projected-configmaps-a336411e-85d7-458d-9c2d-f76d622c7dce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429861ms
Jan 19 21:43:17.207: INFO: The phase of Pod pod-projected-configmaps-a336411e-85d7-458d-9c2d-f76d622c7dce is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:43:19.213: INFO: Pod "pod-projected-configmaps-a336411e-85d7-458d-9c2d-f76d622c7dce": Phase="Running", Reason="", readiness=true. Elapsed: 2.007839127s
Jan 19 21:43:19.213: INFO: The phase of Pod pod-projected-configmaps-a336411e-85d7-458d-9c2d-f76d622c7dce is Running (Ready = true)
Jan 19 21:43:19.213: INFO: Pod "pod-projected-configmaps-a336411e-85d7-458d-9c2d-f76d622c7dce" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-2718ffb3-c0aa-4545-b854-31878da2341e 01/19/23 21:43:19.262
STEP: Updating configmap cm-test-opt-upd-4cb43ccd-79e1-425e-aad6-cef64829ec2a 01/19/23 21:43:19.276
STEP: Creating configMap with name cm-test-opt-create-e80378fa-01bc-4bd6-86ae-8c593d851afa 01/19/23 21:43:19.284
STEP: waiting to observe update in volume 01/19/23 21:43:19.298
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 21:43:23.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9566" for this suite. 01/19/23 21:43:23.33
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":242,"skipped":4493,"failed":0}
------------------------------
• [SLOW TEST] [6.233 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:43:17.103
    Jan 19 21:43:17.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:43:17.103
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:17.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:17.135
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    Jan 19 21:43:17.161: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-2718ffb3-c0aa-4545-b854-31878da2341e 01/19/23 21:43:17.161
    STEP: Creating configMap with name cm-test-opt-upd-4cb43ccd-79e1-425e-aad6-cef64829ec2a 01/19/23 21:43:17.172
    STEP: Creating the pod 01/19/23 21:43:17.184
    Jan 19 21:43:17.205: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a336411e-85d7-458d-9c2d-f76d622c7dce" in namespace "projected-9566" to be "running and ready"
    Jan 19 21:43:17.207: INFO: Pod "pod-projected-configmaps-a336411e-85d7-458d-9c2d-f76d622c7dce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.429861ms
    Jan 19 21:43:17.207: INFO: The phase of Pod pod-projected-configmaps-a336411e-85d7-458d-9c2d-f76d622c7dce is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:43:19.213: INFO: Pod "pod-projected-configmaps-a336411e-85d7-458d-9c2d-f76d622c7dce": Phase="Running", Reason="", readiness=true. Elapsed: 2.007839127s
    Jan 19 21:43:19.213: INFO: The phase of Pod pod-projected-configmaps-a336411e-85d7-458d-9c2d-f76d622c7dce is Running (Ready = true)
    Jan 19 21:43:19.213: INFO: Pod "pod-projected-configmaps-a336411e-85d7-458d-9c2d-f76d622c7dce" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-2718ffb3-c0aa-4545-b854-31878da2341e 01/19/23 21:43:19.262
    STEP: Updating configmap cm-test-opt-upd-4cb43ccd-79e1-425e-aad6-cef64829ec2a 01/19/23 21:43:19.276
    STEP: Creating configMap with name cm-test-opt-create-e80378fa-01bc-4bd6-86ae-8c593d851afa 01/19/23 21:43:19.284
    STEP: waiting to observe update in volume 01/19/23 21:43:19.298
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 21:43:23.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9566" for this suite. 01/19/23 21:43:23.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:43:23.336
Jan 19 21:43:23.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pod-network-test 01/19/23 21:43:23.337
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:23.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:23.356
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-8491 01/19/23 21:43:23.358
STEP: creating a selector 01/19/23 21:43:23.358
STEP: Creating the service pods in kubernetes 01/19/23 21:43:23.358
Jan 19 21:43:23.358: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 21:43:23.518: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8491" to be "running and ready"
Jan 19 21:43:23.528: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.643927ms
Jan 19 21:43:23.528: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:43:25.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013777771s
Jan 19 21:43:25.532: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:43:27.531: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013122598s
Jan 19 21:43:27.531: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:43:29.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013527726s
Jan 19 21:43:29.532: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:43:31.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013892457s
Jan 19 21:43:31.532: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:43:33.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013635094s
Jan 19 21:43:33.532: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:43:35.531: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013393877s
Jan 19 21:43:35.531: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:43:37.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013823347s
Jan 19 21:43:37.532: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:43:39.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013664052s
Jan 19 21:43:39.532: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:43:41.531: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.01319384s
Jan 19 21:43:41.531: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:43:43.533: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014458888s
Jan 19 21:43:43.533: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:43:45.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014336418s
Jan 19 21:43:45.532: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 19 21:43:45.532: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 19 21:43:45.535: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8491" to be "running and ready"
Jan 19 21:43:45.537: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.297355ms
Jan 19 21:43:45.537: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 19 21:43:45.537: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 19 21:43:45.539: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8491" to be "running and ready"
Jan 19 21:43:45.542: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.266296ms
Jan 19 21:43:45.542: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 19 21:43:45.542: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan 19 21:43:45.544: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8491" to be "running and ready"
Jan 19 21:43:45.546: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.210137ms
Jan 19 21:43:45.546: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan 19 21:43:45.546: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jan 19 21:43:45.548: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-8491" to be "running and ready"
Jan 19 21:43:45.550: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 2.117066ms
Jan 19 21:43:45.550: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jan 19 21:43:45.550: INFO: Pod "netserver-4" satisfied condition "running and ready"
Jan 19 21:43:45.553: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-8491" to be "running and ready"
Jan 19 21:43:45.555: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 2.161887ms
Jan 19 21:43:45.555: INFO: The phase of Pod netserver-5 is Running (Ready = true)
Jan 19 21:43:45.555: INFO: Pod "netserver-5" satisfied condition "running and ready"
STEP: Creating test pods 01/19/23 21:43:45.557
Jan 19 21:43:45.568: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8491" to be "running"
Jan 19 21:43:45.571: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.746694ms
Jan 19 21:43:47.575: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007131705s
Jan 19 21:43:47.575: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 19 21:43:47.578: INFO: Setting MaxTries for pod polling to 66 for networking test based on endpoint count 6
Jan 19 21:43:47.578: INFO: Breadth first check of 10.128.10.104 on host 10.0.146.42...
Jan 19 21:43:47.580: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.8.253:9080/dial?request=hostname&protocol=http&host=10.128.10.104&port=8083&tries=1'] Namespace:pod-network-test-8491 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:43:47.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:43:47.581: INFO: ExecWithOptions: Clientset creation
Jan 19 21:43:47.581: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8491/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.8.253%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.10.104%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 21:43:47.642: INFO: Waiting for responses: map[]
Jan 19 21:43:47.642: INFO: reached 10.128.10.104 after 0/1 tries
Jan 19 21:43:47.642: INFO: Breadth first check of 10.128.14.87 on host 10.0.151.158...
Jan 19 21:43:47.645: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.8.253:9080/dial?request=hostname&protocol=http&host=10.128.14.87&port=8083&tries=1'] Namespace:pod-network-test-8491 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:43:47.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:43:47.645: INFO: ExecWithOptions: Clientset creation
Jan 19 21:43:47.645: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8491/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.8.253%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.14.87%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 21:43:47.704: INFO: Waiting for responses: map[]
Jan 19 21:43:47.704: INFO: reached 10.128.14.87 after 0/1 tries
Jan 19 21:43:47.704: INFO: Breadth first check of 10.128.12.139 on host 10.0.171.213...
Jan 19 21:43:47.707: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.8.253:9080/dial?request=hostname&protocol=http&host=10.128.12.139&port=8083&tries=1'] Namespace:pod-network-test-8491 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:43:47.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:43:47.708: INFO: ExecWithOptions: Clientset creation
Jan 19 21:43:47.708: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8491/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.8.253%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.12.139%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 21:43:47.762: INFO: Waiting for responses: map[]
Jan 19 21:43:47.762: INFO: reached 10.128.12.139 after 0/1 tries
Jan 19 21:43:47.762: INFO: Breadth first check of 10.128.8.252 on host 10.0.172.44...
Jan 19 21:43:47.765: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.8.253:9080/dial?request=hostname&protocol=http&host=10.128.8.252&port=8083&tries=1'] Namespace:pod-network-test-8491 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:43:47.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:43:47.765: INFO: ExecWithOptions: Clientset creation
Jan 19 21:43:47.765: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8491/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.8.253%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.8.252%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 21:43:47.824: INFO: Waiting for responses: map[]
Jan 19 21:43:47.824: INFO: reached 10.128.8.252 after 0/1 tries
Jan 19 21:43:47.824: INFO: Breadth first check of 10.128.6.84 on host 10.0.188.71...
Jan 19 21:43:47.827: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.8.253:9080/dial?request=hostname&protocol=http&host=10.128.6.84&port=8083&tries=1'] Namespace:pod-network-test-8491 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:43:47.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:43:47.828: INFO: ExecWithOptions: Clientset creation
Jan 19 21:43:47.828: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8491/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.8.253%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.6.84%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 21:43:47.887: INFO: Waiting for responses: map[]
Jan 19 21:43:47.887: INFO: reached 10.128.6.84 after 0/1 tries
Jan 19 21:43:47.887: INFO: Breadth first check of 10.128.16.135 on host 10.0.207.77...
Jan 19 21:43:47.890: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.8.253:9080/dial?request=hostname&protocol=http&host=10.128.16.135&port=8083&tries=1'] Namespace:pod-network-test-8491 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:43:47.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:43:47.890: INFO: ExecWithOptions: Clientset creation
Jan 19 21:43:47.890: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8491/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.8.253%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.16.135%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 21:43:47.953: INFO: Waiting for responses: map[]
Jan 19 21:43:47.953: INFO: reached 10.128.16.135 after 0/1 tries
Jan 19 21:43:47.953: INFO: Going to retry 0 out of 6 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 19 21:43:47.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8491" for this suite. 01/19/23 21:43:47.957
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":243,"skipped":4512,"failed":0}
------------------------------
• [SLOW TEST] [24.626 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:43:23.336
    Jan 19 21:43:23.337: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pod-network-test 01/19/23 21:43:23.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:23.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:23.356
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-8491 01/19/23 21:43:23.358
    STEP: creating a selector 01/19/23 21:43:23.358
    STEP: Creating the service pods in kubernetes 01/19/23 21:43:23.358
    Jan 19 21:43:23.358: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 19 21:43:23.518: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8491" to be "running and ready"
    Jan 19 21:43:23.528: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.643927ms
    Jan 19 21:43:23.528: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:43:25.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013777771s
    Jan 19 21:43:25.532: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:43:27.531: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013122598s
    Jan 19 21:43:27.531: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:43:29.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.013527726s
    Jan 19 21:43:29.532: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:43:31.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.013892457s
    Jan 19 21:43:31.532: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:43:33.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.013635094s
    Jan 19 21:43:33.532: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:43:35.531: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.013393877s
    Jan 19 21:43:35.531: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:43:37.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.013823347s
    Jan 19 21:43:37.532: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:43:39.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.013664052s
    Jan 19 21:43:39.532: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:43:41.531: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.01319384s
    Jan 19 21:43:41.531: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:43:43.533: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014458888s
    Jan 19 21:43:43.533: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:43:45.532: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014336418s
    Jan 19 21:43:45.532: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 19 21:43:45.532: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 19 21:43:45.535: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8491" to be "running and ready"
    Jan 19 21:43:45.537: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.297355ms
    Jan 19 21:43:45.537: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 19 21:43:45.537: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 19 21:43:45.539: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8491" to be "running and ready"
    Jan 19 21:43:45.542: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.266296ms
    Jan 19 21:43:45.542: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 19 21:43:45.542: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan 19 21:43:45.544: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8491" to be "running and ready"
    Jan 19 21:43:45.546: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.210137ms
    Jan 19 21:43:45.546: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan 19 21:43:45.546: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jan 19 21:43:45.548: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-8491" to be "running and ready"
    Jan 19 21:43:45.550: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 2.117066ms
    Jan 19 21:43:45.550: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jan 19 21:43:45.550: INFO: Pod "netserver-4" satisfied condition "running and ready"
    Jan 19 21:43:45.553: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-8491" to be "running and ready"
    Jan 19 21:43:45.555: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 2.161887ms
    Jan 19 21:43:45.555: INFO: The phase of Pod netserver-5 is Running (Ready = true)
    Jan 19 21:43:45.555: INFO: Pod "netserver-5" satisfied condition "running and ready"
    STEP: Creating test pods 01/19/23 21:43:45.557
    Jan 19 21:43:45.568: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8491" to be "running"
    Jan 19 21:43:45.571: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.746694ms
    Jan 19 21:43:47.575: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007131705s
    Jan 19 21:43:47.575: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 19 21:43:47.578: INFO: Setting MaxTries for pod polling to 66 for networking test based on endpoint count 6
    Jan 19 21:43:47.578: INFO: Breadth first check of 10.128.10.104 on host 10.0.146.42...
    Jan 19 21:43:47.580: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.8.253:9080/dial?request=hostname&protocol=http&host=10.128.10.104&port=8083&tries=1'] Namespace:pod-network-test-8491 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:43:47.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:43:47.581: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:43:47.581: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8491/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.8.253%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.10.104%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 21:43:47.642: INFO: Waiting for responses: map[]
    Jan 19 21:43:47.642: INFO: reached 10.128.10.104 after 0/1 tries
    Jan 19 21:43:47.642: INFO: Breadth first check of 10.128.14.87 on host 10.0.151.158...
    Jan 19 21:43:47.645: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.8.253:9080/dial?request=hostname&protocol=http&host=10.128.14.87&port=8083&tries=1'] Namespace:pod-network-test-8491 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:43:47.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:43:47.645: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:43:47.645: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8491/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.8.253%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.14.87%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 21:43:47.704: INFO: Waiting for responses: map[]
    Jan 19 21:43:47.704: INFO: reached 10.128.14.87 after 0/1 tries
    Jan 19 21:43:47.704: INFO: Breadth first check of 10.128.12.139 on host 10.0.171.213...
    Jan 19 21:43:47.707: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.8.253:9080/dial?request=hostname&protocol=http&host=10.128.12.139&port=8083&tries=1'] Namespace:pod-network-test-8491 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:43:47.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:43:47.708: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:43:47.708: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8491/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.8.253%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.12.139%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 21:43:47.762: INFO: Waiting for responses: map[]
    Jan 19 21:43:47.762: INFO: reached 10.128.12.139 after 0/1 tries
    Jan 19 21:43:47.762: INFO: Breadth first check of 10.128.8.252 on host 10.0.172.44...
    Jan 19 21:43:47.765: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.8.253:9080/dial?request=hostname&protocol=http&host=10.128.8.252&port=8083&tries=1'] Namespace:pod-network-test-8491 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:43:47.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:43:47.765: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:43:47.765: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8491/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.8.253%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.8.252%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 21:43:47.824: INFO: Waiting for responses: map[]
    Jan 19 21:43:47.824: INFO: reached 10.128.8.252 after 0/1 tries
    Jan 19 21:43:47.824: INFO: Breadth first check of 10.128.6.84 on host 10.0.188.71...
    Jan 19 21:43:47.827: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.8.253:9080/dial?request=hostname&protocol=http&host=10.128.6.84&port=8083&tries=1'] Namespace:pod-network-test-8491 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:43:47.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:43:47.828: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:43:47.828: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8491/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.8.253%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.6.84%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 21:43:47.887: INFO: Waiting for responses: map[]
    Jan 19 21:43:47.887: INFO: reached 10.128.6.84 after 0/1 tries
    Jan 19 21:43:47.887: INFO: Breadth first check of 10.128.16.135 on host 10.0.207.77...
    Jan 19 21:43:47.890: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.8.253:9080/dial?request=hostname&protocol=http&host=10.128.16.135&port=8083&tries=1'] Namespace:pod-network-test-8491 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:43:47.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:43:47.890: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:43:47.890: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8491/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.8.253%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.128.16.135%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 21:43:47.953: INFO: Waiting for responses: map[]
    Jan 19 21:43:47.953: INFO: reached 10.128.16.135 after 0/1 tries
    Jan 19 21:43:47.953: INFO: Going to retry 0 out of 6 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 19 21:43:47.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8491" for this suite. 01/19/23 21:43:47.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:43:47.965
Jan 19 21:43:47.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 21:43:47.966
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:47.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:47.993
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 01/19/23 21:43:47.995
Jan 19 21:43:48.033: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-0b63d53c-dbf0-45f3-b967-ded442f10295" in namespace "emptydir-6430" to be "running"
Jan 19 21:43:48.039: INFO: Pod "pod-sharedvolume-0b63d53c-dbf0-45f3-b967-ded442f10295": Phase="Pending", Reason="", readiness=false. Elapsed: 6.494476ms
Jan 19 21:43:50.042: INFO: Pod "pod-sharedvolume-0b63d53c-dbf0-45f3-b967-ded442f10295": Phase="Running", Reason="", readiness=false. Elapsed: 2.009387252s
Jan 19 21:43:50.042: INFO: Pod "pod-sharedvolume-0b63d53c-dbf0-45f3-b967-ded442f10295" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/19/23 21:43:50.042
Jan 19 21:43:50.043: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6430 PodName:pod-sharedvolume-0b63d53c-dbf0-45f3-b967-ded442f10295 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:43:50.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:43:50.043: INFO: ExecWithOptions: Clientset creation
Jan 19 21:43:50.043: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/emptydir-6430/pods/pod-sharedvolume-0b63d53c-dbf0-45f3-b967-ded442f10295/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan 19 21:43:50.096: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 21:43:50.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6430" for this suite. 01/19/23 21:43:50.1
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":244,"skipped":4600,"failed":0}
------------------------------
• [2.140 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:43:47.965
    Jan 19 21:43:47.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 21:43:47.966
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:47.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:47.993
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 01/19/23 21:43:47.995
    Jan 19 21:43:48.033: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-0b63d53c-dbf0-45f3-b967-ded442f10295" in namespace "emptydir-6430" to be "running"
    Jan 19 21:43:48.039: INFO: Pod "pod-sharedvolume-0b63d53c-dbf0-45f3-b967-ded442f10295": Phase="Pending", Reason="", readiness=false. Elapsed: 6.494476ms
    Jan 19 21:43:50.042: INFO: Pod "pod-sharedvolume-0b63d53c-dbf0-45f3-b967-ded442f10295": Phase="Running", Reason="", readiness=false. Elapsed: 2.009387252s
    Jan 19 21:43:50.042: INFO: Pod "pod-sharedvolume-0b63d53c-dbf0-45f3-b967-ded442f10295" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/19/23 21:43:50.042
    Jan 19 21:43:50.043: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6430 PodName:pod-sharedvolume-0b63d53c-dbf0-45f3-b967-ded442f10295 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:43:50.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:43:50.043: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:43:50.043: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/emptydir-6430/pods/pod-sharedvolume-0b63d53c-dbf0-45f3-b967-ded442f10295/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan 19 21:43:50.096: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:43:50.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6430" for this suite. 01/19/23 21:43:50.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:43:50.107
Jan 19 21:43:50.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename endpointslicemirroring 01/19/23 21:43:50.107
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:50.124
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:50.127
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/19/23 21:43:50.146
Jan 19 21:43:50.156: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 01/19/23 21:43:52.166
Jan 19 21:43:52.174: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 01/19/23 21:43:54.177
Jan 19 21:43:54.185: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jan 19 21:43:56.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-6015" for this suite. 01/19/23 21:43:56.192
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":245,"skipped":4619,"failed":0}
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:43:50.107
    Jan 19 21:43:50.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename endpointslicemirroring 01/19/23 21:43:50.107
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:50.124
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:50.127
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/19/23 21:43:50.146
    Jan 19 21:43:50.156: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 01/19/23 21:43:52.166
    Jan 19 21:43:52.174: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 01/19/23 21:43:54.177
    Jan 19 21:43:54.185: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jan 19 21:43:56.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-6015" for this suite. 01/19/23 21:43:56.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:43:56.199
Jan 19 21:43:56.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:43:56.2
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:56.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:56.237
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jan 19 21:43:56.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 create -f -'
Jan 19 21:43:57.650: INFO: stderr: ""
Jan 19 21:43:57.650: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan 19 21:43:57.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 create -f -'
Jan 19 21:43:58.004: INFO: stderr: ""
Jan 19 21:43:58.004: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/19/23 21:43:58.004
Jan 19 21:43:59.007: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 21:43:59.007: INFO: Found 0 / 1
Jan 19 21:44:00.007: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 21:44:00.007: INFO: Found 1 / 1
Jan 19 21:44:00.007: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 19 21:44:00.010: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 21:44:00.010: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 19 21:44:00.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 describe pod agnhost-primary-8fsjh'
Jan 19 21:44:00.073: INFO: stderr: ""
Jan 19 21:44:00.073: INFO: stdout: "Name:             agnhost-primary-8fsjh\nNamespace:        kubectl-7498\nPriority:         0\nService Account:  default\nNode:             ip-10-0-172-44.ec2.internal/10.0.172.44\nStart Time:       Thu, 19 Jan 2023 21:43:57 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      k8s.ovn.org/pod-networks:\n                    {\"default\":{\"ip_addresses\":[\"10.128.8.255/23\"],\"mac_address\":\"0a:58:0a:80:08:ff\",\"gateway_ips\":[\"10.128.8.1\"],\"ip_address\":\"10.128.8.255/2...\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"ovn-kubernetes\",\n                        \"interface\": \"eth0\",\n                        \"ips\": [\n                            \"10.128.8.255\"\n                        ],\n                        \"mac\": \"0a:58:0a:80:08:ff\",\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"ovn-kubernetes\",\n                        \"interface\": \"eth0\",\n                        \"ips\": [\n                            \"10.128.8.255\"\n                        ],\n                        \"mac\": \"0a:58:0a:80:08:ff\",\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               10.128.8.255\nIPs:\n  IP:           10.128.8.255\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://78024fae743ee61e6d77ec620e43355087604c881775cae4e4678d0d711b69d2\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 19 Jan 2023 21:43:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5p7bv (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5p7bv:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-7498/agnhost-primary-8fsjh to ip-10-0-172-44.ec2.internal\n  Normal  AddedInterface  2s    multus             Add eth0 [10.128.8.255/23] from ovn-kubernetes\n  Normal  Pulled          2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created         2s    kubelet            Created container agnhost-primary\n  Normal  Started         2s    kubelet            Started container agnhost-primary\n"
Jan 19 21:44:00.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 describe rc agnhost-primary'
Jan 19 21:44:00.175: INFO: stderr: ""
Jan 19 21:44:00.175: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7498\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-8fsjh\n"
Jan 19 21:44:00.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 describe service agnhost-primary'
Jan 19 21:44:00.240: INFO: stderr: ""
Jan 19 21:44:00.240: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7498\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.30.14.142\nIPs:               172.30.14.142\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.128.8.255:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan 19 21:44:00.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 describe node ip-10-0-145-33.ec2.internal'
Jan 19 21:44:00.730: INFO: stderr: ""
Jan 19 21:44:00.730: INFO: stdout: "Name:               ip-10-0-145-33.ec2.internal\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.2xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-145-33.ec2.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=m5.2xlarge\n                    node.openshift.io/os_id=rhcos\n                    topology.ebs.csi.aws.com/zone=us-east-1a\n                    topology.kubernetes.io/region=us-east-1\n                    topology.kubernetes.io/zone=us-east-1a\nAnnotations:        cloud.network.openshift.io/egress-ipconfig:\n                      [{\"interface\":\"eni-0889be2bb7c9cb71d\",\"ifaddr\":{\"ipv4\":\"10.0.128.0/17\"},\"capacity\":{\"ipv4\":14,\"ipv6\":15}}]\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-02f5085d7672a417e\"}\n                    k8s.ovn.org/host-addresses: [\"10.0.145.33\"]\n                    k8s.ovn.org/l3-gateway-config:\n                      {\"default\":{\"mode\":\"shared\",\"interface-id\":\"br-ex_ip-10-0-145-33.ec2.internal\",\"mac-address\":\"0e:b4:88:bf:65:17\",\"ip-addresses\":[\"10.0.145...\n                    k8s.ovn.org/node-chassis-id: 79265869-8869-4097-bc77-ecbedae57123\n                    k8s.ovn.org/node-gateway-router-lrp-ifaddr: {\"ipv4\":\"100.64.0.4/16\"}\n                    k8s.ovn.org/node-mgmt-port-mac-address: 6e:be:4e:cc:f6:d7\n                    k8s.ovn.org/node-primary-ifaddr: {\"ipv4\":\"10.0.145.33/17\"}\n                    k8s.ovn.org/node-subnets: {\"default\":\"10.128.0.0/23\"}\n                    machine.openshift.io/machine: openshift-machine-api/sdcicd-cncf-412-h5fpx-master-1\n                    machineconfiguration.openshift.io/controlPlaneTopology: HighlyAvailable\n                    machineconfiguration.openshift.io/currentConfig: rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n                    machineconfiguration.openshift.io/desiredConfig: rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n                    machineconfiguration.openshift.io/desiredDrain: uncordon-rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n                    machineconfiguration.openshift.io/lastAppliedDrain: uncordon-rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 19 Jan 2023 18:09:03 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-145-33.ec2.internal\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 19 Jan 2023 21:43:53 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 19 Jan 2023 21:40:59 +0000   Thu, 19 Jan 2023 18:47:26 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 19 Jan 2023 21:40:59 +0000   Thu, 19 Jan 2023 18:47:26 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 19 Jan 2023 21:40:59 +0000   Thu, 19 Jan 2023 18:47:26 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 19 Jan 2023 21:40:59 +0000   Thu, 19 Jan 2023 18:47:26 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.145.33\n  Hostname:     ip-10-0-145-33.ec2.internal\n  InternalDNS:  ip-10-0-145-33.ec2.internal\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         8\n  ephemeral-storage:           366466028Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      32442888Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         7910m\n  ephemeral-storage:           336661349022\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      29194760Ki\n  pods:                        250\nSystem Info:\n  Machine ID:                             ec2966b64317e7235e4a9cddfa04e90c\n  System UUID:                            ec2966b6-4317-e723-5e4a-9cddfa04e90c\n  Boot ID:                                e614ab92-0fdd-4da9-980a-6728469e4541\n  Kernel Version:                         4.18.0-372.40.1.el8_6.x86_64\n  OS Image:                               Red Hat Enterprise Linux CoreOS 412.86.202301061548-0 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.25.1-5.rhaos4.12.git6005903.el8\n  Kubelet Version:                        v1.25.4+77bec7a\n  Kube-Proxy Version:                     v1.25.4+77bec7a\nProviderID:                               aws:///us-east-1a/i-02f5085d7672a417e\nNon-terminated Pods:                      (33 in total)\n  Namespace                               Name                                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                          ------------  ----------  ---------------  -------------  ---\n  openshift-apiserver                     apiserver-cbc988689-wn95d                                     110m (1%)     0 (0%)      250Mi (0%)       0 (0%)         3h1m\n  openshift-authentication                oauth-openshift-7dd4f6fff5-n6jzt                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h1m\n  openshift-cluster-csi-drivers           aws-ebs-csi-driver-node-x2pjf                                 30m (0%)      0 (0%)      150Mi (0%)       0 (0%)         3h32m\n  openshift-cluster-node-tuning-operator  tuned-t6l2k                                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h31m\n  openshift-controller-manager            controller-manager-78d4d9d5f7-r5k5q                           100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         3h1m\n  openshift-dns                           dns-default-k69kv                                             60m (0%)      0 (0%)      110Mi (0%)       0 (0%)         3h32m\n  openshift-dns                           node-resolver-smzlw                                           5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         3h32m\n  openshift-etcd                          etcd-guard-ip-10-0-145-33.ec2.internal                        10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         176m\n  openshift-etcd                          etcd-ip-10-0-145-33.ec2.internal                              360m (4%)     0 (0%)      910Mi (3%)       0 (0%)         3h14m\n  openshift-image-registry                node-ca-vv5vx                                                 10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h25m\n  openshift-kube-apiserver                kube-apiserver-guard-ip-10-0-145-33.ec2.internal              10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         176m\n  openshift-kube-apiserver                kube-apiserver-ip-10-0-145-33.ec2.internal                    290m (3%)     0 (0%)      1224Mi (4%)      0 (0%)         3h2m\n  openshift-kube-controller-manager       kube-controller-manager-guard-ip-10-0-145-33.ec2.internal     10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         176m\n  openshift-kube-controller-manager       kube-controller-manager-ip-10-0-145-33.ec2.internal           80m (1%)      0 (0%)      500Mi (1%)       0 (0%)         175m\n  openshift-kube-scheduler                openshift-kube-scheduler-guard-ip-10-0-145-33.ec2.internal    10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         176m\n  openshift-kube-scheduler                openshift-kube-scheduler-ip-10-0-145-33.ec2.internal          25m (0%)      0 (0%)      150Mi (0%)       0 (0%)         175m\n  openshift-machine-config-operator       machine-config-daemon-6dt2j                                   40m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h32m\n  openshift-machine-config-operator       machine-config-server-nlgvh                                   20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h31m\n  openshift-marketplace                   community-operators-mkbmk                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         49m\n  openshift-monitoring                    node-exporter-fjm4r                                           9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         3h25m\n  openshift-monitoring                    sre-dns-latency-exporter-nj2tz                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h14m\n  openshift-multus                        multus-additional-cni-plugins-v4q6v                           10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h34m\n  openshift-multus                        multus-bkqvw                                                  10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         3h34m\n  openshift-multus                        network-metrics-daemon-wbdfm                                  20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         3h34m\n  openshift-network-diagnostics           network-check-target-mvwh6                                    10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         3h34m\n  openshift-oauth-apiserver               apiserver-bc46d7c9f-5hwxk                                     150m (1%)     0 (0%)      200Mi (0%)       0 (0%)         3h1m\n  openshift-ovn-kubernetes                ovnkube-master-qnsmv                                          60m (0%)      0 (0%)      1520Mi (5%)      0 (0%)         3h34m\n  openshift-ovn-kubernetes                ovnkube-node-wzfjd                                            50m (0%)      0 (0%)      660Mi (2%)       0 (0%)         3h34m\n  openshift-route-controller-manager      route-controller-manager-56dc4c96f9-rm5cl                     100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         3h1m\n  openshift-security                      audit-exporter-lb69h                                          100m (1%)     100m (1%)   256Mi (0%)       256Mi (0%)     3h13m\n  openshift-security                      splunkforwarder-ds-ztkxf                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h10m\n  openshift-validation-webhook            validation-webhook-28fwj                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h14m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-gqvct       0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         1719m (21%)   100m (1%)\n  memory                      6738Mi (23%)  256Mi (0%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  hugepages-1Gi               0 (0%)        0 (0%)\n  hugepages-2Mi               0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\nEvents:\n  Type     Reason                     Age                    From                 Message\n  ----     ------                     ----                   ----                 -------\n  Normal   NodeHasNoDiskPressure      3h34m (x8 over 3h35m)  kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientMemory    3h34m (x8 over 3h35m)  kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeHasSufficientMemory\n  Normal   RegisteredNode             3h34m                  node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n  Warning  ErrorReconcilingNode       3h33m (x2 over 3h33m)  controlplane         [k8s.ovn.org/node-chassis-id annotation not found for node ip-10-0-145-33.ec2.internal, macAddress annotation not found for node \"ip-10-0-145-33.ec2.internal\" , k8s.ovn.org/l3-gateway-config annotation not found for node \"ip-10-0-145-33.ec2.internal\"]\n  Normal   Uncordon                   3h30m                  machineconfigdaemon  Update completed for config rendered-master-4767563adc2c8e9ed43340e5fee7d6ea and node has been uncordoned\n  Normal   NodeDone                   3h30m                  machineconfigdaemon  Setting node ip-10-0-145-33.ec2.internal, currentConfig rendered-master-4767563adc2c8e9ed43340e5fee7d6ea to Done\n  Normal   ConfigDriftMonitorStarted  3h30m                  machineconfigdaemon  Config Drift Monitor started, watching against rendered-master-4767563adc2c8e9ed43340e5fee7d6ea\n  Normal   RegisteredNode             3h25m                  node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n  Warning  ErrorReconcilingNode       3h25m                  controlplane         error creating gateway for node ip-10-0-145-33.ec2.internal: failed to init shared interface gateway: failed to sync stale SNATs on node ip-10-0-145-33.ec2.internal: unable to fetch podIPs for pod openshift-authentication/oauth-openshift-7886664d5d-btrzn\n  Normal   RegisteredNode             3h20m                  node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n  Normal   RegisteredNode             3h17m                  node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n  Normal   RegisteredNode             3h10m                  node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n  Normal   RegisteredNode             3h4m                   node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n  Normal   ConfigDriftMonitorStopped  3h1m                   machineconfigdaemon  Config Drift Monitor stopped\n  Normal   Cordon                     3h1m                   machineconfigdaemon  Cordoned node to apply update\n  Normal   Drain                      3h1m                   machineconfigdaemon  Draining node to update config.\n  Normal   NodeNotSchedulable         3h1m                   kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeNotSchedulable\n  Normal   Reboot                     179m                   machineconfigdaemon  Node will reboot into config rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n  Normal   OSUpdateStaged             179m                   machineconfigdaemon  Changes to OS staged\n  Normal   PendingConfig              179m                   machineconfigdaemon  Written pending config rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n  Normal   OSUpdateStarted            179m                   machineconfigdaemon  \n  Normal   OSUpgradeSkipped           179m                   machineconfigdaemon  OS upgrade skipped; new MachineConfig (rendered-master-e3c6e57e75036edd9a639f4a4f480cf1) has same OS image (quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6db665511f305ef230a2c752d836fe073e80550dc21cede3c55cf44db01db365) as old MachineConfig (rendered-master-4767563adc2c8e9ed43340e5fee7d6ea)\n  Normal   NodeNotReady               179m                   node-controller      Node ip-10-0-145-33.ec2.internal status is now: NodeNotReady\n  Normal   NodeHasNoDiskPressure      176m (x2 over 176m)    kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeHasNoDiskPressure\n  Normal   Starting                   176m                   kubelet              Starting kubelet.\n  Normal   NodeAllocatableEnforced    176m                   kubelet              Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory    176m (x2 over 176m)    kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeHasSufficientMemory\n  Normal   NodeHasSufficientPID       176m (x2 over 176m)    kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeHasSufficientPID\n  Warning  Rebooted                   176m                   kubelet              Node ip-10-0-145-33.ec2.internal has been rebooted, boot id: e614ab92-0fdd-4da9-980a-6728469e4541\n  Normal   NodeReady                  176m                   kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeReady\n  Normal   NodeNotSchedulable         176m                   kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeNotSchedulable\n  Normal   NodeSchedulable            176m                   kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeSchedulable\n  Normal   NodeDone                   176m                   machineconfigdaemon  Setting node ip-10-0-145-33.ec2.internal, currentConfig rendered-master-e3c6e57e75036edd9a639f4a4f480cf1 to Done\n  Normal   Uncordon                   176m                   machineconfigdaemon  Update completed for config rendered-master-e3c6e57e75036edd9a639f4a4f480cf1 and node has been uncordoned\n  Normal   ConfigDriftMonitorStarted  176m                   machineconfigdaemon  Config Drift Monitor started, watching against rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n  Normal   RegisteredNode             174m                   node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n"
Jan 19 21:44:00.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 describe namespace kubectl-7498'
Jan 19 21:44:00.793: INFO: stderr: ""
Jan 19 21:44:00.793: INFO: stdout: "Name:         kubectl-7498\nLabels:       e2e-framework=kubectl\n              e2e-run=f28d6c8f-a821-4fde-a3c3-79708116053c\n              kubernetes.io/metadata.name=kubectl-7498\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c61,c20\n              openshift.io/sa.scc.supplemental-groups: 1003700000/10000\n              openshift.io/sa.scc.uid-range: 1003700000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:44:00.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7498" for this suite. 01/19/23 21:44:00.798
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":246,"skipped":4651,"failed":0}
------------------------------
• [4.604 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:43:56.199
    Jan 19 21:43:56.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:43:56.2
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:43:56.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:43:56.237
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jan 19 21:43:56.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 create -f -'
    Jan 19 21:43:57.650: INFO: stderr: ""
    Jan 19 21:43:57.650: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan 19 21:43:57.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 create -f -'
    Jan 19 21:43:58.004: INFO: stderr: ""
    Jan 19 21:43:58.004: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/19/23 21:43:58.004
    Jan 19 21:43:59.007: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 21:43:59.007: INFO: Found 0 / 1
    Jan 19 21:44:00.007: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 21:44:00.007: INFO: Found 1 / 1
    Jan 19 21:44:00.007: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 19 21:44:00.010: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 21:44:00.010: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 19 21:44:00.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 describe pod agnhost-primary-8fsjh'
    Jan 19 21:44:00.073: INFO: stderr: ""
    Jan 19 21:44:00.073: INFO: stdout: "Name:             agnhost-primary-8fsjh\nNamespace:        kubectl-7498\nPriority:         0\nService Account:  default\nNode:             ip-10-0-172-44.ec2.internal/10.0.172.44\nStart Time:       Thu, 19 Jan 2023 21:43:57 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      k8s.ovn.org/pod-networks:\n                    {\"default\":{\"ip_addresses\":[\"10.128.8.255/23\"],\"mac_address\":\"0a:58:0a:80:08:ff\",\"gateway_ips\":[\"10.128.8.1\"],\"ip_address\":\"10.128.8.255/2...\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"ovn-kubernetes\",\n                        \"interface\": \"eth0\",\n                        \"ips\": [\n                            \"10.128.8.255\"\n                        ],\n                        \"mac\": \"0a:58:0a:80:08:ff\",\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"ovn-kubernetes\",\n                        \"interface\": \"eth0\",\n                        \"ips\": [\n                            \"10.128.8.255\"\n                        ],\n                        \"mac\": \"0a:58:0a:80:08:ff\",\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               10.128.8.255\nIPs:\n  IP:           10.128.8.255\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://78024fae743ee61e6d77ec620e43355087604c881775cae4e4678d0d711b69d2\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 19 Jan 2023 21:43:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-5p7bv (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-5p7bv:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       2s    default-scheduler  Successfully assigned kubectl-7498/agnhost-primary-8fsjh to ip-10-0-172-44.ec2.internal\n  Normal  AddedInterface  2s    multus             Add eth0 [10.128.8.255/23] from ovn-kubernetes\n  Normal  Pulled          2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created         2s    kubelet            Created container agnhost-primary\n  Normal  Started         2s    kubelet            Started container agnhost-primary\n"
    Jan 19 21:44:00.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 describe rc agnhost-primary'
    Jan 19 21:44:00.175: INFO: stderr: ""
    Jan 19 21:44:00.175: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7498\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-8fsjh\n"
    Jan 19 21:44:00.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 describe service agnhost-primary'
    Jan 19 21:44:00.240: INFO: stderr: ""
    Jan 19 21:44:00.240: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7498\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.30.14.142\nIPs:               172.30.14.142\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.128.8.255:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan 19 21:44:00.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 describe node ip-10-0-145-33.ec2.internal'
    Jan 19 21:44:00.730: INFO: stderr: ""
    Jan 19 21:44:00.730: INFO: stdout: "Name:               ip-10-0-145-33.ec2.internal\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.2xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east-1\n                    failure-domain.beta.kubernetes.io/zone=us-east-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-145-33.ec2.internal\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node-role.kubernetes.io/master=\n                    node.kubernetes.io/instance-type=m5.2xlarge\n                    node.openshift.io/os_id=rhcos\n                    topology.ebs.csi.aws.com/zone=us-east-1a\n                    topology.kubernetes.io/region=us-east-1\n                    topology.kubernetes.io/zone=us-east-1a\nAnnotations:        cloud.network.openshift.io/egress-ipconfig:\n                      [{\"interface\":\"eni-0889be2bb7c9cb71d\",\"ifaddr\":{\"ipv4\":\"10.0.128.0/17\"},\"capacity\":{\"ipv4\":14,\"ipv6\":15}}]\n                    csi.volume.kubernetes.io/nodeid: {\"ebs.csi.aws.com\":\"i-02f5085d7672a417e\"}\n                    k8s.ovn.org/host-addresses: [\"10.0.145.33\"]\n                    k8s.ovn.org/l3-gateway-config:\n                      {\"default\":{\"mode\":\"shared\",\"interface-id\":\"br-ex_ip-10-0-145-33.ec2.internal\",\"mac-address\":\"0e:b4:88:bf:65:17\",\"ip-addresses\":[\"10.0.145...\n                    k8s.ovn.org/node-chassis-id: 79265869-8869-4097-bc77-ecbedae57123\n                    k8s.ovn.org/node-gateway-router-lrp-ifaddr: {\"ipv4\":\"100.64.0.4/16\"}\n                    k8s.ovn.org/node-mgmt-port-mac-address: 6e:be:4e:cc:f6:d7\n                    k8s.ovn.org/node-primary-ifaddr: {\"ipv4\":\"10.0.145.33/17\"}\n                    k8s.ovn.org/node-subnets: {\"default\":\"10.128.0.0/23\"}\n                    machine.openshift.io/machine: openshift-machine-api/sdcicd-cncf-412-h5fpx-master-1\n                    machineconfiguration.openshift.io/controlPlaneTopology: HighlyAvailable\n                    machineconfiguration.openshift.io/currentConfig: rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n                    machineconfiguration.openshift.io/desiredConfig: rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n                    machineconfiguration.openshift.io/desiredDrain: uncordon-rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n                    machineconfiguration.openshift.io/lastAppliedDrain: uncordon-rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n                    machineconfiguration.openshift.io/reason: \n                    machineconfiguration.openshift.io/state: Done\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 19 Jan 2023 18:09:03 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-145-33.ec2.internal\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 19 Jan 2023 21:43:53 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 19 Jan 2023 21:40:59 +0000   Thu, 19 Jan 2023 18:47:26 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 19 Jan 2023 21:40:59 +0000   Thu, 19 Jan 2023 18:47:26 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 19 Jan 2023 21:40:59 +0000   Thu, 19 Jan 2023 18:47:26 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 19 Jan 2023 21:40:59 +0000   Thu, 19 Jan 2023 18:47:26 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:   10.0.145.33\n  Hostname:     ip-10-0-145-33.ec2.internal\n  InternalDNS:  ip-10-0-145-33.ec2.internal\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         8\n  ephemeral-storage:           366466028Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      32442888Ki\n  pods:                        250\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         7910m\n  ephemeral-storage:           336661349022\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      29194760Ki\n  pods:                        250\nSystem Info:\n  Machine ID:                             ec2966b64317e7235e4a9cddfa04e90c\n  System UUID:                            ec2966b6-4317-e723-5e4a-9cddfa04e90c\n  Boot ID:                                e614ab92-0fdd-4da9-980a-6728469e4541\n  Kernel Version:                         4.18.0-372.40.1.el8_6.x86_64\n  OS Image:                               Red Hat Enterprise Linux CoreOS 412.86.202301061548-0 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.25.1-5.rhaos4.12.git6005903.el8\n  Kubelet Version:                        v1.25.4+77bec7a\n  Kube-Proxy Version:                     v1.25.4+77bec7a\nProviderID:                               aws:///us-east-1a/i-02f5085d7672a417e\nNon-terminated Pods:                      (33 in total)\n  Namespace                               Name                                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                          ------------  ----------  ---------------  -------------  ---\n  openshift-apiserver                     apiserver-cbc988689-wn95d                                     110m (1%)     0 (0%)      250Mi (0%)       0 (0%)         3h1m\n  openshift-authentication                oauth-openshift-7dd4f6fff5-n6jzt                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h1m\n  openshift-cluster-csi-drivers           aws-ebs-csi-driver-node-x2pjf                                 30m (0%)      0 (0%)      150Mi (0%)       0 (0%)         3h32m\n  openshift-cluster-node-tuning-operator  tuned-t6l2k                                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h31m\n  openshift-controller-manager            controller-manager-78d4d9d5f7-r5k5q                           100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         3h1m\n  openshift-dns                           dns-default-k69kv                                             60m (0%)      0 (0%)      110Mi (0%)       0 (0%)         3h32m\n  openshift-dns                           node-resolver-smzlw                                           5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         3h32m\n  openshift-etcd                          etcd-guard-ip-10-0-145-33.ec2.internal                        10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         176m\n  openshift-etcd                          etcd-ip-10-0-145-33.ec2.internal                              360m (4%)     0 (0%)      910Mi (3%)       0 (0%)         3h14m\n  openshift-image-registry                node-ca-vv5vx                                                 10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h25m\n  openshift-kube-apiserver                kube-apiserver-guard-ip-10-0-145-33.ec2.internal              10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         176m\n  openshift-kube-apiserver                kube-apiserver-ip-10-0-145-33.ec2.internal                    290m (3%)     0 (0%)      1224Mi (4%)      0 (0%)         3h2m\n  openshift-kube-controller-manager       kube-controller-manager-guard-ip-10-0-145-33.ec2.internal     10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         176m\n  openshift-kube-controller-manager       kube-controller-manager-ip-10-0-145-33.ec2.internal           80m (1%)      0 (0%)      500Mi (1%)       0 (0%)         175m\n  openshift-kube-scheduler                openshift-kube-scheduler-guard-ip-10-0-145-33.ec2.internal    10m (0%)      0 (0%)      5Mi (0%)         0 (0%)         176m\n  openshift-kube-scheduler                openshift-kube-scheduler-ip-10-0-145-33.ec2.internal          25m (0%)      0 (0%)      150Mi (0%)       0 (0%)         175m\n  openshift-machine-config-operator       machine-config-daemon-6dt2j                                   40m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h32m\n  openshift-machine-config-operator       machine-config-server-nlgvh                                   20m (0%)      0 (0%)      50Mi (0%)        0 (0%)         3h31m\n  openshift-marketplace                   community-operators-mkbmk                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         49m\n  openshift-monitoring                    node-exporter-fjm4r                                           9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         3h25m\n  openshift-monitoring                    sre-dns-latency-exporter-nj2tz                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h14m\n  openshift-multus                        multus-additional-cni-plugins-v4q6v                           10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         3h34m\n  openshift-multus                        multus-bkqvw                                                  10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         3h34m\n  openshift-multus                        network-metrics-daemon-wbdfm                                  20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         3h34m\n  openshift-network-diagnostics           network-check-target-mvwh6                                    10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         3h34m\n  openshift-oauth-apiserver               apiserver-bc46d7c9f-5hwxk                                     150m (1%)     0 (0%)      200Mi (0%)       0 (0%)         3h1m\n  openshift-ovn-kubernetes                ovnkube-master-qnsmv                                          60m (0%)      0 (0%)      1520Mi (5%)      0 (0%)         3h34m\n  openshift-ovn-kubernetes                ovnkube-node-wzfjd                                            50m (0%)      0 (0%)      660Mi (2%)       0 (0%)         3h34m\n  openshift-route-controller-manager      route-controller-manager-56dc4c96f9-rm5cl                     100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         3h1m\n  openshift-security                      audit-exporter-lb69h                                          100m (1%)     100m (1%)   256Mi (0%)       256Mi (0%)     3h13m\n  openshift-security                      splunkforwarder-ds-ztkxf                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h10m\n  openshift-validation-webhook            validation-webhook-28fwj                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h14m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-gqvct       0 (0%)        0 (0%)      0 (0%)           0 (0%)         74m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests      Limits\n  --------                    --------      ------\n  cpu                         1719m (21%)   100m (1%)\n  memory                      6738Mi (23%)  256Mi (0%)\n  ephemeral-storage           0 (0%)        0 (0%)\n  hugepages-1Gi               0 (0%)        0 (0%)\n  hugepages-2Mi               0 (0%)        0 (0%)\n  attachable-volumes-aws-ebs  0             0\nEvents:\n  Type     Reason                     Age                    From                 Message\n  ----     ------                     ----                   ----                 -------\n  Normal   NodeHasNoDiskPressure      3h34m (x8 over 3h35m)  kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientMemory    3h34m (x8 over 3h35m)  kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeHasSufficientMemory\n  Normal   RegisteredNode             3h34m                  node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n  Warning  ErrorReconcilingNode       3h33m (x2 over 3h33m)  controlplane         [k8s.ovn.org/node-chassis-id annotation not found for node ip-10-0-145-33.ec2.internal, macAddress annotation not found for node \"ip-10-0-145-33.ec2.internal\" , k8s.ovn.org/l3-gateway-config annotation not found for node \"ip-10-0-145-33.ec2.internal\"]\n  Normal   Uncordon                   3h30m                  machineconfigdaemon  Update completed for config rendered-master-4767563adc2c8e9ed43340e5fee7d6ea and node has been uncordoned\n  Normal   NodeDone                   3h30m                  machineconfigdaemon  Setting node ip-10-0-145-33.ec2.internal, currentConfig rendered-master-4767563adc2c8e9ed43340e5fee7d6ea to Done\n  Normal   ConfigDriftMonitorStarted  3h30m                  machineconfigdaemon  Config Drift Monitor started, watching against rendered-master-4767563adc2c8e9ed43340e5fee7d6ea\n  Normal   RegisteredNode             3h25m                  node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n  Warning  ErrorReconcilingNode       3h25m                  controlplane         error creating gateway for node ip-10-0-145-33.ec2.internal: failed to init shared interface gateway: failed to sync stale SNATs on node ip-10-0-145-33.ec2.internal: unable to fetch podIPs for pod openshift-authentication/oauth-openshift-7886664d5d-btrzn\n  Normal   RegisteredNode             3h20m                  node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n  Normal   RegisteredNode             3h17m                  node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n  Normal   RegisteredNode             3h10m                  node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n  Normal   RegisteredNode             3h4m                   node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n  Normal   ConfigDriftMonitorStopped  3h1m                   machineconfigdaemon  Config Drift Monitor stopped\n  Normal   Cordon                     3h1m                   machineconfigdaemon  Cordoned node to apply update\n  Normal   Drain                      3h1m                   machineconfigdaemon  Draining node to update config.\n  Normal   NodeNotSchedulable         3h1m                   kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeNotSchedulable\n  Normal   Reboot                     179m                   machineconfigdaemon  Node will reboot into config rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n  Normal   OSUpdateStaged             179m                   machineconfigdaemon  Changes to OS staged\n  Normal   PendingConfig              179m                   machineconfigdaemon  Written pending config rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n  Normal   OSUpdateStarted            179m                   machineconfigdaemon  \n  Normal   OSUpgradeSkipped           179m                   machineconfigdaemon  OS upgrade skipped; new MachineConfig (rendered-master-e3c6e57e75036edd9a639f4a4f480cf1) has same OS image (quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:6db665511f305ef230a2c752d836fe073e80550dc21cede3c55cf44db01db365) as old MachineConfig (rendered-master-4767563adc2c8e9ed43340e5fee7d6ea)\n  Normal   NodeNotReady               179m                   node-controller      Node ip-10-0-145-33.ec2.internal status is now: NodeNotReady\n  Normal   NodeHasNoDiskPressure      176m (x2 over 176m)    kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeHasNoDiskPressure\n  Normal   Starting                   176m                   kubelet              Starting kubelet.\n  Normal   NodeAllocatableEnforced    176m                   kubelet              Updated Node Allocatable limit across pods\n  Normal   NodeHasSufficientMemory    176m (x2 over 176m)    kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeHasSufficientMemory\n  Normal   NodeHasSufficientPID       176m (x2 over 176m)    kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeHasSufficientPID\n  Warning  Rebooted                   176m                   kubelet              Node ip-10-0-145-33.ec2.internal has been rebooted, boot id: e614ab92-0fdd-4da9-980a-6728469e4541\n  Normal   NodeReady                  176m                   kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeReady\n  Normal   NodeNotSchedulable         176m                   kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeNotSchedulable\n  Normal   NodeSchedulable            176m                   kubelet              Node ip-10-0-145-33.ec2.internal status is now: NodeSchedulable\n  Normal   NodeDone                   176m                   machineconfigdaemon  Setting node ip-10-0-145-33.ec2.internal, currentConfig rendered-master-e3c6e57e75036edd9a639f4a4f480cf1 to Done\n  Normal   Uncordon                   176m                   machineconfigdaemon  Update completed for config rendered-master-e3c6e57e75036edd9a639f4a4f480cf1 and node has been uncordoned\n  Normal   ConfigDriftMonitorStarted  176m                   machineconfigdaemon  Config Drift Monitor started, watching against rendered-master-e3c6e57e75036edd9a639f4a4f480cf1\n  Normal   RegisteredNode             174m                   node-controller      Node ip-10-0-145-33.ec2.internal event: Registered Node ip-10-0-145-33.ec2.internal in Controller\n"
    Jan 19 21:44:00.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-7498 describe namespace kubectl-7498'
    Jan 19 21:44:00.793: INFO: stderr: ""
    Jan 19 21:44:00.793: INFO: stdout: "Name:         kubectl-7498\nLabels:       e2e-framework=kubectl\n              e2e-run=f28d6c8f-a821-4fde-a3c3-79708116053c\n              kubernetes.io/metadata.name=kubectl-7498\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c61,c20\n              openshift.io/sa.scc.supplemental-groups: 1003700000/10000\n              openshift.io/sa.scc.uid-range: 1003700000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:44:00.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7498" for this suite. 01/19/23 21:44:00.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:44:00.804
Jan 19 21:44:00.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 21:44:00.805
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:00.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:00.823
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 01/19/23 21:44:00.825
Jan 19 21:44:00.868: INFO: Waiting up to 5m0s for pod "pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054" in namespace "emptydir-9381" to be "Succeeded or Failed"
Jan 19 21:44:00.873: INFO: Pod "pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054": Phase="Pending", Reason="", readiness=false. Elapsed: 4.556656ms
Jan 19 21:44:02.881: INFO: Pod "pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01265725s
Jan 19 21:44:04.887: INFO: Pod "pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018386445s
STEP: Saw pod success 01/19/23 21:44:04.887
Jan 19 21:44:04.887: INFO: Pod "pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054" satisfied condition "Succeeded or Failed"
Jan 19 21:44:04.889: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054 container test-container: <nil>
STEP: delete the pod 01/19/23 21:44:04.9
Jan 19 21:44:04.913: INFO: Waiting for pod pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054 to disappear
Jan 19 21:44:04.916: INFO: Pod pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 21:44:04.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9381" for this suite. 01/19/23 21:44:04.921
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":247,"skipped":4662,"failed":0}
------------------------------
• [4.123 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:44:00.804
    Jan 19 21:44:00.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 21:44:00.805
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:00.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:00.823
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 01/19/23 21:44:00.825
    Jan 19 21:44:00.868: INFO: Waiting up to 5m0s for pod "pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054" in namespace "emptydir-9381" to be "Succeeded or Failed"
    Jan 19 21:44:00.873: INFO: Pod "pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054": Phase="Pending", Reason="", readiness=false. Elapsed: 4.556656ms
    Jan 19 21:44:02.881: INFO: Pod "pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01265725s
    Jan 19 21:44:04.887: INFO: Pod "pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018386445s
    STEP: Saw pod success 01/19/23 21:44:04.887
    Jan 19 21:44:04.887: INFO: Pod "pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054" satisfied condition "Succeeded or Failed"
    Jan 19 21:44:04.889: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054 container test-container: <nil>
    STEP: delete the pod 01/19/23 21:44:04.9
    Jan 19 21:44:04.913: INFO: Waiting for pod pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054 to disappear
    Jan 19 21:44:04.916: INFO: Pod pod-dc4ad0a7-bd0f-49c3-8bab-f43ddfb2e054 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:44:04.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9381" for this suite. 01/19/23 21:44:04.921
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:44:04.927
Jan 19 21:44:04.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubelet-test 01/19/23 21:44:04.928
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:04.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:04.963
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
W0119 21:44:04.998427      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:44:04.998: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92" in namespace "kubelet-test-6542" to be "running and ready"
Jan 19 21:44:05.000: INFO: Pod "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.360834ms
Jan 19 21:44:05.000: INFO: The phase of Pod busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:44:07.003: INFO: Pod "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92": Phase="Running", Reason="", readiness=true. Elapsed: 2.005493313s
Jan 19 21:44:07.004: INFO: The phase of Pod busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92 is Running (Ready = true)
Jan 19 21:44:07.004: INFO: Pod "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 19 21:44:07.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6542" for this suite. 01/19/23 21:44:07.015
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":248,"skipped":4662,"failed":0}
------------------------------
• [2.092 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:44:04.927
    Jan 19 21:44:04.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubelet-test 01/19/23 21:44:04.928
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:04.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:04.963
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    W0119 21:44:04.998427      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:44:04.998: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92" in namespace "kubelet-test-6542" to be "running and ready"
    Jan 19 21:44:05.000: INFO: Pod "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92": Phase="Pending", Reason="", readiness=false. Elapsed: 2.360834ms
    Jan 19 21:44:05.000: INFO: The phase of Pod busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:44:07.003: INFO: Pod "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92": Phase="Running", Reason="", readiness=true. Elapsed: 2.005493313s
    Jan 19 21:44:07.004: INFO: The phase of Pod busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92 is Running (Ready = true)
    Jan 19 21:44:07.004: INFO: Pod "busybox-readonly-fsb52f3667-8403-42ed-b787-d0e9ad04ca92" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 19 21:44:07.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6542" for this suite. 01/19/23 21:44:07.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:44:07.021
Jan 19 21:44:07.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 21:44:07.021
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:07.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:07.041
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/19/23 21:44:07.043
Jan 19 21:44:07.070: INFO: Waiting up to 5m0s for pod "pod-96432054-1b79-4b90-90b5-a24cbd39d7cd" in namespace "emptydir-5661" to be "Succeeded or Failed"
Jan 19 21:44:07.076: INFO: Pod "pod-96432054-1b79-4b90-90b5-a24cbd39d7cd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274336ms
Jan 19 21:44:09.079: INFO: Pod "pod-96432054-1b79-4b90-90b5-a24cbd39d7cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009562665s
Jan 19 21:44:11.079: INFO: Pod "pod-96432054-1b79-4b90-90b5-a24cbd39d7cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009626733s
STEP: Saw pod success 01/19/23 21:44:11.079
Jan 19 21:44:11.080: INFO: Pod "pod-96432054-1b79-4b90-90b5-a24cbd39d7cd" satisfied condition "Succeeded or Failed"
Jan 19 21:44:11.082: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-96432054-1b79-4b90-90b5-a24cbd39d7cd container test-container: <nil>
STEP: delete the pod 01/19/23 21:44:11.087
Jan 19 21:44:11.096: INFO: Waiting for pod pod-96432054-1b79-4b90-90b5-a24cbd39d7cd to disappear
Jan 19 21:44:11.098: INFO: Pod pod-96432054-1b79-4b90-90b5-a24cbd39d7cd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 21:44:11.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5661" for this suite. 01/19/23 21:44:11.102
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":249,"skipped":4684,"failed":0}
------------------------------
• [4.086 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:44:07.021
    Jan 19 21:44:07.021: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 21:44:07.021
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:07.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:07.041
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/19/23 21:44:07.043
    Jan 19 21:44:07.070: INFO: Waiting up to 5m0s for pod "pod-96432054-1b79-4b90-90b5-a24cbd39d7cd" in namespace "emptydir-5661" to be "Succeeded or Failed"
    Jan 19 21:44:07.076: INFO: Pod "pod-96432054-1b79-4b90-90b5-a24cbd39d7cd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.274336ms
    Jan 19 21:44:09.079: INFO: Pod "pod-96432054-1b79-4b90-90b5-a24cbd39d7cd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009562665s
    Jan 19 21:44:11.079: INFO: Pod "pod-96432054-1b79-4b90-90b5-a24cbd39d7cd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009626733s
    STEP: Saw pod success 01/19/23 21:44:11.079
    Jan 19 21:44:11.080: INFO: Pod "pod-96432054-1b79-4b90-90b5-a24cbd39d7cd" satisfied condition "Succeeded or Failed"
    Jan 19 21:44:11.082: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-96432054-1b79-4b90-90b5-a24cbd39d7cd container test-container: <nil>
    STEP: delete the pod 01/19/23 21:44:11.087
    Jan 19 21:44:11.096: INFO: Waiting for pod pod-96432054-1b79-4b90-90b5-a24cbd39d7cd to disappear
    Jan 19 21:44:11.098: INFO: Pod pod-96432054-1b79-4b90-90b5-a24cbd39d7cd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:44:11.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5661" for this suite. 01/19/23 21:44:11.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:44:11.108
Jan 19 21:44:11.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename namespaces 01/19/23 21:44:11.108
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:11.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:11.131
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 01/19/23 21:44:11.134
Jan 19 21:44:11.142: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/19/23 21:44:11.142
Jan 19 21:44:11.156: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/19/23 21:44:11.156
Jan 19 21:44:11.170: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:44:11.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-15" for this suite. 01/19/23 21:44:11.174
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":250,"skipped":4700,"failed":0}
------------------------------
• [0.085 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:44:11.108
    Jan 19 21:44:11.108: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename namespaces 01/19/23 21:44:11.108
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:11.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:11.131
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 01/19/23 21:44:11.134
    Jan 19 21:44:11.142: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/19/23 21:44:11.142
    Jan 19 21:44:11.156: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/19/23 21:44:11.156
    Jan 19 21:44:11.170: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:44:11.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-15" for this suite. 01/19/23 21:44:11.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:44:11.195
Jan 19 21:44:11.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:44:11.195
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:11.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:11.239
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-4407a035-f458-47b8-b789-93710b1ae27b 01/19/23 21:44:11.242
STEP: Creating a pod to test consume secrets 01/19/23 21:44:11.251
Jan 19 21:44:11.297: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56" in namespace "projected-1009" to be "Succeeded or Failed"
Jan 19 21:44:11.304: INFO: Pod "pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.94447ms
Jan 19 21:44:13.308: INFO: Pod "pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010709902s
Jan 19 21:44:15.308: INFO: Pod "pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011262538s
STEP: Saw pod success 01/19/23 21:44:15.308
Jan 19 21:44:15.308: INFO: Pod "pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56" satisfied condition "Succeeded or Failed"
Jan 19 21:44:15.311: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/19/23 21:44:15.318
Jan 19 21:44:15.329: INFO: Waiting for pod pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56 to disappear
Jan 19 21:44:15.332: INFO: Pod pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 21:44:15.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1009" for this suite. 01/19/23 21:44:15.336
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":251,"skipped":4768,"failed":0}
------------------------------
• [4.146 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:44:11.195
    Jan 19 21:44:11.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:44:11.195
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:11.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:11.239
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-4407a035-f458-47b8-b789-93710b1ae27b 01/19/23 21:44:11.242
    STEP: Creating a pod to test consume secrets 01/19/23 21:44:11.251
    Jan 19 21:44:11.297: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56" in namespace "projected-1009" to be "Succeeded or Failed"
    Jan 19 21:44:11.304: INFO: Pod "pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56": Phase="Pending", Reason="", readiness=false. Elapsed: 6.94447ms
    Jan 19 21:44:13.308: INFO: Pod "pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010709902s
    Jan 19 21:44:15.308: INFO: Pod "pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011262538s
    STEP: Saw pod success 01/19/23 21:44:15.308
    Jan 19 21:44:15.308: INFO: Pod "pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56" satisfied condition "Succeeded or Failed"
    Jan 19 21:44:15.311: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:44:15.318
    Jan 19 21:44:15.329: INFO: Waiting for pod pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56 to disappear
    Jan 19 21:44:15.332: INFO: Pod pod-projected-secrets-f16a258c-8ace-484d-9c88-5361b2f52c56 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 21:44:15.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1009" for this suite. 01/19/23 21:44:15.336
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:44:15.341
Jan 19 21:44:15.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:44:15.342
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:15.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:15.362
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jan 19 21:44:15.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/19/23 21:44:25.458
Jan 19 21:44:25.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-5720 --namespace=crd-publish-openapi-5720 create -f -'
Jan 19 21:44:27.082: INFO: stderr: ""
Jan 19 21:44:27.082: INFO: stdout: "e2e-test-crd-publish-openapi-7945-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 19 21:44:27.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-5720 --namespace=crd-publish-openapi-5720 delete e2e-test-crd-publish-openapi-7945-crds test-cr'
Jan 19 21:44:27.142: INFO: stderr: ""
Jan 19 21:44:27.142: INFO: stdout: "e2e-test-crd-publish-openapi-7945-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan 19 21:44:27.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-5720 --namespace=crd-publish-openapi-5720 apply -f -'
Jan 19 21:44:28.524: INFO: stderr: ""
Jan 19 21:44:28.524: INFO: stdout: "e2e-test-crd-publish-openapi-7945-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan 19 21:44:28.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-5720 --namespace=crd-publish-openapi-5720 delete e2e-test-crd-publish-openapi-7945-crds test-cr'
Jan 19 21:44:28.587: INFO: stderr: ""
Jan 19 21:44:28.587: INFO: stdout: "e2e-test-crd-publish-openapi-7945-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/19/23 21:44:28.587
Jan 19 21:44:28.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-5720 explain e2e-test-crd-publish-openapi-7945-crds'
Jan 19 21:44:28.955: INFO: stderr: ""
Jan 19 21:44:28.956: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7945-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:44:37.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5720" for this suite. 01/19/23 21:44:37.985
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":252,"skipped":4771,"failed":0}
------------------------------
• [SLOW TEST] [22.651 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:44:15.341
    Jan 19 21:44:15.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:44:15.342
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:15.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:15.362
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jan 19 21:44:15.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/19/23 21:44:25.458
    Jan 19 21:44:25.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-5720 --namespace=crd-publish-openapi-5720 create -f -'
    Jan 19 21:44:27.082: INFO: stderr: ""
    Jan 19 21:44:27.082: INFO: stdout: "e2e-test-crd-publish-openapi-7945-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 19 21:44:27.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-5720 --namespace=crd-publish-openapi-5720 delete e2e-test-crd-publish-openapi-7945-crds test-cr'
    Jan 19 21:44:27.142: INFO: stderr: ""
    Jan 19 21:44:27.142: INFO: stdout: "e2e-test-crd-publish-openapi-7945-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan 19 21:44:27.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-5720 --namespace=crd-publish-openapi-5720 apply -f -'
    Jan 19 21:44:28.524: INFO: stderr: ""
    Jan 19 21:44:28.524: INFO: stdout: "e2e-test-crd-publish-openapi-7945-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan 19 21:44:28.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-5720 --namespace=crd-publish-openapi-5720 delete e2e-test-crd-publish-openapi-7945-crds test-cr'
    Jan 19 21:44:28.587: INFO: stderr: ""
    Jan 19 21:44:28.587: INFO: stdout: "e2e-test-crd-publish-openapi-7945-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/19/23 21:44:28.587
    Jan 19 21:44:28.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-5720 explain e2e-test-crd-publish-openapi-7945-crds'
    Jan 19 21:44:28.955: INFO: stderr: ""
    Jan 19 21:44:28.956: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7945-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:44:37.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5720" for this suite. 01/19/23 21:44:37.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:44:37.993
Jan 19 21:44:37.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename gc 01/19/23 21:44:37.993
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:38.013
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:38.015
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/19/23 21:44:38.018
W0119 21:44:38.042965      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc 01/19/23 21:44:43.046
STEP: wait for all pods to be garbage collected 01/19/23 21:44:43.051
STEP: Gathering metrics 01/19/23 21:44:48.056
W0119 21:44:48.058878      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0119 21:44:48.058893      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 21:44:48.058: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 21:44:48.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1641" for this suite. 01/19/23 21:44:48.063
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":253,"skipped":4783,"failed":0}
------------------------------
• [SLOW TEST] [10.076 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:44:37.993
    Jan 19 21:44:37.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename gc 01/19/23 21:44:37.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:38.013
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:38.015
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/19/23 21:44:38.018
    W0119 21:44:38.042965      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: delete the rc 01/19/23 21:44:43.046
    STEP: wait for all pods to be garbage collected 01/19/23 21:44:43.051
    STEP: Gathering metrics 01/19/23 21:44:48.056
    W0119 21:44:48.058878      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0119 21:44:48.058893      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 19 21:44:48.058: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 21:44:48.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-1641" for this suite. 01/19/23 21:44:48.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:44:48.07
Jan 19 21:44:48.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:44:48.07
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:48.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:48.094
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-1e60e706-becf-402a-a81d-4842bad79bf5 01/19/23 21:44:48.096
STEP: Creating a pod to test consume configMaps 01/19/23 21:44:48.108
W0119 21:44:48.157938      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:44:48.158: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf" in namespace "projected-1554" to be "Succeeded or Failed"
Jan 19 21:44:48.162: INFO: Pod "pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.770965ms
Jan 19 21:44:50.166: INFO: Pod "pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008041377s
Jan 19 21:44:52.166: INFO: Pod "pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008108026s
Jan 19 21:44:54.166: INFO: Pod "pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008464995s
STEP: Saw pod success 01/19/23 21:44:54.166
Jan 19 21:44:54.166: INFO: Pod "pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf" satisfied condition "Succeeded or Failed"
Jan 19 21:44:54.168: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:44:54.177
Jan 19 21:44:54.185: INFO: Waiting for pod pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf to disappear
Jan 19 21:44:54.188: INFO: Pod pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 21:44:54.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1554" for this suite. 01/19/23 21:44:54.192
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":254,"skipped":4795,"failed":0}
------------------------------
• [SLOW TEST] [6.128 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:44:48.07
    Jan 19 21:44:48.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:44:48.07
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:48.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:48.094
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-1e60e706-becf-402a-a81d-4842bad79bf5 01/19/23 21:44:48.096
    STEP: Creating a pod to test consume configMaps 01/19/23 21:44:48.108
    W0119 21:44:48.157938      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "agnhost-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "agnhost-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "agnhost-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:44:48.158: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf" in namespace "projected-1554" to be "Succeeded or Failed"
    Jan 19 21:44:48.162: INFO: Pod "pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.770965ms
    Jan 19 21:44:50.166: INFO: Pod "pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008041377s
    Jan 19 21:44:52.166: INFO: Pod "pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008108026s
    Jan 19 21:44:54.166: INFO: Pod "pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008464995s
    STEP: Saw pod success 01/19/23 21:44:54.166
    Jan 19 21:44:54.166: INFO: Pod "pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf" satisfied condition "Succeeded or Failed"
    Jan 19 21:44:54.168: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:44:54.177
    Jan 19 21:44:54.185: INFO: Waiting for pod pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf to disappear
    Jan 19 21:44:54.188: INFO: Pod pod-projected-configmaps-db25116c-becc-4ee8-af61-0c4a979b62cf no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 21:44:54.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1554" for this suite. 01/19/23 21:44:54.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:44:54.198
Jan 19 21:44:54.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename job 01/19/23 21:44:54.199
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:54.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:54.212
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 01/19/23 21:44:54.219
W0119 21:44:54.230159      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Patching the Job 01/19/23 21:44:54.23
STEP: Watching for Job to be patched 01/19/23 21:44:54.25
Jan 19 21:44:54.251: INFO: Event ADDED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 19 21:44:54.251: INFO: Event MODIFIED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan 19 21:44:54.251: INFO: Event MODIFIED found for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/19/23 21:44:54.251
STEP: Watching for Job to be updated 01/19/23 21:44:54.263
Jan 19 21:44:54.264: INFO: Event MODIFIED found for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 21:44:54.264: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/19/23 21:44:54.264
Jan 19 21:44:54.269: INFO: Job: e2e-nvp9t as labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched]
STEP: Waiting for job to complete 01/19/23 21:44:54.269
STEP: Delete a job collection with a labelselector 01/19/23 21:45:04.275
STEP: Watching for Job to be deleted 01/19/23 21:45:04.283
Jan 19 21:45:04.284: INFO: Event MODIFIED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 21:45:04.284: INFO: Event MODIFIED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 21:45:04.284: INFO: Event MODIFIED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 21:45:04.284: INFO: Event MODIFIED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 21:45:04.284: INFO: Event MODIFIED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan 19 21:45:04.284: INFO: Event DELETED found for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/19/23 21:45:04.284
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 19 21:45:04.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1306" for this suite. 01/19/23 21:45:04.294
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":255,"skipped":4809,"failed":0}
------------------------------
• [SLOW TEST] [10.106 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:44:54.198
    Jan 19 21:44:54.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename job 01/19/23 21:44:54.199
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:44:54.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:44:54.212
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 01/19/23 21:44:54.219
    W0119 21:44:54.230159      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Patching the Job 01/19/23 21:44:54.23
    STEP: Watching for Job to be patched 01/19/23 21:44:54.25
    Jan 19 21:44:54.251: INFO: Event ADDED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 19 21:44:54.251: INFO: Event MODIFIED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan 19 21:44:54.251: INFO: Event MODIFIED found for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/19/23 21:44:54.251
    STEP: Watching for Job to be updated 01/19/23 21:44:54.263
    Jan 19 21:44:54.264: INFO: Event MODIFIED found for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 21:44:54.264: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/19/23 21:44:54.264
    Jan 19 21:44:54.269: INFO: Job: e2e-nvp9t as labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched]
    STEP: Waiting for job to complete 01/19/23 21:44:54.269
    STEP: Delete a job collection with a labelselector 01/19/23 21:45:04.275
    STEP: Watching for Job to be deleted 01/19/23 21:45:04.283
    Jan 19 21:45:04.284: INFO: Event MODIFIED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 21:45:04.284: INFO: Event MODIFIED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 21:45:04.284: INFO: Event MODIFIED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 21:45:04.284: INFO: Event MODIFIED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 21:45:04.284: INFO: Event MODIFIED observed for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan 19 21:45:04.284: INFO: Event DELETED found for Job e2e-nvp9t in namespace job-1306 with labels: map[e2e-job-label:e2e-nvp9t e2e-nvp9t:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/19/23 21:45:04.284
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 19 21:45:04.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1306" for this suite. 01/19/23 21:45:04.294
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:45:04.304
Jan 19 21:45:04.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename resourcequota 01/19/23 21:45:04.305
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:45:04.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:45:04.319
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 01/19/23 21:45:04.325
STEP: Creating a ResourceQuota 01/19/23 21:45:09.328
STEP: Ensuring resource quota status is calculated 01/19/23 21:45:09.332
STEP: Creating a Service 01/19/23 21:45:11.335
STEP: Creating a NodePort Service 01/19/23 21:45:11.35
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/19/23 21:45:11.372
STEP: Ensuring resource quota status captures service creation 01/19/23 21:45:11.409
STEP: Deleting Services 01/19/23 21:45:13.412
STEP: Ensuring resource quota status released usage 01/19/23 21:45:13.45
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 21:45:15.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8651" for this suite. 01/19/23 21:45:15.46
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":256,"skipped":4810,"failed":0}
------------------------------
• [SLOW TEST] [11.161 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:45:04.304
    Jan 19 21:45:04.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename resourcequota 01/19/23 21:45:04.305
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:45:04.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:45:04.319
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 01/19/23 21:45:04.325
    STEP: Creating a ResourceQuota 01/19/23 21:45:09.328
    STEP: Ensuring resource quota status is calculated 01/19/23 21:45:09.332
    STEP: Creating a Service 01/19/23 21:45:11.335
    STEP: Creating a NodePort Service 01/19/23 21:45:11.35
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/19/23 21:45:11.372
    STEP: Ensuring resource quota status captures service creation 01/19/23 21:45:11.409
    STEP: Deleting Services 01/19/23 21:45:13.412
    STEP: Ensuring resource quota status released usage 01/19/23 21:45:13.45
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 21:45:15.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8651" for this suite. 01/19/23 21:45:15.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:45:15.468
Jan 19 21:45:15.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename resourcequota 01/19/23 21:45:15.469
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:45:15.481
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:45:15.483
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 01/19/23 21:45:15.486
STEP: Creating a ResourceQuota 01/19/23 21:45:20.489
STEP: Ensuring resource quota status is calculated 01/19/23 21:45:20.494
STEP: Creating a ReplicaSet 01/19/23 21:45:22.497
STEP: Ensuring resource quota status captures replicaset creation 01/19/23 21:45:22.519
STEP: Deleting a ReplicaSet 01/19/23 21:45:24.522
STEP: Ensuring resource quota status released usage 01/19/23 21:45:24.527
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 21:45:26.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9108" for this suite. 01/19/23 21:45:26.535
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":257,"skipped":4917,"failed":0}
------------------------------
• [SLOW TEST] [11.073 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:45:15.468
    Jan 19 21:45:15.468: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename resourcequota 01/19/23 21:45:15.469
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:45:15.481
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:45:15.483
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 01/19/23 21:45:15.486
    STEP: Creating a ResourceQuota 01/19/23 21:45:20.489
    STEP: Ensuring resource quota status is calculated 01/19/23 21:45:20.494
    STEP: Creating a ReplicaSet 01/19/23 21:45:22.497
    STEP: Ensuring resource quota status captures replicaset creation 01/19/23 21:45:22.519
    STEP: Deleting a ReplicaSet 01/19/23 21:45:24.522
    STEP: Ensuring resource quota status released usage 01/19/23 21:45:24.527
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 21:45:26.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9108" for this suite. 01/19/23 21:45:26.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:45:26.541
Jan 19 21:45:26.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:45:26.542
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:45:26.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:45:26.57
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:45:26.634
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:45:26.927
STEP: Deploying the webhook pod 01/19/23 21:45:26.942
STEP: Wait for the deployment to be ready 01/19/23 21:45:26.968
Jan 19 21:45:26.974: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/19/23 21:45:28.982
STEP: Verifying the service has paired with the endpoint 01/19/23 21:45:28.992
Jan 19 21:45:29.992: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 01/19/23 21:45:29.995
STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 21:45:30.007
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/19/23 21:45:30.013
STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 21:45:30.021
STEP: Patching a validating webhook configuration's rules to include the create operation 01/19/23 21:45:30.033
STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 21:45:30.039
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:45:30.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7412" for this suite. 01/19/23 21:45:30.051
STEP: Destroying namespace "webhook-7412-markers" for this suite. 01/19/23 21:45:30.055
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":258,"skipped":4923,"failed":0}
------------------------------
• [3.571 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:45:26.541
    Jan 19 21:45:26.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:45:26.542
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:45:26.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:45:26.57
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:45:26.634
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:45:26.927
    STEP: Deploying the webhook pod 01/19/23 21:45:26.942
    STEP: Wait for the deployment to be ready 01/19/23 21:45:26.968
    Jan 19 21:45:26.974: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/19/23 21:45:28.982
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:45:28.992
    Jan 19 21:45:29.992: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 01/19/23 21:45:29.995
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 21:45:30.007
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/19/23 21:45:30.013
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 21:45:30.021
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/19/23 21:45:30.033
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/19/23 21:45:30.039
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:45:30.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7412" for this suite. 01/19/23 21:45:30.051
    STEP: Destroying namespace "webhook-7412-markers" for this suite. 01/19/23 21:45:30.055
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:45:30.112
Jan 19 21:45:30.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:45:30.113
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:45:30.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:45:30.134
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jan 19 21:45:30.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/19/23 21:45:39.231
Jan 19 21:45:39.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 create -f -'
Jan 19 21:45:40.863: INFO: stderr: ""
Jan 19 21:45:40.863: INFO: stdout: "e2e-test-crd-publish-openapi-4153-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 19 21:45:40.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 delete e2e-test-crd-publish-openapi-4153-crds test-foo'
Jan 19 21:45:40.927: INFO: stderr: ""
Jan 19 21:45:40.927: INFO: stdout: "e2e-test-crd-publish-openapi-4153-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan 19 21:45:40.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 apply -f -'
Jan 19 21:45:42.355: INFO: stderr: ""
Jan 19 21:45:42.355: INFO: stdout: "e2e-test-crd-publish-openapi-4153-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan 19 21:45:42.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 delete e2e-test-crd-publish-openapi-4153-crds test-foo'
Jan 19 21:45:42.419: INFO: stderr: ""
Jan 19 21:45:42.419: INFO: stdout: "e2e-test-crd-publish-openapi-4153-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/19/23 21:45:42.419
Jan 19 21:45:42.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 create -f -'
Jan 19 21:45:43.755: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/19/23 21:45:43.755
Jan 19 21:45:43.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 create -f -'
Jan 19 21:45:44.102: INFO: rc: 1
Jan 19 21:45:44.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 apply -f -'
Jan 19 21:45:44.483: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/19/23 21:45:44.483
Jan 19 21:45:44.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 create -f -'
Jan 19 21:45:44.847: INFO: rc: 1
Jan 19 21:45:44.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 apply -f -'
Jan 19 21:45:45.225: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/19/23 21:45:45.225
Jan 19 21:45:45.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 explain e2e-test-crd-publish-openapi-4153-crds'
Jan 19 21:45:45.585: INFO: stderr: ""
Jan 19 21:45:45.585: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4153-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/19/23 21:45:45.585
Jan 19 21:45:45.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 explain e2e-test-crd-publish-openapi-4153-crds.metadata'
Jan 19 21:45:45.936: INFO: stderr: ""
Jan 19 21:45:45.936: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4153-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan 19 21:45:45.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 explain e2e-test-crd-publish-openapi-4153-crds.spec'
Jan 19 21:45:46.306: INFO: stderr: ""
Jan 19 21:45:46.306: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4153-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan 19 21:45:46.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 explain e2e-test-crd-publish-openapi-4153-crds.spec.bars'
Jan 19 21:45:46.678: INFO: stderr: ""
Jan 19 21:45:46.678: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4153-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/19/23 21:45:46.679
Jan 19 21:45:46.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 explain e2e-test-crd-publish-openapi-4153-crds.spec.bars2'
Jan 19 21:45:47.056: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:45:55.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9250" for this suite. 01/19/23 21:45:55.985
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":259,"skipped":4923,"failed":0}
------------------------------
• [SLOW TEST] [25.880 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:45:30.112
    Jan 19 21:45:30.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:45:30.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:45:30.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:45:30.134
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jan 19 21:45:30.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/19/23 21:45:39.231
    Jan 19 21:45:39.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 create -f -'
    Jan 19 21:45:40.863: INFO: stderr: ""
    Jan 19 21:45:40.863: INFO: stdout: "e2e-test-crd-publish-openapi-4153-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 19 21:45:40.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 delete e2e-test-crd-publish-openapi-4153-crds test-foo'
    Jan 19 21:45:40.927: INFO: stderr: ""
    Jan 19 21:45:40.927: INFO: stdout: "e2e-test-crd-publish-openapi-4153-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan 19 21:45:40.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 apply -f -'
    Jan 19 21:45:42.355: INFO: stderr: ""
    Jan 19 21:45:42.355: INFO: stdout: "e2e-test-crd-publish-openapi-4153-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan 19 21:45:42.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 delete e2e-test-crd-publish-openapi-4153-crds test-foo'
    Jan 19 21:45:42.419: INFO: stderr: ""
    Jan 19 21:45:42.419: INFO: stdout: "e2e-test-crd-publish-openapi-4153-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/19/23 21:45:42.419
    Jan 19 21:45:42.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 create -f -'
    Jan 19 21:45:43.755: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/19/23 21:45:43.755
    Jan 19 21:45:43.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 create -f -'
    Jan 19 21:45:44.102: INFO: rc: 1
    Jan 19 21:45:44.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 apply -f -'
    Jan 19 21:45:44.483: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/19/23 21:45:44.483
    Jan 19 21:45:44.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 create -f -'
    Jan 19 21:45:44.847: INFO: rc: 1
    Jan 19 21:45:44.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 --namespace=crd-publish-openapi-9250 apply -f -'
    Jan 19 21:45:45.225: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/19/23 21:45:45.225
    Jan 19 21:45:45.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 explain e2e-test-crd-publish-openapi-4153-crds'
    Jan 19 21:45:45.585: INFO: stderr: ""
    Jan 19 21:45:45.585: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4153-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/19/23 21:45:45.585
    Jan 19 21:45:45.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 explain e2e-test-crd-publish-openapi-4153-crds.metadata'
    Jan 19 21:45:45.936: INFO: stderr: ""
    Jan 19 21:45:45.936: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4153-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan 19 21:45:45.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 explain e2e-test-crd-publish-openapi-4153-crds.spec'
    Jan 19 21:45:46.306: INFO: stderr: ""
    Jan 19 21:45:46.306: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4153-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan 19 21:45:46.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 explain e2e-test-crd-publish-openapi-4153-crds.spec.bars'
    Jan 19 21:45:46.678: INFO: stderr: ""
    Jan 19 21:45:46.678: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4153-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/19/23 21:45:46.679
    Jan 19 21:45:46.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=crd-publish-openapi-9250 explain e2e-test-crd-publish-openapi-4153-crds.spec.bars2'
    Jan 19 21:45:47.056: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:45:55.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9250" for this suite. 01/19/23 21:45:55.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:45:55.992
Jan 19 21:45:55.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:45:55.993
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:45:56.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:45:56.015
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-c3ab0250-8c3a-4584-8b93-3fa9f1b4c5e2 01/19/23 21:45:56.018
STEP: Creating a pod to test consume configMaps 01/19/23 21:45:56.03
Jan 19 21:45:56.065: INFO: Waiting up to 5m0s for pod "pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc" in namespace "configmap-3454" to be "Succeeded or Failed"
Jan 19 21:45:56.077: INFO: Pod "pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.679133ms
Jan 19 21:45:58.080: INFO: Pod "pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014971486s
Jan 19 21:46:00.081: INFO: Pod "pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015730352s
STEP: Saw pod success 01/19/23 21:46:00.081
Jan 19 21:46:00.081: INFO: Pod "pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc" satisfied condition "Succeeded or Failed"
Jan 19 21:46:00.083: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:46:00.092
Jan 19 21:46:00.104: INFO: Waiting for pod pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc to disappear
Jan 19 21:46:00.106: INFO: Pod pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:46:00.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3454" for this suite. 01/19/23 21:46:00.11
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":260,"skipped":4929,"failed":0}
------------------------------
• [4.124 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:45:55.992
    Jan 19 21:45:55.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:45:55.993
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:45:56.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:45:56.015
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-c3ab0250-8c3a-4584-8b93-3fa9f1b4c5e2 01/19/23 21:45:56.018
    STEP: Creating a pod to test consume configMaps 01/19/23 21:45:56.03
    Jan 19 21:45:56.065: INFO: Waiting up to 5m0s for pod "pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc" in namespace "configmap-3454" to be "Succeeded or Failed"
    Jan 19 21:45:56.077: INFO: Pod "pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc": Phase="Pending", Reason="", readiness=false. Elapsed: 11.679133ms
    Jan 19 21:45:58.080: INFO: Pod "pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014971486s
    Jan 19 21:46:00.081: INFO: Pod "pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015730352s
    STEP: Saw pod success 01/19/23 21:46:00.081
    Jan 19 21:46:00.081: INFO: Pod "pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc" satisfied condition "Succeeded or Failed"
    Jan 19 21:46:00.083: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:46:00.092
    Jan 19 21:46:00.104: INFO: Waiting for pod pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc to disappear
    Jan 19 21:46:00.106: INFO: Pod pod-configmaps-8841c9c2-4ce5-4ef7-b58b-e61c1c8222cc no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:46:00.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3454" for this suite. 01/19/23 21:46:00.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:00.117
Jan 19 21:46:00.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubelet-test 01/19/23 21:46:00.118
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:00.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:00.135
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 19 21:46:04.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8355" for this suite. 01/19/23 21:46:04.199
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":261,"skipped":4945,"failed":0}
------------------------------
• [4.089 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:00.117
    Jan 19 21:46:00.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubelet-test 01/19/23 21:46:00.118
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:00.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:00.135
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 19 21:46:04.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8355" for this suite. 01/19/23 21:46:04.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:04.206
Jan 19 21:46:04.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:46:04.207
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:04.228
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:04.232
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-f390c215-bb17-46de-b869-c9eec0478529 01/19/23 21:46:04.234
STEP: Creating a pod to test consume configMaps 01/19/23 21:46:04.264
Jan 19 21:46:04.297: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e" in namespace "projected-2060" to be "Succeeded or Failed"
Jan 19 21:46:04.309: INFO: Pod "pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.028442ms
Jan 19 21:46:06.312: INFO: Pod "pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015497675s
Jan 19 21:46:08.312: INFO: Pod "pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015408322s
STEP: Saw pod success 01/19/23 21:46:08.312
Jan 19 21:46:08.312: INFO: Pod "pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e" satisfied condition "Succeeded or Failed"
Jan 19 21:46:08.315: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:46:08.32
Jan 19 21:46:08.331: INFO: Waiting for pod pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e to disappear
Jan 19 21:46:08.333: INFO: Pod pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 21:46:08.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2060" for this suite. 01/19/23 21:46:08.339
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":262,"skipped":4971,"failed":0}
------------------------------
• [4.137 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:04.206
    Jan 19 21:46:04.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:46:04.207
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:04.228
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:04.232
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-f390c215-bb17-46de-b869-c9eec0478529 01/19/23 21:46:04.234
    STEP: Creating a pod to test consume configMaps 01/19/23 21:46:04.264
    Jan 19 21:46:04.297: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e" in namespace "projected-2060" to be "Succeeded or Failed"
    Jan 19 21:46:04.309: INFO: Pod "pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.028442ms
    Jan 19 21:46:06.312: INFO: Pod "pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015497675s
    Jan 19 21:46:08.312: INFO: Pod "pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015408322s
    STEP: Saw pod success 01/19/23 21:46:08.312
    Jan 19 21:46:08.312: INFO: Pod "pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e" satisfied condition "Succeeded or Failed"
    Jan 19 21:46:08.315: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:46:08.32
    Jan 19 21:46:08.331: INFO: Waiting for pod pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e to disappear
    Jan 19 21:46:08.333: INFO: Pod pod-projected-configmaps-cf358261-7014-4ff6-a03b-d168c23de57e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 21:46:08.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2060" for this suite. 01/19/23 21:46:08.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:08.344
Jan 19 21:46:08.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename podtemplate 01/19/23 21:46:08.345
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:08.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:08.366
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/19/23 21:46:08.369
W0119 21:46:08.375809      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:46:08.375: INFO: created test-podtemplate-1
W0119 21:46:08.386933      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:46:08.386: INFO: created test-podtemplate-2
W0119 21:46:08.393716      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:46:08.393: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/19/23 21:46:08.393
STEP: delete collection of pod templates 01/19/23 21:46:08.399
Jan 19 21:46:08.399: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/19/23 21:46:08.427
Jan 19 21:46:08.427: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jan 19 21:46:08.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3057" for this suite. 01/19/23 21:46:08.442
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":263,"skipped":4995,"failed":0}
------------------------------
• [0.105 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:08.344
    Jan 19 21:46:08.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename podtemplate 01/19/23 21:46:08.345
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:08.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:08.366
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/19/23 21:46:08.369
    W0119 21:46:08.375809      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:46:08.375: INFO: created test-podtemplate-1
    W0119 21:46:08.386933      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:46:08.386: INFO: created test-podtemplate-2
    W0119 21:46:08.393716      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "token-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "token-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "token-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "token-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:46:08.393: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/19/23 21:46:08.393
    STEP: delete collection of pod templates 01/19/23 21:46:08.399
    Jan 19 21:46:08.399: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/19/23 21:46:08.427
    Jan 19 21:46:08.427: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jan 19 21:46:08.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3057" for this suite. 01/19/23 21:46:08.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:08.449
Jan 19 21:46:08.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename limitrange 01/19/23 21:46:08.45
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:08.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:08.477
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 01/19/23 21:46:08.48
STEP: Setting up watch 01/19/23 21:46:08.481
STEP: Submitting a LimitRange 01/19/23 21:46:08.592
STEP: Verifying LimitRange creation was observed 01/19/23 21:46:08.599
STEP: Fetching the LimitRange to ensure it has proper values 01/19/23 21:46:08.599
Jan 19 21:46:08.607: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 19 21:46:08.607: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/19/23 21:46:08.607
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/19/23 21:46:08.62
Jan 19 21:46:08.625: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan 19 21:46:08.625: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/19/23 21:46:08.625
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/19/23 21:46:08.637
Jan 19 21:46:08.640: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan 19 21:46:08.640: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/19/23 21:46:08.64
STEP: Failing to create a Pod with more than max resources 01/19/23 21:46:08.648
STEP: Updating a LimitRange 01/19/23 21:46:08.653
STEP: Verifying LimitRange updating is effective 01/19/23 21:46:08.66
STEP: Creating a Pod with less than former min resources 01/19/23 21:46:10.73
STEP: Failing to create a Pod with more than max resources 01/19/23 21:46:10.742
STEP: Deleting a LimitRange 01/19/23 21:46:10.749
STEP: Verifying the LimitRange was deleted 01/19/23 21:46:10.754
Jan 19 21:46:15.758: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/19/23 21:46:15.758
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jan 19 21:46:15.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3651" for this suite. 01/19/23 21:46:15.775
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":264,"skipped":5000,"failed":0}
------------------------------
• [SLOW TEST] [7.331 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:08.449
    Jan 19 21:46:08.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename limitrange 01/19/23 21:46:08.45
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:08.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:08.477
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 01/19/23 21:46:08.48
    STEP: Setting up watch 01/19/23 21:46:08.481
    STEP: Submitting a LimitRange 01/19/23 21:46:08.592
    STEP: Verifying LimitRange creation was observed 01/19/23 21:46:08.599
    STEP: Fetching the LimitRange to ensure it has proper values 01/19/23 21:46:08.599
    Jan 19 21:46:08.607: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 19 21:46:08.607: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/19/23 21:46:08.607
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/19/23 21:46:08.62
    Jan 19 21:46:08.625: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan 19 21:46:08.625: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/19/23 21:46:08.625
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/19/23 21:46:08.637
    Jan 19 21:46:08.640: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan 19 21:46:08.640: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/19/23 21:46:08.64
    STEP: Failing to create a Pod with more than max resources 01/19/23 21:46:08.648
    STEP: Updating a LimitRange 01/19/23 21:46:08.653
    STEP: Verifying LimitRange updating is effective 01/19/23 21:46:08.66
    STEP: Creating a Pod with less than former min resources 01/19/23 21:46:10.73
    STEP: Failing to create a Pod with more than max resources 01/19/23 21:46:10.742
    STEP: Deleting a LimitRange 01/19/23 21:46:10.749
    STEP: Verifying the LimitRange was deleted 01/19/23 21:46:10.754
    Jan 19 21:46:15.758: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/19/23 21:46:15.758
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jan 19 21:46:15.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-3651" for this suite. 01/19/23 21:46:15.775
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:15.782
Jan 19 21:46:15.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 21:46:15.783
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:15.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:15.821
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:46:15.829
Jan 19 21:46:15.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b" in namespace "downward-api-6091" to be "Succeeded or Failed"
Jan 19 21:46:15.882: INFO: Pod "downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b": Phase="Pending", Reason="", readiness=false. Elapsed: 26.781861ms
Jan 19 21:46:17.887: INFO: Pod "downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031248989s
Jan 19 21:46:19.885: INFO: Pod "downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030001152s
STEP: Saw pod success 01/19/23 21:46:19.885
Jan 19 21:46:19.885: INFO: Pod "downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b" satisfied condition "Succeeded or Failed"
Jan 19 21:46:19.888: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b container client-container: <nil>
STEP: delete the pod 01/19/23 21:46:19.898
Jan 19 21:46:19.910: INFO: Waiting for pod downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b to disappear
Jan 19 21:46:19.912: INFO: Pod downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 21:46:19.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6091" for this suite. 01/19/23 21:46:19.917
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":265,"skipped":5050,"failed":0}
------------------------------
• [4.142 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:15.782
    Jan 19 21:46:15.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 21:46:15.783
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:15.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:15.821
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:46:15.829
    Jan 19 21:46:15.855: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b" in namespace "downward-api-6091" to be "Succeeded or Failed"
    Jan 19 21:46:15.882: INFO: Pod "downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b": Phase="Pending", Reason="", readiness=false. Elapsed: 26.781861ms
    Jan 19 21:46:17.887: INFO: Pod "downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031248989s
    Jan 19 21:46:19.885: INFO: Pod "downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030001152s
    STEP: Saw pod success 01/19/23 21:46:19.885
    Jan 19 21:46:19.885: INFO: Pod "downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b" satisfied condition "Succeeded or Failed"
    Jan 19 21:46:19.888: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b container client-container: <nil>
    STEP: delete the pod 01/19/23 21:46:19.898
    Jan 19 21:46:19.910: INFO: Waiting for pod downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b to disappear
    Jan 19 21:46:19.912: INFO: Pod downwardapi-volume-7534f71d-1312-4502-bcf9-ca504b6e744b no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 21:46:19.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6091" for this suite. 01/19/23 21:46:19.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:19.925
Jan 19 21:46:19.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:46:19.926
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:19.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:19.951
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jan 19 21:46:19.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5715 version'
Jan 19 21:46:20.000: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan 19 21:46:20.000: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4+77bec7a\", GitCommit:\"b6d1f054747e9886f61dd85316deac3415e2726f\", GitTreeState:\"clean\", BuildDate:\"2022-12-14T20:18:42Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:46:20.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5715" for this suite. 01/19/23 21:46:20.005
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":266,"skipped":5080,"failed":0}
------------------------------
• [0.085 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:19.925
    Jan 19 21:46:19.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:46:19.926
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:19.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:19.951
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jan 19 21:46:19.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5715 version'
    Jan 19 21:46:20.000: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan 19 21:46:20.000: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4\", GitCommit:\"872a965c6c6526caa949f0c6ac028ef7aff3fb78\", GitTreeState:\"clean\", BuildDate:\"2022-11-09T13:36:36Z\", GoVersion:\"go1.19.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.4+77bec7a\", GitCommit:\"b6d1f054747e9886f61dd85316deac3415e2726f\", GitTreeState:\"clean\", BuildDate:\"2022-12-14T20:18:42Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:46:20.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5715" for this suite. 01/19/23 21:46:20.005
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:20.01
Jan 19 21:46:20.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/19/23 21:46:20.011
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:20.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:20.031
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/19/23 21:46:20.034
STEP: Creating hostNetwork=false pod 01/19/23 21:46:20.034
Jan 19 21:46:20.066: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9743" to be "running and ready"
Jan 19 21:46:20.069: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.08252ms
Jan 19 21:46:20.069: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:46:22.073: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00642587s
Jan 19 21:46:22.073: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan 19 21:46:22.073: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/19/23 21:46:22.075
Jan 19 21:46:22.087: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9743" to be "running and ready"
Jan 19 21:46:22.089: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050661ms
Jan 19 21:46:22.089: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:46:24.103: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015673756s
Jan 19 21:46:24.103: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan 19 21:46:24.103: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/19/23 21:46:24.106
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/19/23 21:46:24.106
Jan 19 21:46:24.106: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:46:24.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:46:24.107: INFO: ExecWithOptions: Clientset creation
Jan 19 21:46:24.107: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 19 21:46:24.189: INFO: Exec stderr: ""
Jan 19 21:46:24.189: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:46:24.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:46:24.189: INFO: ExecWithOptions: Clientset creation
Jan 19 21:46:24.189: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 19 21:46:24.251: INFO: Exec stderr: ""
Jan 19 21:46:24.251: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:46:24.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:46:24.251: INFO: ExecWithOptions: Clientset creation
Jan 19 21:46:24.251: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 19 21:46:24.305: INFO: Exec stderr: ""
Jan 19 21:46:24.305: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:46:24.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:46:24.306: INFO: ExecWithOptions: Clientset creation
Jan 19 21:46:24.306: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 19 21:46:24.360: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/19/23 21:46:24.36
Jan 19 21:46:24.360: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:46:24.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:46:24.361: INFO: ExecWithOptions: Clientset creation
Jan 19 21:46:24.361: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 19 21:46:24.412: INFO: Exec stderr: ""
Jan 19 21:46:24.412: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:46:24.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:46:24.413: INFO: ExecWithOptions: Clientset creation
Jan 19 21:46:24.413: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan 19 21:46:24.480: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/19/23 21:46:24.48
Jan 19 21:46:24.480: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:46:24.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:46:24.480: INFO: ExecWithOptions: Clientset creation
Jan 19 21:46:24.480: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 19 21:46:24.533: INFO: Exec stderr: ""
Jan 19 21:46:24.533: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:46:24.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:46:24.533: INFO: ExecWithOptions: Clientset creation
Jan 19 21:46:24.533: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan 19 21:46:24.585: INFO: Exec stderr: ""
Jan 19 21:46:24.585: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:46:24.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:46:24.586: INFO: ExecWithOptions: Clientset creation
Jan 19 21:46:24.586: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 19 21:46:24.635: INFO: Exec stderr: ""
Jan 19 21:46:24.635: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:46:24.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:46:24.635: INFO: ExecWithOptions: Clientset creation
Jan 19 21:46:24.635: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan 19 21:46:24.690: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jan 19 21:46:24.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9743" for this suite. 01/19/23 21:46:24.695
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":267,"skipped":5084,"failed":0}
------------------------------
• [4.692 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:20.01
    Jan 19 21:46:20.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/19/23 21:46:20.011
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:20.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:20.031
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/19/23 21:46:20.034
    STEP: Creating hostNetwork=false pod 01/19/23 21:46:20.034
    Jan 19 21:46:20.066: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9743" to be "running and ready"
    Jan 19 21:46:20.069: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.08252ms
    Jan 19 21:46:20.069: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:46:22.073: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00642587s
    Jan 19 21:46:22.073: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan 19 21:46:22.073: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/19/23 21:46:22.075
    Jan 19 21:46:22.087: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9743" to be "running and ready"
    Jan 19 21:46:22.089: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.050661ms
    Jan 19 21:46:22.089: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:46:24.103: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.015673756s
    Jan 19 21:46:24.103: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan 19 21:46:24.103: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/19/23 21:46:24.106
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/19/23 21:46:24.106
    Jan 19 21:46:24.106: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:46:24.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:46:24.107: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:46:24.107: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 19 21:46:24.189: INFO: Exec stderr: ""
    Jan 19 21:46:24.189: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:46:24.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:46:24.189: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:46:24.189: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 19 21:46:24.251: INFO: Exec stderr: ""
    Jan 19 21:46:24.251: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:46:24.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:46:24.251: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:46:24.251: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 19 21:46:24.305: INFO: Exec stderr: ""
    Jan 19 21:46:24.305: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:46:24.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:46:24.306: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:46:24.306: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 19 21:46:24.360: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/19/23 21:46:24.36
    Jan 19 21:46:24.360: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:46:24.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:46:24.361: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:46:24.361: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 19 21:46:24.412: INFO: Exec stderr: ""
    Jan 19 21:46:24.412: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:46:24.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:46:24.413: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:46:24.413: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan 19 21:46:24.480: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/19/23 21:46:24.48
    Jan 19 21:46:24.480: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:46:24.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:46:24.480: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:46:24.480: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 19 21:46:24.533: INFO: Exec stderr: ""
    Jan 19 21:46:24.533: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:46:24.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:46:24.533: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:46:24.533: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan 19 21:46:24.585: INFO: Exec stderr: ""
    Jan 19 21:46:24.585: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:46:24.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:46:24.586: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:46:24.586: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 19 21:46:24.635: INFO: Exec stderr: ""
    Jan 19 21:46:24.635: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9743 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:46:24.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:46:24.635: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:46:24.635: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9743/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan 19 21:46:24.690: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jan 19 21:46:24.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-9743" for this suite. 01/19/23 21:46:24.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:24.703
Jan 19 21:46:24.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename watch 01/19/23 21:46:24.704
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:24.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:24.723
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/19/23 21:46:24.725
STEP: modifying the configmap once 01/19/23 21:46:24.741
STEP: modifying the configmap a second time 01/19/23 21:46:24.766
STEP: deleting the configmap 01/19/23 21:46:24.779
STEP: creating a watch on configmaps from the resource version returned by the first update 01/19/23 21:46:24.794
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/19/23 21:46:24.795
Jan 19 21:46:24.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6303  2897c796-ace0-4b12-b021-e125259584ba 216320 0 2023-01-19 21:46:24 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-19 21:46:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 21:46:24.795: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6303  2897c796-ace0-4b12-b021-e125259584ba 216324 0 2023-01-19 21:46:24 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-19 21:46:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 19 21:46:24.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6303" for this suite. 01/19/23 21:46:24.8
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":268,"skipped":5126,"failed":0}
------------------------------
• [0.105 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:24.703
    Jan 19 21:46:24.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename watch 01/19/23 21:46:24.704
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:24.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:24.723
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/19/23 21:46:24.725
    STEP: modifying the configmap once 01/19/23 21:46:24.741
    STEP: modifying the configmap a second time 01/19/23 21:46:24.766
    STEP: deleting the configmap 01/19/23 21:46:24.779
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/19/23 21:46:24.794
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/19/23 21:46:24.795
    Jan 19 21:46:24.795: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6303  2897c796-ace0-4b12-b021-e125259584ba 216320 0 2023-01-19 21:46:24 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-19 21:46:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 21:46:24.795: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6303  2897c796-ace0-4b12-b021-e125259584ba 216324 0 2023-01-19 21:46:24 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-19 21:46:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 19 21:46:24.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6303" for this suite. 01/19/23 21:46:24.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:24.809
Jan 19 21:46:24.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename svcaccounts 01/19/23 21:46:24.81
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:24.854
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:24.888
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jan 19 21:46:24.918: INFO: Got root ca configmap in namespace "svcaccounts-5760"
Jan 19 21:46:24.959: INFO: Deleted root ca configmap in namespace "svcaccounts-5760"
STEP: waiting for a new root ca configmap created 01/19/23 21:46:25.46
Jan 19 21:46:25.463: INFO: Recreated root ca configmap in namespace "svcaccounts-5760"
Jan 19 21:46:25.469: INFO: Updated root ca configmap in namespace "svcaccounts-5760"
STEP: waiting for the root ca configmap reconciled 01/19/23 21:46:25.97
Jan 19 21:46:25.984: INFO: Reconciled root ca configmap in namespace "svcaccounts-5760"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jan 19 21:46:25.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5760" for this suite. 01/19/23 21:46:25.989
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":269,"skipped":5133,"failed":0}
------------------------------
• [1.190 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:24.809
    Jan 19 21:46:24.809: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename svcaccounts 01/19/23 21:46:24.81
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:24.854
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:24.888
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jan 19 21:46:24.918: INFO: Got root ca configmap in namespace "svcaccounts-5760"
    Jan 19 21:46:24.959: INFO: Deleted root ca configmap in namespace "svcaccounts-5760"
    STEP: waiting for a new root ca configmap created 01/19/23 21:46:25.46
    Jan 19 21:46:25.463: INFO: Recreated root ca configmap in namespace "svcaccounts-5760"
    Jan 19 21:46:25.469: INFO: Updated root ca configmap in namespace "svcaccounts-5760"
    STEP: waiting for the root ca configmap reconciled 01/19/23 21:46:25.97
    Jan 19 21:46:25.984: INFO: Reconciled root ca configmap in namespace "svcaccounts-5760"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jan 19 21:46:25.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5760" for this suite. 01/19/23 21:46:25.989
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:25.999
Jan 19 21:46:25.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 21:46:26
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:26.032
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:26.037
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/19/23 21:46:26.039
W0119 21:46:26.108931      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:46:26.109: INFO: Waiting up to 5m0s for pod "pod-526d2545-215a-447b-a2f7-75a724139e6a" in namespace "emptydir-744" to be "Succeeded or Failed"
Jan 19 21:46:26.111: INFO: Pod "pod-526d2545-215a-447b-a2f7-75a724139e6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597878ms
Jan 19 21:46:28.114: INFO: Pod "pod-526d2545-215a-447b-a2f7-75a724139e6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005786441s
Jan 19 21:46:30.115: INFO: Pod "pod-526d2545-215a-447b-a2f7-75a724139e6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006303194s
STEP: Saw pod success 01/19/23 21:46:30.115
Jan 19 21:46:30.115: INFO: Pod "pod-526d2545-215a-447b-a2f7-75a724139e6a" satisfied condition "Succeeded or Failed"
Jan 19 21:46:30.117: INFO: Trying to get logs from node ip-10-0-146-42.ec2.internal pod pod-526d2545-215a-447b-a2f7-75a724139e6a container test-container: <nil>
STEP: delete the pod 01/19/23 21:46:30.13
Jan 19 21:46:30.142: INFO: Waiting for pod pod-526d2545-215a-447b-a2f7-75a724139e6a to disappear
Jan 19 21:46:30.144: INFO: Pod pod-526d2545-215a-447b-a2f7-75a724139e6a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 21:46:30.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-744" for this suite. 01/19/23 21:46:30.149
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":270,"skipped":5139,"failed":0}
------------------------------
• [4.156 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:25.999
    Jan 19 21:46:25.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 21:46:26
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:26.032
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:26.037
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/19/23 21:46:26.039
    W0119 21:46:26.108931      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:46:26.109: INFO: Waiting up to 5m0s for pod "pod-526d2545-215a-447b-a2f7-75a724139e6a" in namespace "emptydir-744" to be "Succeeded or Failed"
    Jan 19 21:46:26.111: INFO: Pod "pod-526d2545-215a-447b-a2f7-75a724139e6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.597878ms
    Jan 19 21:46:28.114: INFO: Pod "pod-526d2545-215a-447b-a2f7-75a724139e6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005786441s
    Jan 19 21:46:30.115: INFO: Pod "pod-526d2545-215a-447b-a2f7-75a724139e6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006303194s
    STEP: Saw pod success 01/19/23 21:46:30.115
    Jan 19 21:46:30.115: INFO: Pod "pod-526d2545-215a-447b-a2f7-75a724139e6a" satisfied condition "Succeeded or Failed"
    Jan 19 21:46:30.117: INFO: Trying to get logs from node ip-10-0-146-42.ec2.internal pod pod-526d2545-215a-447b-a2f7-75a724139e6a container test-container: <nil>
    STEP: delete the pod 01/19/23 21:46:30.13
    Jan 19 21:46:30.142: INFO: Waiting for pod pod-526d2545-215a-447b-a2f7-75a724139e6a to disappear
    Jan 19 21:46:30.144: INFO: Pod pod-526d2545-215a-447b-a2f7-75a724139e6a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:46:30.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-744" for this suite. 01/19/23 21:46:30.149
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:30.155
Jan 19 21:46:30.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename runtimeclass 01/19/23 21:46:30.156
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:30.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:30.18
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 19 21:46:30.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-8300" for this suite. 01/19/23 21:46:30.219
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":271,"skipped":5145,"failed":0}
------------------------------
• [0.069 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:30.155
    Jan 19 21:46:30.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename runtimeclass 01/19/23 21:46:30.156
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:30.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:30.18
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 19 21:46:30.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-8300" for this suite. 01/19/23 21:46:30.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:30.225
Jan 19 21:46:30.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pods 01/19/23 21:46:30.226
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:30.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:30.273
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jan 19 21:46:30.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: creating the pod 01/19/23 21:46:30.277
STEP: submitting the pod to kubernetes 01/19/23 21:46:30.277
Jan 19 21:46:30.315: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5ab60dd3-58d1-444e-979b-2a1a0b891dd0" in namespace "pods-539" to be "running and ready"
Jan 19 21:46:30.331: INFO: Pod "pod-logs-websocket-5ab60dd3-58d1-444e-979b-2a1a0b891dd0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.690981ms
Jan 19 21:46:30.331: INFO: The phase of Pod pod-logs-websocket-5ab60dd3-58d1-444e-979b-2a1a0b891dd0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:46:32.336: INFO: Pod "pod-logs-websocket-5ab60dd3-58d1-444e-979b-2a1a0b891dd0": Phase="Running", Reason="", readiness=true. Elapsed: 2.021738535s
Jan 19 21:46:32.336: INFO: The phase of Pod pod-logs-websocket-5ab60dd3-58d1-444e-979b-2a1a0b891dd0 is Running (Ready = true)
Jan 19 21:46:32.336: INFO: Pod "pod-logs-websocket-5ab60dd3-58d1-444e-979b-2a1a0b891dd0" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 21:46:32.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-539" for this suite. 01/19/23 21:46:32.366
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":272,"skipped":5181,"failed":0}
------------------------------
• [2.147 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:30.225
    Jan 19 21:46:30.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pods 01/19/23 21:46:30.226
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:30.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:30.273
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jan 19 21:46:30.276: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: creating the pod 01/19/23 21:46:30.277
    STEP: submitting the pod to kubernetes 01/19/23 21:46:30.277
    Jan 19 21:46:30.315: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5ab60dd3-58d1-444e-979b-2a1a0b891dd0" in namespace "pods-539" to be "running and ready"
    Jan 19 21:46:30.331: INFO: Pod "pod-logs-websocket-5ab60dd3-58d1-444e-979b-2a1a0b891dd0": Phase="Pending", Reason="", readiness=false. Elapsed: 16.690981ms
    Jan 19 21:46:30.331: INFO: The phase of Pod pod-logs-websocket-5ab60dd3-58d1-444e-979b-2a1a0b891dd0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:46:32.336: INFO: Pod "pod-logs-websocket-5ab60dd3-58d1-444e-979b-2a1a0b891dd0": Phase="Running", Reason="", readiness=true. Elapsed: 2.021738535s
    Jan 19 21:46:32.336: INFO: The phase of Pod pod-logs-websocket-5ab60dd3-58d1-444e-979b-2a1a0b891dd0 is Running (Ready = true)
    Jan 19 21:46:32.336: INFO: Pod "pod-logs-websocket-5ab60dd3-58d1-444e-979b-2a1a0b891dd0" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 21:46:32.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-539" for this suite. 01/19/23 21:46:32.366
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:32.372
Jan 19 21:46:32.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename proxy 01/19/23 21:46:32.373
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:32.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:32.432
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan 19 21:46:32.438: INFO: Creating pod...
Jan 19 21:46:32.527: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9543" to be "running"
Jan 19 21:46:32.545: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 18.769684ms
Jan 19 21:46:34.548: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.021211488s
Jan 19 21:46:34.548: INFO: Pod "agnhost" satisfied condition "running"
Jan 19 21:46:34.548: INFO: Creating service...
Jan 19 21:46:34.557: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=DELETE
Jan 19 21:46:34.562: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 19 21:46:34.562: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=OPTIONS
Jan 19 21:46:34.566: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 19 21:46:34.566: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=PATCH
Jan 19 21:46:34.570: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 19 21:46:34.570: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=POST
Jan 19 21:46:34.574: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 19 21:46:34.574: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=PUT
Jan 19 21:46:34.577: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 19 21:46:34.577: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=DELETE
Jan 19 21:46:34.584: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 19 21:46:34.584: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan 19 21:46:34.590: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 19 21:46:34.590: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=PATCH
Jan 19 21:46:34.595: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 19 21:46:34.595: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=POST
Jan 19 21:46:34.600: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 19 21:46:34.600: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=PUT
Jan 19 21:46:34.623: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 19 21:46:34.623: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=GET
Jan 19 21:46:34.627: INFO: http.Client request:GET StatusCode:301
Jan 19 21:46:34.627: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=GET
Jan 19 21:46:34.631: INFO: http.Client request:GET StatusCode:301
Jan 19 21:46:34.631: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=HEAD
Jan 19 21:46:34.633: INFO: http.Client request:HEAD StatusCode:301
Jan 19 21:46:34.633: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=HEAD
Jan 19 21:46:34.637: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 19 21:46:34.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9543" for this suite. 01/19/23 21:46:34.642
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":273,"skipped":5185,"failed":0}
------------------------------
• [2.277 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:32.372
    Jan 19 21:46:32.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename proxy 01/19/23 21:46:32.373
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:32.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:32.432
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan 19 21:46:32.438: INFO: Creating pod...
    Jan 19 21:46:32.527: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9543" to be "running"
    Jan 19 21:46:32.545: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 18.769684ms
    Jan 19 21:46:34.548: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.021211488s
    Jan 19 21:46:34.548: INFO: Pod "agnhost" satisfied condition "running"
    Jan 19 21:46:34.548: INFO: Creating service...
    Jan 19 21:46:34.557: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=DELETE
    Jan 19 21:46:34.562: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 19 21:46:34.562: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=OPTIONS
    Jan 19 21:46:34.566: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 19 21:46:34.566: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=PATCH
    Jan 19 21:46:34.570: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 19 21:46:34.570: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=POST
    Jan 19 21:46:34.574: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 19 21:46:34.574: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=PUT
    Jan 19 21:46:34.577: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 19 21:46:34.577: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan 19 21:46:34.584: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 19 21:46:34.584: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan 19 21:46:34.590: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 19 21:46:34.590: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan 19 21:46:34.595: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 19 21:46:34.595: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=POST
    Jan 19 21:46:34.600: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 19 21:46:34.600: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=PUT
    Jan 19 21:46:34.623: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 19 21:46:34.623: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=GET
    Jan 19 21:46:34.627: INFO: http.Client request:GET StatusCode:301
    Jan 19 21:46:34.627: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=GET
    Jan 19 21:46:34.631: INFO: http.Client request:GET StatusCode:301
    Jan 19 21:46:34.631: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/pods/agnhost/proxy?method=HEAD
    Jan 19 21:46:34.633: INFO: http.Client request:HEAD StatusCode:301
    Jan 19 21:46:34.633: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-9543/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan 19 21:46:34.637: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 19 21:46:34.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-9543" for this suite. 01/19/23 21:46:34.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:34.65
Jan 19 21:46:34.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 21:46:34.651
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:34.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:34.674
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-8ef8d582-dd4f-4a5a-991d-b0b60b84013c 01/19/23 21:46:34.677
STEP: Creating a pod to test consume secrets 01/19/23 21:46:34.684
Jan 19 21:46:34.715: INFO: Waiting up to 5m0s for pod "pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347" in namespace "secrets-502" to be "Succeeded or Failed"
Jan 19 21:46:34.719: INFO: Pod "pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11437ms
Jan 19 21:46:36.724: INFO: Pod "pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008402617s
Jan 19 21:46:38.724: INFO: Pod "pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009017341s
STEP: Saw pod success 01/19/23 21:46:38.724
Jan 19 21:46:38.724: INFO: Pod "pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347" satisfied condition "Succeeded or Failed"
Jan 19 21:46:38.727: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347 container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 21:46:38.732
Jan 19 21:46:38.741: INFO: Waiting for pod pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347 to disappear
Jan 19 21:46:38.743: INFO: Pod pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 21:46:38.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-502" for this suite. 01/19/23 21:46:38.749
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":274,"skipped":5199,"failed":0}
------------------------------
• [4.104 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:34.65
    Jan 19 21:46:34.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 21:46:34.651
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:34.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:34.674
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-8ef8d582-dd4f-4a5a-991d-b0b60b84013c 01/19/23 21:46:34.677
    STEP: Creating a pod to test consume secrets 01/19/23 21:46:34.684
    Jan 19 21:46:34.715: INFO: Waiting up to 5m0s for pod "pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347" in namespace "secrets-502" to be "Succeeded or Failed"
    Jan 19 21:46:34.719: INFO: Pod "pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347": Phase="Pending", Reason="", readiness=false. Elapsed: 4.11437ms
    Jan 19 21:46:36.724: INFO: Pod "pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008402617s
    Jan 19 21:46:38.724: INFO: Pod "pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009017341s
    STEP: Saw pod success 01/19/23 21:46:38.724
    Jan 19 21:46:38.724: INFO: Pod "pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347" satisfied condition "Succeeded or Failed"
    Jan 19 21:46:38.727: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347 container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:46:38.732
    Jan 19 21:46:38.741: INFO: Waiting for pod pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347 to disappear
    Jan 19 21:46:38.743: INFO: Pod pod-secrets-8bc42557-cdb4-4b03-943c-0a435186f347 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 21:46:38.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-502" for this suite. 01/19/23 21:46:38.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:38.755
Jan 19 21:46:38.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:46:38.755
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:38.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:38.777
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
Jan 19 21:46:38.798: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-ade60934-fa37-402d-a0e7-0fa957da2087 01/19/23 21:46:38.798
STEP: Creating the pod 01/19/23 21:46:38.805
Jan 19 21:46:38.817: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b2c361f8-240e-4278-a144-62ba58967919" in namespace "projected-7993" to be "running and ready"
Jan 19 21:46:38.837: INFO: Pod "pod-projected-configmaps-b2c361f8-240e-4278-a144-62ba58967919": Phase="Pending", Reason="", readiness=false. Elapsed: 19.61212ms
Jan 19 21:46:38.837: INFO: The phase of Pod pod-projected-configmaps-b2c361f8-240e-4278-a144-62ba58967919 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:46:40.852: INFO: Pod "pod-projected-configmaps-b2c361f8-240e-4278-a144-62ba58967919": Phase="Running", Reason="", readiness=true. Elapsed: 2.034739287s
Jan 19 21:46:40.852: INFO: The phase of Pod pod-projected-configmaps-b2c361f8-240e-4278-a144-62ba58967919 is Running (Ready = true)
Jan 19 21:46:40.852: INFO: Pod "pod-projected-configmaps-b2c361f8-240e-4278-a144-62ba58967919" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-ade60934-fa37-402d-a0e7-0fa957da2087 01/19/23 21:46:40.869
STEP: waiting to observe update in volume 01/19/23 21:46:40.884
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 21:46:42.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7993" for this suite. 01/19/23 21:46:42.917
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":275,"skipped":5207,"failed":0}
------------------------------
• [4.167 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:38.755
    Jan 19 21:46:38.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:46:38.755
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:38.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:38.777
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    Jan 19 21:46:38.798: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-ade60934-fa37-402d-a0e7-0fa957da2087 01/19/23 21:46:38.798
    STEP: Creating the pod 01/19/23 21:46:38.805
    Jan 19 21:46:38.817: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b2c361f8-240e-4278-a144-62ba58967919" in namespace "projected-7993" to be "running and ready"
    Jan 19 21:46:38.837: INFO: Pod "pod-projected-configmaps-b2c361f8-240e-4278-a144-62ba58967919": Phase="Pending", Reason="", readiness=false. Elapsed: 19.61212ms
    Jan 19 21:46:38.837: INFO: The phase of Pod pod-projected-configmaps-b2c361f8-240e-4278-a144-62ba58967919 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:46:40.852: INFO: Pod "pod-projected-configmaps-b2c361f8-240e-4278-a144-62ba58967919": Phase="Running", Reason="", readiness=true. Elapsed: 2.034739287s
    Jan 19 21:46:40.852: INFO: The phase of Pod pod-projected-configmaps-b2c361f8-240e-4278-a144-62ba58967919 is Running (Ready = true)
    Jan 19 21:46:40.852: INFO: Pod "pod-projected-configmaps-b2c361f8-240e-4278-a144-62ba58967919" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-ade60934-fa37-402d-a0e7-0fa957da2087 01/19/23 21:46:40.869
    STEP: waiting to observe update in volume 01/19/23 21:46:40.884
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 21:46:42.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7993" for this suite. 01/19/23 21:46:42.917
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:46:42.922
Jan 19 21:46:42.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:46:42.923
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:42.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:42.946
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 01/19/23 21:46:42.949
Jan 19 21:46:42.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: rename a version 01/19/23 21:47:01.475
STEP: check the new version name is served 01/19/23 21:47:01.487
STEP: check the old version name is removed 01/19/23 21:47:10.011
STEP: check the other version is not changed 01/19/23 21:47:13.08
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:47:28.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4884" for this suite. 01/19/23 21:47:28.019
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":276,"skipped":5210,"failed":0}
------------------------------
• [SLOW TEST] [45.102 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:46:42.922
    Jan 19 21:46:42.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename crd-publish-openapi 01/19/23 21:46:42.923
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:46:42.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:46:42.946
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 01/19/23 21:46:42.949
    Jan 19 21:46:42.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: rename a version 01/19/23 21:47:01.475
    STEP: check the new version name is served 01/19/23 21:47:01.487
    STEP: check the old version name is removed 01/19/23 21:47:10.011
    STEP: check the other version is not changed 01/19/23 21:47:13.08
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:47:28.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4884" for this suite. 01/19/23 21:47:28.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:47:28.025
Jan 19 21:47:28.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename events 01/19/23 21:47:28.026
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:28.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:28.039
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/19/23 21:47:28.046
STEP: listing all events in all namespaces 01/19/23 21:47:28.064
STEP: patching the test event 01/19/23 21:47:28.126
STEP: fetching the test event 01/19/23 21:47:28.136
STEP: updating the test event 01/19/23 21:47:28.146
STEP: getting the test event 01/19/23 21:47:28.156
STEP: deleting the test event 01/19/23 21:47:28.158
STEP: listing all events in all namespaces 01/19/23 21:47:28.17
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jan 19 21:47:28.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-768" for this suite. 01/19/23 21:47:28.22
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":277,"skipped":5222,"failed":0}
------------------------------
• [0.201 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:47:28.025
    Jan 19 21:47:28.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename events 01/19/23 21:47:28.026
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:28.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:28.039
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/19/23 21:47:28.046
    STEP: listing all events in all namespaces 01/19/23 21:47:28.064
    STEP: patching the test event 01/19/23 21:47:28.126
    STEP: fetching the test event 01/19/23 21:47:28.136
    STEP: updating the test event 01/19/23 21:47:28.146
    STEP: getting the test event 01/19/23 21:47:28.156
    STEP: deleting the test event 01/19/23 21:47:28.158
    STEP: listing all events in all namespaces 01/19/23 21:47:28.17
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jan 19 21:47:28.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-768" for this suite. 01/19/23 21:47:28.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:47:28.227
Jan 19 21:47:28.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 21:47:28.228
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:28.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:28.243
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 01/19/23 21:47:28.247
Jan 19 21:47:28.295: INFO: Waiting up to 5m0s for pod "downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4" in namespace "downward-api-12" to be "Succeeded or Failed"
Jan 19 21:47:28.298: INFO: Pod "downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.428065ms
Jan 19 21:47:30.303: INFO: Pod "downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008320467s
Jan 19 21:47:32.301: INFO: Pod "downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005839883s
STEP: Saw pod success 01/19/23 21:47:32.301
Jan 19 21:47:32.301: INFO: Pod "downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4" satisfied condition "Succeeded or Failed"
Jan 19 21:47:32.303: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4 container dapi-container: <nil>
STEP: delete the pod 01/19/23 21:47:32.314
Jan 19 21:47:32.323: INFO: Waiting for pod downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4 to disappear
Jan 19 21:47:32.325: INFO: Pod downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jan 19 21:47:32.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-12" for this suite. 01/19/23 21:47:32.329
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":278,"skipped":5232,"failed":0}
------------------------------
• [4.107 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:47:28.227
    Jan 19 21:47:28.227: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 21:47:28.228
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:28.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:28.243
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 01/19/23 21:47:28.247
    Jan 19 21:47:28.295: INFO: Waiting up to 5m0s for pod "downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4" in namespace "downward-api-12" to be "Succeeded or Failed"
    Jan 19 21:47:28.298: INFO: Pod "downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.428065ms
    Jan 19 21:47:30.303: INFO: Pod "downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008320467s
    Jan 19 21:47:32.301: INFO: Pod "downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005839883s
    STEP: Saw pod success 01/19/23 21:47:32.301
    Jan 19 21:47:32.301: INFO: Pod "downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4" satisfied condition "Succeeded or Failed"
    Jan 19 21:47:32.303: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4 container dapi-container: <nil>
    STEP: delete the pod 01/19/23 21:47:32.314
    Jan 19 21:47:32.323: INFO: Waiting for pod downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4 to disappear
    Jan 19 21:47:32.325: INFO: Pod downward-api-53dfb4f8-5c5a-4353-bcc2-eca4a518b7c4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jan 19 21:47:32.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-12" for this suite. 01/19/23 21:47:32.329
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:47:32.334
Jan 19 21:47:32.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:47:32.335
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:32.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:32.355
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 01/19/23 21:47:32.357
Jan 19 21:47:32.357: INFO: namespace kubectl-5355
Jan 19 21:47:32.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5355 create -f -'
Jan 19 21:47:34.239: INFO: stderr: ""
Jan 19 21:47:34.239: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/19/23 21:47:34.239
Jan 19 21:47:35.241: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 21:47:35.241: INFO: Found 0 / 1
Jan 19 21:47:36.242: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 21:47:36.242: INFO: Found 1 / 1
Jan 19 21:47:36.242: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan 19 21:47:36.245: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 21:47:36.245: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 19 21:47:36.245: INFO: wait on agnhost-primary startup in kubectl-5355 
Jan 19 21:47:36.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5355 logs agnhost-primary-wfzhj agnhost-primary'
Jan 19 21:47:36.304: INFO: stderr: ""
Jan 19 21:47:36.304: INFO: stdout: "Paused\n"
STEP: exposing RC 01/19/23 21:47:36.304
Jan 19 21:47:36.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5355 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan 19 21:47:36.373: INFO: stderr: ""
Jan 19 21:47:36.373: INFO: stdout: "service/rm2 exposed\n"
Jan 19 21:47:36.376: INFO: Service rm2 in namespace kubectl-5355 found.
STEP: exposing service 01/19/23 21:47:38.382
Jan 19 21:47:38.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5355 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan 19 21:47:38.456: INFO: stderr: ""
Jan 19 21:47:38.456: INFO: stdout: "service/rm3 exposed\n"
Jan 19 21:47:38.460: INFO: Service rm3 in namespace kubectl-5355 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:47:40.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5355" for this suite. 01/19/23 21:47:40.469
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":279,"skipped":5236,"failed":0}
------------------------------
• [SLOW TEST] [8.140 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:47:32.334
    Jan 19 21:47:32.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:47:32.335
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:32.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:32.355
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 01/19/23 21:47:32.357
    Jan 19 21:47:32.357: INFO: namespace kubectl-5355
    Jan 19 21:47:32.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5355 create -f -'
    Jan 19 21:47:34.239: INFO: stderr: ""
    Jan 19 21:47:34.239: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/19/23 21:47:34.239
    Jan 19 21:47:35.241: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 21:47:35.241: INFO: Found 0 / 1
    Jan 19 21:47:36.242: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 21:47:36.242: INFO: Found 1 / 1
    Jan 19 21:47:36.242: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan 19 21:47:36.245: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 21:47:36.245: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 19 21:47:36.245: INFO: wait on agnhost-primary startup in kubectl-5355 
    Jan 19 21:47:36.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5355 logs agnhost-primary-wfzhj agnhost-primary'
    Jan 19 21:47:36.304: INFO: stderr: ""
    Jan 19 21:47:36.304: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/19/23 21:47:36.304
    Jan 19 21:47:36.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5355 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan 19 21:47:36.373: INFO: stderr: ""
    Jan 19 21:47:36.373: INFO: stdout: "service/rm2 exposed\n"
    Jan 19 21:47:36.376: INFO: Service rm2 in namespace kubectl-5355 found.
    STEP: exposing service 01/19/23 21:47:38.382
    Jan 19 21:47:38.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5355 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan 19 21:47:38.456: INFO: stderr: ""
    Jan 19 21:47:38.456: INFO: stdout: "service/rm3 exposed\n"
    Jan 19 21:47:38.460: INFO: Service rm3 in namespace kubectl-5355 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:47:40.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5355" for this suite. 01/19/23 21:47:40.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:47:40.475
Jan 19 21:47:40.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 21:47:40.475
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:40.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:40.49
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 21:47:40.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1718" for this suite. 01/19/23 21:47:40.518
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":280,"skipped":5241,"failed":0}
------------------------------
• [0.063 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:47:40.475
    Jan 19 21:47:40.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 21:47:40.475
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:40.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:40.49
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 21:47:40.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1718" for this suite. 01/19/23 21:47:40.518
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:47:40.538
Jan 19 21:47:40.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:47:40.539
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:40.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:40.569
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:47:40.58
Jan 19 21:47:40.617: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110" in namespace "projected-3953" to be "Succeeded or Failed"
Jan 19 21:47:40.619: INFO: Pod "downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327679ms
Jan 19 21:47:42.623: INFO: Pod "downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005668095s
Jan 19 21:47:44.622: INFO: Pod "downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005213399s
STEP: Saw pod success 01/19/23 21:47:44.622
Jan 19 21:47:44.622: INFO: Pod "downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110" satisfied condition "Succeeded or Failed"
Jan 19 21:47:44.625: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110 container client-container: <nil>
STEP: delete the pod 01/19/23 21:47:44.633
Jan 19 21:47:44.642: INFO: Waiting for pod downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110 to disappear
Jan 19 21:47:44.644: INFO: Pod downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 21:47:44.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3953" for this suite. 01/19/23 21:47:44.648
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":281,"skipped":5246,"failed":0}
------------------------------
• [4.115 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:47:40.538
    Jan 19 21:47:40.538: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:47:40.539
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:40.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:40.569
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:47:40.58
    Jan 19 21:47:40.617: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110" in namespace "projected-3953" to be "Succeeded or Failed"
    Jan 19 21:47:40.619: INFO: Pod "downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327679ms
    Jan 19 21:47:42.623: INFO: Pod "downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005668095s
    Jan 19 21:47:44.622: INFO: Pod "downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005213399s
    STEP: Saw pod success 01/19/23 21:47:44.622
    Jan 19 21:47:44.622: INFO: Pod "downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110" satisfied condition "Succeeded or Failed"
    Jan 19 21:47:44.625: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110 container client-container: <nil>
    STEP: delete the pod 01/19/23 21:47:44.633
    Jan 19 21:47:44.642: INFO: Waiting for pod downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110 to disappear
    Jan 19 21:47:44.644: INFO: Pod downwardapi-volume-2d50bdb4-e1bd-4462-88b0-f05da5793110 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 21:47:44.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3953" for this suite. 01/19/23 21:47:44.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:47:44.654
Jan 19 21:47:44.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:47:44.654
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:44.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:44.666
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 21:47:44.671
Jan 19 21:47:44.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9909 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jan 19 21:47:44.750: INFO: stderr: ""
Jan 19 21:47:44.750: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/19/23 21:47:44.75
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jan 19 21:47:44.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9909 delete pods e2e-test-httpd-pod'
Jan 19 21:47:47.442: INFO: stderr: ""
Jan 19 21:47:47.442: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:47:47.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9909" for this suite. 01/19/23 21:47:47.447
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":282,"skipped":5268,"failed":0}
------------------------------
• [2.799 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:47:44.654
    Jan 19 21:47:44.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:47:44.654
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:44.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:44.666
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 01/19/23 21:47:44.671
    Jan 19 21:47:44.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9909 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jan 19 21:47:44.750: INFO: stderr: ""
    Jan 19 21:47:44.750: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/19/23 21:47:44.75
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jan 19 21:47:44.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9909 delete pods e2e-test-httpd-pod'
    Jan 19 21:47:47.442: INFO: stderr: ""
    Jan 19 21:47:47.442: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:47:47.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9909" for this suite. 01/19/23 21:47:47.447
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:47:47.453
Jan 19 21:47:47.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename sched-pred 01/19/23 21:47:47.454
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:47.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:47.473
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 19 21:47:47.503: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 21:47:47.519: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 21:47:47.531: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-146-42.ec2.internal before test
Jan 19 21:47:47.570: INFO: aws-ebs-csi-driver-node-vxgq2 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:41 +0000 UTC (3 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 21:47:47.570: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 21:47:47.570: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 21:47:47.570: INFO: tuned-nbzt4 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container tuned ready: true, restart count 1
Jan 19 21:47:47.570: INFO: downloads-8d695cd69-lcsd9 from openshift-console started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container download-server ready: true, restart count 0
Jan 19 21:47:47.570: INFO: dns-default-c5qwq from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container dns ready: true, restart count 1
Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.570: INFO: node-resolver-g5jk6 from openshift-dns started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 21:47:47.570: INFO: node-ca-crm7t from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 21:47:47.570: INFO: ingress-canary-nfwhl from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 21:47:47.570: INFO: migrator-5c54d8d69d-l5lss from openshift-kube-storage-version-migrator started at 2023-01-19 18:42:53 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container migrator ready: true, restart count 0
Jan 19 21:47:47.570: INFO: machine-config-daemon-cc29b from openshift-machine-config-operator started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 21:47:47.570: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 21:47:47.570: INFO: node-exporter-9zc87 from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.570: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 21:47:47.570: INFO: sre-dns-latency-exporter-jsn9x from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container main ready: true, restart count 1
Jan 19 21:47:47.570: INFO: multus-5zck8 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 21:47:47.570: INFO: multus-additional-cni-plugins-88bc2 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 21:47:47.570: INFO: network-metrics-daemon-zvctv from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.570: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 21:47:47.570: INFO: network-check-source-746dd6c885-bmf5k from openshift-network-diagnostics started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 19 21:47:47.570: INFO: network-check-target-g5qsk from openshift-network-diagnostics started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 21:47:47.570: INFO: obo-prometheus-operator-64989cfc68-hdxb9 from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 21:47:47.570: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-gqhlz from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 21:47:47.570: INFO: osd-metrics-exporter-registry-qdjtk from openshift-osd-metrics started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.570: INFO: ovnkube-node-ftv9n from openshift-ovn-kubernetes started at 2023-01-19 18:15:41 +0000 UTC (5 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 21:47:47.570: INFO: 	Container ovn-acl-logging ready: true, restart count 2
Jan 19 21:47:47.570: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 21:47:47.570: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 21:47:47.570: INFO: rbac-permissions-operator-registry-rc4js from openshift-rbac-permissions started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.570: INFO: splunkforwarder-ds-7rpx7 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 21:47:47.570: INFO: prometheus-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (6 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 21:47:47.570: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 21:47:47.570: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 21:47:47.570: INFO: thanos-ruler-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (3 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:47:47.570: INFO: 	Container thanos-ruler ready: true, restart count 0
Jan 19 21:47:47.570: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
Jan 19 21:47:47.570: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-chbvs from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.570: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:47:47.570: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 21:47:47.570: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-151-158.ec2.internal before test
Jan 19 21:47:47.608: INFO: addon-operator-manager-84ff88fc6-fmhnb from openshift-addon-operator started at 2023-01-19 18:46:50 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container manager ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container metrics-relay-server ready: true, restart count 0
Jan 19 21:47:47.608: INFO: addon-operator-webhooks-67d9d47489-4kbk2 from openshift-addon-operator started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container webhook ready: true, restart count 0
Jan 19 21:47:47.608: INFO: aws-ebs-csi-driver-node-c8m6c from openshift-cluster-csi-drivers started at 2023-01-19 18:32:48 +0000 UTC (3 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 21:47:47.608: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 21:47:47.608: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 21:47:47.608: INFO: tuned-5h69k from openshift-cluster-node-tuning-operator started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container tuned ready: true, restart count 1
Jan 19 21:47:47.608: INFO: custom-domains-operator-5764f4df49-8nbdk from openshift-custom-domains-operator started at 2023-01-19 19:15:08 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container custom-domains-operator ready: true, restart count 0
Jan 19 21:47:47.608: INFO: deployment-validation-operator-7b8fd479cb-wtzg7 from openshift-deployment-validation-operator started at 2023-01-19 18:46:50 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container deployment-validation-operator ready: true, restart count 0
Jan 19 21:47:47.608: INFO: dns-default-d9szl from openshift-dns started at 2023-01-19 21:45:22 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container dns ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:47:47.608: INFO: node-resolver-9cwpx from openshift-dns started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 21:47:47.608: INFO: image-pruner-27902580-lzd8r from openshift-image-registry started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 21:47:47.608: INFO: image-pruner-27902640-c6mlt from openshift-image-registry started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 21:47:47.608: INFO: image-pruner-27902700-cf7x6 from openshift-image-registry started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 21:47:47.608: INFO: image-registry-656cfd9bd9-9x67r from openshift-image-registry started at 2023-01-19 19:15:11 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container registry ready: true, restart count 0
Jan 19 21:47:47.608: INFO: node-ca-k89p7 from openshift-image-registry started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 21:47:47.608: INFO: ingress-canary-v2tjj from openshift-ingress-canary started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 21:47:47.608: INFO: router-default-6bf5ff7dcb-5nzvc from openshift-ingress started at 2023-01-19 18:47:44 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container router ready: true, restart count 0
Jan 19 21:47:47.608: INFO: machine-config-daemon-28fzw from openshift-machine-config-operator started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 21:47:47.608: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 21:47:47.608: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container alertmanager ready: true, restart count 1
Jan 19 21:47:47.608: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 21:47:47.608: INFO: configure-alertmanager-operator-7d94bbbcf-8h7xs from openshift-monitoring started at 2023-01-19 19:16:27 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Jan 19 21:47:47.608: INFO: kube-state-metrics-666f4cbf77-5w8rk from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 19 21:47:47.608: INFO: node-exporter-7t5b7 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.608: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 21:47:47.608: INFO: openshift-state-metrics-7d5c9d867c-97gzd from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 19 21:47:47.608: INFO: prometheus-adapter-5bc6487f7c-q2ng7 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 19 21:47:47.608: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 21:47:47.608: INFO: prometheus-operator-564b78d5ff-v4xnk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 21:47:47.608: INFO: prometheus-operator-admission-webhook-749df5cf4f-zj8xj from openshift-monitoring started at 2023-01-19 18:46:40 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 21:47:47.608: INFO: sre-dns-latency-exporter-2q899 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container main ready: true, restart count 1
Jan 19 21:47:47.608: INFO: sre-stuck-ebs-vols-1-vdlrh from openshift-monitoring started at 2023-01-19 19:17:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container main ready: true, restart count 0
Jan 19 21:47:47.608: INFO: telemeter-client-54bbbbf598-d9fdk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (3 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container reload ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 19 21:47:47.608: INFO: thanos-querier-fcd5b6c8d-ql8qb from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container thanos-query ready: true, restart count 0
Jan 19 21:47:47.608: INFO: multus-additional-cni-plugins-vslld from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 21:47:47.608: INFO: multus-zqhtq from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 21:47:47.608: INFO: network-metrics-daemon-677lg from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.608: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 21:47:47.608: INFO: must-gather-operator-56b466776c-qv8sh from openshift-must-gather-operator started at 2023-01-19 19:17:38 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container must-gather-operator ready: true, restart count 0
Jan 19 21:47:47.608: INFO: network-check-target-xrr4d from openshift-network-diagnostics started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 21:47:47.608: INFO: ocm-agent-ddfdc6544-4fxrp from openshift-ocm-agent-operator started at 2023-01-19 19:17:41 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container ocm-agent ready: true, restart count 0
Jan 19 21:47:47.608: INFO: ocm-agent-operator-6cdb45fd86-4clcj from openshift-ocm-agent-operator started at 2023-01-19 19:17:43 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container ocm-agent-operator ready: true, restart count 0
Jan 19 21:47:47.608: INFO: ovnkube-node-gvwrr from openshift-ovn-kubernetes started at 2023-01-19 18:32:48 +0000 UTC (5 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 21:47:47.608: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 21:47:47.608: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 21:47:47.608: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 21:47:47.608: INFO: rbac-permissions-operator-5fb57974d8-htnjj from openshift-rbac-permissions started at 2023-01-19 19:17:50 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Jan 19 21:47:47.608: INFO: splunkforwarder-ds-ztlpn from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 21:47:47.608: INFO: splunk-forwarder-operator-7d66568cf7-bksrn from openshift-splunk-forwarder-operator started at 2023-01-19 19:17:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Jan 19 21:47:47.608: INFO: managed-velero-operator-788754d4b6-n89fw from openshift-velero started at 2023-01-19 19:17:56 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container managed-velero-operator ready: true, restart count 0
Jan 19 21:47:47.608: INFO: velero-7c64598b8c-8blx4 from openshift-velero started at 2023-01-19 19:18:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container velero ready: true, restart count 0
Jan 19 21:47:47.608: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-8mc8n from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.608: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 21:47:47.608: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-171-213.ec2.internal before test
Jan 19 21:47:47.641: INFO: aws-ebs-csi-driver-node-svd2s from openshift-cluster-csi-drivers started at 2023-01-19 18:15:58 +0000 UTC (3 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 21:47:47.641: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 21:47:47.641: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 21:47:47.641: INFO: tuned-9p2db from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container tuned ready: true, restart count 1
Jan 19 21:47:47.641: INFO: dns-default-zbx44 from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container dns ready: true, restart count 1
Jan 19 21:47:47.641: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.641: INFO: node-resolver-5mb95 from openshift-dns started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 21:47:47.641: INFO: node-ca-7c5fq from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 21:47:47.641: INFO: ingress-canary-6sc9q from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 21:47:47.641: INFO: machine-config-daemon-xxm6j from openshift-machine-config-operator started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 21:47:47.641: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 21:47:47.641: INFO: node-exporter-h445b from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.641: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 21:47:47.641: INFO: sre-dns-latency-exporter-vhwwx from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container main ready: true, restart count 1
Jan 19 21:47:47.641: INFO: multus-additional-cni-plugins-nhwfz from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 21:47:47.641: INFO: multus-j4ngm from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 21:47:47.641: INFO: network-metrics-daemon-hp85h from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.641: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 21:47:47.641: INFO: network-check-target-mg7p4 from openshift-network-diagnostics started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 21:47:47.641: INFO: ovnkube-node-kk5t9 from openshift-ovn-kubernetes started at 2023-01-19 18:15:58 +0000 UTC (5 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.641: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 21:47:47.641: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 21:47:47.641: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 21:47:47.641: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 21:47:47.641: INFO: splunkforwarder-ds-66kjq from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 21:47:47.641: INFO: sonobuoy-e2e-job-4d801e056c4540b8 from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container e2e ready: true, restart count 0
Jan 19 21:47:47.641: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:47:47.641: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-m8vnk from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.641: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:47:47.641: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 21:47:47.641: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-172-44.ec2.internal before test
Jan 19 21:47:47.675: INFO: aws-ebs-csi-driver-node-z6klb from openshift-cluster-csi-drivers started at 2023-01-19 18:15:33 +0000 UTC (3 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 21:47:47.675: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 21:47:47.675: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 21:47:47.675: INFO: tuned-gjnt5 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container tuned ready: true, restart count 1
Jan 19 21:47:47.675: INFO: dns-default-dfndw from openshift-dns started at 2023-01-19 21:22:44 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container dns ready: true, restart count 0
Jan 19 21:47:47.675: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:47:47.675: INFO: node-resolver-b85qs from openshift-dns started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 21:47:47.675: INFO: node-ca-dsxvs from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 21:47:47.675: INFO: ingress-canary-cs8pq from openshift-ingress-canary started at 2023-01-19 21:22:44 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 19 21:47:47.675: INFO: machine-config-daemon-cllj8 from openshift-machine-config-operator started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 21:47:47.675: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 21:47:47.675: INFO: node-exporter-2vp9v from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.675: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 21:47:47.675: INFO: sre-dns-latency-exporter-ctmgr from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container main ready: true, restart count 1
Jan 19 21:47:47.675: INFO: multus-additional-cni-plugins-p5pjp from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 21:47:47.675: INFO: multus-xzxwg from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 21:47:47.675: INFO: network-metrics-daemon-hz5pj from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.675: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 21:47:47.675: INFO: network-check-target-nq9ln from openshift-network-diagnostics started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 21:47:47.675: INFO: collect-profiles-27902730-zm5kk from openshift-operator-lifecycle-manager started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 21:47:47.675: INFO: collect-profiles-27902745-9gctc from openshift-operator-lifecycle-manager started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 21:47:47.675: INFO: ovnkube-node-zmvs8 from openshift-ovn-kubernetes started at 2023-01-19 18:15:33 +0000 UTC (5 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.675: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 21:47:47.675: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 21:47:47.675: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 21:47:47.675: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 21:47:47.675: INFO: splunkforwarder-ds-pj5q8 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 21:47:47.675: INFO: sonobuoy from sonobuoy started at 2023-01-19 20:29:23 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.675: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 19 21:47:47.675: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-cbfjg from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:47:47.676: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 21:47:47.676: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-188-71.ec2.internal before test
Jan 19 21:47:47.711: INFO: addon-operator-catalog-zxs8v from openshift-addon-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: cloud-ingress-operator-registry-pqwm7 from openshift-cloud-ingress-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: aws-ebs-csi-driver-node-wmvd4 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:13 +0000 UTC (3 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 21:47:47.711: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 21:47:47.711: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 21:47:47.711: INFO: tuned-7x9t2 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container tuned ready: true, restart count 1
Jan 19 21:47:47.711: INFO: downloads-8d695cd69-hltgd from openshift-console started at 2023-01-19 18:39:20 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container download-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: custom-domains-operator-registry-kdw7s from openshift-custom-domains-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: deployment-validation-operator-catalog-nq94w from openshift-deployment-validation-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: dns-default-tzrx6 from openshift-dns started at 2023-01-19 18:15:43 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container dns ready: true, restart count 1
Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.711: INFO: node-resolver-l2fzw from openshift-dns started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 21:47:47.711: INFO: node-ca-kzsvk from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 21:47:47.711: INFO: ingress-canary-t6tqp from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 21:47:47.711: INFO: machine-config-daemon-zhllf from openshift-machine-config-operator started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 21:47:47.711: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 21:47:47.711: INFO: managed-node-metadata-operator-registry-m6dxz from openshift-managed-node-metadata-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: managed-upgrade-operator-catalog-thxts from openshift-managed-upgrade-operator started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: configure-alertmanager-operator-registry-sjcb5 from openshift-monitoring started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: node-exporter-dc59d from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.711: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 21:47:47.711: INFO: osd-cluster-ready-nb8xk from openshift-monitoring started at 2023-01-19 18:39:21 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container osd-cluster-ready ready: false, restart count 7
Jan 19 21:47:47.711: INFO: sre-dns-latency-exporter-5dz25 from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container main ready: true, restart count 1
Jan 19 21:47:47.711: INFO: multus-additional-cni-plugins-hvpd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 21:47:47.711: INFO: multus-mptd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 21:47:47.711: INFO: network-metrics-daemon-s2l94 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.711: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 21:47:47.711: INFO: must-gather-operator-registry-n7fvg from openshift-must-gather-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: network-check-target-d8gvt from openshift-network-diagnostics started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 21:47:47.711: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-b4r6s from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 21:47:47.711: INFO: observability-operator-6dd79df7dc-z7frh from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container operator ready: true, restart count 0
Jan 19 21:47:47.711: INFO: observability-operator-catalog-vrnkr from openshift-observability-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: ocm-agent-operator-registry-dwfc4 from openshift-ocm-agent-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: collect-profiles-27902715-lq9hm from openshift-operator-lifecycle-manager started at 2023-01-19 21:15:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 21:47:47.711: INFO: ovnkube-node-wcrc4 from openshift-ovn-kubernetes started at 2023-01-19 18:15:13 +0000 UTC (5 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 21:47:47.711: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 21:47:47.711: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 21:47:47.711: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 21:47:47.711: INFO: route-monitor-operator-registry-whpgh from openshift-route-monitor-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: splunkforwarder-ds-xzbdk from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 21:47:47.711: INFO: splunk-forwarder-operator-catalog-fqp2b from openshift-splunk-forwarder-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: prometheus-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:37:05 +0000 UTC (6 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 21:47:47.711: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 21:47:47.711: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 21:47:47.711: INFO: thanos-ruler-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:37:04 +0000 UTC (3 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:47:47.711: INFO: 	Container thanos-ruler ready: true, restart count 0
Jan 19 21:47:47.711: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
Jan 19 21:47:47.711: INFO: managed-velero-operator-registry-6lc6b from openshift-velero started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:47:47.711: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-q5j5c from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.711: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:47:47.711: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 21:47:47.711: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-207-77.ec2.internal before test
Jan 19 21:47:47.755: INFO: addon-operator-webhooks-67d9d47489-f6z86 from openshift-addon-operator started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container webhook ready: true, restart count 0
Jan 19 21:47:47.755: INFO: osd-delete-ownerrefs-serviceaccounts-27902677-st5fv from openshift-backplane-srep started at 2023-01-19 20:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 21:47:47.755: INFO: osd-delete-ownerrefs-serviceaccounts-27902707-pxrnh from openshift-backplane-srep started at 2023-01-19 21:07:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 21:47:47.755: INFO: osd-delete-ownerrefs-serviceaccounts-27902737-ww4vb from openshift-backplane-srep started at 2023-01-19 21:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 21:47:47.755: INFO: osd-delete-backplane-serviceaccounts-27902720-brqcs from openshift-backplane started at 2023-01-19 21:20:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 21:47:47.755: INFO: osd-delete-backplane-serviceaccounts-27902730-ph7cv from openshift-backplane started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 21:47:47.755: INFO: osd-delete-backplane-serviceaccounts-27902740-x6pck from openshift-backplane started at 2023-01-19 21:40:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 21:47:47.755: INFO: sre-build-test-27902711-r8jct from openshift-build-test started at 2023-01-19 21:11:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container sre-build-test ready: false, restart count 0
Jan 19 21:47:47.755: INFO: cloud-ingress-operator-5b5df787b5-bdk4j from openshift-cloud-ingress-operator started at 2023-01-19 19:15:07 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Jan 19 21:47:47.755: INFO: aws-ebs-csi-driver-node-9sjjv from openshift-cluster-csi-drivers started at 2023-01-19 18:33:36 +0000 UTC (3 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 21:47:47.755: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 21:47:47.755: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 21:47:47.755: INFO: tuned-qkjmb from openshift-cluster-node-tuning-operator started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container tuned ready: true, restart count 1
Jan 19 21:47:47.755: INFO: dns-default-bcrdt from openshift-dns started at 2023-01-19 21:45:58 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container dns ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:47:47.755: INFO: node-resolver-m4bvp from openshift-dns started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 21:47:47.755: INFO: image-registry-656cfd9bd9-bj4wn from openshift-image-registry started at 2023-01-19 19:15:47 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container registry ready: true, restart count 0
Jan 19 21:47:47.755: INFO: node-ca-kwgfx from openshift-image-registry started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 21:47:47.755: INFO: ingress-canary-6xnfp from openshift-ingress-canary started at 2023-01-19 18:34:10 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 21:47:47.755: INFO: router-default-6bf5ff7dcb-6vrcg from openshift-ingress started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container router ready: true, restart count 0
Jan 19 21:47:47.755: INFO: osd-disable-cpms-27902745-qqjbm from openshift-machine-api started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-disable-cpms ready: false, restart count 0
Jan 19 21:47:47.755: INFO: machine-config-daemon-2bt5b from openshift-machine-config-operator started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 21:47:47.755: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 21:47:47.755: INFO: managed-node-metadata-operator-649f646d54-smsh7 from openshift-managed-node-metadata-operator started at 2023-01-19 19:16:23 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
Jan 19 21:47:47.755: INFO: osd-patch-subscription-source-27902580-hz9tz from openshift-marketplace started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 21:47:47.755: INFO: osd-patch-subscription-source-27902640-qrd58 from openshift-marketplace started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 21:47:47.755: INFO: osd-patch-subscription-source-27902700-smwhh from openshift-marketplace started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 21:47:47.755: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container alertmanager ready: true, restart count 1
Jan 19 21:47:47.755: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 21:47:47.755: INFO: node-exporter-s77nf from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.755: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 21:47:47.755: INFO: osd-rebalance-infra-nodes-27902715-hm9rx from openshift-monitoring started at 2023-01-19 21:15:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 21:47:47.755: INFO: osd-rebalance-infra-nodes-27902730-h7lrq from openshift-monitoring started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 21:47:47.755: INFO: osd-rebalance-infra-nodes-27902745-cwdn2 from openshift-monitoring started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 21:47:47.755: INFO: prometheus-adapter-5bc6487f7c-sv96c from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 19 21:47:47.755: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 21:47:47.755: INFO: prometheus-operator-admission-webhook-749df5cf4f-bmj4n from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 21:47:47.755: INFO: sre-dns-latency-exporter-4lksg from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container main ready: true, restart count 1
Jan 19 21:47:47.755: INFO: sre-ebs-iops-reporter-1-sg7b2 from openshift-monitoring started at 2023-01-19 19:16:28 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container main ready: true, restart count 0
Jan 19 21:47:47.755: INFO: thanos-querier-fcd5b6c8d-9jjkp from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 21:47:47.755: INFO: 	Container thanos-query ready: true, restart count 0
Jan 19 21:47:47.755: INFO: token-refresher-6f7c46d758-mld9t from openshift-monitoring started at 2023-01-19 19:17:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container token-refresher ready: true, restart count 0
Jan 19 21:47:47.755: INFO: multus-additional-cni-plugins-476gw from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 21:47:47.755: INFO: multus-mczpf from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 21:47:47.755: INFO: network-metrics-daemon-ddkb6 from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.755: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 21:47:47.755: INFO: network-check-target-d8k5s from openshift-network-diagnostics started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 21:47:47.755: INFO: osd-metrics-exporter-679d75d598-rsv49 from openshift-osd-metrics started at 2023-01-19 19:17:47 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
Jan 19 21:47:47.755: INFO: ovnkube-node-gq9vc from openshift-ovn-kubernetes started at 2023-01-19 18:33:36 +0000 UTC (5 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 21:47:47.755: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 21:47:47.755: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 21:47:47.755: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 21:47:47.755: INFO: blackbox-exporter-5f7f7bf859-l8ptw from openshift-route-monitor-operator started at 2023-01-19 19:17:51 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 19 21:47:47.755: INFO: route-monitor-operator-controller-manager-bfbff575-pfqc4 from openshift-route-monitor-operator started at 2023-01-19 19:17:53 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container manager ready: true, restart count 0
Jan 19 21:47:47.755: INFO: splunkforwarder-ds-h56tr from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 21:47:47.755: INFO: builds-pruner-27902580-p9bnq from openshift-sre-pruning started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 21:47:47.755: INFO: builds-pruner-27902640-bj57l from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 21:47:47.755: INFO: builds-pruner-27902700-t7v8n from openshift-sre-pruning started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.755: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 21:47:47.755: INFO: deployments-pruner-27902580-qfgtd from openshift-sre-pruning started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.756: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 21:47:47.756: INFO: deployments-pruner-27902640-vhqsv from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.756: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 21:47:47.756: INFO: deployments-pruner-27902700-fvhm7 from openshift-sre-pruning started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:47:47.756: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 21:47:47.756: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-hj9fv from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:47:47.756: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:47:47.756: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node ip-10-0-146-42.ec2.internal 01/19/23 21:47:47.851
STEP: verifying the node has the label node ip-10-0-151-158.ec2.internal 01/19/23 21:47:47.873
STEP: verifying the node has the label node ip-10-0-171-213.ec2.internal 01/19/23 21:47:47.891
STEP: verifying the node has the label node ip-10-0-172-44.ec2.internal 01/19/23 21:47:47.914
STEP: verifying the node has the label node ip-10-0-188-71.ec2.internal 01/19/23 21:47:47.942
STEP: verifying the node has the label node ip-10-0-207-77.ec2.internal 01/19/23 21:47:47.972
Jan 19 21:47:48.067: INFO: Pod addon-operator-catalog-zxs8v requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.067: INFO: Pod addon-operator-manager-84ff88fc6-fmhnb requesting resource cpu=200m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.067: INFO: Pod addon-operator-webhooks-67d9d47489-4kbk2 requesting resource cpu=100m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.067: INFO: Pod addon-operator-webhooks-67d9d47489-f6z86 requesting resource cpu=100m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.067: INFO: Pod cloud-ingress-operator-5b5df787b5-bdk4j requesting resource cpu=200m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.067: INFO: Pod cloud-ingress-operator-registry-pqwm7 requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.067: INFO: Pod aws-ebs-csi-driver-node-9sjjv requesting resource cpu=30m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.067: INFO: Pod aws-ebs-csi-driver-node-c8m6c requesting resource cpu=30m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.067: INFO: Pod aws-ebs-csi-driver-node-svd2s requesting resource cpu=30m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.067: INFO: Pod aws-ebs-csi-driver-node-vxgq2 requesting resource cpu=30m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.067: INFO: Pod aws-ebs-csi-driver-node-wmvd4 requesting resource cpu=30m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.067: INFO: Pod aws-ebs-csi-driver-node-z6klb requesting resource cpu=30m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.067: INFO: Pod tuned-5h69k requesting resource cpu=10m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.067: INFO: Pod tuned-7x9t2 requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.067: INFO: Pod tuned-9p2db requesting resource cpu=10m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.067: INFO: Pod tuned-gjnt5 requesting resource cpu=10m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.067: INFO: Pod tuned-nbzt4 requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.067: INFO: Pod tuned-qkjmb requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.067: INFO: Pod downloads-8d695cd69-hltgd requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.067: INFO: Pod downloads-8d695cd69-lcsd9 requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.067: INFO: Pod custom-domains-operator-5764f4df49-8nbdk requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.067: INFO: Pod custom-domains-operator-registry-kdw7s requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.067: INFO: Pod deployment-validation-operator-7b8fd479cb-wtzg7 requesting resource cpu=100m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod deployment-validation-operator-catalog-nq94w requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod dns-default-bcrdt requesting resource cpu=60m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod dns-default-c5qwq requesting resource cpu=60m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod dns-default-d9szl requesting resource cpu=60m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod dns-default-dfndw requesting resource cpu=60m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod dns-default-tzrx6 requesting resource cpu=60m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod dns-default-zbx44 requesting resource cpu=60m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-resolver-5mb95 requesting resource cpu=5m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-resolver-9cwpx requesting resource cpu=5m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-resolver-b85qs requesting resource cpu=5m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-resolver-g5jk6 requesting resource cpu=5m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-resolver-l2fzw requesting resource cpu=5m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-resolver-m4bvp requesting resource cpu=5m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod image-registry-656cfd9bd9-9x67r requesting resource cpu=100m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod image-registry-656cfd9bd9-bj4wn requesting resource cpu=100m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-ca-7c5fq requesting resource cpu=10m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-ca-crm7t requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-ca-dsxvs requesting resource cpu=10m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-ca-k89p7 requesting resource cpu=10m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-ca-kwgfx requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-ca-kzsvk requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ingress-canary-6sc9q requesting resource cpu=10m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ingress-canary-6xnfp requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ingress-canary-cs8pq requesting resource cpu=10m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ingress-canary-nfwhl requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ingress-canary-t6tqp requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ingress-canary-v2tjj requesting resource cpu=10m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod router-default-6bf5ff7dcb-5nzvc requesting resource cpu=100m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod router-default-6bf5ff7dcb-6vrcg requesting resource cpu=100m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod migrator-5c54d8d69d-l5lss requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod machine-config-daemon-28fzw requesting resource cpu=40m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod machine-config-daemon-2bt5b requesting resource cpu=40m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod machine-config-daemon-cc29b requesting resource cpu=40m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod machine-config-daemon-cllj8 requesting resource cpu=40m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod machine-config-daemon-xxm6j requesting resource cpu=40m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod machine-config-daemon-zhllf requesting resource cpu=40m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod managed-node-metadata-operator-649f646d54-smsh7 requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod managed-node-metadata-operator-registry-m6dxz requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod managed-upgrade-operator-catalog-thxts requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod configure-alertmanager-operator-7d94bbbcf-8h7xs requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod configure-alertmanager-operator-registry-sjcb5 requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod kube-state-metrics-666f4cbf77-5w8rk requesting resource cpu=4m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-exporter-2vp9v requesting resource cpu=9m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-exporter-7t5b7 requesting resource cpu=9m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-exporter-9zc87 requesting resource cpu=9m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-exporter-dc59d requesting resource cpu=9m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-exporter-h445b requesting resource cpu=9m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod node-exporter-s77nf requesting resource cpu=9m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod openshift-state-metrics-7d5c9d867c-97gzd requesting resource cpu=3m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod prometheus-adapter-5bc6487f7c-q2ng7 requesting resource cpu=1m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod prometheus-adapter-5bc6487f7c-sv96c requesting resource cpu=1m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod prometheus-operator-564b78d5ff-v4xnk requesting resource cpu=6m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod prometheus-operator-admission-webhook-749df5cf4f-bmj4n requesting resource cpu=5m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod prometheus-operator-admission-webhook-749df5cf4f-zj8xj requesting resource cpu=5m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sre-dns-latency-exporter-2q899 requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sre-dns-latency-exporter-4lksg requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sre-dns-latency-exporter-5dz25 requesting resource cpu=0m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sre-dns-latency-exporter-ctmgr requesting resource cpu=0m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sre-dns-latency-exporter-jsn9x requesting resource cpu=0m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sre-dns-latency-exporter-vhwwx requesting resource cpu=0m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sre-ebs-iops-reporter-1-sg7b2 requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sre-stuck-ebs-vols-1-vdlrh requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod telemeter-client-54bbbbf598-d9fdk requesting resource cpu=3m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod thanos-querier-fcd5b6c8d-9jjkp requesting resource cpu=15m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod thanos-querier-fcd5b6c8d-ql8qb requesting resource cpu=15m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod token-refresher-6f7c46d758-mld9t requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod multus-5zck8 requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod multus-additional-cni-plugins-476gw requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod multus-additional-cni-plugins-88bc2 requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod multus-additional-cni-plugins-hvpd7 requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod multus-additional-cni-plugins-nhwfz requesting resource cpu=10m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod multus-additional-cni-plugins-p5pjp requesting resource cpu=10m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod multus-additional-cni-plugins-vslld requesting resource cpu=10m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod multus-j4ngm requesting resource cpu=10m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod multus-mczpf requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod multus-mptd7 requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod multus-xzxwg requesting resource cpu=10m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod multus-zqhtq requesting resource cpu=10m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-metrics-daemon-677lg requesting resource cpu=20m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-metrics-daemon-ddkb6 requesting resource cpu=20m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-metrics-daemon-hp85h requesting resource cpu=20m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-metrics-daemon-hz5pj requesting resource cpu=20m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-metrics-daemon-s2l94 requesting resource cpu=20m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-metrics-daemon-zvctv requesting resource cpu=20m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod must-gather-operator-56b466776c-qv8sh requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod must-gather-operator-registry-n7fvg requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-check-source-746dd6c885-bmf5k requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-check-target-d8gvt requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-check-target-d8k5s requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-check-target-g5qsk requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-check-target-mg7p4 requesting resource cpu=10m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-check-target-nq9ln requesting resource cpu=10m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod network-check-target-xrr4d requesting resource cpu=10m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod obo-prometheus-operator-64989cfc68-hdxb9 requesting resource cpu=5m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod obo-prometheus-operator-admission-webhook-558d9c7d6-b4r6s requesting resource cpu=50m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod obo-prometheus-operator-admission-webhook-558d9c7d6-gqhlz requesting resource cpu=50m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod observability-operator-6dd79df7dc-z7frh requesting resource cpu=5m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod observability-operator-catalog-vrnkr requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ocm-agent-ddfdc6544-4fxrp requesting resource cpu=1m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ocm-agent-operator-6cdb45fd86-4clcj requesting resource cpu=1m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ocm-agent-operator-registry-dwfc4 requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod osd-metrics-exporter-679d75d598-rsv49 requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod osd-metrics-exporter-registry-qdjtk requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ovnkube-node-ftv9n requesting resource cpu=50m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ovnkube-node-gq9vc requesting resource cpu=50m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ovnkube-node-gvwrr requesting resource cpu=50m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ovnkube-node-kk5t9 requesting resource cpu=50m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ovnkube-node-wcrc4 requesting resource cpu=50m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod ovnkube-node-zmvs8 requesting resource cpu=50m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod rbac-permissions-operator-5fb57974d8-htnjj requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod rbac-permissions-operator-registry-rc4js requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod blackbox-exporter-5f7f7bf859-l8ptw requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod route-monitor-operator-controller-manager-bfbff575-pfqc4 requesting resource cpu=100m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod route-monitor-operator-registry-whpgh requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod splunkforwarder-ds-66kjq requesting resource cpu=0m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod splunkforwarder-ds-7rpx7 requesting resource cpu=0m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod splunkforwarder-ds-h56tr requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod splunkforwarder-ds-pj5q8 requesting resource cpu=0m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod splunkforwarder-ds-xzbdk requesting resource cpu=0m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod splunkforwarder-ds-ztlpn requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod splunk-forwarder-operator-7d66568cf7-bksrn requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod splunk-forwarder-operator-catalog-fqp2b requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod prometheus-user-workload-0 requesting resource cpu=11m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod prometheus-user-workload-1 requesting resource cpu=11m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod thanos-ruler-user-workload-0 requesting resource cpu=3m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod thanos-ruler-user-workload-1 requesting resource cpu=3m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod managed-velero-operator-788754d4b6-n89fw requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod managed-velero-operator-registry-6lc6b requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.068: INFO: Pod velero-7c64598b8c-8blx4 requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sonobuoy-e2e-job-4d801e056c4540b8 requesting resource cpu=0m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-8mc8n requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-cbfjg requesting resource cpu=0m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-chbvs requesting resource cpu=0m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-hj9fv requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-m8vnk requesting resource cpu=0m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-q5j5c requesting resource cpu=0m on Node ip-10-0-188-71.ec2.internal
STEP: Starting Pods to consume most of the cluster CPU. 01/19/23 21:47:48.068
Jan 19 21:47:48.068: INFO: Creating a pod which consumes cpu=2405m on Node ip-10-0-188-71.ec2.internal
Jan 19 21:47:48.084: INFO: Creating a pod which consumes cpu=2051m on Node ip-10-0-207-77.ec2.internal
Jan 19 21:47:48.100: INFO: Creating a pod which consumes cpu=2468m on Node ip-10-0-146-42.ec2.internal
Jan 19 21:47:48.115: INFO: Creating a pod which consumes cpu=2046m on Node ip-10-0-151-158.ec2.internal
Jan 19 21:47:48.130: INFO: Creating a pod which consumes cpu=2552m on Node ip-10-0-171-213.ec2.internal
Jan 19 21:47:48.150: INFO: Creating a pod which consumes cpu=2552m on Node ip-10-0-172-44.ec2.internal
Jan 19 21:47:48.171: INFO: Waiting up to 5m0s for pod "filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462" in namespace "sched-pred-4697" to be "running"
Jan 19 21:47:48.182: INFO: Pod "filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462": Phase="Pending", Reason="", readiness=false. Elapsed: 11.389441ms
Jan 19 21:47:50.186: INFO: Pod "filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462": Phase="Running", Reason="", readiness=true. Elapsed: 2.01468169s
Jan 19 21:47:50.186: INFO: Pod "filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462" satisfied condition "running"
Jan 19 21:47:50.186: INFO: Waiting up to 5m0s for pod "filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245" in namespace "sched-pred-4697" to be "running"
Jan 19 21:47:50.188: INFO: Pod "filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245": Phase="Running", Reason="", readiness=true. Elapsed: 2.235948ms
Jan 19 21:47:50.188: INFO: Pod "filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245" satisfied condition "running"
Jan 19 21:47:50.188: INFO: Waiting up to 5m0s for pod "filler-pod-74458b47-100d-4308-b8f4-298ca388adba" in namespace "sched-pred-4697" to be "running"
Jan 19 21:47:50.190: INFO: Pod "filler-pod-74458b47-100d-4308-b8f4-298ca388adba": Phase="Running", Reason="", readiness=true. Elapsed: 2.317566ms
Jan 19 21:47:50.190: INFO: Pod "filler-pod-74458b47-100d-4308-b8f4-298ca388adba" satisfied condition "running"
Jan 19 21:47:50.190: INFO: Waiting up to 5m0s for pod "filler-pod-714e512c-f5e9-4a61-983b-3d050de03763" in namespace "sched-pred-4697" to be "running"
Jan 19 21:47:50.192: INFO: Pod "filler-pod-714e512c-f5e9-4a61-983b-3d050de03763": Phase="Running", Reason="", readiness=true. Elapsed: 2.120328ms
Jan 19 21:47:50.193: INFO: Pod "filler-pod-714e512c-f5e9-4a61-983b-3d050de03763" satisfied condition "running"
Jan 19 21:47:50.193: INFO: Waiting up to 5m0s for pod "filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655" in namespace "sched-pred-4697" to be "running"
Jan 19 21:47:50.195: INFO: Pod "filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655": Phase="Running", Reason="", readiness=true. Elapsed: 2.12885ms
Jan 19 21:47:50.195: INFO: Pod "filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655" satisfied condition "running"
Jan 19 21:47:50.195: INFO: Waiting up to 5m0s for pod "filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157" in namespace "sched-pred-4697" to be "running"
Jan 19 21:47:50.197: INFO: Pod "filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157": Phase="Running", Reason="", readiness=true. Elapsed: 1.895873ms
Jan 19 21:47:50.197: INFO: Pod "filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/19/23 21:47:50.197
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157.173bd413d7268ffe], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4697/filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157 to ip-10-0-172-44.ec2.internal] 01/19/23 21:47:50.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157.173bd413fc066dcb], Reason = [AddedInterface], Message = [Add eth0 [10.128.9.18/23] from ovn-kubernetes] 01/19/23 21:47:50.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157.173bd413fd63322b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 21:47:50.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157.173bd414078d60da], Reason = [Created], Message = [Created container filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157] 01/19/23 21:47:50.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157.173bd4140973495e], Reason = [Started], Message = [Started container filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157] 01/19/23 21:47:50.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245.173bd413d2f5a914], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4697/filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245 to ip-10-0-207-77.ec2.internal] 01/19/23 21:47:50.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245.173bd413fde71560], Reason = [AddedInterface], Message = [Add eth0 [10.128.16.139/23] from ovn-kubernetes] 01/19/23 21:47:50.2
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245.173bd413ffaa505f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245.173bd4140acb3605], Reason = [Created], Message = [Created container filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245.173bd4140ce1fa7d], Reason = [Started], Message = [Started container filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462.173bd413d201a3f7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4697/filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462 to ip-10-0-188-71.ec2.internal] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462.173bd413f66b664e], Reason = [AddedInterface], Message = [Add eth0 [10.128.6.85/23] from ovn-kubernetes] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462.173bd413f7a11e49], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462.173bd41400c2344a], Reason = [Created], Message = [Created container filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462.173bd41402140528], Reason = [Started], Message = [Started container filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-714e512c-f5e9-4a61-983b-3d050de03763.173bd413d4ca42ee], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4697/filler-pod-714e512c-f5e9-4a61-983b-3d050de03763 to ip-10-0-151-158.ec2.internal] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-714e512c-f5e9-4a61-983b-3d050de03763.173bd413f9a9ced2], Reason = [AddedInterface], Message = [Add eth0 [10.128.14.89/23] from ovn-kubernetes] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-714e512c-f5e9-4a61-983b-3d050de03763.173bd413fb07bfcd], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-714e512c-f5e9-4a61-983b-3d050de03763.173bd41406c74aab], Reason = [Created], Message = [Created container filler-pod-714e512c-f5e9-4a61-983b-3d050de03763] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-714e512c-f5e9-4a61-983b-3d050de03763.173bd41408f653c9], Reason = [Started], Message = [Started container filler-pod-714e512c-f5e9-4a61-983b-3d050de03763] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-74458b47-100d-4308-b8f4-298ca388adba.173bd413d3b809fa], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4697/filler-pod-74458b47-100d-4308-b8f4-298ca388adba to ip-10-0-146-42.ec2.internal] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-74458b47-100d-4308-b8f4-298ca388adba.173bd413f5c2b34d], Reason = [AddedInterface], Message = [Add eth0 [10.128.10.109/23] from ovn-kubernetes] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-74458b47-100d-4308-b8f4-298ca388adba.173bd413f7138206], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-74458b47-100d-4308-b8f4-298ca388adba.173bd414015f8ab3], Reason = [Created], Message = [Created container filler-pod-74458b47-100d-4308-b8f4-298ca388adba] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-74458b47-100d-4308-b8f4-298ca388adba.173bd414030bd9a4], Reason = [Started], Message = [Started container filler-pod-74458b47-100d-4308-b8f4-298ca388adba] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655.173bd413d6380c08], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4697/filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655 to ip-10-0-171-213.ec2.internal] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655.173bd413f7f48af8], Reason = [AddedInterface], Message = [Add eth0 [10.128.12.148/23] from ovn-kubernetes] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655.173bd413f92f32f1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 21:47:50.201
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655.173bd414020eb9fc], Reason = [Created], Message = [Created container filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655] 01/19/23 21:47:50.202
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655.173bd414032c78b4], Reason = [Started], Message = [Started container filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655] 01/19/23 21:47:50.202
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173bd4145085b35c], Reason = [FailedScheduling], Message = [0/9 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 Insufficient cpu. preemption: 0/9 nodes are available: 3 Preemption is not helpful for scheduling, 6 No preemption victims found for incoming pod.] 01/19/23 21:47:50.218
STEP: removing the label node off the node ip-10-0-146-42.ec2.internal 01/19/23 21:47:51.219
STEP: verifying the node doesn't have the label node 01/19/23 21:47:51.234
STEP: removing the label node off the node ip-10-0-151-158.ec2.internal 01/19/23 21:47:51.239
STEP: verifying the node doesn't have the label node 01/19/23 21:47:51.261
STEP: removing the label node off the node ip-10-0-171-213.ec2.internal 01/19/23 21:47:51.271
STEP: verifying the node doesn't have the label node 01/19/23 21:47:51.298
STEP: removing the label node off the node ip-10-0-172-44.ec2.internal 01/19/23 21:47:51.301
STEP: verifying the node doesn't have the label node 01/19/23 21:47:51.322
STEP: removing the label node off the node ip-10-0-188-71.ec2.internal 01/19/23 21:47:51.328
STEP: verifying the node doesn't have the label node 01/19/23 21:47:51.344
STEP: removing the label node off the node ip-10-0-207-77.ec2.internal 01/19/23 21:47:51.347
STEP: verifying the node doesn't have the label node 01/19/23 21:47:51.364
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:47:51.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4697" for this suite. 01/19/23 21:47:51.38
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":283,"skipped":5272,"failed":0}
------------------------------
• [3.934 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:47:47.453
    Jan 19 21:47:47.453: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename sched-pred 01/19/23 21:47:47.454
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:47.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:47.473
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 19 21:47:47.503: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 19 21:47:47.519: INFO: Waiting for terminating namespaces to be deleted...
    Jan 19 21:47:47.531: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-146-42.ec2.internal before test
    Jan 19 21:47:47.570: INFO: aws-ebs-csi-driver-node-vxgq2 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:41 +0000 UTC (3 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: tuned-nbzt4 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: downloads-8d695cd69-lcsd9 from openshift-console started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container download-server ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: dns-default-c5qwq from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container dns ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: node-resolver-g5jk6 from openshift-dns started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: node-ca-crm7t from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: ingress-canary-nfwhl from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: migrator-5c54d8d69d-l5lss from openshift-kube-storage-version-migrator started at 2023-01-19 18:42:53 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container migrator ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: machine-config-daemon-cc29b from openshift-machine-config-operator started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: node-exporter-9zc87 from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: sre-dns-latency-exporter-jsn9x from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container main ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: multus-5zck8 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: multus-additional-cni-plugins-88bc2 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: network-metrics-daemon-zvctv from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: network-check-source-746dd6c885-bmf5k from openshift-network-diagnostics started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container check-endpoints ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: network-check-target-g5qsk from openshift-network-diagnostics started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: obo-prometheus-operator-64989cfc68-hdxb9 from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container prometheus-operator ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-gqhlz from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: osd-metrics-exporter-registry-qdjtk from openshift-osd-metrics started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: ovnkube-node-ftv9n from openshift-ovn-kubernetes started at 2023-01-19 18:15:41 +0000 UTC (5 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: 	Container ovn-acl-logging ready: true, restart count 2
    Jan 19 21:47:47.570: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: rbac-permissions-operator-registry-rc4js from openshift-rbac-permissions started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: splunkforwarder-ds-7rpx7 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 21:47:47.570: INFO: prometheus-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (6 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: thanos-ruler-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (3 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: 	Container thanos-ruler ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-chbvs from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.570: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 21:47:47.570: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-151-158.ec2.internal before test
    Jan 19 21:47:47.608: INFO: addon-operator-manager-84ff88fc6-fmhnb from openshift-addon-operator started at 2023-01-19 18:46:50 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container manager ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container metrics-relay-server ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: addon-operator-webhooks-67d9d47489-4kbk2 from openshift-addon-operator started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container webhook ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: aws-ebs-csi-driver-node-c8m6c from openshift-cluster-csi-drivers started at 2023-01-19 18:32:48 +0000 UTC (3 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: tuned-5h69k from openshift-cluster-node-tuning-operator started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: custom-domains-operator-5764f4df49-8nbdk from openshift-custom-domains-operator started at 2023-01-19 19:15:08 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container custom-domains-operator ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: deployment-validation-operator-7b8fd479cb-wtzg7 from openshift-deployment-validation-operator started at 2023-01-19 18:46:50 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container deployment-validation-operator ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: dns-default-d9szl from openshift-dns started at 2023-01-19 21:45:22 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container dns ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: node-resolver-9cwpx from openshift-dns started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: image-pruner-27902580-lzd8r from openshift-image-registry started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container image-pruner ready: false, restart count 0
    Jan 19 21:47:47.608: INFO: image-pruner-27902640-c6mlt from openshift-image-registry started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container image-pruner ready: false, restart count 0
    Jan 19 21:47:47.608: INFO: image-pruner-27902700-cf7x6 from openshift-image-registry started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container image-pruner ready: false, restart count 0
    Jan 19 21:47:47.608: INFO: image-registry-656cfd9bd9-9x67r from openshift-image-registry started at 2023-01-19 19:15:11 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container registry ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: node-ca-k89p7 from openshift-image-registry started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: ingress-canary-v2tjj from openshift-ingress-canary started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: router-default-6bf5ff7dcb-5nzvc from openshift-ingress started at 2023-01-19 18:47:44 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container router ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: machine-config-daemon-28fzw from openshift-machine-config-operator started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: configure-alertmanager-operator-7d94bbbcf-8h7xs from openshift-monitoring started at 2023-01-19 19:16:27 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: kube-state-metrics-666f4cbf77-5w8rk from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: node-exporter-7t5b7 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: openshift-state-metrics-7d5c9d867c-97gzd from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: prometheus-adapter-5bc6487f7c-q2ng7 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: prometheus-operator-564b78d5ff-v4xnk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container prometheus-operator ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: prometheus-operator-admission-webhook-749df5cf4f-zj8xj from openshift-monitoring started at 2023-01-19 18:46:40 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: sre-dns-latency-exporter-2q899 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container main ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: sre-stuck-ebs-vols-1-vdlrh from openshift-monitoring started at 2023-01-19 19:17:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container main ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: telemeter-client-54bbbbf598-d9fdk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (3 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container reload ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container telemeter-client ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: thanos-querier-fcd5b6c8d-ql8qb from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: multus-additional-cni-plugins-vslld from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: multus-zqhtq from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: network-metrics-daemon-677lg from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: must-gather-operator-56b466776c-qv8sh from openshift-must-gather-operator started at 2023-01-19 19:17:38 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container must-gather-operator ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: network-check-target-xrr4d from openshift-network-diagnostics started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: ocm-agent-ddfdc6544-4fxrp from openshift-ocm-agent-operator started at 2023-01-19 19:17:41 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container ocm-agent ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: ocm-agent-operator-6cdb45fd86-4clcj from openshift-ocm-agent-operator started at 2023-01-19 19:17:43 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container ocm-agent-operator ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: ovnkube-node-gvwrr from openshift-ovn-kubernetes started at 2023-01-19 18:32:48 +0000 UTC (5 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: rbac-permissions-operator-5fb57974d8-htnjj from openshift-rbac-permissions started at 2023-01-19 19:17:50 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: splunkforwarder-ds-ztlpn from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 21:47:47.608: INFO: splunk-forwarder-operator-7d66568cf7-bksrn from openshift-splunk-forwarder-operator started at 2023-01-19 19:17:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: managed-velero-operator-788754d4b6-n89fw from openshift-velero started at 2023-01-19 19:17:56 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container managed-velero-operator ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: velero-7c64598b8c-8blx4 from openshift-velero started at 2023-01-19 19:18:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container velero ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-8mc8n from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.608: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 21:47:47.608: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-171-213.ec2.internal before test
    Jan 19 21:47:47.641: INFO: aws-ebs-csi-driver-node-svd2s from openshift-cluster-csi-drivers started at 2023-01-19 18:15:58 +0000 UTC (3 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: tuned-9p2db from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: dns-default-zbx44 from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container dns ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: node-resolver-5mb95 from openshift-dns started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: node-ca-7c5fq from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: ingress-canary-6sc9q from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: machine-config-daemon-xxm6j from openshift-machine-config-operator started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: node-exporter-h445b from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: sre-dns-latency-exporter-vhwwx from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container main ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: multus-additional-cni-plugins-nhwfz from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: multus-j4ngm from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: network-metrics-daemon-hp85h from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: network-check-target-mg7p4 from openshift-network-diagnostics started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: ovnkube-node-kk5t9 from openshift-ovn-kubernetes started at 2023-01-19 18:15:58 +0000 UTC (5 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: splunkforwarder-ds-66kjq from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 21:47:47.641: INFO: sonobuoy-e2e-job-4d801e056c4540b8 from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container e2e ready: true, restart count 0
    Jan 19 21:47:47.641: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:47:47.641: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-m8vnk from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.641: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:47:47.641: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 21:47:47.641: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-172-44.ec2.internal before test
    Jan 19 21:47:47.675: INFO: aws-ebs-csi-driver-node-z6klb from openshift-cluster-csi-drivers started at 2023-01-19 18:15:33 +0000 UTC (3 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: tuned-gjnt5 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: dns-default-dfndw from openshift-dns started at 2023-01-19 21:22:44 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container dns ready: true, restart count 0
    Jan 19 21:47:47.675: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:47:47.675: INFO: node-resolver-b85qs from openshift-dns started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: node-ca-dsxvs from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: ingress-canary-cs8pq from openshift-ingress-canary started at 2023-01-19 21:22:44 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 19 21:47:47.675: INFO: machine-config-daemon-cllj8 from openshift-machine-config-operator started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: node-exporter-2vp9v from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: sre-dns-latency-exporter-ctmgr from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container main ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: multus-additional-cni-plugins-p5pjp from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: multus-xzxwg from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: network-metrics-daemon-hz5pj from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: network-check-target-nq9ln from openshift-network-diagnostics started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: collect-profiles-27902730-zm5kk from openshift-operator-lifecycle-manager started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 19 21:47:47.675: INFO: collect-profiles-27902745-9gctc from openshift-operator-lifecycle-manager started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 19 21:47:47.675: INFO: ovnkube-node-zmvs8 from openshift-ovn-kubernetes started at 2023-01-19 18:15:33 +0000 UTC (5 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: splunkforwarder-ds-pj5q8 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 21:47:47.675: INFO: sonobuoy from sonobuoy started at 2023-01-19 20:29:23 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.675: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 19 21:47:47.675: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-cbfjg from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.676: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:47:47.676: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 21:47:47.676: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-188-71.ec2.internal before test
    Jan 19 21:47:47.711: INFO: addon-operator-catalog-zxs8v from openshift-addon-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: cloud-ingress-operator-registry-pqwm7 from openshift-cloud-ingress-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: aws-ebs-csi-driver-node-wmvd4 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:13 +0000 UTC (3 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: tuned-7x9t2 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: downloads-8d695cd69-hltgd from openshift-console started at 2023-01-19 18:39:20 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container download-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: custom-domains-operator-registry-kdw7s from openshift-custom-domains-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: deployment-validation-operator-catalog-nq94w from openshift-deployment-validation-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: dns-default-tzrx6 from openshift-dns started at 2023-01-19 18:15:43 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container dns ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: node-resolver-l2fzw from openshift-dns started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: node-ca-kzsvk from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: ingress-canary-t6tqp from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: machine-config-daemon-zhllf from openshift-machine-config-operator started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: managed-node-metadata-operator-registry-m6dxz from openshift-managed-node-metadata-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: managed-upgrade-operator-catalog-thxts from openshift-managed-upgrade-operator started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: configure-alertmanager-operator-registry-sjcb5 from openshift-monitoring started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: node-exporter-dc59d from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: osd-cluster-ready-nb8xk from openshift-monitoring started at 2023-01-19 18:39:21 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container osd-cluster-ready ready: false, restart count 7
    Jan 19 21:47:47.711: INFO: sre-dns-latency-exporter-5dz25 from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container main ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: multus-additional-cni-plugins-hvpd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: multus-mptd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: network-metrics-daemon-s2l94 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: must-gather-operator-registry-n7fvg from openshift-must-gather-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: network-check-target-d8gvt from openshift-network-diagnostics started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-b4r6s from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: observability-operator-6dd79df7dc-z7frh from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container operator ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: observability-operator-catalog-vrnkr from openshift-observability-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: ocm-agent-operator-registry-dwfc4 from openshift-ocm-agent-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: collect-profiles-27902715-lq9hm from openshift-operator-lifecycle-manager started at 2023-01-19 21:15:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 19 21:47:47.711: INFO: ovnkube-node-wcrc4 from openshift-ovn-kubernetes started at 2023-01-19 18:15:13 +0000 UTC (5 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: route-monitor-operator-registry-whpgh from openshift-route-monitor-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: splunkforwarder-ds-xzbdk from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 21:47:47.711: INFO: splunk-forwarder-operator-catalog-fqp2b from openshift-splunk-forwarder-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: prometheus-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:37:05 +0000 UTC (6 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: thanos-ruler-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:37:04 +0000 UTC (3 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: 	Container thanos-ruler ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: managed-velero-operator-registry-6lc6b from openshift-velero started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-q5j5c from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.711: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 21:47:47.711: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-207-77.ec2.internal before test
    Jan 19 21:47:47.755: INFO: addon-operator-webhooks-67d9d47489-f6z86 from openshift-addon-operator started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container webhook ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: osd-delete-ownerrefs-serviceaccounts-27902677-st5fv from openshift-backplane-srep started at 2023-01-19 20:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: osd-delete-ownerrefs-serviceaccounts-27902707-pxrnh from openshift-backplane-srep started at 2023-01-19 21:07:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: osd-delete-ownerrefs-serviceaccounts-27902737-ww4vb from openshift-backplane-srep started at 2023-01-19 21:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: osd-delete-backplane-serviceaccounts-27902720-brqcs from openshift-backplane started at 2023-01-19 21:20:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: osd-delete-backplane-serviceaccounts-27902730-ph7cv from openshift-backplane started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: osd-delete-backplane-serviceaccounts-27902740-x6pck from openshift-backplane started at 2023-01-19 21:40:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: sre-build-test-27902711-r8jct from openshift-build-test started at 2023-01-19 21:11:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container sre-build-test ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: cloud-ingress-operator-5b5df787b5-bdk4j from openshift-cloud-ingress-operator started at 2023-01-19 19:15:07 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: aws-ebs-csi-driver-node-9sjjv from openshift-cluster-csi-drivers started at 2023-01-19 18:33:36 +0000 UTC (3 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: tuned-qkjmb from openshift-cluster-node-tuning-operator started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: dns-default-bcrdt from openshift-dns started at 2023-01-19 21:45:58 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container dns ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: node-resolver-m4bvp from openshift-dns started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: image-registry-656cfd9bd9-bj4wn from openshift-image-registry started at 2023-01-19 19:15:47 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container registry ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: node-ca-kwgfx from openshift-image-registry started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: ingress-canary-6xnfp from openshift-ingress-canary started at 2023-01-19 18:34:10 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: router-default-6bf5ff7dcb-6vrcg from openshift-ingress started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container router ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: osd-disable-cpms-27902745-qqjbm from openshift-machine-api started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-disable-cpms ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: machine-config-daemon-2bt5b from openshift-machine-config-operator started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: managed-node-metadata-operator-649f646d54-smsh7 from openshift-managed-node-metadata-operator started at 2023-01-19 19:16:23 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: osd-patch-subscription-source-27902580-hz9tz from openshift-marketplace started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: osd-patch-subscription-source-27902640-qrd58 from openshift-marketplace started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: osd-patch-subscription-source-27902700-smwhh from openshift-marketplace started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: node-exporter-s77nf from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: osd-rebalance-infra-nodes-27902715-hm9rx from openshift-monitoring started at 2023-01-19 21:15:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: osd-rebalance-infra-nodes-27902730-h7lrq from openshift-monitoring started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: osd-rebalance-infra-nodes-27902745-cwdn2 from openshift-monitoring started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: prometheus-adapter-5bc6487f7c-sv96c from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: prometheus-operator-admission-webhook-749df5cf4f-bmj4n from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: sre-dns-latency-exporter-4lksg from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container main ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: sre-ebs-iops-reporter-1-sg7b2 from openshift-monitoring started at 2023-01-19 19:16:28 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container main ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: thanos-querier-fcd5b6c8d-9jjkp from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: token-refresher-6f7c46d758-mld9t from openshift-monitoring started at 2023-01-19 19:17:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container token-refresher ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: multus-additional-cni-plugins-476gw from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: multus-mczpf from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: network-metrics-daemon-ddkb6 from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: network-check-target-d8k5s from openshift-network-diagnostics started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: osd-metrics-exporter-679d75d598-rsv49 from openshift-osd-metrics started at 2023-01-19 19:17:47 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: ovnkube-node-gq9vc from openshift-ovn-kubernetes started at 2023-01-19 18:33:36 +0000 UTC (5 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: blackbox-exporter-5f7f7bf859-l8ptw from openshift-route-monitor-operator started at 2023-01-19 19:17:51 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: route-monitor-operator-controller-manager-bfbff575-pfqc4 from openshift-route-monitor-operator started at 2023-01-19 19:17:53 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container manager ready: true, restart count 0
    Jan 19 21:47:47.755: INFO: splunkforwarder-ds-h56tr from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 21:47:47.755: INFO: builds-pruner-27902580-p9bnq from openshift-sre-pruning started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container builds-pruner ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: builds-pruner-27902640-bj57l from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container builds-pruner ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: builds-pruner-27902700-t7v8n from openshift-sre-pruning started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.755: INFO: 	Container builds-pruner ready: false, restart count 0
    Jan 19 21:47:47.755: INFO: deployments-pruner-27902580-qfgtd from openshift-sre-pruning started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.756: INFO: 	Container deployments-pruner ready: false, restart count 0
    Jan 19 21:47:47.756: INFO: deployments-pruner-27902640-vhqsv from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.756: INFO: 	Container deployments-pruner ready: false, restart count 0
    Jan 19 21:47:47.756: INFO: deployments-pruner-27902700-fvhm7 from openshift-sre-pruning started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:47:47.756: INFO: 	Container deployments-pruner ready: false, restart count 0
    Jan 19 21:47:47.756: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-hj9fv from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:47:47.756: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:47:47.756: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node ip-10-0-146-42.ec2.internal 01/19/23 21:47:47.851
    STEP: verifying the node has the label node ip-10-0-151-158.ec2.internal 01/19/23 21:47:47.873
    STEP: verifying the node has the label node ip-10-0-171-213.ec2.internal 01/19/23 21:47:47.891
    STEP: verifying the node has the label node ip-10-0-172-44.ec2.internal 01/19/23 21:47:47.914
    STEP: verifying the node has the label node ip-10-0-188-71.ec2.internal 01/19/23 21:47:47.942
    STEP: verifying the node has the label node ip-10-0-207-77.ec2.internal 01/19/23 21:47:47.972
    Jan 19 21:47:48.067: INFO: Pod addon-operator-catalog-zxs8v requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod addon-operator-manager-84ff88fc6-fmhnb requesting resource cpu=200m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod addon-operator-webhooks-67d9d47489-4kbk2 requesting resource cpu=100m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod addon-operator-webhooks-67d9d47489-f6z86 requesting resource cpu=100m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod cloud-ingress-operator-5b5df787b5-bdk4j requesting resource cpu=200m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod cloud-ingress-operator-registry-pqwm7 requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod aws-ebs-csi-driver-node-9sjjv requesting resource cpu=30m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod aws-ebs-csi-driver-node-c8m6c requesting resource cpu=30m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod aws-ebs-csi-driver-node-svd2s requesting resource cpu=30m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod aws-ebs-csi-driver-node-vxgq2 requesting resource cpu=30m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod aws-ebs-csi-driver-node-wmvd4 requesting resource cpu=30m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod aws-ebs-csi-driver-node-z6klb requesting resource cpu=30m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod tuned-5h69k requesting resource cpu=10m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod tuned-7x9t2 requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod tuned-9p2db requesting resource cpu=10m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod tuned-gjnt5 requesting resource cpu=10m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod tuned-nbzt4 requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod tuned-qkjmb requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod downloads-8d695cd69-hltgd requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod downloads-8d695cd69-lcsd9 requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod custom-domains-operator-5764f4df49-8nbdk requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod custom-domains-operator-registry-kdw7s requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.067: INFO: Pod deployment-validation-operator-7b8fd479cb-wtzg7 requesting resource cpu=100m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod deployment-validation-operator-catalog-nq94w requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod dns-default-bcrdt requesting resource cpu=60m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod dns-default-c5qwq requesting resource cpu=60m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod dns-default-d9szl requesting resource cpu=60m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod dns-default-dfndw requesting resource cpu=60m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod dns-default-tzrx6 requesting resource cpu=60m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod dns-default-zbx44 requesting resource cpu=60m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-resolver-5mb95 requesting resource cpu=5m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-resolver-9cwpx requesting resource cpu=5m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-resolver-b85qs requesting resource cpu=5m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-resolver-g5jk6 requesting resource cpu=5m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-resolver-l2fzw requesting resource cpu=5m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-resolver-m4bvp requesting resource cpu=5m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod image-registry-656cfd9bd9-9x67r requesting resource cpu=100m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod image-registry-656cfd9bd9-bj4wn requesting resource cpu=100m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-ca-7c5fq requesting resource cpu=10m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-ca-crm7t requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-ca-dsxvs requesting resource cpu=10m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-ca-k89p7 requesting resource cpu=10m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-ca-kwgfx requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-ca-kzsvk requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ingress-canary-6sc9q requesting resource cpu=10m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ingress-canary-6xnfp requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ingress-canary-cs8pq requesting resource cpu=10m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ingress-canary-nfwhl requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ingress-canary-t6tqp requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ingress-canary-v2tjj requesting resource cpu=10m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod router-default-6bf5ff7dcb-5nzvc requesting resource cpu=100m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod router-default-6bf5ff7dcb-6vrcg requesting resource cpu=100m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod migrator-5c54d8d69d-l5lss requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod machine-config-daemon-28fzw requesting resource cpu=40m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod machine-config-daemon-2bt5b requesting resource cpu=40m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod machine-config-daemon-cc29b requesting resource cpu=40m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod machine-config-daemon-cllj8 requesting resource cpu=40m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod machine-config-daemon-xxm6j requesting resource cpu=40m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod machine-config-daemon-zhllf requesting resource cpu=40m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod managed-node-metadata-operator-649f646d54-smsh7 requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod managed-node-metadata-operator-registry-m6dxz requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod managed-upgrade-operator-catalog-thxts requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod configure-alertmanager-operator-7d94bbbcf-8h7xs requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod configure-alertmanager-operator-registry-sjcb5 requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod kube-state-metrics-666f4cbf77-5w8rk requesting resource cpu=4m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-exporter-2vp9v requesting resource cpu=9m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-exporter-7t5b7 requesting resource cpu=9m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-exporter-9zc87 requesting resource cpu=9m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-exporter-dc59d requesting resource cpu=9m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-exporter-h445b requesting resource cpu=9m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod node-exporter-s77nf requesting resource cpu=9m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod openshift-state-metrics-7d5c9d867c-97gzd requesting resource cpu=3m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod prometheus-adapter-5bc6487f7c-q2ng7 requesting resource cpu=1m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod prometheus-adapter-5bc6487f7c-sv96c requesting resource cpu=1m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod prometheus-operator-564b78d5ff-v4xnk requesting resource cpu=6m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod prometheus-operator-admission-webhook-749df5cf4f-bmj4n requesting resource cpu=5m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod prometheus-operator-admission-webhook-749df5cf4f-zj8xj requesting resource cpu=5m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sre-dns-latency-exporter-2q899 requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sre-dns-latency-exporter-4lksg requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sre-dns-latency-exporter-5dz25 requesting resource cpu=0m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sre-dns-latency-exporter-ctmgr requesting resource cpu=0m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sre-dns-latency-exporter-jsn9x requesting resource cpu=0m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sre-dns-latency-exporter-vhwwx requesting resource cpu=0m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sre-ebs-iops-reporter-1-sg7b2 requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sre-stuck-ebs-vols-1-vdlrh requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod telemeter-client-54bbbbf598-d9fdk requesting resource cpu=3m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod thanos-querier-fcd5b6c8d-9jjkp requesting resource cpu=15m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod thanos-querier-fcd5b6c8d-ql8qb requesting resource cpu=15m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod token-refresher-6f7c46d758-mld9t requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod multus-5zck8 requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod multus-additional-cni-plugins-476gw requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod multus-additional-cni-plugins-88bc2 requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod multus-additional-cni-plugins-hvpd7 requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod multus-additional-cni-plugins-nhwfz requesting resource cpu=10m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod multus-additional-cni-plugins-p5pjp requesting resource cpu=10m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod multus-additional-cni-plugins-vslld requesting resource cpu=10m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod multus-j4ngm requesting resource cpu=10m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod multus-mczpf requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod multus-mptd7 requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod multus-xzxwg requesting resource cpu=10m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod multus-zqhtq requesting resource cpu=10m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-metrics-daemon-677lg requesting resource cpu=20m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-metrics-daemon-ddkb6 requesting resource cpu=20m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-metrics-daemon-hp85h requesting resource cpu=20m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-metrics-daemon-hz5pj requesting resource cpu=20m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-metrics-daemon-s2l94 requesting resource cpu=20m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-metrics-daemon-zvctv requesting resource cpu=20m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod must-gather-operator-56b466776c-qv8sh requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod must-gather-operator-registry-n7fvg requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-check-source-746dd6c885-bmf5k requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-check-target-d8gvt requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-check-target-d8k5s requesting resource cpu=10m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-check-target-g5qsk requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-check-target-mg7p4 requesting resource cpu=10m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-check-target-nq9ln requesting resource cpu=10m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod network-check-target-xrr4d requesting resource cpu=10m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod obo-prometheus-operator-64989cfc68-hdxb9 requesting resource cpu=5m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod obo-prometheus-operator-admission-webhook-558d9c7d6-b4r6s requesting resource cpu=50m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod obo-prometheus-operator-admission-webhook-558d9c7d6-gqhlz requesting resource cpu=50m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod observability-operator-6dd79df7dc-z7frh requesting resource cpu=5m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod observability-operator-catalog-vrnkr requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ocm-agent-ddfdc6544-4fxrp requesting resource cpu=1m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ocm-agent-operator-6cdb45fd86-4clcj requesting resource cpu=1m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ocm-agent-operator-registry-dwfc4 requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod osd-metrics-exporter-679d75d598-rsv49 requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod osd-metrics-exporter-registry-qdjtk requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ovnkube-node-ftv9n requesting resource cpu=50m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ovnkube-node-gq9vc requesting resource cpu=50m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ovnkube-node-gvwrr requesting resource cpu=50m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ovnkube-node-kk5t9 requesting resource cpu=50m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ovnkube-node-wcrc4 requesting resource cpu=50m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod ovnkube-node-zmvs8 requesting resource cpu=50m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod rbac-permissions-operator-5fb57974d8-htnjj requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod rbac-permissions-operator-registry-rc4js requesting resource cpu=10m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod blackbox-exporter-5f7f7bf859-l8ptw requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod route-monitor-operator-controller-manager-bfbff575-pfqc4 requesting resource cpu=100m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod route-monitor-operator-registry-whpgh requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod splunkforwarder-ds-66kjq requesting resource cpu=0m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod splunkforwarder-ds-7rpx7 requesting resource cpu=0m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod splunkforwarder-ds-h56tr requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod splunkforwarder-ds-pj5q8 requesting resource cpu=0m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod splunkforwarder-ds-xzbdk requesting resource cpu=0m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod splunkforwarder-ds-ztlpn requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod splunk-forwarder-operator-7d66568cf7-bksrn requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod splunk-forwarder-operator-catalog-fqp2b requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod prometheus-user-workload-0 requesting resource cpu=11m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod prometheus-user-workload-1 requesting resource cpu=11m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod thanos-ruler-user-workload-0 requesting resource cpu=3m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod thanos-ruler-user-workload-1 requesting resource cpu=3m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod managed-velero-operator-788754d4b6-n89fw requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod managed-velero-operator-registry-6lc6b requesting resource cpu=10m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod velero-7c64598b8c-8blx4 requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sonobuoy-e2e-job-4d801e056c4540b8 requesting resource cpu=0m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-8mc8n requesting resource cpu=0m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-cbfjg requesting resource cpu=0m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-chbvs requesting resource cpu=0m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-hj9fv requesting resource cpu=0m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-m8vnk requesting resource cpu=0m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.068: INFO: Pod sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-q5j5c requesting resource cpu=0m on Node ip-10-0-188-71.ec2.internal
    STEP: Starting Pods to consume most of the cluster CPU. 01/19/23 21:47:48.068
    Jan 19 21:47:48.068: INFO: Creating a pod which consumes cpu=2405m on Node ip-10-0-188-71.ec2.internal
    Jan 19 21:47:48.084: INFO: Creating a pod which consumes cpu=2051m on Node ip-10-0-207-77.ec2.internal
    Jan 19 21:47:48.100: INFO: Creating a pod which consumes cpu=2468m on Node ip-10-0-146-42.ec2.internal
    Jan 19 21:47:48.115: INFO: Creating a pod which consumes cpu=2046m on Node ip-10-0-151-158.ec2.internal
    Jan 19 21:47:48.130: INFO: Creating a pod which consumes cpu=2552m on Node ip-10-0-171-213.ec2.internal
    Jan 19 21:47:48.150: INFO: Creating a pod which consumes cpu=2552m on Node ip-10-0-172-44.ec2.internal
    Jan 19 21:47:48.171: INFO: Waiting up to 5m0s for pod "filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462" in namespace "sched-pred-4697" to be "running"
    Jan 19 21:47:48.182: INFO: Pod "filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462": Phase="Pending", Reason="", readiness=false. Elapsed: 11.389441ms
    Jan 19 21:47:50.186: INFO: Pod "filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462": Phase="Running", Reason="", readiness=true. Elapsed: 2.01468169s
    Jan 19 21:47:50.186: INFO: Pod "filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462" satisfied condition "running"
    Jan 19 21:47:50.186: INFO: Waiting up to 5m0s for pod "filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245" in namespace "sched-pred-4697" to be "running"
    Jan 19 21:47:50.188: INFO: Pod "filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245": Phase="Running", Reason="", readiness=true. Elapsed: 2.235948ms
    Jan 19 21:47:50.188: INFO: Pod "filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245" satisfied condition "running"
    Jan 19 21:47:50.188: INFO: Waiting up to 5m0s for pod "filler-pod-74458b47-100d-4308-b8f4-298ca388adba" in namespace "sched-pred-4697" to be "running"
    Jan 19 21:47:50.190: INFO: Pod "filler-pod-74458b47-100d-4308-b8f4-298ca388adba": Phase="Running", Reason="", readiness=true. Elapsed: 2.317566ms
    Jan 19 21:47:50.190: INFO: Pod "filler-pod-74458b47-100d-4308-b8f4-298ca388adba" satisfied condition "running"
    Jan 19 21:47:50.190: INFO: Waiting up to 5m0s for pod "filler-pod-714e512c-f5e9-4a61-983b-3d050de03763" in namespace "sched-pred-4697" to be "running"
    Jan 19 21:47:50.192: INFO: Pod "filler-pod-714e512c-f5e9-4a61-983b-3d050de03763": Phase="Running", Reason="", readiness=true. Elapsed: 2.120328ms
    Jan 19 21:47:50.193: INFO: Pod "filler-pod-714e512c-f5e9-4a61-983b-3d050de03763" satisfied condition "running"
    Jan 19 21:47:50.193: INFO: Waiting up to 5m0s for pod "filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655" in namespace "sched-pred-4697" to be "running"
    Jan 19 21:47:50.195: INFO: Pod "filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655": Phase="Running", Reason="", readiness=true. Elapsed: 2.12885ms
    Jan 19 21:47:50.195: INFO: Pod "filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655" satisfied condition "running"
    Jan 19 21:47:50.195: INFO: Waiting up to 5m0s for pod "filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157" in namespace "sched-pred-4697" to be "running"
    Jan 19 21:47:50.197: INFO: Pod "filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157": Phase="Running", Reason="", readiness=true. Elapsed: 1.895873ms
    Jan 19 21:47:50.197: INFO: Pod "filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/19/23 21:47:50.197
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157.173bd413d7268ffe], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4697/filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157 to ip-10-0-172-44.ec2.internal] 01/19/23 21:47:50.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157.173bd413fc066dcb], Reason = [AddedInterface], Message = [Add eth0 [10.128.9.18/23] from ovn-kubernetes] 01/19/23 21:47:50.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157.173bd413fd63322b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 21:47:50.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157.173bd414078d60da], Reason = [Created], Message = [Created container filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157] 01/19/23 21:47:50.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157.173bd4140973495e], Reason = [Started], Message = [Started container filler-pod-05d0cb2e-eef9-4b8a-a247-94eb51c9a157] 01/19/23 21:47:50.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245.173bd413d2f5a914], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4697/filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245 to ip-10-0-207-77.ec2.internal] 01/19/23 21:47:50.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245.173bd413fde71560], Reason = [AddedInterface], Message = [Add eth0 [10.128.16.139/23] from ovn-kubernetes] 01/19/23 21:47:50.2
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245.173bd413ffaa505f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245.173bd4140acb3605], Reason = [Created], Message = [Created container filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245.173bd4140ce1fa7d], Reason = [Started], Message = [Started container filler-pod-1370e0cf-f934-49d5-bc2b-ea85caa07245] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462.173bd413d201a3f7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4697/filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462 to ip-10-0-188-71.ec2.internal] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462.173bd413f66b664e], Reason = [AddedInterface], Message = [Add eth0 [10.128.6.85/23] from ovn-kubernetes] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462.173bd413f7a11e49], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462.173bd41400c2344a], Reason = [Created], Message = [Created container filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462.173bd41402140528], Reason = [Started], Message = [Started container filler-pod-54b0f54d-b505-4176-bab0-3a26e0354462] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-714e512c-f5e9-4a61-983b-3d050de03763.173bd413d4ca42ee], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4697/filler-pod-714e512c-f5e9-4a61-983b-3d050de03763 to ip-10-0-151-158.ec2.internal] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-714e512c-f5e9-4a61-983b-3d050de03763.173bd413f9a9ced2], Reason = [AddedInterface], Message = [Add eth0 [10.128.14.89/23] from ovn-kubernetes] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-714e512c-f5e9-4a61-983b-3d050de03763.173bd413fb07bfcd], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-714e512c-f5e9-4a61-983b-3d050de03763.173bd41406c74aab], Reason = [Created], Message = [Created container filler-pod-714e512c-f5e9-4a61-983b-3d050de03763] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-714e512c-f5e9-4a61-983b-3d050de03763.173bd41408f653c9], Reason = [Started], Message = [Started container filler-pod-714e512c-f5e9-4a61-983b-3d050de03763] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-74458b47-100d-4308-b8f4-298ca388adba.173bd413d3b809fa], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4697/filler-pod-74458b47-100d-4308-b8f4-298ca388adba to ip-10-0-146-42.ec2.internal] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-74458b47-100d-4308-b8f4-298ca388adba.173bd413f5c2b34d], Reason = [AddedInterface], Message = [Add eth0 [10.128.10.109/23] from ovn-kubernetes] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-74458b47-100d-4308-b8f4-298ca388adba.173bd413f7138206], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-74458b47-100d-4308-b8f4-298ca388adba.173bd414015f8ab3], Reason = [Created], Message = [Created container filler-pod-74458b47-100d-4308-b8f4-298ca388adba] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-74458b47-100d-4308-b8f4-298ca388adba.173bd414030bd9a4], Reason = [Started], Message = [Started container filler-pod-74458b47-100d-4308-b8f4-298ca388adba] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655.173bd413d6380c08], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4697/filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655 to ip-10-0-171-213.ec2.internal] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655.173bd413f7f48af8], Reason = [AddedInterface], Message = [Add eth0 [10.128.12.148/23] from ovn-kubernetes] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655.173bd413f92f32f1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 01/19/23 21:47:50.201
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655.173bd414020eb9fc], Reason = [Created], Message = [Created container filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655] 01/19/23 21:47:50.202
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655.173bd414032c78b4], Reason = [Started], Message = [Started container filler-pod-ff6cd77e-6d1d-4dd3-bdae-7317f13c5655] 01/19/23 21:47:50.202
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173bd4145085b35c], Reason = [FailedScheduling], Message = [0/9 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/master: }, 6 Insufficient cpu. preemption: 0/9 nodes are available: 3 Preemption is not helpful for scheduling, 6 No preemption victims found for incoming pod.] 01/19/23 21:47:50.218
    STEP: removing the label node off the node ip-10-0-146-42.ec2.internal 01/19/23 21:47:51.219
    STEP: verifying the node doesn't have the label node 01/19/23 21:47:51.234
    STEP: removing the label node off the node ip-10-0-151-158.ec2.internal 01/19/23 21:47:51.239
    STEP: verifying the node doesn't have the label node 01/19/23 21:47:51.261
    STEP: removing the label node off the node ip-10-0-171-213.ec2.internal 01/19/23 21:47:51.271
    STEP: verifying the node doesn't have the label node 01/19/23 21:47:51.298
    STEP: removing the label node off the node ip-10-0-172-44.ec2.internal 01/19/23 21:47:51.301
    STEP: verifying the node doesn't have the label node 01/19/23 21:47:51.322
    STEP: removing the label node off the node ip-10-0-188-71.ec2.internal 01/19/23 21:47:51.328
    STEP: verifying the node doesn't have the label node 01/19/23 21:47:51.344
    STEP: removing the label node off the node ip-10-0-207-77.ec2.internal 01/19/23 21:47:51.347
    STEP: verifying the node doesn't have the label node 01/19/23 21:47:51.364
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:47:51.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4697" for this suite. 01/19/23 21:47:51.38
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:47:51.387
Jan 19 21:47:51.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 21:47:51.388
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:51.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:51.402
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6114 01/19/23 21:47:51.407
STEP: changing the ExternalName service to type=ClusterIP 01/19/23 21:47:51.411
STEP: creating replication controller externalname-service in namespace services-6114 01/19/23 21:47:51.455
I0119 21:47:51.461055      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6114, replica count: 2
I0119 21:47:54.513657      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 21:47:54.513: INFO: Creating new exec pod
Jan 19 21:47:54.527: INFO: Waiting up to 5m0s for pod "execpodjnk79" in namespace "services-6114" to be "running"
Jan 19 21:47:54.529: INFO: Pod "execpodjnk79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.284379ms
Jan 19 21:47:56.532: INFO: Pod "execpodjnk79": Phase="Running", Reason="", readiness=true. Elapsed: 2.005036084s
Jan 19 21:47:56.532: INFO: Pod "execpodjnk79" satisfied condition "running"
Jan 19 21:47:57.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6114 exec execpodjnk79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jan 19 21:47:57.645: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan 19 21:47:57.645: INFO: stdout: "externalname-service-2msxf"
Jan 19 21:47:57.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6114 exec execpodjnk79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.119.102 80'
Jan 19 21:47:57.769: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.119.102 80\nConnection to 172.30.119.102 80 port [tcp/http] succeeded!\n"
Jan 19 21:47:57.769: INFO: stdout: ""
Jan 19 21:47:58.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6114 exec execpodjnk79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.119.102 80'
Jan 19 21:47:58.895: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.119.102 80\nConnection to 172.30.119.102 80 port [tcp/http] succeeded!\n"
Jan 19 21:47:58.895: INFO: stdout: ""
Jan 19 21:47:59.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6114 exec execpodjnk79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.119.102 80'
Jan 19 21:47:59.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.119.102 80\nConnection to 172.30.119.102 80 port [tcp/http] succeeded!\n"
Jan 19 21:47:59.890: INFO: stdout: "externalname-service-2msxf"
Jan 19 21:47:59.890: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 21:47:59.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6114" for this suite. 01/19/23 21:47:59.915
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":284,"skipped":5277,"failed":0}
------------------------------
• [SLOW TEST] [8.547 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:47:51.387
    Jan 19 21:47:51.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 21:47:51.388
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:51.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:51.402
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6114 01/19/23 21:47:51.407
    STEP: changing the ExternalName service to type=ClusterIP 01/19/23 21:47:51.411
    STEP: creating replication controller externalname-service in namespace services-6114 01/19/23 21:47:51.455
    I0119 21:47:51.461055      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6114, replica count: 2
    I0119 21:47:54.513657      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 21:47:54.513: INFO: Creating new exec pod
    Jan 19 21:47:54.527: INFO: Waiting up to 5m0s for pod "execpodjnk79" in namespace "services-6114" to be "running"
    Jan 19 21:47:54.529: INFO: Pod "execpodjnk79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.284379ms
    Jan 19 21:47:56.532: INFO: Pod "execpodjnk79": Phase="Running", Reason="", readiness=true. Elapsed: 2.005036084s
    Jan 19 21:47:56.532: INFO: Pod "execpodjnk79" satisfied condition "running"
    Jan 19 21:47:57.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6114 exec execpodjnk79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jan 19 21:47:57.645: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan 19 21:47:57.645: INFO: stdout: "externalname-service-2msxf"
    Jan 19 21:47:57.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6114 exec execpodjnk79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.119.102 80'
    Jan 19 21:47:57.769: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.119.102 80\nConnection to 172.30.119.102 80 port [tcp/http] succeeded!\n"
    Jan 19 21:47:57.769: INFO: stdout: ""
    Jan 19 21:47:58.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6114 exec execpodjnk79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.119.102 80'
    Jan 19 21:47:58.895: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.119.102 80\nConnection to 172.30.119.102 80 port [tcp/http] succeeded!\n"
    Jan 19 21:47:58.895: INFO: stdout: ""
    Jan 19 21:47:59.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-6114 exec execpodjnk79 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.119.102 80'
    Jan 19 21:47:59.890: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.119.102 80\nConnection to 172.30.119.102 80 port [tcp/http] succeeded!\n"
    Jan 19 21:47:59.890: INFO: stdout: "externalname-service-2msxf"
    Jan 19 21:47:59.890: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 21:47:59.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-6114" for this suite. 01/19/23 21:47:59.915
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:47:59.947
Jan 19 21:47:59.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename runtimeclass 01/19/23 21:47:59.948
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:59.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:59.979
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-7314-delete-me 01/19/23 21:47:59.989
STEP: Waiting for the RuntimeClass to disappear 01/19/23 21:48:00.006
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jan 19 21:48:00.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7314" for this suite. 01/19/23 21:48:00.026
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":285,"skipped":5317,"failed":0}
------------------------------
• [0.086 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:47:59.947
    Jan 19 21:47:59.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename runtimeclass 01/19/23 21:47:59.948
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:47:59.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:47:59.979
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-7314-delete-me 01/19/23 21:47:59.989
    STEP: Waiting for the RuntimeClass to disappear 01/19/23 21:48:00.006
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jan 19 21:48:00.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7314" for this suite. 01/19/23 21:48:00.026
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:48:00.034
Jan 19 21:48:00.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename resourcequota 01/19/23 21:48:00.034
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:00.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:00.072
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 01/19/23 21:48:17.119
STEP: Creating a ResourceQuota 01/19/23 21:48:22.123
STEP: Ensuring resource quota status is calculated 01/19/23 21:48:22.128
STEP: Creating a ConfigMap 01/19/23 21:48:24.131
STEP: Ensuring resource quota status captures configMap creation 01/19/23 21:48:24.143
STEP: Deleting a ConfigMap 01/19/23 21:48:26.146
STEP: Ensuring resource quota status released usage 01/19/23 21:48:26.152
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 21:48:28.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1183" for this suite. 01/19/23 21:48:28.161
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":286,"skipped":5318,"failed":0}
------------------------------
• [SLOW TEST] [28.132 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:48:00.034
    Jan 19 21:48:00.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename resourcequota 01/19/23 21:48:00.034
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:00.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:00.072
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 01/19/23 21:48:17.119
    STEP: Creating a ResourceQuota 01/19/23 21:48:22.123
    STEP: Ensuring resource quota status is calculated 01/19/23 21:48:22.128
    STEP: Creating a ConfigMap 01/19/23 21:48:24.131
    STEP: Ensuring resource quota status captures configMap creation 01/19/23 21:48:24.143
    STEP: Deleting a ConfigMap 01/19/23 21:48:26.146
    STEP: Ensuring resource quota status released usage 01/19/23 21:48:26.152
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 21:48:28.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1183" for this suite. 01/19/23 21:48:28.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:48:28.168
Jan 19 21:48:28.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubelet-test 01/19/23 21:48:28.169
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:28.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:28.184
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/19/23 21:48:29.23
Jan 19 21:48:29.230: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases3bf1a092-a4b8-4212-b5c4-ccd0179a20de" in namespace "kubelet-test-6422" to be "completed"
Jan 19 21:48:29.232: INFO: Pod "agnhost-host-aliases3bf1a092-a4b8-4212-b5c4-ccd0179a20de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.239423ms
Jan 19 21:48:31.235: INFO: Pod "agnhost-host-aliases3bf1a092-a4b8-4212-b5c4-ccd0179a20de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00518374s
Jan 19 21:48:33.235: INFO: Pod "agnhost-host-aliases3bf1a092-a4b8-4212-b5c4-ccd0179a20de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005215698s
Jan 19 21:48:33.235: INFO: Pod "agnhost-host-aliases3bf1a092-a4b8-4212-b5c4-ccd0179a20de" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jan 19 21:48:33.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6422" for this suite. 01/19/23 21:48:33.249
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":287,"skipped":5365,"failed":0}
------------------------------
• [SLOW TEST] [5.086 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:48:28.168
    Jan 19 21:48:28.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubelet-test 01/19/23 21:48:28.169
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:28.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:28.184
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/19/23 21:48:29.23
    Jan 19 21:48:29.230: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases3bf1a092-a4b8-4212-b5c4-ccd0179a20de" in namespace "kubelet-test-6422" to be "completed"
    Jan 19 21:48:29.232: INFO: Pod "agnhost-host-aliases3bf1a092-a4b8-4212-b5c4-ccd0179a20de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.239423ms
    Jan 19 21:48:31.235: INFO: Pod "agnhost-host-aliases3bf1a092-a4b8-4212-b5c4-ccd0179a20de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00518374s
    Jan 19 21:48:33.235: INFO: Pod "agnhost-host-aliases3bf1a092-a4b8-4212-b5c4-ccd0179a20de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005215698s
    Jan 19 21:48:33.235: INFO: Pod "agnhost-host-aliases3bf1a092-a4b8-4212-b5c4-ccd0179a20de" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jan 19 21:48:33.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6422" for this suite. 01/19/23 21:48:33.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:48:33.254
Jan 19 21:48:33.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 21:48:33.255
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:33.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:33.269
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 01/19/23 21:48:33.273
W0119 21:48:33.309336      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:48:33.309: INFO: Waiting up to 5m0s for pod "pod-7d6cde06-073e-4eb2-a658-dffce4d26292" in namespace "emptydir-9053" to be "Succeeded or Failed"
Jan 19 21:48:33.312: INFO: Pod "pod-7d6cde06-073e-4eb2-a658-dffce4d26292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.609413ms
Jan 19 21:48:35.315: INFO: Pod "pod-7d6cde06-073e-4eb2-a658-dffce4d26292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006068549s
Jan 19 21:48:37.316: INFO: Pod "pod-7d6cde06-073e-4eb2-a658-dffce4d26292": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006701449s
STEP: Saw pod success 01/19/23 21:48:37.316
Jan 19 21:48:37.316: INFO: Pod "pod-7d6cde06-073e-4eb2-a658-dffce4d26292" satisfied condition "Succeeded or Failed"
Jan 19 21:48:37.318: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-7d6cde06-073e-4eb2-a658-dffce4d26292 container test-container: <nil>
STEP: delete the pod 01/19/23 21:48:37.323
Jan 19 21:48:37.334: INFO: Waiting for pod pod-7d6cde06-073e-4eb2-a658-dffce4d26292 to disappear
Jan 19 21:48:37.336: INFO: Pod pod-7d6cde06-073e-4eb2-a658-dffce4d26292 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 21:48:37.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9053" for this suite. 01/19/23 21:48:37.342
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":288,"skipped":5376,"failed":0}
------------------------------
• [4.093 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:48:33.254
    Jan 19 21:48:33.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 21:48:33.255
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:33.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:33.269
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/19/23 21:48:33.273
    W0119 21:48:33.309336      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-container" must set securityContext.capabilities.drop=["ALL"]), seccompProfile (pod or container "test-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:48:33.309: INFO: Waiting up to 5m0s for pod "pod-7d6cde06-073e-4eb2-a658-dffce4d26292" in namespace "emptydir-9053" to be "Succeeded or Failed"
    Jan 19 21:48:33.312: INFO: Pod "pod-7d6cde06-073e-4eb2-a658-dffce4d26292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.609413ms
    Jan 19 21:48:35.315: INFO: Pod "pod-7d6cde06-073e-4eb2-a658-dffce4d26292": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006068549s
    Jan 19 21:48:37.316: INFO: Pod "pod-7d6cde06-073e-4eb2-a658-dffce4d26292": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006701449s
    STEP: Saw pod success 01/19/23 21:48:37.316
    Jan 19 21:48:37.316: INFO: Pod "pod-7d6cde06-073e-4eb2-a658-dffce4d26292" satisfied condition "Succeeded or Failed"
    Jan 19 21:48:37.318: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-7d6cde06-073e-4eb2-a658-dffce4d26292 container test-container: <nil>
    STEP: delete the pod 01/19/23 21:48:37.323
    Jan 19 21:48:37.334: INFO: Waiting for pod pod-7d6cde06-073e-4eb2-a658-dffce4d26292 to disappear
    Jan 19 21:48:37.336: INFO: Pod pod-7d6cde06-073e-4eb2-a658-dffce4d26292 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:48:37.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9053" for this suite. 01/19/23 21:48:37.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:48:37.347
Jan 19 21:48:37.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename replication-controller 01/19/23 21:48:37.348
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:37.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:37.368
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e 01/19/23 21:48:37.37
W0119 21:48:37.380671      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:48:37.383: INFO: Pod name my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e: Found 0 pods out of 1
Jan 19 21:48:42.386: INFO: Pod name my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e: Found 1 pods out of 1
Jan 19 21:48:42.386: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e" are running
Jan 19 21:48:42.386: INFO: Waiting up to 5m0s for pod "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e-6l6h6" in namespace "replication-controller-9740" to be "running"
Jan 19 21:48:42.389: INFO: Pod "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e-6l6h6": Phase="Running", Reason="", readiness=true. Elapsed: 2.496876ms
Jan 19 21:48:42.389: INFO: Pod "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e-6l6h6" satisfied condition "running"
Jan 19 21:48:42.389: INFO: Pod "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e-6l6h6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:48:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:48:38 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:48:38 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:48:37 +0000 UTC Reason: Message:}])
Jan 19 21:48:42.389: INFO: Trying to dial the pod
Jan 19 21:48:47.401: INFO: Controller my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e: Got expected result from replica 1 [my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e-6l6h6]: "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e-6l6h6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jan 19 21:48:47.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9740" for this suite. 01/19/23 21:48:47.405
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":289,"skipped":5391,"failed":0}
------------------------------
• [SLOW TEST] [10.063 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:48:37.347
    Jan 19 21:48:37.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename replication-controller 01/19/23 21:48:37.348
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:37.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:37.368
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e 01/19/23 21:48:37.37
    W0119 21:48:37.380671      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:48:37.383: INFO: Pod name my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e: Found 0 pods out of 1
    Jan 19 21:48:42.386: INFO: Pod name my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e: Found 1 pods out of 1
    Jan 19 21:48:42.386: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e" are running
    Jan 19 21:48:42.386: INFO: Waiting up to 5m0s for pod "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e-6l6h6" in namespace "replication-controller-9740" to be "running"
    Jan 19 21:48:42.389: INFO: Pod "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e-6l6h6": Phase="Running", Reason="", readiness=true. Elapsed: 2.496876ms
    Jan 19 21:48:42.389: INFO: Pod "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e-6l6h6" satisfied condition "running"
    Jan 19 21:48:42.389: INFO: Pod "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e-6l6h6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:48:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:48:38 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:48:38 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-19 21:48:37 +0000 UTC Reason: Message:}])
    Jan 19 21:48:42.389: INFO: Trying to dial the pod
    Jan 19 21:48:47.401: INFO: Controller my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e: Got expected result from replica 1 [my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e-6l6h6]: "my-hostname-basic-ca0ddbff-b0d8-43d1-adc7-51647437951e-6l6h6", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jan 19 21:48:47.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-9740" for this suite. 01/19/23 21:48:47.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:48:47.412
Jan 19 21:48:47.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:48:47.413
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:47.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:47.43
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:48:47.457
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:48:47.813
STEP: Deploying the webhook pod 01/19/23 21:48:47.822
STEP: Wait for the deployment to be ready 01/19/23 21:48:47.832
Jan 19 21:48:47.837: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/19/23 21:48:49.845
STEP: Verifying the service has paired with the endpoint 01/19/23 21:48:49.857
Jan 19 21:48:50.857: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/19/23 21:48:50.86
STEP: create a namespace for the webhook 01/19/23 21:48:50.873
STEP: create a configmap should be unconditionally rejected by the webhook 01/19/23 21:48:50.882
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:48:50.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-299" for this suite. 01/19/23 21:48:50.929
STEP: Destroying namespace "webhook-299-markers" for this suite. 01/19/23 21:48:50.942
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":290,"skipped":5428,"failed":0}
------------------------------
• [3.581 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:48:47.412
    Jan 19 21:48:47.412: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:48:47.413
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:47.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:47.43
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:48:47.457
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:48:47.813
    STEP: Deploying the webhook pod 01/19/23 21:48:47.822
    STEP: Wait for the deployment to be ready 01/19/23 21:48:47.832
    Jan 19 21:48:47.837: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/19/23 21:48:49.845
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:48:49.857
    Jan 19 21:48:50.857: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/19/23 21:48:50.86
    STEP: create a namespace for the webhook 01/19/23 21:48:50.873
    STEP: create a configmap should be unconditionally rejected by the webhook 01/19/23 21:48:50.882
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:48:50.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-299" for this suite. 01/19/23 21:48:50.929
    STEP: Destroying namespace "webhook-299-markers" for this suite. 01/19/23 21:48:50.942
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:48:50.994
Jan 19 21:48:50.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 21:48:50.995
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:51.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:51.013
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/19/23 21:48:51.016
Jan 19 21:48:52.032: INFO: Waiting up to 5m0s for pod "pod-d3cb144f-41ad-47ec-85fc-a862d6111dde" in namespace "emptydir-5174" to be "Succeeded or Failed"
Jan 19 21:48:52.034: INFO: Pod "pod-d3cb144f-41ad-47ec-85fc-a862d6111dde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.416348ms
Jan 19 21:48:54.038: INFO: Pod "pod-d3cb144f-41ad-47ec-85fc-a862d6111dde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005961993s
Jan 19 21:48:56.039: INFO: Pod "pod-d3cb144f-41ad-47ec-85fc-a862d6111dde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007171626s
STEP: Saw pod success 01/19/23 21:48:56.039
Jan 19 21:48:56.039: INFO: Pod "pod-d3cb144f-41ad-47ec-85fc-a862d6111dde" satisfied condition "Succeeded or Failed"
Jan 19 21:48:56.042: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-d3cb144f-41ad-47ec-85fc-a862d6111dde container test-container: <nil>
STEP: delete the pod 01/19/23 21:48:56.051
Jan 19 21:48:56.066: INFO: Waiting for pod pod-d3cb144f-41ad-47ec-85fc-a862d6111dde to disappear
Jan 19 21:48:56.071: INFO: Pod pod-d3cb144f-41ad-47ec-85fc-a862d6111dde no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 21:48:56.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5174" for this suite. 01/19/23 21:48:56.076
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":291,"skipped":5449,"failed":0}
------------------------------
• [SLOW TEST] [5.087 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:48:50.994
    Jan 19 21:48:50.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 21:48:50.995
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:51.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:51.013
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/19/23 21:48:51.016
    Jan 19 21:48:52.032: INFO: Waiting up to 5m0s for pod "pod-d3cb144f-41ad-47ec-85fc-a862d6111dde" in namespace "emptydir-5174" to be "Succeeded or Failed"
    Jan 19 21:48:52.034: INFO: Pod "pod-d3cb144f-41ad-47ec-85fc-a862d6111dde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.416348ms
    Jan 19 21:48:54.038: INFO: Pod "pod-d3cb144f-41ad-47ec-85fc-a862d6111dde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005961993s
    Jan 19 21:48:56.039: INFO: Pod "pod-d3cb144f-41ad-47ec-85fc-a862d6111dde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007171626s
    STEP: Saw pod success 01/19/23 21:48:56.039
    Jan 19 21:48:56.039: INFO: Pod "pod-d3cb144f-41ad-47ec-85fc-a862d6111dde" satisfied condition "Succeeded or Failed"
    Jan 19 21:48:56.042: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-d3cb144f-41ad-47ec-85fc-a862d6111dde container test-container: <nil>
    STEP: delete the pod 01/19/23 21:48:56.051
    Jan 19 21:48:56.066: INFO: Waiting for pod pod-d3cb144f-41ad-47ec-85fc-a862d6111dde to disappear
    Jan 19 21:48:56.071: INFO: Pod pod-d3cb144f-41ad-47ec-85fc-a862d6111dde no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:48:56.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5174" for this suite. 01/19/23 21:48:56.076
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:48:56.082
Jan 19 21:48:56.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename discovery 01/19/23 21:48:56.083
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:56.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:56.102
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/19/23 21:48:56.105
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan 19 21:48:56.392: INFO: Checking APIGroup: apiregistration.k8s.io
Jan 19 21:48:56.393: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan 19 21:48:56.393: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan 19 21:48:56.393: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan 19 21:48:56.393: INFO: Checking APIGroup: apps
Jan 19 21:48:56.393: INFO: PreferredVersion.GroupVersion: apps/v1
Jan 19 21:48:56.393: INFO: Versions found [{apps/v1 v1}]
Jan 19 21:48:56.393: INFO: apps/v1 matches apps/v1
Jan 19 21:48:56.393: INFO: Checking APIGroup: events.k8s.io
Jan 19 21:48:56.394: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan 19 21:48:56.394: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan 19 21:48:56.394: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan 19 21:48:56.394: INFO: Checking APIGroup: authentication.k8s.io
Jan 19 21:48:56.395: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan 19 21:48:56.395: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan 19 21:48:56.395: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan 19 21:48:56.395: INFO: Checking APIGroup: authorization.k8s.io
Jan 19 21:48:56.395: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan 19 21:48:56.395: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan 19 21:48:56.395: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan 19 21:48:56.395: INFO: Checking APIGroup: autoscaling
Jan 19 21:48:56.396: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan 19 21:48:56.396: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jan 19 21:48:56.396: INFO: autoscaling/v2 matches autoscaling/v2
Jan 19 21:48:56.396: INFO: Checking APIGroup: batch
Jan 19 21:48:56.397: INFO: PreferredVersion.GroupVersion: batch/v1
Jan 19 21:48:56.397: INFO: Versions found [{batch/v1 v1}]
Jan 19 21:48:56.397: INFO: batch/v1 matches batch/v1
Jan 19 21:48:56.397: INFO: Checking APIGroup: certificates.k8s.io
Jan 19 21:48:56.397: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan 19 21:48:56.397: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan 19 21:48:56.397: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan 19 21:48:56.397: INFO: Checking APIGroup: networking.k8s.io
Jan 19 21:48:56.398: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan 19 21:48:56.398: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan 19 21:48:56.398: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan 19 21:48:56.398: INFO: Checking APIGroup: policy
Jan 19 21:48:56.398: INFO: PreferredVersion.GroupVersion: policy/v1
Jan 19 21:48:56.398: INFO: Versions found [{policy/v1 v1}]
Jan 19 21:48:56.398: INFO: policy/v1 matches policy/v1
Jan 19 21:48:56.398: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan 19 21:48:56.399: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan 19 21:48:56.399: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan 19 21:48:56.399: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan 19 21:48:56.399: INFO: Checking APIGroup: storage.k8s.io
Jan 19 21:48:56.400: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan 19 21:48:56.400: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan 19 21:48:56.400: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan 19 21:48:56.400: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan 19 21:48:56.400: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan 19 21:48:56.400: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan 19 21:48:56.400: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan 19 21:48:56.400: INFO: Checking APIGroup: apiextensions.k8s.io
Jan 19 21:48:56.401: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan 19 21:48:56.401: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan 19 21:48:56.401: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan 19 21:48:56.401: INFO: Checking APIGroup: scheduling.k8s.io
Jan 19 21:48:56.402: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan 19 21:48:56.402: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan 19 21:48:56.402: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan 19 21:48:56.402: INFO: Checking APIGroup: coordination.k8s.io
Jan 19 21:48:56.402: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan 19 21:48:56.402: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan 19 21:48:56.402: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan 19 21:48:56.402: INFO: Checking APIGroup: node.k8s.io
Jan 19 21:48:56.403: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan 19 21:48:56.403: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan 19 21:48:56.403: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan 19 21:48:56.403: INFO: Checking APIGroup: discovery.k8s.io
Jan 19 21:48:56.404: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan 19 21:48:56.404: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan 19 21:48:56.404: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan 19 21:48:56.404: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan 19 21:48:56.405: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jan 19 21:48:56.405: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jan 19 21:48:56.405: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jan 19 21:48:56.405: INFO: Checking APIGroup: apps.openshift.io
Jan 19 21:48:56.405: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Jan 19 21:48:56.405: INFO: Versions found [{apps.openshift.io/v1 v1}]
Jan 19 21:48:56.405: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Jan 19 21:48:56.405: INFO: Checking APIGroup: authorization.openshift.io
Jan 19 21:48:56.406: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Jan 19 21:48:56.406: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Jan 19 21:48:56.406: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Jan 19 21:48:56.406: INFO: Checking APIGroup: build.openshift.io
Jan 19 21:48:56.407: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Jan 19 21:48:56.407: INFO: Versions found [{build.openshift.io/v1 v1}]
Jan 19 21:48:56.407: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Jan 19 21:48:56.407: INFO: Checking APIGroup: image.openshift.io
Jan 19 21:48:56.407: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Jan 19 21:48:56.407: INFO: Versions found [{image.openshift.io/v1 v1}]
Jan 19 21:48:56.407: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Jan 19 21:48:56.407: INFO: Checking APIGroup: oauth.openshift.io
Jan 19 21:48:56.408: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Jan 19 21:48:56.408: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Jan 19 21:48:56.408: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Jan 19 21:48:56.408: INFO: Checking APIGroup: project.openshift.io
Jan 19 21:48:56.408: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Jan 19 21:48:56.408: INFO: Versions found [{project.openshift.io/v1 v1}]
Jan 19 21:48:56.408: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Jan 19 21:48:56.408: INFO: Checking APIGroup: quota.openshift.io
Jan 19 21:48:56.409: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Jan 19 21:48:56.409: INFO: Versions found [{quota.openshift.io/v1 v1}]
Jan 19 21:48:56.409: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Jan 19 21:48:56.409: INFO: Checking APIGroup: route.openshift.io
Jan 19 21:48:56.410: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Jan 19 21:48:56.410: INFO: Versions found [{route.openshift.io/v1 v1}]
Jan 19 21:48:56.410: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Jan 19 21:48:56.410: INFO: Checking APIGroup: security.openshift.io
Jan 19 21:48:56.410: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Jan 19 21:48:56.410: INFO: Versions found [{security.openshift.io/v1 v1}]
Jan 19 21:48:56.410: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Jan 19 21:48:56.410: INFO: Checking APIGroup: template.openshift.io
Jan 19 21:48:56.411: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Jan 19 21:48:56.411: INFO: Versions found [{template.openshift.io/v1 v1}]
Jan 19 21:48:56.411: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Jan 19 21:48:56.411: INFO: Checking APIGroup: user.openshift.io
Jan 19 21:48:56.412: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Jan 19 21:48:56.412: INFO: Versions found [{user.openshift.io/v1 v1}]
Jan 19 21:48:56.412: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Jan 19 21:48:56.412: INFO: Checking APIGroup: packages.operators.coreos.com
Jan 19 21:48:56.412: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Jan 19 21:48:56.412: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Jan 19 21:48:56.412: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Jan 19 21:48:56.412: INFO: Checking APIGroup: config.openshift.io
Jan 19 21:48:56.413: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Jan 19 21:48:56.413: INFO: Versions found [{config.openshift.io/v1 v1}]
Jan 19 21:48:56.413: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Jan 19 21:48:56.413: INFO: Checking APIGroup: operator.openshift.io
Jan 19 21:48:56.414: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Jan 19 21:48:56.414: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Jan 19 21:48:56.414: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Jan 19 21:48:56.414: INFO: Checking APIGroup: apiserver.openshift.io
Jan 19 21:48:56.414: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Jan 19 21:48:56.414: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Jan 19 21:48:56.414: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Jan 19 21:48:56.414: INFO: Checking APIGroup: autoscaling.openshift.io
Jan 19 21:48:56.415: INFO: PreferredVersion.GroupVersion: autoscaling.openshift.io/v1
Jan 19 21:48:56.415: INFO: Versions found [{autoscaling.openshift.io/v1 v1} {autoscaling.openshift.io/v1beta1 v1beta1}]
Jan 19 21:48:56.415: INFO: autoscaling.openshift.io/v1 matches autoscaling.openshift.io/v1
Jan 19 21:48:56.415: INFO: Checking APIGroup: cloud.network.openshift.io
Jan 19 21:48:56.416: INFO: PreferredVersion.GroupVersion: cloud.network.openshift.io/v1
Jan 19 21:48:56.416: INFO: Versions found [{cloud.network.openshift.io/v1 v1}]
Jan 19 21:48:56.416: INFO: cloud.network.openshift.io/v1 matches cloud.network.openshift.io/v1
Jan 19 21:48:56.416: INFO: Checking APIGroup: cloudcredential.openshift.io
Jan 19 21:48:56.416: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Jan 19 21:48:56.416: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Jan 19 21:48:56.416: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Jan 19 21:48:56.416: INFO: Checking APIGroup: console.openshift.io
Jan 19 21:48:56.417: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Jan 19 21:48:56.417: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Jan 19 21:48:56.417: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Jan 19 21:48:56.417: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Jan 19 21:48:56.418: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Jan 19 21:48:56.418: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Jan 19 21:48:56.418: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Jan 19 21:48:56.418: INFO: Checking APIGroup: ingress.operator.openshift.io
Jan 19 21:48:56.418: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Jan 19 21:48:56.418: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Jan 19 21:48:56.418: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Jan 19 21:48:56.418: INFO: Checking APIGroup: k8s.cni.cncf.io
Jan 19 21:48:56.419: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Jan 19 21:48:56.419: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Jan 19 21:48:56.419: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Jan 19 21:48:56.419: INFO: Checking APIGroup: k8s.ovn.org
Jan 19 21:48:56.420: INFO: PreferredVersion.GroupVersion: k8s.ovn.org/v1
Jan 19 21:48:56.420: INFO: Versions found [{k8s.ovn.org/v1 v1}]
Jan 19 21:48:56.420: INFO: k8s.ovn.org/v1 matches k8s.ovn.org/v1
Jan 19 21:48:56.420: INFO: Checking APIGroup: machine.openshift.io
Jan 19 21:48:56.420: INFO: PreferredVersion.GroupVersion: machine.openshift.io/v1
Jan 19 21:48:56.420: INFO: Versions found [{machine.openshift.io/v1 v1} {machine.openshift.io/v1beta1 v1beta1}]
Jan 19 21:48:56.420: INFO: machine.openshift.io/v1 matches machine.openshift.io/v1
Jan 19 21:48:56.420: INFO: Checking APIGroup: machineconfiguration.openshift.io
Jan 19 21:48:56.421: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Jan 19 21:48:56.421: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Jan 19 21:48:56.421: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Jan 19 21:48:56.421: INFO: Checking APIGroup: monitoring.coreos.com
Jan 19 21:48:56.422: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jan 19 21:48:56.422: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jan 19 21:48:56.422: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jan 19 21:48:56.422: INFO: Checking APIGroup: monitoring.rhobs
Jan 19 21:48:56.422: INFO: PreferredVersion.GroupVersion: monitoring.rhobs/v1
Jan 19 21:48:56.422: INFO: Versions found [{monitoring.rhobs/v1 v1} {monitoring.rhobs/v1alpha1 v1alpha1}]
Jan 19 21:48:56.422: INFO: monitoring.rhobs/v1 matches monitoring.rhobs/v1
Jan 19 21:48:56.422: INFO: Checking APIGroup: network.operator.openshift.io
Jan 19 21:48:56.423: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Jan 19 21:48:56.423: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Jan 19 21:48:56.423: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Jan 19 21:48:56.423: INFO: Checking APIGroup: operators.coreos.com
Jan 19 21:48:56.424: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Jan 19 21:48:56.424: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Jan 19 21:48:56.424: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Jan 19 21:48:56.424: INFO: Checking APIGroup: performance.openshift.io
Jan 19 21:48:56.425: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
Jan 19 21:48:56.425: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
Jan 19 21:48:56.425: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
Jan 19 21:48:56.425: INFO: Checking APIGroup: samples.operator.openshift.io
Jan 19 21:48:56.442: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Jan 19 21:48:56.442: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Jan 19 21:48:56.442: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Jan 19 21:48:56.442: INFO: Checking APIGroup: security.internal.openshift.io
Jan 19 21:48:56.492: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Jan 19 21:48:56.492: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Jan 19 21:48:56.492: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Jan 19 21:48:56.492: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan 19 21:48:56.542: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan 19 21:48:56.542: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Jan 19 21:48:56.542: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan 19 21:48:56.542: INFO: Checking APIGroup: tuned.openshift.io
Jan 19 21:48:56.592: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Jan 19 21:48:56.592: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Jan 19 21:48:56.592: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Jan 19 21:48:56.592: INFO: Checking APIGroup: velero.io
Jan 19 21:48:56.642: INFO: PreferredVersion.GroupVersion: velero.io/v1
Jan 19 21:48:56.643: INFO: Versions found [{velero.io/v1 v1}]
Jan 19 21:48:56.643: INFO: velero.io/v1 matches velero.io/v1
Jan 19 21:48:56.643: INFO: Checking APIGroup: addons.managed.openshift.io
Jan 19 21:48:56.693: INFO: PreferredVersion.GroupVersion: addons.managed.openshift.io/v1alpha1
Jan 19 21:48:56.693: INFO: Versions found [{addons.managed.openshift.io/v1alpha1 v1alpha1}]
Jan 19 21:48:56.693: INFO: addons.managed.openshift.io/v1alpha1 matches addons.managed.openshift.io/v1alpha1
Jan 19 21:48:56.693: INFO: Checking APIGroup: cloudingress.managed.openshift.io
Jan 19 21:48:56.742: INFO: PreferredVersion.GroupVersion: cloudingress.managed.openshift.io/v1alpha1
Jan 19 21:48:56.743: INFO: Versions found [{cloudingress.managed.openshift.io/v1alpha1 v1alpha1}]
Jan 19 21:48:56.743: INFO: cloudingress.managed.openshift.io/v1alpha1 matches cloudingress.managed.openshift.io/v1alpha1
Jan 19 21:48:56.743: INFO: Checking APIGroup: controlplane.operator.openshift.io
Jan 19 21:48:56.793: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Jan 19 21:48:56.793: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Jan 19 21:48:56.793: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Jan 19 21:48:56.793: INFO: Checking APIGroup: managed.openshift.io
Jan 19 21:48:56.842: INFO: PreferredVersion.GroupVersion: managed.openshift.io/v1alpha2
Jan 19 21:48:56.842: INFO: Versions found [{managed.openshift.io/v1alpha2 v1alpha2} {managed.openshift.io/v1alpha1 v1alpha1}]
Jan 19 21:48:56.842: INFO: managed.openshift.io/v1alpha2 matches managed.openshift.io/v1alpha2
Jan 19 21:48:56.842: INFO: Checking APIGroup: metal3.io
Jan 19 21:48:56.892: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
Jan 19 21:48:56.892: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
Jan 19 21:48:56.892: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
Jan 19 21:48:56.892: INFO: Checking APIGroup: migration.k8s.io
Jan 19 21:48:56.942: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Jan 19 21:48:56.942: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Jan 19 21:48:56.942: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Jan 19 21:48:56.942: INFO: Checking APIGroup: monitoring.openshift.io
Jan 19 21:48:56.992: INFO: PreferredVersion.GroupVersion: monitoring.openshift.io/v1alpha1
Jan 19 21:48:56.992: INFO: Versions found [{monitoring.openshift.io/v1alpha1 v1alpha1}]
Jan 19 21:48:56.992: INFO: monitoring.openshift.io/v1alpha1 matches monitoring.openshift.io/v1alpha1
Jan 19 21:48:56.992: INFO: Checking APIGroup: ocmagent.managed.openshift.io
Jan 19 21:48:57.042: INFO: PreferredVersion.GroupVersion: ocmagent.managed.openshift.io/v1alpha1
Jan 19 21:48:57.042: INFO: Versions found [{ocmagent.managed.openshift.io/v1alpha1 v1alpha1}]
Jan 19 21:48:57.042: INFO: ocmagent.managed.openshift.io/v1alpha1 matches ocmagent.managed.openshift.io/v1alpha1
Jan 19 21:48:57.042: INFO: Checking APIGroup: splunkforwarder.managed.openshift.io
Jan 19 21:48:57.092: INFO: PreferredVersion.GroupVersion: splunkforwarder.managed.openshift.io/v1alpha1
Jan 19 21:48:57.092: INFO: Versions found [{splunkforwarder.managed.openshift.io/v1alpha1 v1alpha1}]
Jan 19 21:48:57.092: INFO: splunkforwarder.managed.openshift.io/v1alpha1 matches splunkforwarder.managed.openshift.io/v1alpha1
Jan 19 21:48:57.092: INFO: Checking APIGroup: upgrade.managed.openshift.io
Jan 19 21:48:57.143: INFO: PreferredVersion.GroupVersion: upgrade.managed.openshift.io/v1alpha1
Jan 19 21:48:57.143: INFO: Versions found [{upgrade.managed.openshift.io/v1alpha1 v1alpha1}]
Jan 19 21:48:57.143: INFO: upgrade.managed.openshift.io/v1alpha1 matches upgrade.managed.openshift.io/v1alpha1
Jan 19 21:48:57.143: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Jan 19 21:48:57.193: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Jan 19 21:48:57.193: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Jan 19 21:48:57.193: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Jan 19 21:48:57.193: INFO: Checking APIGroup: helm.openshift.io
Jan 19 21:48:57.242: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Jan 19 21:48:57.242: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Jan 19 21:48:57.242: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Jan 19 21:48:57.242: INFO: Checking APIGroup: metrics.k8s.io
Jan 19 21:48:57.292: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan 19 21:48:57.292: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan 19 21:48:57.292: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jan 19 21:48:57.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-4055" for this suite. 01/19/23 21:48:57.346
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":292,"skipped":5460,"failed":0}
------------------------------
• [1.317 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:48:56.082
    Jan 19 21:48:56.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename discovery 01/19/23 21:48:56.083
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:56.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:56.102
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/19/23 21:48:56.105
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan 19 21:48:56.392: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan 19 21:48:56.393: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan 19 21:48:56.393: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan 19 21:48:56.393: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan 19 21:48:56.393: INFO: Checking APIGroup: apps
    Jan 19 21:48:56.393: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan 19 21:48:56.393: INFO: Versions found [{apps/v1 v1}]
    Jan 19 21:48:56.393: INFO: apps/v1 matches apps/v1
    Jan 19 21:48:56.393: INFO: Checking APIGroup: events.k8s.io
    Jan 19 21:48:56.394: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan 19 21:48:56.394: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan 19 21:48:56.394: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan 19 21:48:56.394: INFO: Checking APIGroup: authentication.k8s.io
    Jan 19 21:48:56.395: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan 19 21:48:56.395: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan 19 21:48:56.395: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan 19 21:48:56.395: INFO: Checking APIGroup: authorization.k8s.io
    Jan 19 21:48:56.395: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan 19 21:48:56.395: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan 19 21:48:56.395: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan 19 21:48:56.395: INFO: Checking APIGroup: autoscaling
    Jan 19 21:48:56.396: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan 19 21:48:56.396: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jan 19 21:48:56.396: INFO: autoscaling/v2 matches autoscaling/v2
    Jan 19 21:48:56.396: INFO: Checking APIGroup: batch
    Jan 19 21:48:56.397: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan 19 21:48:56.397: INFO: Versions found [{batch/v1 v1}]
    Jan 19 21:48:56.397: INFO: batch/v1 matches batch/v1
    Jan 19 21:48:56.397: INFO: Checking APIGroup: certificates.k8s.io
    Jan 19 21:48:56.397: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan 19 21:48:56.397: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan 19 21:48:56.397: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan 19 21:48:56.397: INFO: Checking APIGroup: networking.k8s.io
    Jan 19 21:48:56.398: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan 19 21:48:56.398: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan 19 21:48:56.398: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan 19 21:48:56.398: INFO: Checking APIGroup: policy
    Jan 19 21:48:56.398: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan 19 21:48:56.398: INFO: Versions found [{policy/v1 v1}]
    Jan 19 21:48:56.398: INFO: policy/v1 matches policy/v1
    Jan 19 21:48:56.398: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan 19 21:48:56.399: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan 19 21:48:56.399: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan 19 21:48:56.399: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan 19 21:48:56.399: INFO: Checking APIGroup: storage.k8s.io
    Jan 19 21:48:56.400: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan 19 21:48:56.400: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan 19 21:48:56.400: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan 19 21:48:56.400: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan 19 21:48:56.400: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan 19 21:48:56.400: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan 19 21:48:56.400: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan 19 21:48:56.400: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan 19 21:48:56.401: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan 19 21:48:56.401: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan 19 21:48:56.401: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan 19 21:48:56.401: INFO: Checking APIGroup: scheduling.k8s.io
    Jan 19 21:48:56.402: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan 19 21:48:56.402: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan 19 21:48:56.402: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan 19 21:48:56.402: INFO: Checking APIGroup: coordination.k8s.io
    Jan 19 21:48:56.402: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan 19 21:48:56.402: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan 19 21:48:56.402: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan 19 21:48:56.402: INFO: Checking APIGroup: node.k8s.io
    Jan 19 21:48:56.403: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan 19 21:48:56.403: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan 19 21:48:56.403: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan 19 21:48:56.403: INFO: Checking APIGroup: discovery.k8s.io
    Jan 19 21:48:56.404: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan 19 21:48:56.404: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan 19 21:48:56.404: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan 19 21:48:56.404: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan 19 21:48:56.405: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jan 19 21:48:56.405: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jan 19 21:48:56.405: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jan 19 21:48:56.405: INFO: Checking APIGroup: apps.openshift.io
    Jan 19 21:48:56.405: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
    Jan 19 21:48:56.405: INFO: Versions found [{apps.openshift.io/v1 v1}]
    Jan 19 21:48:56.405: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
    Jan 19 21:48:56.405: INFO: Checking APIGroup: authorization.openshift.io
    Jan 19 21:48:56.406: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
    Jan 19 21:48:56.406: INFO: Versions found [{authorization.openshift.io/v1 v1}]
    Jan 19 21:48:56.406: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
    Jan 19 21:48:56.406: INFO: Checking APIGroup: build.openshift.io
    Jan 19 21:48:56.407: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
    Jan 19 21:48:56.407: INFO: Versions found [{build.openshift.io/v1 v1}]
    Jan 19 21:48:56.407: INFO: build.openshift.io/v1 matches build.openshift.io/v1
    Jan 19 21:48:56.407: INFO: Checking APIGroup: image.openshift.io
    Jan 19 21:48:56.407: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
    Jan 19 21:48:56.407: INFO: Versions found [{image.openshift.io/v1 v1}]
    Jan 19 21:48:56.407: INFO: image.openshift.io/v1 matches image.openshift.io/v1
    Jan 19 21:48:56.407: INFO: Checking APIGroup: oauth.openshift.io
    Jan 19 21:48:56.408: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
    Jan 19 21:48:56.408: INFO: Versions found [{oauth.openshift.io/v1 v1}]
    Jan 19 21:48:56.408: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
    Jan 19 21:48:56.408: INFO: Checking APIGroup: project.openshift.io
    Jan 19 21:48:56.408: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
    Jan 19 21:48:56.408: INFO: Versions found [{project.openshift.io/v1 v1}]
    Jan 19 21:48:56.408: INFO: project.openshift.io/v1 matches project.openshift.io/v1
    Jan 19 21:48:56.408: INFO: Checking APIGroup: quota.openshift.io
    Jan 19 21:48:56.409: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
    Jan 19 21:48:56.409: INFO: Versions found [{quota.openshift.io/v1 v1}]
    Jan 19 21:48:56.409: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
    Jan 19 21:48:56.409: INFO: Checking APIGroup: route.openshift.io
    Jan 19 21:48:56.410: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
    Jan 19 21:48:56.410: INFO: Versions found [{route.openshift.io/v1 v1}]
    Jan 19 21:48:56.410: INFO: route.openshift.io/v1 matches route.openshift.io/v1
    Jan 19 21:48:56.410: INFO: Checking APIGroup: security.openshift.io
    Jan 19 21:48:56.410: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
    Jan 19 21:48:56.410: INFO: Versions found [{security.openshift.io/v1 v1}]
    Jan 19 21:48:56.410: INFO: security.openshift.io/v1 matches security.openshift.io/v1
    Jan 19 21:48:56.410: INFO: Checking APIGroup: template.openshift.io
    Jan 19 21:48:56.411: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
    Jan 19 21:48:56.411: INFO: Versions found [{template.openshift.io/v1 v1}]
    Jan 19 21:48:56.411: INFO: template.openshift.io/v1 matches template.openshift.io/v1
    Jan 19 21:48:56.411: INFO: Checking APIGroup: user.openshift.io
    Jan 19 21:48:56.412: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
    Jan 19 21:48:56.412: INFO: Versions found [{user.openshift.io/v1 v1}]
    Jan 19 21:48:56.412: INFO: user.openshift.io/v1 matches user.openshift.io/v1
    Jan 19 21:48:56.412: INFO: Checking APIGroup: packages.operators.coreos.com
    Jan 19 21:48:56.412: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
    Jan 19 21:48:56.412: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
    Jan 19 21:48:56.412: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
    Jan 19 21:48:56.412: INFO: Checking APIGroup: config.openshift.io
    Jan 19 21:48:56.413: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
    Jan 19 21:48:56.413: INFO: Versions found [{config.openshift.io/v1 v1}]
    Jan 19 21:48:56.413: INFO: config.openshift.io/v1 matches config.openshift.io/v1
    Jan 19 21:48:56.413: INFO: Checking APIGroup: operator.openshift.io
    Jan 19 21:48:56.414: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
    Jan 19 21:48:56.414: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.414: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
    Jan 19 21:48:56.414: INFO: Checking APIGroup: apiserver.openshift.io
    Jan 19 21:48:56.414: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
    Jan 19 21:48:56.414: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
    Jan 19 21:48:56.414: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
    Jan 19 21:48:56.414: INFO: Checking APIGroup: autoscaling.openshift.io
    Jan 19 21:48:56.415: INFO: PreferredVersion.GroupVersion: autoscaling.openshift.io/v1
    Jan 19 21:48:56.415: INFO: Versions found [{autoscaling.openshift.io/v1 v1} {autoscaling.openshift.io/v1beta1 v1beta1}]
    Jan 19 21:48:56.415: INFO: autoscaling.openshift.io/v1 matches autoscaling.openshift.io/v1
    Jan 19 21:48:56.415: INFO: Checking APIGroup: cloud.network.openshift.io
    Jan 19 21:48:56.416: INFO: PreferredVersion.GroupVersion: cloud.network.openshift.io/v1
    Jan 19 21:48:56.416: INFO: Versions found [{cloud.network.openshift.io/v1 v1}]
    Jan 19 21:48:56.416: INFO: cloud.network.openshift.io/v1 matches cloud.network.openshift.io/v1
    Jan 19 21:48:56.416: INFO: Checking APIGroup: cloudcredential.openshift.io
    Jan 19 21:48:56.416: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
    Jan 19 21:48:56.416: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
    Jan 19 21:48:56.416: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
    Jan 19 21:48:56.416: INFO: Checking APIGroup: console.openshift.io
    Jan 19 21:48:56.417: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
    Jan 19 21:48:56.417: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.417: INFO: console.openshift.io/v1 matches console.openshift.io/v1
    Jan 19 21:48:56.417: INFO: Checking APIGroup: imageregistry.operator.openshift.io
    Jan 19 21:48:56.418: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
    Jan 19 21:48:56.418: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
    Jan 19 21:48:56.418: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
    Jan 19 21:48:56.418: INFO: Checking APIGroup: ingress.operator.openshift.io
    Jan 19 21:48:56.418: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
    Jan 19 21:48:56.418: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
    Jan 19 21:48:56.418: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
    Jan 19 21:48:56.418: INFO: Checking APIGroup: k8s.cni.cncf.io
    Jan 19 21:48:56.419: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    Jan 19 21:48:56.419: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    Jan 19 21:48:56.419: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    Jan 19 21:48:56.419: INFO: Checking APIGroup: k8s.ovn.org
    Jan 19 21:48:56.420: INFO: PreferredVersion.GroupVersion: k8s.ovn.org/v1
    Jan 19 21:48:56.420: INFO: Versions found [{k8s.ovn.org/v1 v1}]
    Jan 19 21:48:56.420: INFO: k8s.ovn.org/v1 matches k8s.ovn.org/v1
    Jan 19 21:48:56.420: INFO: Checking APIGroup: machine.openshift.io
    Jan 19 21:48:56.420: INFO: PreferredVersion.GroupVersion: machine.openshift.io/v1
    Jan 19 21:48:56.420: INFO: Versions found [{machine.openshift.io/v1 v1} {machine.openshift.io/v1beta1 v1beta1}]
    Jan 19 21:48:56.420: INFO: machine.openshift.io/v1 matches machine.openshift.io/v1
    Jan 19 21:48:56.420: INFO: Checking APIGroup: machineconfiguration.openshift.io
    Jan 19 21:48:56.421: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
    Jan 19 21:48:56.421: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
    Jan 19 21:48:56.421: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
    Jan 19 21:48:56.421: INFO: Checking APIGroup: monitoring.coreos.com
    Jan 19 21:48:56.422: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Jan 19 21:48:56.422: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.422: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Jan 19 21:48:56.422: INFO: Checking APIGroup: monitoring.rhobs
    Jan 19 21:48:56.422: INFO: PreferredVersion.GroupVersion: monitoring.rhobs/v1
    Jan 19 21:48:56.422: INFO: Versions found [{monitoring.rhobs/v1 v1} {monitoring.rhobs/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.422: INFO: monitoring.rhobs/v1 matches monitoring.rhobs/v1
    Jan 19 21:48:56.422: INFO: Checking APIGroup: network.operator.openshift.io
    Jan 19 21:48:56.423: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
    Jan 19 21:48:56.423: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
    Jan 19 21:48:56.423: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
    Jan 19 21:48:56.423: INFO: Checking APIGroup: operators.coreos.com
    Jan 19 21:48:56.424: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
    Jan 19 21:48:56.424: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.424: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
    Jan 19 21:48:56.424: INFO: Checking APIGroup: performance.openshift.io
    Jan 19 21:48:56.425: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
    Jan 19 21:48:56.425: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.425: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
    Jan 19 21:48:56.425: INFO: Checking APIGroup: samples.operator.openshift.io
    Jan 19 21:48:56.442: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
    Jan 19 21:48:56.442: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
    Jan 19 21:48:56.442: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
    Jan 19 21:48:56.442: INFO: Checking APIGroup: security.internal.openshift.io
    Jan 19 21:48:56.492: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
    Jan 19 21:48:56.492: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
    Jan 19 21:48:56.492: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
    Jan 19 21:48:56.492: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jan 19 21:48:56.542: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jan 19 21:48:56.542: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Jan 19 21:48:56.542: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jan 19 21:48:56.542: INFO: Checking APIGroup: tuned.openshift.io
    Jan 19 21:48:56.592: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
    Jan 19 21:48:56.592: INFO: Versions found [{tuned.openshift.io/v1 v1}]
    Jan 19 21:48:56.592: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
    Jan 19 21:48:56.592: INFO: Checking APIGroup: velero.io
    Jan 19 21:48:56.642: INFO: PreferredVersion.GroupVersion: velero.io/v1
    Jan 19 21:48:56.643: INFO: Versions found [{velero.io/v1 v1}]
    Jan 19 21:48:56.643: INFO: velero.io/v1 matches velero.io/v1
    Jan 19 21:48:56.643: INFO: Checking APIGroup: addons.managed.openshift.io
    Jan 19 21:48:56.693: INFO: PreferredVersion.GroupVersion: addons.managed.openshift.io/v1alpha1
    Jan 19 21:48:56.693: INFO: Versions found [{addons.managed.openshift.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.693: INFO: addons.managed.openshift.io/v1alpha1 matches addons.managed.openshift.io/v1alpha1
    Jan 19 21:48:56.693: INFO: Checking APIGroup: cloudingress.managed.openshift.io
    Jan 19 21:48:56.742: INFO: PreferredVersion.GroupVersion: cloudingress.managed.openshift.io/v1alpha1
    Jan 19 21:48:56.743: INFO: Versions found [{cloudingress.managed.openshift.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.743: INFO: cloudingress.managed.openshift.io/v1alpha1 matches cloudingress.managed.openshift.io/v1alpha1
    Jan 19 21:48:56.743: INFO: Checking APIGroup: controlplane.operator.openshift.io
    Jan 19 21:48:56.793: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
    Jan 19 21:48:56.793: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.793: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
    Jan 19 21:48:56.793: INFO: Checking APIGroup: managed.openshift.io
    Jan 19 21:48:56.842: INFO: PreferredVersion.GroupVersion: managed.openshift.io/v1alpha2
    Jan 19 21:48:56.842: INFO: Versions found [{managed.openshift.io/v1alpha2 v1alpha2} {managed.openshift.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.842: INFO: managed.openshift.io/v1alpha2 matches managed.openshift.io/v1alpha2
    Jan 19 21:48:56.842: INFO: Checking APIGroup: metal3.io
    Jan 19 21:48:56.892: INFO: PreferredVersion.GroupVersion: metal3.io/v1alpha1
    Jan 19 21:48:56.892: INFO: Versions found [{metal3.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.892: INFO: metal3.io/v1alpha1 matches metal3.io/v1alpha1
    Jan 19 21:48:56.892: INFO: Checking APIGroup: migration.k8s.io
    Jan 19 21:48:56.942: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
    Jan 19 21:48:56.942: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.942: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
    Jan 19 21:48:56.942: INFO: Checking APIGroup: monitoring.openshift.io
    Jan 19 21:48:56.992: INFO: PreferredVersion.GroupVersion: monitoring.openshift.io/v1alpha1
    Jan 19 21:48:56.992: INFO: Versions found [{monitoring.openshift.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:56.992: INFO: monitoring.openshift.io/v1alpha1 matches monitoring.openshift.io/v1alpha1
    Jan 19 21:48:56.992: INFO: Checking APIGroup: ocmagent.managed.openshift.io
    Jan 19 21:48:57.042: INFO: PreferredVersion.GroupVersion: ocmagent.managed.openshift.io/v1alpha1
    Jan 19 21:48:57.042: INFO: Versions found [{ocmagent.managed.openshift.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:57.042: INFO: ocmagent.managed.openshift.io/v1alpha1 matches ocmagent.managed.openshift.io/v1alpha1
    Jan 19 21:48:57.042: INFO: Checking APIGroup: splunkforwarder.managed.openshift.io
    Jan 19 21:48:57.092: INFO: PreferredVersion.GroupVersion: splunkforwarder.managed.openshift.io/v1alpha1
    Jan 19 21:48:57.092: INFO: Versions found [{splunkforwarder.managed.openshift.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:57.092: INFO: splunkforwarder.managed.openshift.io/v1alpha1 matches splunkforwarder.managed.openshift.io/v1alpha1
    Jan 19 21:48:57.092: INFO: Checking APIGroup: upgrade.managed.openshift.io
    Jan 19 21:48:57.143: INFO: PreferredVersion.GroupVersion: upgrade.managed.openshift.io/v1alpha1
    Jan 19 21:48:57.143: INFO: Versions found [{upgrade.managed.openshift.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:57.143: INFO: upgrade.managed.openshift.io/v1alpha1 matches upgrade.managed.openshift.io/v1alpha1
    Jan 19 21:48:57.143: INFO: Checking APIGroup: whereabouts.cni.cncf.io
    Jan 19 21:48:57.193: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
    Jan 19 21:48:57.193: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
    Jan 19 21:48:57.193: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
    Jan 19 21:48:57.193: INFO: Checking APIGroup: helm.openshift.io
    Jan 19 21:48:57.242: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
    Jan 19 21:48:57.242: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
    Jan 19 21:48:57.242: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
    Jan 19 21:48:57.242: INFO: Checking APIGroup: metrics.k8s.io
    Jan 19 21:48:57.292: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan 19 21:48:57.292: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan 19 21:48:57.292: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jan 19 21:48:57.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-4055" for this suite. 01/19/23 21:48:57.346
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:48:57.4
Jan 19 21:48:57.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:48:57.4
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:57.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:57.415
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-95a6c83a-f61c-4638-a445-6320508bd9f0 01/19/23 21:48:57.421
STEP: Creating a pod to test consume configMaps 01/19/23 21:48:57.431
Jan 19 21:48:57.459: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222" in namespace "projected-2429" to be "Succeeded or Failed"
Jan 19 21:48:57.464: INFO: Pod "pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222": Phase="Pending", Reason="", readiness=false. Elapsed: 4.22385ms
Jan 19 21:48:59.473: INFO: Pod "pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013563931s
Jan 19 21:49:01.468: INFO: Pod "pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008785605s
STEP: Saw pod success 01/19/23 21:49:01.468
Jan 19 21:49:01.468: INFO: Pod "pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222" satisfied condition "Succeeded or Failed"
Jan 19 21:49:01.472: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222 container agnhost-container: <nil>
STEP: delete the pod 01/19/23 21:49:01.479
Jan 19 21:49:01.489: INFO: Waiting for pod pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222 to disappear
Jan 19 21:49:01.491: INFO: Pod pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 21:49:01.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2429" for this suite. 01/19/23 21:49:01.496
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":293,"skipped":5468,"failed":0}
------------------------------
• [4.101 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:48:57.4
    Jan 19 21:48:57.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:48:57.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:48:57.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:48:57.415
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-95a6c83a-f61c-4638-a445-6320508bd9f0 01/19/23 21:48:57.421
    STEP: Creating a pod to test consume configMaps 01/19/23 21:48:57.431
    Jan 19 21:48:57.459: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222" in namespace "projected-2429" to be "Succeeded or Failed"
    Jan 19 21:48:57.464: INFO: Pod "pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222": Phase="Pending", Reason="", readiness=false. Elapsed: 4.22385ms
    Jan 19 21:48:59.473: INFO: Pod "pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013563931s
    Jan 19 21:49:01.468: INFO: Pod "pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008785605s
    STEP: Saw pod success 01/19/23 21:49:01.468
    Jan 19 21:49:01.468: INFO: Pod "pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222" satisfied condition "Succeeded or Failed"
    Jan 19 21:49:01.472: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222 container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 21:49:01.479
    Jan 19 21:49:01.489: INFO: Waiting for pod pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222 to disappear
    Jan 19 21:49:01.491: INFO: Pod pod-projected-configmaps-94f9a3ce-b1d7-4a30-aa71-aecbf7cf0222 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 21:49:01.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2429" for this suite. 01/19/23 21:49:01.496
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:01.502
Jan 19 21:49:01.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:49:01.503
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:01.517
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:01.519
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-125843c1-f078-4954-b325-c40c4d06e860 01/19/23 21:49:01.523
STEP: Creating a pod to test consume secrets 01/19/23 21:49:01.528
W0119 21:49:01.564005      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:49:01.564: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94" in namespace "projected-9005" to be "Succeeded or Failed"
Jan 19 21:49:01.568: INFO: Pod "pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94": Phase="Pending", Reason="", readiness=false. Elapsed: 4.391107ms
Jan 19 21:49:03.571: INFO: Pod "pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007519074s
Jan 19 21:49:05.572: INFO: Pod "pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008292869s
STEP: Saw pod success 01/19/23 21:49:05.572
Jan 19 21:49:05.572: INFO: Pod "pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94" satisfied condition "Succeeded or Failed"
Jan 19 21:49:05.574: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/19/23 21:49:05.579
Jan 19 21:49:05.595: INFO: Waiting for pod pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94 to disappear
Jan 19 21:49:05.609: INFO: Pod pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jan 19 21:49:05.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9005" for this suite. 01/19/23 21:49:05.613
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":294,"skipped":5500,"failed":0}
------------------------------
• [4.117 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:01.502
    Jan 19 21:49:01.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:49:01.503
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:01.517
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:01.519
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-125843c1-f078-4954-b325-c40c4d06e860 01/19/23 21:49:01.523
    STEP: Creating a pod to test consume secrets 01/19/23 21:49:01.528
    W0119 21:49:01.564005      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "projected-secret-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "projected-secret-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "projected-secret-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "projected-secret-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:49:01.564: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94" in namespace "projected-9005" to be "Succeeded or Failed"
    Jan 19 21:49:01.568: INFO: Pod "pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94": Phase="Pending", Reason="", readiness=false. Elapsed: 4.391107ms
    Jan 19 21:49:03.571: INFO: Pod "pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007519074s
    Jan 19 21:49:05.572: INFO: Pod "pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008292869s
    STEP: Saw pod success 01/19/23 21:49:05.572
    Jan 19 21:49:05.572: INFO: Pod "pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94" satisfied condition "Succeeded or Failed"
    Jan 19 21:49:05.574: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:49:05.579
    Jan 19 21:49:05.595: INFO: Waiting for pod pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94 to disappear
    Jan 19 21:49:05.609: INFO: Pod pod-projected-secrets-304fde60-0f9a-4fa2-a8e3-009dec0a9e94 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jan 19 21:49:05.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9005" for this suite. 01/19/23 21:49:05.613
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:05.619
Jan 19 21:49:05.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename crd-webhook 01/19/23 21:49:05.62
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:05.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:05.646
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/19/23 21:49:05.648
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/19/23 21:49:05.9
STEP: Deploying the custom resource conversion webhook pod 01/19/23 21:49:05.906
STEP: Wait for the deployment to be ready 01/19/23 21:49:05.918
Jan 19 21:49:05.925: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/19/23 21:49:07.935
STEP: Verifying the service has paired with the endpoint 01/19/23 21:49:07.945
Jan 19 21:49:08.945: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan 19 21:49:08.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Creating a v1 custom resource 01/19/23 21:49:11.52
STEP: v2 custom resource should be converted 01/19/23 21:49:11.524
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:49:12.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-4485" for this suite. 01/19/23 21:49:12.043
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":295,"skipped":5503,"failed":0}
------------------------------
• [SLOW TEST] [6.489 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:05.619
    Jan 19 21:49:05.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename crd-webhook 01/19/23 21:49:05.62
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:05.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:05.646
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/19/23 21:49:05.648
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/19/23 21:49:05.9
    STEP: Deploying the custom resource conversion webhook pod 01/19/23 21:49:05.906
    STEP: Wait for the deployment to be ready 01/19/23 21:49:05.918
    Jan 19 21:49:05.925: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/19/23 21:49:07.935
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:49:07.945
    Jan 19 21:49:08.945: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan 19 21:49:08.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Creating a v1 custom resource 01/19/23 21:49:11.52
    STEP: v2 custom resource should be converted 01/19/23 21:49:11.524
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:49:12.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-4485" for this suite. 01/19/23 21:49:12.043
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:12.11
Jan 19 21:49:12.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 21:49:12.11
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:12.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:12.171
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 01/19/23 21:49:12.174
Jan 19 21:49:12.200: INFO: Waiting up to 5m0s for pod "labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a" in namespace "downward-api-4110" to be "running and ready"
Jan 19 21:49:12.203: INFO: Pod "labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.510929ms
Jan 19 21:49:12.203: INFO: The phase of Pod labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:49:14.206: INFO: Pod "labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a": Phase="Running", Reason="", readiness=true. Elapsed: 2.005965662s
Jan 19 21:49:14.206: INFO: The phase of Pod labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a is Running (Ready = true)
Jan 19 21:49:14.206: INFO: Pod "labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a" satisfied condition "running and ready"
Jan 19 21:49:14.730: INFO: Successfully updated pod "labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 21:49:18.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4110" for this suite. 01/19/23 21:49:18.753
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":296,"skipped":5549,"failed":0}
------------------------------
• [SLOW TEST] [6.649 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:12.11
    Jan 19 21:49:12.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 21:49:12.11
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:12.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:12.171
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 01/19/23 21:49:12.174
    Jan 19 21:49:12.200: INFO: Waiting up to 5m0s for pod "labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a" in namespace "downward-api-4110" to be "running and ready"
    Jan 19 21:49:12.203: INFO: Pod "labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.510929ms
    Jan 19 21:49:12.203: INFO: The phase of Pod labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:49:14.206: INFO: Pod "labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a": Phase="Running", Reason="", readiness=true. Elapsed: 2.005965662s
    Jan 19 21:49:14.206: INFO: The phase of Pod labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a is Running (Ready = true)
    Jan 19 21:49:14.206: INFO: Pod "labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a" satisfied condition "running and ready"
    Jan 19 21:49:14.730: INFO: Successfully updated pod "labelsupdatedc0ceea7-eed0-419d-8cd2-481f79845c4a"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 21:49:18.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4110" for this suite. 01/19/23 21:49:18.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:18.759
Jan 19 21:49:18.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:49:18.76
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:18.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:18.776
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-5462f640-69bd-4475-92b6-03768c04f24d 01/19/23 21:49:18.78
STEP: Creating a pod to test consume configMaps 01/19/23 21:49:18.786
W0119 21:49:18.822660      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:49:18.822: INFO: Waiting up to 5m0s for pod "pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc" in namespace "configmap-5374" to be "Succeeded or Failed"
Jan 19 21:49:18.825: INFO: Pod "pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.520218ms
Jan 19 21:49:20.828: INFO: Pod "pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005861353s
Jan 19 21:49:22.827: INFO: Pod "pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005175028s
STEP: Saw pod success 01/19/23 21:49:22.827
Jan 19 21:49:22.828: INFO: Pod "pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc" satisfied condition "Succeeded or Failed"
Jan 19 21:49:22.832: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc container configmap-volume-test: <nil>
STEP: delete the pod 01/19/23 21:49:22.838
Jan 19 21:49:22.848: INFO: Waiting for pod pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc to disappear
Jan 19 21:49:22.850: INFO: Pod pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:49:22.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5374" for this suite. 01/19/23 21:49:22.855
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":297,"skipped":5557,"failed":0}
------------------------------
• [4.101 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:18.759
    Jan 19 21:49:18.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:49:18.76
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:18.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:18.776
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-5462f640-69bd-4475-92b6-03768c04f24d 01/19/23 21:49:18.78
    STEP: Creating a pod to test consume configMaps 01/19/23 21:49:18.786
    W0119 21:49:18.822660      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "configmap-volume-test" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "configmap-volume-test" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "configmap-volume-test" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "configmap-volume-test" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:49:18.822: INFO: Waiting up to 5m0s for pod "pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc" in namespace "configmap-5374" to be "Succeeded or Failed"
    Jan 19 21:49:18.825: INFO: Pod "pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.520218ms
    Jan 19 21:49:20.828: INFO: Pod "pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005861353s
    Jan 19 21:49:22.827: INFO: Pod "pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005175028s
    STEP: Saw pod success 01/19/23 21:49:22.827
    Jan 19 21:49:22.828: INFO: Pod "pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc" satisfied condition "Succeeded or Failed"
    Jan 19 21:49:22.832: INFO: Trying to get logs from node ip-10-0-171-213.ec2.internal pod pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc container configmap-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:49:22.838
    Jan 19 21:49:22.848: INFO: Waiting for pod pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc to disappear
    Jan 19 21:49:22.850: INFO: Pod pod-configmaps-94c2c8d1-ad31-44c5-bcf4-0ba85ac402bc no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:49:22.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5374" for this suite. 01/19/23 21:49:22.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:22.861
Jan 19 21:49:22.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename gc 01/19/23 21:49:22.862
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:22.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:22.877
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/19/23 21:49:22.887
W0119 21:49:22.896963      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: delete the rc 01/19/23 21:49:27.9
STEP: wait for the rc to be deleted 01/19/23 21:49:27.908
STEP: Gathering metrics 01/19/23 21:49:28.914
W0119 21:49:28.916534      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0119 21:49:28.916550      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 21:49:28.916: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 21:49:28.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2383" for this suite. 01/19/23 21:49:28.92
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":298,"skipped":5571,"failed":0}
------------------------------
• [SLOW TEST] [6.065 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:22.861
    Jan 19 21:49:22.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename gc 01/19/23 21:49:22.862
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:22.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:22.877
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/19/23 21:49:22.887
    W0119 21:49:22.896963      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: delete the rc 01/19/23 21:49:27.9
    STEP: wait for the rc to be deleted 01/19/23 21:49:27.908
    STEP: Gathering metrics 01/19/23 21:49:28.914
    W0119 21:49:28.916534      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0119 21:49:28.916550      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 19 21:49:28.916: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 21:49:28.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2383" for this suite. 01/19/23 21:49:28.92
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:28.926
Jan 19 21:49:28.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename gc 01/19/23 21:49:28.927
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:28.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:28.939
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/19/23 21:49:28.946
W0119 21:49:28.956045      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Wait for the Deployment to create new ReplicaSet 01/19/23 21:49:28.956
STEP: delete the deployment 01/19/23 21:49:28.967
STEP: wait for all rs to be garbage collected 01/19/23 21:49:28.979
STEP: expected 0 rs, got 1 rs 01/19/23 21:49:28.986
STEP: Gathering metrics 01/19/23 21:49:29.501
W0119 21:49:29.504156      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
W0119 21:49:29.504179      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan 19 21:49:29.504: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jan 19 21:49:29.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5329" for this suite. 01/19/23 21:49:29.509
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":299,"skipped":5571,"failed":0}
------------------------------
• [0.588 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:28.926
    Jan 19 21:49:28.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename gc 01/19/23 21:49:28.927
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:28.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:28.939
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/19/23 21:49:28.946
    W0119 21:49:28.956045      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "nginx" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "nginx" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "nginx" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "nginx" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Wait for the Deployment to create new ReplicaSet 01/19/23 21:49:28.956
    STEP: delete the deployment 01/19/23 21:49:28.967
    STEP: wait for all rs to be garbage collected 01/19/23 21:49:28.979
    STEP: expected 0 rs, got 1 rs 01/19/23 21:49:28.986
    STEP: Gathering metrics 01/19/23 21:49:29.501
    W0119 21:49:29.504156      22 metrics_grabber.go:110] Can't find any pods in namespace kube-system to grab metrics from
    W0119 21:49:29.504179      22 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan 19 21:49:29.504: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jan 19 21:49:29.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5329" for this suite. 01/19/23 21:49:29.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:29.516
Jan 19 21:49:29.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:49:29.517
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:29.529
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:29.531
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 01/19/23 21:49:29.554
Jan 19 21:49:29.594: INFO: Waiting up to 5m0s for pod "annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e" in namespace "projected-2235" to be "running and ready"
Jan 19 21:49:29.607: INFO: Pod "annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.908404ms
Jan 19 21:49:29.607: INFO: The phase of Pod annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:49:31.615: INFO: Pod "annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021459782s
Jan 19 21:49:31.615: INFO: The phase of Pod annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:49:33.610: INFO: Pod "annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e": Phase="Running", Reason="", readiness=true. Elapsed: 4.01561768s
Jan 19 21:49:33.610: INFO: The phase of Pod annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e is Running (Ready = true)
Jan 19 21:49:33.610: INFO: Pod "annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e" satisfied condition "running and ready"
Jan 19 21:49:34.133: INFO: Successfully updated pod "annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 21:49:38.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2235" for this suite. 01/19/23 21:49:38.157
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":300,"skipped":5605,"failed":0}
------------------------------
• [SLOW TEST] [8.645 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:29.516
    Jan 19 21:49:29.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:49:29.517
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:29.529
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:29.531
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 01/19/23 21:49:29.554
    Jan 19 21:49:29.594: INFO: Waiting up to 5m0s for pod "annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e" in namespace "projected-2235" to be "running and ready"
    Jan 19 21:49:29.607: INFO: Pod "annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.908404ms
    Jan 19 21:49:29.607: INFO: The phase of Pod annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:49:31.615: INFO: Pod "annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021459782s
    Jan 19 21:49:31.615: INFO: The phase of Pod annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:49:33.610: INFO: Pod "annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e": Phase="Running", Reason="", readiness=true. Elapsed: 4.01561768s
    Jan 19 21:49:33.610: INFO: The phase of Pod annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e is Running (Ready = true)
    Jan 19 21:49:33.610: INFO: Pod "annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e" satisfied condition "running and ready"
    Jan 19 21:49:34.133: INFO: Successfully updated pod "annotationupdate2da8fe88-27eb-4abb-ac5d-8fbd44cc423e"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 21:49:38.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2235" for this suite. 01/19/23 21:49:38.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:38.162
Jan 19 21:49:38.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename sysctl 01/19/23 21:49:38.163
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:38.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:38.182
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/19/23 21:49:38.185
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 21:49:38.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3941" for this suite. 01/19/23 21:49:38.215
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":301,"skipped":5612,"failed":0}
------------------------------
• [0.059 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:38.162
    Jan 19 21:49:38.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename sysctl 01/19/23 21:49:38.163
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:38.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:38.182
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/19/23 21:49:38.185
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 21:49:38.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-3941" for this suite. 01/19/23 21:49:38.215
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:38.222
Jan 19 21:49:38.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:49:38.223
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:38.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:38.246
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 01/19/23 21:49:38.249
STEP: fetching the ConfigMap 01/19/23 21:49:38.255
STEP: patching the ConfigMap 01/19/23 21:49:38.264
STEP: listing all ConfigMaps in all namespaces with a label selector 01/19/23 21:49:38.274
STEP: deleting the ConfigMap by collection with a label selector 01/19/23 21:49:38.364
STEP: listing all ConfigMaps in test namespace 01/19/23 21:49:38.393
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:49:38.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-263" for this suite. 01/19/23 21:49:38.401
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":302,"skipped":5629,"failed":0}
------------------------------
• [0.186 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:38.222
    Jan 19 21:49:38.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:49:38.223
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:38.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:38.246
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 01/19/23 21:49:38.249
    STEP: fetching the ConfigMap 01/19/23 21:49:38.255
    STEP: patching the ConfigMap 01/19/23 21:49:38.264
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/19/23 21:49:38.274
    STEP: deleting the ConfigMap by collection with a label selector 01/19/23 21:49:38.364
    STEP: listing all ConfigMaps in test namespace 01/19/23 21:49:38.393
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:49:38.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-263" for this suite. 01/19/23 21:49:38.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:38.409
Jan 19 21:49:38.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename events 01/19/23 21:49:38.409
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:38.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:38.433
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/19/23 21:49:38.437
STEP: get a list of Events with a label in the current namespace 01/19/23 21:49:38.482
STEP: delete a list of events 01/19/23 21:49:38.489
Jan 19 21:49:38.489: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/19/23 21:49:38.556
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jan 19 21:49:38.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-772" for this suite. 01/19/23 21:49:38.564
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":303,"skipped":5637,"failed":0}
------------------------------
• [0.160 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:38.409
    Jan 19 21:49:38.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename events 01/19/23 21:49:38.409
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:38.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:38.433
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/19/23 21:49:38.437
    STEP: get a list of Events with a label in the current namespace 01/19/23 21:49:38.482
    STEP: delete a list of events 01/19/23 21:49:38.489
    Jan 19 21:49:38.489: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/19/23 21:49:38.556
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jan 19 21:49:38.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-772" for this suite. 01/19/23 21:49:38.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:38.57
Jan 19 21:49:38.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename proxy 01/19/23 21:49:38.57
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:38.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:38.586
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/19/23 21:49:38.615
STEP: creating replication controller proxy-service-twm8j in namespace proxy-84 01/19/23 21:49:38.615
I0119 21:49:38.623669      22 runners.go:193] Created replication controller with name: proxy-service-twm8j, namespace: proxy-84, replica count: 1
I0119 21:49:39.674319      22 runners.go:193] proxy-service-twm8j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0119 21:49:40.674638      22 runners.go:193] proxy-service-twm8j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0119 21:49:41.675510      22 runners.go:193] proxy-service-twm8j Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 21:49:41.678: INFO: setup took 3.087266154s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/19/23 21:49:41.678
Jan 19 21:49:41.683: INFO: (0) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 5.555039ms)
Jan 19 21:49:41.684: INFO: (0) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 6.245895ms)
Jan 19 21:49:41.684: INFO: (0) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 6.154149ms)
Jan 19 21:49:41.685: INFO: (0) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 7.456516ms)
Jan 19 21:49:41.685: INFO: (0) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 7.76113ms)
Jan 19 21:49:41.686: INFO: (0) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 7.682488ms)
Jan 19 21:49:41.686: INFO: (0) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 7.808454ms)
Jan 19 21:49:41.686: INFO: (0) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 7.832296ms)
Jan 19 21:49:41.686: INFO: (0) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 7.834251ms)
Jan 19 21:49:41.686: INFO: (0) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 8.159009ms)
Jan 19 21:49:41.687: INFO: (0) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 8.770632ms)
Jan 19 21:49:41.687: INFO: (0) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 9.024698ms)
Jan 19 21:49:41.687: INFO: (0) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 8.937244ms)
Jan 19 21:49:41.687: INFO: (0) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 9.227467ms)
Jan 19 21:49:41.687: INFO: (0) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 9.249658ms)
Jan 19 21:49:41.688: INFO: (0) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 10.312837ms)
Jan 19 21:49:41.691: INFO: (1) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.651013ms)
Jan 19 21:49:41.691: INFO: (1) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.941134ms)
Jan 19 21:49:41.691: INFO: (1) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.148502ms)
Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.273815ms)
Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.49211ms)
Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.453252ms)
Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.689484ms)
Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.71609ms)
Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 4.715729ms)
Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.782335ms)
Jan 19 21:49:41.694: INFO: (1) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.330065ms)
Jan 19 21:49:41.694: INFO: (1) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.471582ms)
Jan 19 21:49:41.694: INFO: (1) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.511777ms)
Jan 19 21:49:41.694: INFO: (1) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.674481ms)
Jan 19 21:49:41.695: INFO: (1) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 6.116775ms)
Jan 19 21:49:41.695: INFO: (1) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 6.698126ms)
Jan 19 21:49:41.698: INFO: (2) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 2.617534ms)
Jan 19 21:49:41.698: INFO: (2) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.612214ms)
Jan 19 21:49:41.698: INFO: (2) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 2.970308ms)
Jan 19 21:49:41.698: INFO: (2) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.281469ms)
Jan 19 21:49:41.698: INFO: (2) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.217669ms)
Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.751521ms)
Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.953686ms)
Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.230549ms)
Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.059171ms)
Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.085789ms)
Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.197054ms)
Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.205098ms)
Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.483384ms)
Jan 19 21:49:41.700: INFO: (2) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.323267ms)
Jan 19 21:49:41.700: INFO: (2) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.311419ms)
Jan 19 21:49:41.701: INFO: (2) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 6.043135ms)
Jan 19 21:49:41.704: INFO: (3) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.540219ms)
Jan 19 21:49:41.704: INFO: (3) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 2.673062ms)
Jan 19 21:49:41.704: INFO: (3) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.790284ms)
Jan 19 21:49:41.704: INFO: (3) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.857087ms)
Jan 19 21:49:41.705: INFO: (3) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.098877ms)
Jan 19 21:49:41.705: INFO: (3) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.036104ms)
Jan 19 21:49:41.705: INFO: (3) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.123419ms)
Jan 19 21:49:41.705: INFO: (3) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.113717ms)
Jan 19 21:49:41.705: INFO: (3) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.020945ms)
Jan 19 21:49:41.706: INFO: (3) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.369991ms)
Jan 19 21:49:41.706: INFO: (3) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.458784ms)
Jan 19 21:49:41.706: INFO: (3) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.392388ms)
Jan 19 21:49:41.706: INFO: (3) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.73016ms)
Jan 19 21:49:41.706: INFO: (3) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.896424ms)
Jan 19 21:49:41.706: INFO: (3) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.932334ms)
Jan 19 21:49:41.707: INFO: (3) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.184031ms)
Jan 19 21:49:41.709: INFO: (4) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.478969ms)
Jan 19 21:49:41.710: INFO: (4) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.812808ms)
Jan 19 21:49:41.710: INFO: (4) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.135121ms)
Jan 19 21:49:41.710: INFO: (4) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.576366ms)
Jan 19 21:49:41.710: INFO: (4) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 3.59254ms)
Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.011755ms)
Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.041899ms)
Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.270789ms)
Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.287057ms)
Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.328038ms)
Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.446224ms)
Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.529143ms)
Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.491497ms)
Jan 19 21:49:41.712: INFO: (4) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 4.931912ms)
Jan 19 21:49:41.712: INFO: (4) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.33779ms)
Jan 19 21:49:41.713: INFO: (4) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.763218ms)
Jan 19 21:49:41.715: INFO: (5) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.501165ms)
Jan 19 21:49:41.716: INFO: (5) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 2.997364ms)
Jan 19 21:49:41.716: INFO: (5) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.521042ms)
Jan 19 21:49:41.716: INFO: (5) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.612328ms)
Jan 19 21:49:41.716: INFO: (5) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 3.648165ms)
Jan 19 21:49:41.716: INFO: (5) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.586338ms)
Jan 19 21:49:41.717: INFO: (5) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 3.916592ms)
Jan 19 21:49:41.717: INFO: (5) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.000652ms)
Jan 19 21:49:41.718: INFO: (5) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.147756ms)
Jan 19 21:49:41.718: INFO: (5) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 5.221319ms)
Jan 19 21:49:41.718: INFO: (5) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 5.259658ms)
Jan 19 21:49:41.718: INFO: (5) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 5.313335ms)
Jan 19 21:49:41.718: INFO: (5) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 5.283236ms)
Jan 19 21:49:41.718: INFO: (5) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.656655ms)
Jan 19 21:49:41.719: INFO: (5) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 6.473548ms)
Jan 19 21:49:41.719: INFO: (5) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 6.304265ms)
Jan 19 21:49:41.722: INFO: (6) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 2.651497ms)
Jan 19 21:49:41.722: INFO: (6) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 2.988231ms)
Jan 19 21:49:41.722: INFO: (6) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.063523ms)
Jan 19 21:49:41.723: INFO: (6) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.433117ms)
Jan 19 21:49:41.723: INFO: (6) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.951747ms)
Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.655338ms)
Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.404226ms)
Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.477711ms)
Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.837657ms)
Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 4.793826ms)
Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.645535ms)
Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 4.762283ms)
Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.912133ms)
Jan 19 21:49:41.725: INFO: (6) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.655633ms)
Jan 19 21:49:41.725: INFO: (6) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 5.661568ms)
Jan 19 21:49:41.726: INFO: (6) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 6.486953ms)
Jan 19 21:49:41.729: INFO: (7) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.130923ms)
Jan 19 21:49:41.729: INFO: (7) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.551648ms)
Jan 19 21:49:41.729: INFO: (7) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 3.562076ms)
Jan 19 21:49:41.730: INFO: (7) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.251092ms)
Jan 19 21:49:41.730: INFO: (7) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.488946ms)
Jan 19 21:49:41.730: INFO: (7) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.705988ms)
Jan 19 21:49:41.731: INFO: (7) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.304151ms)
Jan 19 21:49:41.732: INFO: (7) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.791682ms)
Jan 19 21:49:41.732: INFO: (7) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 6.008595ms)
Jan 19 21:49:41.732: INFO: (7) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 6.018246ms)
Jan 19 21:49:41.732: INFO: (7) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 6.10946ms)
Jan 19 21:49:41.732: INFO: (7) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 5.977748ms)
Jan 19 21:49:41.732: INFO: (7) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 6.017923ms)
Jan 19 21:49:41.733: INFO: (7) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 6.810515ms)
Jan 19 21:49:41.733: INFO: (7) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 7.107811ms)
Jan 19 21:49:41.733: INFO: (7) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 7.410971ms)
Jan 19 21:49:41.736: INFO: (8) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 2.500602ms)
Jan 19 21:49:41.736: INFO: (8) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 2.610784ms)
Jan 19 21:49:41.736: INFO: (8) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.807846ms)
Jan 19 21:49:41.736: INFO: (8) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.842764ms)
Jan 19 21:49:41.737: INFO: (8) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.367211ms)
Jan 19 21:49:41.737: INFO: (8) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.847617ms)
Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.240958ms)
Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.998951ms)
Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.235621ms)
Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.198285ms)
Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.317136ms)
Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.355538ms)
Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.716193ms)
Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.042038ms)
Jan 19 21:49:41.739: INFO: (8) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.781241ms)
Jan 19 21:49:41.739: INFO: (8) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.797646ms)
Jan 19 21:49:41.742: INFO: (9) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 2.908385ms)
Jan 19 21:49:41.742: INFO: (9) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.83207ms)
Jan 19 21:49:41.742: INFO: (9) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.932789ms)
Jan 19 21:49:41.743: INFO: (9) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.52075ms)
Jan 19 21:49:41.743: INFO: (9) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 3.514905ms)
Jan 19 21:49:41.743: INFO: (9) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.616937ms)
Jan 19 21:49:41.743: INFO: (9) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 3.68901ms)
Jan 19 21:49:41.743: INFO: (9) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.871451ms)
Jan 19 21:49:41.743: INFO: (9) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.734643ms)
Jan 19 21:49:41.744: INFO: (9) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.376228ms)
Jan 19 21:49:41.744: INFO: (9) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.296233ms)
Jan 19 21:49:41.744: INFO: (9) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.291227ms)
Jan 19 21:49:41.744: INFO: (9) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.624825ms)
Jan 19 21:49:41.745: INFO: (9) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.150017ms)
Jan 19 21:49:41.745: INFO: (9) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 5.782112ms)
Jan 19 21:49:41.745: INFO: (9) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.680828ms)
Jan 19 21:49:41.748: INFO: (10) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.745472ms)
Jan 19 21:49:41.748: INFO: (10) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 2.903085ms)
Jan 19 21:49:41.749: INFO: (10) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.430107ms)
Jan 19 21:49:41.750: INFO: (10) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.10003ms)
Jan 19 21:49:41.750: INFO: (10) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.196286ms)
Jan 19 21:49:41.750: INFO: (10) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.241152ms)
Jan 19 21:49:41.750: INFO: (10) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.323912ms)
Jan 19 21:49:41.750: INFO: (10) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 4.194609ms)
Jan 19 21:49:41.750: INFO: (10) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.995593ms)
Jan 19 21:49:41.751: INFO: (10) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.371897ms)
Jan 19 21:49:41.751: INFO: (10) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.908806ms)
Jan 19 21:49:41.758: INFO: (10) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 12.960836ms)
Jan 19 21:49:41.759: INFO: (10) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 13.702941ms)
Jan 19 21:49:41.759: INFO: (10) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 13.920396ms)
Jan 19 21:49:41.760: INFO: (10) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 15.123371ms)
Jan 19 21:49:41.760: INFO: (10) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 15.018686ms)
Jan 19 21:49:41.763: INFO: (11) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.757831ms)
Jan 19 21:49:41.764: INFO: (11) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.45461ms)
Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.940412ms)
Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.349799ms)
Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.207121ms)
Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.105711ms)
Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.238836ms)
Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.367872ms)
Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.421806ms)
Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.464053ms)
Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 4.704258ms)
Jan 19 21:49:41.766: INFO: (11) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.932751ms)
Jan 19 21:49:41.766: INFO: (11) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.572739ms)
Jan 19 21:49:41.766: INFO: (11) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.634306ms)
Jan 19 21:49:41.766: INFO: (11) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.621586ms)
Jan 19 21:49:41.766: INFO: (11) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 5.819771ms)
Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 3.205568ms)
Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.390355ms)
Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 3.319586ms)
Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.289476ms)
Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 3.418329ms)
Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.825421ms)
Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.90208ms)
Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.184407ms)
Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.3975ms)
Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.265923ms)
Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.46149ms)
Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.512063ms)
Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 4.957107ms)
Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.982404ms)
Jan 19 21:49:41.772: INFO: (12) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.171053ms)
Jan 19 21:49:41.772: INFO: (12) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.087453ms)
Jan 19 21:49:41.774: INFO: (13) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 2.495614ms)
Jan 19 21:49:41.774: INFO: (13) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.643072ms)
Jan 19 21:49:41.774: INFO: (13) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.676054ms)
Jan 19 21:49:41.775: INFO: (13) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.232808ms)
Jan 19 21:49:41.775: INFO: (13) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 3.374465ms)
Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.075674ms)
Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.955647ms)
Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.03216ms)
Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 4.160268ms)
Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.258307ms)
Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.263118ms)
Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.227785ms)
Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.168044ms)
Jan 19 21:49:41.777: INFO: (13) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.791801ms)
Jan 19 21:49:41.777: INFO: (13) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.082703ms)
Jan 19 21:49:41.777: INFO: (13) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.552918ms)
Jan 19 21:49:41.780: INFO: (14) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 2.521243ms)
Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 3.005217ms)
Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 3.470832ms)
Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.611198ms)
Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.738224ms)
Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.77308ms)
Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.711254ms)
Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.058135ms)
Jan 19 21:49:41.782: INFO: (14) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.163133ms)
Jan 19 21:49:41.782: INFO: (14) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.278565ms)
Jan 19 21:49:41.782: INFO: (14) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.160501ms)
Jan 19 21:49:41.782: INFO: (14) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 4.225121ms)
Jan 19 21:49:41.782: INFO: (14) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.677043ms)
Jan 19 21:49:41.783: INFO: (14) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.140203ms)
Jan 19 21:49:41.783: INFO: (14) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.333155ms)
Jan 19 21:49:41.784: INFO: (14) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 5.997625ms)
Jan 19 21:49:41.786: INFO: (15) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 2.673635ms)
Jan 19 21:49:41.786: INFO: (15) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.646441ms)
Jan 19 21:49:41.787: INFO: (15) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.298879ms)
Jan 19 21:49:41.787: INFO: (15) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 3.29902ms)
Jan 19 21:49:41.787: INFO: (15) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 3.571013ms)
Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 3.976449ms)
Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.749363ms)
Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.913203ms)
Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.047054ms)
Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.019826ms)
Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 4.189503ms)
Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.346281ms)
Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.348456ms)
Jan 19 21:49:41.789: INFO: (15) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 5.574994ms)
Jan 19 21:49:41.789: INFO: (15) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.274994ms)
Jan 19 21:49:41.789: INFO: (15) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.27267ms)
Jan 19 21:49:41.792: INFO: (16) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 2.712465ms)
Jan 19 21:49:41.793: INFO: (16) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.532788ms)
Jan 19 21:49:41.793: INFO: (16) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.47313ms)
Jan 19 21:49:41.793: INFO: (16) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.66397ms)
Jan 19 21:49:41.793: INFO: (16) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.541326ms)
Jan 19 21:49:41.793: INFO: (16) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 3.998423ms)
Jan 19 21:49:41.794: INFO: (16) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.207653ms)
Jan 19 21:49:41.795: INFO: (16) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 5.669166ms)
Jan 19 21:49:41.795: INFO: (16) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.911377ms)
Jan 19 21:49:41.795: INFO: (16) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 6.048168ms)
Jan 19 21:49:41.795: INFO: (16) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 6.067099ms)
Jan 19 21:49:41.795: INFO: (16) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 6.151932ms)
Jan 19 21:49:41.796: INFO: (16) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 6.204016ms)
Jan 19 21:49:41.796: INFO: (16) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 6.960307ms)
Jan 19 21:49:41.797: INFO: (16) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 7.209713ms)
Jan 19 21:49:41.797: INFO: (16) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 7.628139ms)
Jan 19 21:49:41.800: INFO: (17) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.476172ms)
Jan 19 21:49:41.800: INFO: (17) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 2.849214ms)
Jan 19 21:49:41.800: INFO: (17) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.863079ms)
Jan 19 21:49:41.801: INFO: (17) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.311331ms)
Jan 19 21:49:41.801: INFO: (17) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 3.563428ms)
Jan 19 21:49:41.801: INFO: (17) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.99721ms)
Jan 19 21:49:41.801: INFO: (17) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.065648ms)
Jan 19 21:49:41.801: INFO: (17) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.169193ms)
Jan 19 21:49:41.801: INFO: (17) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.317227ms)
Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 4.33178ms)
Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.506684ms)
Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.484842ms)
Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.778105ms)
Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.994614ms)
Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.008346ms)
Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.334747ms)
Jan 19 21:49:41.805: INFO: (18) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.363046ms)
Jan 19 21:49:41.805: INFO: (18) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.831056ms)
Jan 19 21:49:41.805: INFO: (18) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 2.697814ms)
Jan 19 21:49:41.806: INFO: (18) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.992652ms)
Jan 19 21:49:41.806: INFO: (18) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.581402ms)
Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.970991ms)
Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.611784ms)
Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.610206ms)
Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.595281ms)
Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.708963ms)
Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.839714ms)
Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.775642ms)
Jan 19 21:49:41.808: INFO: (18) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.860758ms)
Jan 19 21:49:41.808: INFO: (18) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.298014ms)
Jan 19 21:49:41.808: INFO: (18) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.521933ms)
Jan 19 21:49:41.808: INFO: (18) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.575967ms)
Jan 19 21:49:41.811: INFO: (19) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.483964ms)
Jan 19 21:49:41.811: INFO: (19) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 2.902616ms)
Jan 19 21:49:41.811: INFO: (19) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.961698ms)
Jan 19 21:49:41.811: INFO: (19) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 3.000528ms)
Jan 19 21:49:41.812: INFO: (19) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.569975ms)
Jan 19 21:49:41.812: INFO: (19) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.575124ms)
Jan 19 21:49:41.813: INFO: (19) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.250628ms)
Jan 19 21:49:41.813: INFO: (19) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.378616ms)
Jan 19 21:49:41.813: INFO: (19) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.883963ms)
Jan 19 21:49:41.813: INFO: (19) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.800085ms)
Jan 19 21:49:41.813: INFO: (19) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.777303ms)
Jan 19 21:49:41.813: INFO: (19) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.902979ms)
Jan 19 21:49:41.814: INFO: (19) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.364192ms)
Jan 19 21:49:41.814: INFO: (19) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.400261ms)
Jan 19 21:49:41.814: INFO: (19) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.258397ms)
Jan 19 21:49:41.814: INFO: (19) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.699305ms)
STEP: deleting ReplicationController proxy-service-twm8j in namespace proxy-84, will wait for the garbage collector to delete the pods 01/19/23 21:49:41.814
Jan 19 21:49:41.873: INFO: Deleting ReplicationController proxy-service-twm8j took: 5.724702ms
Jan 19 21:49:41.974: INFO: Terminating ReplicationController proxy-service-twm8j pods took: 100.819129ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 19 21:49:44.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-84" for this suite. 01/19/23 21:49:44.102
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":304,"skipped":5653,"failed":0}
------------------------------
• [SLOW TEST] [5.544 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:38.57
    Jan 19 21:49:38.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename proxy 01/19/23 21:49:38.57
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:38.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:38.586
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/19/23 21:49:38.615
    STEP: creating replication controller proxy-service-twm8j in namespace proxy-84 01/19/23 21:49:38.615
    I0119 21:49:38.623669      22 runners.go:193] Created replication controller with name: proxy-service-twm8j, namespace: proxy-84, replica count: 1
    I0119 21:49:39.674319      22 runners.go:193] proxy-service-twm8j Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0119 21:49:40.674638      22 runners.go:193] proxy-service-twm8j Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0119 21:49:41.675510      22 runners.go:193] proxy-service-twm8j Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 21:49:41.678: INFO: setup took 3.087266154s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/19/23 21:49:41.678
    Jan 19 21:49:41.683: INFO: (0) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 5.555039ms)
    Jan 19 21:49:41.684: INFO: (0) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 6.245895ms)
    Jan 19 21:49:41.684: INFO: (0) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 6.154149ms)
    Jan 19 21:49:41.685: INFO: (0) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 7.456516ms)
    Jan 19 21:49:41.685: INFO: (0) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 7.76113ms)
    Jan 19 21:49:41.686: INFO: (0) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 7.682488ms)
    Jan 19 21:49:41.686: INFO: (0) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 7.808454ms)
    Jan 19 21:49:41.686: INFO: (0) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 7.832296ms)
    Jan 19 21:49:41.686: INFO: (0) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 7.834251ms)
    Jan 19 21:49:41.686: INFO: (0) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 8.159009ms)
    Jan 19 21:49:41.687: INFO: (0) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 8.770632ms)
    Jan 19 21:49:41.687: INFO: (0) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 9.024698ms)
    Jan 19 21:49:41.687: INFO: (0) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 8.937244ms)
    Jan 19 21:49:41.687: INFO: (0) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 9.227467ms)
    Jan 19 21:49:41.687: INFO: (0) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 9.249658ms)
    Jan 19 21:49:41.688: INFO: (0) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 10.312837ms)
    Jan 19 21:49:41.691: INFO: (1) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.651013ms)
    Jan 19 21:49:41.691: INFO: (1) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.941134ms)
    Jan 19 21:49:41.691: INFO: (1) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.148502ms)
    Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.273815ms)
    Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.49211ms)
    Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.453252ms)
    Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.689484ms)
    Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.71609ms)
    Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 4.715729ms)
    Jan 19 21:49:41.693: INFO: (1) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.782335ms)
    Jan 19 21:49:41.694: INFO: (1) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.330065ms)
    Jan 19 21:49:41.694: INFO: (1) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.471582ms)
    Jan 19 21:49:41.694: INFO: (1) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.511777ms)
    Jan 19 21:49:41.694: INFO: (1) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.674481ms)
    Jan 19 21:49:41.695: INFO: (1) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 6.116775ms)
    Jan 19 21:49:41.695: INFO: (1) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 6.698126ms)
    Jan 19 21:49:41.698: INFO: (2) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 2.617534ms)
    Jan 19 21:49:41.698: INFO: (2) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.612214ms)
    Jan 19 21:49:41.698: INFO: (2) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 2.970308ms)
    Jan 19 21:49:41.698: INFO: (2) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.281469ms)
    Jan 19 21:49:41.698: INFO: (2) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.217669ms)
    Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.751521ms)
    Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.953686ms)
    Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.230549ms)
    Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.059171ms)
    Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.085789ms)
    Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.197054ms)
    Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.205098ms)
    Jan 19 21:49:41.699: INFO: (2) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.483384ms)
    Jan 19 21:49:41.700: INFO: (2) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.323267ms)
    Jan 19 21:49:41.700: INFO: (2) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.311419ms)
    Jan 19 21:49:41.701: INFO: (2) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 6.043135ms)
    Jan 19 21:49:41.704: INFO: (3) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.540219ms)
    Jan 19 21:49:41.704: INFO: (3) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 2.673062ms)
    Jan 19 21:49:41.704: INFO: (3) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.790284ms)
    Jan 19 21:49:41.704: INFO: (3) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.857087ms)
    Jan 19 21:49:41.705: INFO: (3) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.098877ms)
    Jan 19 21:49:41.705: INFO: (3) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.036104ms)
    Jan 19 21:49:41.705: INFO: (3) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.123419ms)
    Jan 19 21:49:41.705: INFO: (3) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.113717ms)
    Jan 19 21:49:41.705: INFO: (3) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.020945ms)
    Jan 19 21:49:41.706: INFO: (3) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.369991ms)
    Jan 19 21:49:41.706: INFO: (3) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.458784ms)
    Jan 19 21:49:41.706: INFO: (3) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.392388ms)
    Jan 19 21:49:41.706: INFO: (3) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.73016ms)
    Jan 19 21:49:41.706: INFO: (3) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.896424ms)
    Jan 19 21:49:41.706: INFO: (3) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.932334ms)
    Jan 19 21:49:41.707: INFO: (3) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.184031ms)
    Jan 19 21:49:41.709: INFO: (4) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.478969ms)
    Jan 19 21:49:41.710: INFO: (4) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.812808ms)
    Jan 19 21:49:41.710: INFO: (4) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.135121ms)
    Jan 19 21:49:41.710: INFO: (4) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.576366ms)
    Jan 19 21:49:41.710: INFO: (4) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 3.59254ms)
    Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.011755ms)
    Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.041899ms)
    Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.270789ms)
    Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.287057ms)
    Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.328038ms)
    Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.446224ms)
    Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.529143ms)
    Jan 19 21:49:41.711: INFO: (4) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.491497ms)
    Jan 19 21:49:41.712: INFO: (4) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 4.931912ms)
    Jan 19 21:49:41.712: INFO: (4) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.33779ms)
    Jan 19 21:49:41.713: INFO: (4) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.763218ms)
    Jan 19 21:49:41.715: INFO: (5) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.501165ms)
    Jan 19 21:49:41.716: INFO: (5) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 2.997364ms)
    Jan 19 21:49:41.716: INFO: (5) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.521042ms)
    Jan 19 21:49:41.716: INFO: (5) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.612328ms)
    Jan 19 21:49:41.716: INFO: (5) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 3.648165ms)
    Jan 19 21:49:41.716: INFO: (5) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.586338ms)
    Jan 19 21:49:41.717: INFO: (5) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 3.916592ms)
    Jan 19 21:49:41.717: INFO: (5) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.000652ms)
    Jan 19 21:49:41.718: INFO: (5) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.147756ms)
    Jan 19 21:49:41.718: INFO: (5) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 5.221319ms)
    Jan 19 21:49:41.718: INFO: (5) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 5.259658ms)
    Jan 19 21:49:41.718: INFO: (5) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 5.313335ms)
    Jan 19 21:49:41.718: INFO: (5) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 5.283236ms)
    Jan 19 21:49:41.718: INFO: (5) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.656655ms)
    Jan 19 21:49:41.719: INFO: (5) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 6.473548ms)
    Jan 19 21:49:41.719: INFO: (5) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 6.304265ms)
    Jan 19 21:49:41.722: INFO: (6) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 2.651497ms)
    Jan 19 21:49:41.722: INFO: (6) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 2.988231ms)
    Jan 19 21:49:41.722: INFO: (6) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.063523ms)
    Jan 19 21:49:41.723: INFO: (6) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.433117ms)
    Jan 19 21:49:41.723: INFO: (6) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.951747ms)
    Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.655338ms)
    Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.404226ms)
    Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.477711ms)
    Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.837657ms)
    Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 4.793826ms)
    Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.645535ms)
    Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 4.762283ms)
    Jan 19 21:49:41.724: INFO: (6) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.912133ms)
    Jan 19 21:49:41.725: INFO: (6) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.655633ms)
    Jan 19 21:49:41.725: INFO: (6) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 5.661568ms)
    Jan 19 21:49:41.726: INFO: (6) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 6.486953ms)
    Jan 19 21:49:41.729: INFO: (7) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.130923ms)
    Jan 19 21:49:41.729: INFO: (7) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.551648ms)
    Jan 19 21:49:41.729: INFO: (7) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 3.562076ms)
    Jan 19 21:49:41.730: INFO: (7) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.251092ms)
    Jan 19 21:49:41.730: INFO: (7) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.488946ms)
    Jan 19 21:49:41.730: INFO: (7) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.705988ms)
    Jan 19 21:49:41.731: INFO: (7) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.304151ms)
    Jan 19 21:49:41.732: INFO: (7) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.791682ms)
    Jan 19 21:49:41.732: INFO: (7) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 6.008595ms)
    Jan 19 21:49:41.732: INFO: (7) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 6.018246ms)
    Jan 19 21:49:41.732: INFO: (7) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 6.10946ms)
    Jan 19 21:49:41.732: INFO: (7) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 5.977748ms)
    Jan 19 21:49:41.732: INFO: (7) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 6.017923ms)
    Jan 19 21:49:41.733: INFO: (7) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 6.810515ms)
    Jan 19 21:49:41.733: INFO: (7) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 7.107811ms)
    Jan 19 21:49:41.733: INFO: (7) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 7.410971ms)
    Jan 19 21:49:41.736: INFO: (8) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 2.500602ms)
    Jan 19 21:49:41.736: INFO: (8) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 2.610784ms)
    Jan 19 21:49:41.736: INFO: (8) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.807846ms)
    Jan 19 21:49:41.736: INFO: (8) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.842764ms)
    Jan 19 21:49:41.737: INFO: (8) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.367211ms)
    Jan 19 21:49:41.737: INFO: (8) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.847617ms)
    Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.240958ms)
    Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.998951ms)
    Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.235621ms)
    Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.198285ms)
    Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.317136ms)
    Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.355538ms)
    Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.716193ms)
    Jan 19 21:49:41.738: INFO: (8) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.042038ms)
    Jan 19 21:49:41.739: INFO: (8) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.781241ms)
    Jan 19 21:49:41.739: INFO: (8) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.797646ms)
    Jan 19 21:49:41.742: INFO: (9) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 2.908385ms)
    Jan 19 21:49:41.742: INFO: (9) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.83207ms)
    Jan 19 21:49:41.742: INFO: (9) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.932789ms)
    Jan 19 21:49:41.743: INFO: (9) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.52075ms)
    Jan 19 21:49:41.743: INFO: (9) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 3.514905ms)
    Jan 19 21:49:41.743: INFO: (9) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.616937ms)
    Jan 19 21:49:41.743: INFO: (9) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 3.68901ms)
    Jan 19 21:49:41.743: INFO: (9) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.871451ms)
    Jan 19 21:49:41.743: INFO: (9) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.734643ms)
    Jan 19 21:49:41.744: INFO: (9) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.376228ms)
    Jan 19 21:49:41.744: INFO: (9) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.296233ms)
    Jan 19 21:49:41.744: INFO: (9) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.291227ms)
    Jan 19 21:49:41.744: INFO: (9) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.624825ms)
    Jan 19 21:49:41.745: INFO: (9) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.150017ms)
    Jan 19 21:49:41.745: INFO: (9) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 5.782112ms)
    Jan 19 21:49:41.745: INFO: (9) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.680828ms)
    Jan 19 21:49:41.748: INFO: (10) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.745472ms)
    Jan 19 21:49:41.748: INFO: (10) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 2.903085ms)
    Jan 19 21:49:41.749: INFO: (10) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.430107ms)
    Jan 19 21:49:41.750: INFO: (10) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.10003ms)
    Jan 19 21:49:41.750: INFO: (10) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.196286ms)
    Jan 19 21:49:41.750: INFO: (10) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.241152ms)
    Jan 19 21:49:41.750: INFO: (10) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.323912ms)
    Jan 19 21:49:41.750: INFO: (10) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 4.194609ms)
    Jan 19 21:49:41.750: INFO: (10) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.995593ms)
    Jan 19 21:49:41.751: INFO: (10) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.371897ms)
    Jan 19 21:49:41.751: INFO: (10) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.908806ms)
    Jan 19 21:49:41.758: INFO: (10) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 12.960836ms)
    Jan 19 21:49:41.759: INFO: (10) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 13.702941ms)
    Jan 19 21:49:41.759: INFO: (10) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 13.920396ms)
    Jan 19 21:49:41.760: INFO: (10) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 15.123371ms)
    Jan 19 21:49:41.760: INFO: (10) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 15.018686ms)
    Jan 19 21:49:41.763: INFO: (11) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.757831ms)
    Jan 19 21:49:41.764: INFO: (11) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.45461ms)
    Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.940412ms)
    Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.349799ms)
    Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.207121ms)
    Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.105711ms)
    Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.238836ms)
    Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.367872ms)
    Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.421806ms)
    Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.464053ms)
    Jan 19 21:49:41.765: INFO: (11) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 4.704258ms)
    Jan 19 21:49:41.766: INFO: (11) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.932751ms)
    Jan 19 21:49:41.766: INFO: (11) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.572739ms)
    Jan 19 21:49:41.766: INFO: (11) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.634306ms)
    Jan 19 21:49:41.766: INFO: (11) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.621586ms)
    Jan 19 21:49:41.766: INFO: (11) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 5.819771ms)
    Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 3.205568ms)
    Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.390355ms)
    Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 3.319586ms)
    Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.289476ms)
    Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 3.418329ms)
    Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.825421ms)
    Jan 19 21:49:41.770: INFO: (12) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.90208ms)
    Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.184407ms)
    Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.3975ms)
    Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.265923ms)
    Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.46149ms)
    Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.512063ms)
    Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 4.957107ms)
    Jan 19 21:49:41.771: INFO: (12) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.982404ms)
    Jan 19 21:49:41.772: INFO: (12) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.171053ms)
    Jan 19 21:49:41.772: INFO: (12) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.087453ms)
    Jan 19 21:49:41.774: INFO: (13) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 2.495614ms)
    Jan 19 21:49:41.774: INFO: (13) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.643072ms)
    Jan 19 21:49:41.774: INFO: (13) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.676054ms)
    Jan 19 21:49:41.775: INFO: (13) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.232808ms)
    Jan 19 21:49:41.775: INFO: (13) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 3.374465ms)
    Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.075674ms)
    Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.955647ms)
    Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.03216ms)
    Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 4.160268ms)
    Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.258307ms)
    Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.263118ms)
    Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.227785ms)
    Jan 19 21:49:41.776: INFO: (13) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.168044ms)
    Jan 19 21:49:41.777: INFO: (13) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 4.791801ms)
    Jan 19 21:49:41.777: INFO: (13) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.082703ms)
    Jan 19 21:49:41.777: INFO: (13) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.552918ms)
    Jan 19 21:49:41.780: INFO: (14) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 2.521243ms)
    Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 3.005217ms)
    Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 3.470832ms)
    Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.611198ms)
    Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.738224ms)
    Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.77308ms)
    Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.711254ms)
    Jan 19 21:49:41.781: INFO: (14) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.058135ms)
    Jan 19 21:49:41.782: INFO: (14) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.163133ms)
    Jan 19 21:49:41.782: INFO: (14) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.278565ms)
    Jan 19 21:49:41.782: INFO: (14) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.160501ms)
    Jan 19 21:49:41.782: INFO: (14) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 4.225121ms)
    Jan 19 21:49:41.782: INFO: (14) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.677043ms)
    Jan 19 21:49:41.783: INFO: (14) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.140203ms)
    Jan 19 21:49:41.783: INFO: (14) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.333155ms)
    Jan 19 21:49:41.784: INFO: (14) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 5.997625ms)
    Jan 19 21:49:41.786: INFO: (15) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 2.673635ms)
    Jan 19 21:49:41.786: INFO: (15) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.646441ms)
    Jan 19 21:49:41.787: INFO: (15) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.298879ms)
    Jan 19 21:49:41.787: INFO: (15) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 3.29902ms)
    Jan 19 21:49:41.787: INFO: (15) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 3.571013ms)
    Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 3.976449ms)
    Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.749363ms)
    Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.913203ms)
    Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.047054ms)
    Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.019826ms)
    Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 4.189503ms)
    Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.346281ms)
    Jan 19 21:49:41.788: INFO: (15) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.348456ms)
    Jan 19 21:49:41.789: INFO: (15) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 5.574994ms)
    Jan 19 21:49:41.789: INFO: (15) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.274994ms)
    Jan 19 21:49:41.789: INFO: (15) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.27267ms)
    Jan 19 21:49:41.792: INFO: (16) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 2.712465ms)
    Jan 19 21:49:41.793: INFO: (16) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.532788ms)
    Jan 19 21:49:41.793: INFO: (16) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.47313ms)
    Jan 19 21:49:41.793: INFO: (16) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.66397ms)
    Jan 19 21:49:41.793: INFO: (16) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 3.541326ms)
    Jan 19 21:49:41.793: INFO: (16) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 3.998423ms)
    Jan 19 21:49:41.794: INFO: (16) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.207653ms)
    Jan 19 21:49:41.795: INFO: (16) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 5.669166ms)
    Jan 19 21:49:41.795: INFO: (16) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.911377ms)
    Jan 19 21:49:41.795: INFO: (16) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 6.048168ms)
    Jan 19 21:49:41.795: INFO: (16) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 6.067099ms)
    Jan 19 21:49:41.795: INFO: (16) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 6.151932ms)
    Jan 19 21:49:41.796: INFO: (16) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 6.204016ms)
    Jan 19 21:49:41.796: INFO: (16) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 6.960307ms)
    Jan 19 21:49:41.797: INFO: (16) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 7.209713ms)
    Jan 19 21:49:41.797: INFO: (16) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 7.628139ms)
    Jan 19 21:49:41.800: INFO: (17) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.476172ms)
    Jan 19 21:49:41.800: INFO: (17) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 2.849214ms)
    Jan 19 21:49:41.800: INFO: (17) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.863079ms)
    Jan 19 21:49:41.801: INFO: (17) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 3.311331ms)
    Jan 19 21:49:41.801: INFO: (17) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 3.563428ms)
    Jan 19 21:49:41.801: INFO: (17) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 3.99721ms)
    Jan 19 21:49:41.801: INFO: (17) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.065648ms)
    Jan 19 21:49:41.801: INFO: (17) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 4.169193ms)
    Jan 19 21:49:41.801: INFO: (17) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.317227ms)
    Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 4.33178ms)
    Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.506684ms)
    Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.484842ms)
    Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.778105ms)
    Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.994614ms)
    Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.008346ms)
    Jan 19 21:49:41.802: INFO: (17) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.334747ms)
    Jan 19 21:49:41.805: INFO: (18) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.363046ms)
    Jan 19 21:49:41.805: INFO: (18) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 2.831056ms)
    Jan 19 21:49:41.805: INFO: (18) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 2.697814ms)
    Jan 19 21:49:41.806: INFO: (18) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.992652ms)
    Jan 19 21:49:41.806: INFO: (18) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.581402ms)
    Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.970991ms)
    Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 4.611784ms)
    Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 4.610206ms)
    Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.595281ms)
    Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 4.708963ms)
    Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 4.839714ms)
    Jan 19 21:49:41.807: INFO: (18) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.775642ms)
    Jan 19 21:49:41.808: INFO: (18) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 4.860758ms)
    Jan 19 21:49:41.808: INFO: (18) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.298014ms)
    Jan 19 21:49:41.808: INFO: (18) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.521933ms)
    Jan 19 21:49:41.808: INFO: (18) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 5.575967ms)
    Jan 19 21:49:41.811: INFO: (19) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 2.483964ms)
    Jan 19 21:49:41.811: INFO: (19) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:443/proxy/tlsrewriteme"... (200; 2.902616ms)
    Jan 19 21:49:41.811: INFO: (19) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">te... (200; 2.961698ms)
    Jan 19 21:49:41.811: INFO: (19) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:460/proxy/: tls baz (200; 3.000528ms)
    Jan 19 21:49:41.812: INFO: (19) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:160/proxy/: foo (200; 3.569975ms)
    Jan 19 21:49:41.812: INFO: (19) /api/v1/namespaces/proxy-84/pods/https:proxy-service-twm8j-2r8j9:462/proxy/: tls qux (200; 3.575124ms)
    Jan 19 21:49:41.813: INFO: (19) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname2/proxy/: bar (200; 4.250628ms)
    Jan 19 21:49:41.813: INFO: (19) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:1080/proxy/rewriteme">test</a... (200; 4.378616ms)
    Jan 19 21:49:41.813: INFO: (19) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname1/proxy/: tls baz (200; 4.883963ms)
    Jan 19 21:49:41.813: INFO: (19) /api/v1/namespaces/proxy-84/pods/http:proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.800085ms)
    Jan 19 21:49:41.813: INFO: (19) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/: <a href="/api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9/proxy/rewriteme">test</a> (200; 4.777303ms)
    Jan 19 21:49:41.813: INFO: (19) /api/v1/namespaces/proxy-84/pods/proxy-service-twm8j-2r8j9:162/proxy/: bar (200; 4.902979ms)
    Jan 19 21:49:41.814: INFO: (19) /api/v1/namespaces/proxy-84/services/http:proxy-service-twm8j:portname1/proxy/: foo (200; 5.364192ms)
    Jan 19 21:49:41.814: INFO: (19) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname2/proxy/: bar (200; 5.400261ms)
    Jan 19 21:49:41.814: INFO: (19) /api/v1/namespaces/proxy-84/services/proxy-service-twm8j:portname1/proxy/: foo (200; 5.258397ms)
    Jan 19 21:49:41.814: INFO: (19) /api/v1/namespaces/proxy-84/services/https:proxy-service-twm8j:tlsportname2/proxy/: tls qux (200; 5.699305ms)
    STEP: deleting ReplicationController proxy-service-twm8j in namespace proxy-84, will wait for the garbage collector to delete the pods 01/19/23 21:49:41.814
    Jan 19 21:49:41.873: INFO: Deleting ReplicationController proxy-service-twm8j took: 5.724702ms
    Jan 19 21:49:41.974: INFO: Terminating ReplicationController proxy-service-twm8j pods took: 100.819129ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 19 21:49:44.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-84" for this suite. 01/19/23 21:49:44.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:44.117
Jan 19 21:49:44.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename sysctl 01/19/23 21:49:44.117
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:44.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:44.148
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/19/23 21:49:44.15
STEP: Watching for error events or started pod 01/19/23 21:49:45.177
STEP: Waiting for pod completion 01/19/23 21:49:47.181
Jan 19 21:49:47.181: INFO: Waiting up to 3m0s for pod "sysctl-7a7e3223-eaac-4d40-a03c-60560a4cf5d3" in namespace "sysctl-162" to be "completed"
Jan 19 21:49:47.183: INFO: Pod "sysctl-7a7e3223-eaac-4d40-a03c-60560a4cf5d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.371844ms
Jan 19 21:49:49.188: INFO: Pod "sysctl-7a7e3223-eaac-4d40-a03c-60560a4cf5d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007005387s
Jan 19 21:49:49.188: INFO: Pod "sysctl-7a7e3223-eaac-4d40-a03c-60560a4cf5d3" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/19/23 21:49:49.191
STEP: Getting logs from the pod 01/19/23 21:49:49.191
STEP: Checking that the sysctl is actually updated 01/19/23 21:49:49.196
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 21:49:49.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-162" for this suite. 01/19/23 21:49:49.201
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":305,"skipped":5733,"failed":0}
------------------------------
• [SLOW TEST] [5.089 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:44.117
    Jan 19 21:49:44.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename sysctl 01/19/23 21:49:44.117
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:44.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:44.148
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/19/23 21:49:44.15
    STEP: Watching for error events or started pod 01/19/23 21:49:45.177
    STEP: Waiting for pod completion 01/19/23 21:49:47.181
    Jan 19 21:49:47.181: INFO: Waiting up to 3m0s for pod "sysctl-7a7e3223-eaac-4d40-a03c-60560a4cf5d3" in namespace "sysctl-162" to be "completed"
    Jan 19 21:49:47.183: INFO: Pod "sysctl-7a7e3223-eaac-4d40-a03c-60560a4cf5d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.371844ms
    Jan 19 21:49:49.188: INFO: Pod "sysctl-7a7e3223-eaac-4d40-a03c-60560a4cf5d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007005387s
    Jan 19 21:49:49.188: INFO: Pod "sysctl-7a7e3223-eaac-4d40-a03c-60560a4cf5d3" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/19/23 21:49:49.191
    STEP: Getting logs from the pod 01/19/23 21:49:49.191
    STEP: Checking that the sysctl is actually updated 01/19/23 21:49:49.196
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 21:49:49.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-162" for this suite. 01/19/23 21:49:49.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:49.206
Jan 19 21:49:49.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename dns 01/19/23 21:49:49.206
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:49.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:49.22
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/19/23 21:49:49.222
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6902.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6902.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/19/23 21:49:49.227
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6902.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6902.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/19/23 21:49:49.227
STEP: creating a pod to probe DNS 01/19/23 21:49:49.227
STEP: submitting the pod to kubernetes 01/19/23 21:49:49.227
Jan 19 21:49:49.274: INFO: Waiting up to 15m0s for pod "dns-test-955cbc09-0192-4769-9a2b-45848ef7fe0b" in namespace "dns-6902" to be "running"
Jan 19 21:49:49.277: INFO: Pod "dns-test-955cbc09-0192-4769-9a2b-45848ef7fe0b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.252696ms
Jan 19 21:49:51.280: INFO: Pod "dns-test-955cbc09-0192-4769-9a2b-45848ef7fe0b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006162666s
Jan 19 21:49:51.280: INFO: Pod "dns-test-955cbc09-0192-4769-9a2b-45848ef7fe0b" satisfied condition "running"
STEP: retrieving the pod 01/19/23 21:49:51.28
STEP: looking for the results for each expected name from probers 01/19/23 21:49:51.283
Jan 19 21:49:51.298: INFO: DNS probes using dns-6902/dns-test-955cbc09-0192-4769-9a2b-45848ef7fe0b succeeded

STEP: deleting the pod 01/19/23 21:49:51.298
STEP: deleting the test headless service 01/19/23 21:49:51.309
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 21:49:51.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6902" for this suite. 01/19/23 21:49:51.328
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":306,"skipped":5740,"failed":0}
------------------------------
• [2.127 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:49.206
    Jan 19 21:49:49.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename dns 01/19/23 21:49:49.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:49.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:49.22
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/19/23 21:49:49.222
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6902.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6902.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/19/23 21:49:49.227
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6902.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6902.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/19/23 21:49:49.227
    STEP: creating a pod to probe DNS 01/19/23 21:49:49.227
    STEP: submitting the pod to kubernetes 01/19/23 21:49:49.227
    Jan 19 21:49:49.274: INFO: Waiting up to 15m0s for pod "dns-test-955cbc09-0192-4769-9a2b-45848ef7fe0b" in namespace "dns-6902" to be "running"
    Jan 19 21:49:49.277: INFO: Pod "dns-test-955cbc09-0192-4769-9a2b-45848ef7fe0b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.252696ms
    Jan 19 21:49:51.280: INFO: Pod "dns-test-955cbc09-0192-4769-9a2b-45848ef7fe0b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006162666s
    Jan 19 21:49:51.280: INFO: Pod "dns-test-955cbc09-0192-4769-9a2b-45848ef7fe0b" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 21:49:51.28
    STEP: looking for the results for each expected name from probers 01/19/23 21:49:51.283
    Jan 19 21:49:51.298: INFO: DNS probes using dns-6902/dns-test-955cbc09-0192-4769-9a2b-45848ef7fe0b succeeded

    STEP: deleting the pod 01/19/23 21:49:51.298
    STEP: deleting the test headless service 01/19/23 21:49:51.309
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 21:49:51.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6902" for this suite. 01/19/23 21:49:51.328
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:49:51.333
Jan 19 21:49:51.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename resourcequota 01/19/23 21:49:51.334
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:51.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:51.355
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 01/19/23 21:49:51.359
STEP: Creating a ResourceQuota 01/19/23 21:49:56.362
STEP: Ensuring resource quota status is calculated 01/19/23 21:49:56.367
STEP: Creating a ReplicationController 01/19/23 21:49:58.37
STEP: Ensuring resource quota status captures replication controller creation 01/19/23 21:49:58.395
STEP: Deleting a ReplicationController 01/19/23 21:50:00.399
STEP: Ensuring resource quota status released usage 01/19/23 21:50:00.406
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 21:50:02.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7937" for this suite. 01/19/23 21:50:02.412
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":307,"skipped":5740,"failed":0}
------------------------------
• [SLOW TEST] [11.084 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:49:51.333
    Jan 19 21:49:51.333: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename resourcequota 01/19/23 21:49:51.334
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:49:51.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:49:51.355
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 01/19/23 21:49:51.359
    STEP: Creating a ResourceQuota 01/19/23 21:49:56.362
    STEP: Ensuring resource quota status is calculated 01/19/23 21:49:56.367
    STEP: Creating a ReplicationController 01/19/23 21:49:58.37
    STEP: Ensuring resource quota status captures replication controller creation 01/19/23 21:49:58.395
    STEP: Deleting a ReplicationController 01/19/23 21:50:00.399
    STEP: Ensuring resource quota status released usage 01/19/23 21:50:00.406
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 21:50:02.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7937" for this suite. 01/19/23 21:50:02.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:50:02.418
Jan 19 21:50:02.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename statefulset 01/19/23 21:50:02.419
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:50:02.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:50:02.433
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9931 01/19/23 21:50:02.438
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 01/19/23 21:50:02.447
STEP: Creating stateful set ss in namespace statefulset-9931 01/19/23 21:50:02.461
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9931 01/19/23 21:50:02.474
Jan 19 21:50:02.484: INFO: Found 0 stateful pods, waiting for 1
Jan 19 21:50:12.488: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/19/23 21:50:12.488
Jan 19 21:50:12.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 21:50:12.614: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 21:50:12.614: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 21:50:12.614: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 21:50:12.616: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan 19 21:50:22.621: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 21:50:22.621: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 21:50:22.636: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999735s
Jan 19 21:50:23.639: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995953393s
Jan 19 21:50:24.642: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993430996s
Jan 19 21:50:25.645: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990576887s
Jan 19 21:50:26.648: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.987239799s
Jan 19 21:50:27.651: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.984157064s
Jan 19 21:50:28.654: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.981344599s
Jan 19 21:50:29.657: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.978600891s
Jan 19 21:50:30.663: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.975234916s
Jan 19 21:50:31.666: INFO: Verifying statefulset ss doesn't scale past 1 for another 969.308004ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9931 01/19/23 21:50:32.667
Jan 19 21:50:32.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 21:50:32.821: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 21:50:32.821: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 21:50:32.821: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 21:50:32.824: INFO: Found 1 stateful pods, waiting for 3
Jan 19 21:50:42.829: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 21:50:42.829: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 21:50:42.829: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/19/23 21:50:42.829
STEP: Scale down will halt with unhealthy stateful pod 01/19/23 21:50:42.829
Jan 19 21:50:42.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 21:50:42.959: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 21:50:42.959: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 21:50:42.959: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 21:50:42.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 21:50:43.089: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 21:50:43.089: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 21:50:43.089: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 21:50:43.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan 19 21:50:43.222: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan 19 21:50:43.222: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan 19 21:50:43.222: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan 19 21:50:43.222: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 21:50:43.225: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jan 19 21:50:53.233: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 21:50:53.233: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 21:50:53.233: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan 19 21:50:53.258: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999804s
Jan 19 21:50:54.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98249786s
Jan 19 21:50:55.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97951706s
Jan 19 21:50:56.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976310151s
Jan 19 21:50:57.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972592467s
Jan 19 21:50:58.275: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96907196s
Jan 19 21:50:59.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965825676s
Jan 19 21:51:00.282: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961743586s
Jan 19 21:51:01.286: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957706642s
Jan 19 21:51:02.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.365512ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9931 01/19/23 21:51:03.29
Jan 19 21:51:03.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 21:51:03.403: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 21:51:03.403: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 21:51:03.403: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 21:51:03.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 21:51:03.530: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 21:51:03.530: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 21:51:03.530: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 21:51:03.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan 19 21:51:03.637: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan 19 21:51:03.637: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan 19 21:51:03.637: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan 19 21:51:03.637: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/19/23 21:51:13.649
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 21:51:13.650: INFO: Deleting all statefulset in ns statefulset-9931
Jan 19 21:51:13.652: INFO: Scaling statefulset ss to 0
Jan 19 21:51:13.660: INFO: Waiting for statefulset status.replicas updated to 0
Jan 19 21:51:13.662: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 21:51:13.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9931" for this suite. 01/19/23 21:51:13.677
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":308,"skipped":5766,"failed":0}
------------------------------
• [SLOW TEST] [71.273 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:50:02.418
    Jan 19 21:50:02.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename statefulset 01/19/23 21:50:02.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:50:02.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:50:02.433
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9931 01/19/23 21:50:02.438
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/19/23 21:50:02.447
    STEP: Creating stateful set ss in namespace statefulset-9931 01/19/23 21:50:02.461
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9931 01/19/23 21:50:02.474
    Jan 19 21:50:02.484: INFO: Found 0 stateful pods, waiting for 1
    Jan 19 21:50:12.488: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/19/23 21:50:12.488
    Jan 19 21:50:12.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 21:50:12.614: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 21:50:12.614: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 21:50:12.614: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 21:50:12.616: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan 19 21:50:22.621: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 21:50:22.621: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 21:50:22.636: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999735s
    Jan 19 21:50:23.639: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995953393s
    Jan 19 21:50:24.642: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993430996s
    Jan 19 21:50:25.645: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.990576887s
    Jan 19 21:50:26.648: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.987239799s
    Jan 19 21:50:27.651: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.984157064s
    Jan 19 21:50:28.654: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.981344599s
    Jan 19 21:50:29.657: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.978600891s
    Jan 19 21:50:30.663: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.975234916s
    Jan 19 21:50:31.666: INFO: Verifying statefulset ss doesn't scale past 1 for another 969.308004ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9931 01/19/23 21:50:32.667
    Jan 19 21:50:32.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 21:50:32.821: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 21:50:32.821: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 21:50:32.821: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 21:50:32.824: INFO: Found 1 stateful pods, waiting for 3
    Jan 19 21:50:42.829: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 21:50:42.829: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 21:50:42.829: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/19/23 21:50:42.829
    STEP: Scale down will halt with unhealthy stateful pod 01/19/23 21:50:42.829
    Jan 19 21:50:42.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 21:50:42.959: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 21:50:42.959: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 21:50:42.959: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 21:50:42.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 21:50:43.089: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 21:50:43.089: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 21:50:43.089: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 21:50:43.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan 19 21:50:43.222: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan 19 21:50:43.222: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan 19 21:50:43.222: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan 19 21:50:43.222: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 21:50:43.225: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jan 19 21:50:53.233: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 21:50:53.233: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 21:50:53.233: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan 19 21:50:53.258: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999804s
    Jan 19 21:50:54.261: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98249786s
    Jan 19 21:50:55.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.97951706s
    Jan 19 21:50:56.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.976310151s
    Jan 19 21:50:57.272: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.972592467s
    Jan 19 21:50:58.275: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96907196s
    Jan 19 21:50:59.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965825676s
    Jan 19 21:51:00.282: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961743586s
    Jan 19 21:51:01.286: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957706642s
    Jan 19 21:51:02.290: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.365512ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9931 01/19/23 21:51:03.29
    Jan 19 21:51:03.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 21:51:03.403: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 21:51:03.403: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 21:51:03.403: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 21:51:03.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 21:51:03.530: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 21:51:03.530: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 21:51:03.530: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 21:51:03.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=statefulset-9931 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan 19 21:51:03.637: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan 19 21:51:03.637: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan 19 21:51:03.637: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan 19 21:51:03.637: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/19/23 21:51:13.649
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 21:51:13.650: INFO: Deleting all statefulset in ns statefulset-9931
    Jan 19 21:51:13.652: INFO: Scaling statefulset ss to 0
    Jan 19 21:51:13.660: INFO: Waiting for statefulset status.replicas updated to 0
    Jan 19 21:51:13.662: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 21:51:13.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9931" for this suite. 01/19/23 21:51:13.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:51:13.693
Jan 19 21:51:13.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pods 01/19/23 21:51:13.694
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:51:13.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:51:13.711
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 01/19/23 21:51:13.715
Jan 19 21:51:13.783: INFO: created test-pod-1
Jan 19 21:51:13.826: INFO: created test-pod-2
Jan 19 21:51:13.885: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/19/23 21:51:13.886
Jan 19 21:51:13.886: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6853' to be running and ready
Jan 19 21:51:13.908: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 19 21:51:13.908: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 19 21:51:13.908: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan 19 21:51:13.908: INFO: 0 / 3 pods in namespace 'pods-6853' are running and ready (0 seconds elapsed)
Jan 19 21:51:13.908: INFO: expected 0 pod replicas in namespace 'pods-6853', 0 are Running and Ready.
Jan 19 21:51:13.908: INFO: POD         NODE                          PHASE    GRACE  CONDITIONS
Jan 19 21:51:13.908: INFO: test-pod-1  ip-10-0-172-44.ec2.internal   Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC  }]
Jan 19 21:51:13.908: INFO: test-pod-2  ip-10-0-171-213.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC  }]
Jan 19 21:51:13.908: INFO: test-pod-3  ip-10-0-172-44.ec2.internal   Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC  }]
Jan 19 21:51:13.908: INFO: 
Jan 19 21:51:15.916: INFO: 3 / 3 pods in namespace 'pods-6853' are running and ready (2 seconds elapsed)
Jan 19 21:51:15.916: INFO: expected 0 pod replicas in namespace 'pods-6853', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/19/23 21:51:15.934
Jan 19 21:51:15.937: INFO: Pod quantity 3 is different from expected quantity 0
Jan 19 21:51:16.939: INFO: Pod quantity 3 is different from expected quantity 0
Jan 19 21:51:17.941: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jan 19 21:51:18.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6853" for this suite. 01/19/23 21:51:18.953
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":309,"skipped":5805,"failed":0}
------------------------------
• [SLOW TEST] [5.266 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:51:13.693
    Jan 19 21:51:13.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pods 01/19/23 21:51:13.694
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:51:13.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:51:13.711
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 01/19/23 21:51:13.715
    Jan 19 21:51:13.783: INFO: created test-pod-1
    Jan 19 21:51:13.826: INFO: created test-pod-2
    Jan 19 21:51:13.885: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/19/23 21:51:13.886
    Jan 19 21:51:13.886: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6853' to be running and ready
    Jan 19 21:51:13.908: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 19 21:51:13.908: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 19 21:51:13.908: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan 19 21:51:13.908: INFO: 0 / 3 pods in namespace 'pods-6853' are running and ready (0 seconds elapsed)
    Jan 19 21:51:13.908: INFO: expected 0 pod replicas in namespace 'pods-6853', 0 are Running and Ready.
    Jan 19 21:51:13.908: INFO: POD         NODE                          PHASE    GRACE  CONDITIONS
    Jan 19 21:51:13.908: INFO: test-pod-1  ip-10-0-172-44.ec2.internal   Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC  }]
    Jan 19 21:51:13.908: INFO: test-pod-2  ip-10-0-171-213.ec2.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC  }]
    Jan 19 21:51:13.908: INFO: test-pod-3  ip-10-0-172-44.ec2.internal   Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-19 21:51:13 +0000 UTC  }]
    Jan 19 21:51:13.908: INFO: 
    Jan 19 21:51:15.916: INFO: 3 / 3 pods in namespace 'pods-6853' are running and ready (2 seconds elapsed)
    Jan 19 21:51:15.916: INFO: expected 0 pod replicas in namespace 'pods-6853', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/19/23 21:51:15.934
    Jan 19 21:51:15.937: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 19 21:51:16.939: INFO: Pod quantity 3 is different from expected quantity 0
    Jan 19 21:51:17.941: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jan 19 21:51:18.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6853" for this suite. 01/19/23 21:51:18.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:51:18.959
Jan 19 21:51:18.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename ephemeral-containers-test 01/19/23 21:51:18.96
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:51:18.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:51:18.986
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/19/23 21:51:18.988
Jan 19 21:51:19.032: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4608" to be "running and ready"
Jan 19 21:51:19.035: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.045633ms
Jan 19 21:51:19.035: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:51:21.038: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006256829s
Jan 19 21:51:21.038: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan 19 21:51:21.038: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/19/23 21:51:21.041
Jan 19 21:51:21.059: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4608" to be "container debugger running"
Jan 19 21:51:21.061: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.342049ms
Jan 19 21:51:23.065: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005940621s
Jan 19 21:51:25.065: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005711728s
Jan 19 21:51:25.065: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/19/23 21:51:25.065
Jan 19 21:51:25.065: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4608 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:51:25.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:51:25.065: INFO: ExecWithOptions: Clientset creation
Jan 19 21:51:25.065: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/ephemeral-containers-test-4608/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan 19 21:51:25.134: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jan 19 21:51:25.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-4608" for this suite. 01/19/23 21:51:25.146
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":310,"skipped":5816,"failed":0}
------------------------------
• [SLOW TEST] [6.192 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:51:18.959
    Jan 19 21:51:18.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/19/23 21:51:18.96
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:51:18.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:51:18.986
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/19/23 21:51:18.988
    Jan 19 21:51:19.032: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4608" to be "running and ready"
    Jan 19 21:51:19.035: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.045633ms
    Jan 19 21:51:19.035: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:51:21.038: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006256829s
    Jan 19 21:51:21.038: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan 19 21:51:21.038: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/19/23 21:51:21.041
    Jan 19 21:51:21.059: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4608" to be "container debugger running"
    Jan 19 21:51:21.061: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.342049ms
    Jan 19 21:51:23.065: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005940621s
    Jan 19 21:51:25.065: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.005711728s
    Jan 19 21:51:25.065: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/19/23 21:51:25.065
    Jan 19 21:51:25.065: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4608 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:51:25.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:51:25.065: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:51:25.065: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/ephemeral-containers-test-4608/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan 19 21:51:25.134: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jan 19 21:51:25.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-4608" for this suite. 01/19/23 21:51:25.146
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:51:25.152
Jan 19 21:51:25.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:51:25.153
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:51:25.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:51:25.178
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:51:25.22
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:51:25.381
STEP: Deploying the webhook pod 01/19/23 21:51:25.392
STEP: Wait for the deployment to be ready 01/19/23 21:51:25.401
Jan 19 21:51:25.409: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/19/23 21:51:27.417
STEP: Verifying the service has paired with the endpoint 01/19/23 21:51:27.426
Jan 19 21:51:28.426: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 01/19/23 21:51:28.429
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/19/23 21:51:28.43
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/19/23 21:51:28.43
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/19/23 21:51:28.43
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/19/23 21:51:28.431
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/19/23 21:51:28.431
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/19/23 21:51:28.431
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:51:28.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4197" for this suite. 01/19/23 21:51:28.435
STEP: Destroying namespace "webhook-4197-markers" for this suite. 01/19/23 21:51:28.441
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":311,"skipped":5819,"failed":0}
------------------------------
• [3.367 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:51:25.152
    Jan 19 21:51:25.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:51:25.153
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:51:25.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:51:25.178
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:51:25.22
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:51:25.381
    STEP: Deploying the webhook pod 01/19/23 21:51:25.392
    STEP: Wait for the deployment to be ready 01/19/23 21:51:25.401
    Jan 19 21:51:25.409: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/19/23 21:51:27.417
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:51:27.426
    Jan 19 21:51:28.426: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 01/19/23 21:51:28.429
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/19/23 21:51:28.43
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/19/23 21:51:28.43
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/19/23 21:51:28.43
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/19/23 21:51:28.431
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/19/23 21:51:28.431
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/19/23 21:51:28.431
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:51:28.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4197" for this suite. 01/19/23 21:51:28.435
    STEP: Destroying namespace "webhook-4197-markers" for this suite. 01/19/23 21:51:28.441
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:51:28.519
Jan 19 21:51:28.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename sched-pred 01/19/23 21:51:28.52
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:51:28.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:51:28.544
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 19 21:51:28.546: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 21:51:28.576: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 21:51:28.588: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-146-42.ec2.internal before test
Jan 19 21:51:28.626: INFO: aws-ebs-csi-driver-node-vxgq2 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:41 +0000 UTC (3 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 21:51:28.626: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 21:51:28.626: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 21:51:28.626: INFO: tuned-nbzt4 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container tuned ready: true, restart count 1
Jan 19 21:51:28.626: INFO: downloads-8d695cd69-lcsd9 from openshift-console started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container download-server ready: true, restart count 0
Jan 19 21:51:28.626: INFO: dns-default-c5qwq from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container dns ready: true, restart count 1
Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.626: INFO: node-resolver-g5jk6 from openshift-dns started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 21:51:28.626: INFO: node-ca-crm7t from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 21:51:28.626: INFO: ingress-canary-nfwhl from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 21:51:28.626: INFO: migrator-5c54d8d69d-l5lss from openshift-kube-storage-version-migrator started at 2023-01-19 18:42:53 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container migrator ready: true, restart count 0
Jan 19 21:51:28.626: INFO: machine-config-daemon-cc29b from openshift-machine-config-operator started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 21:51:28.626: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 21:51:28.626: INFO: node-exporter-9zc87 from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.626: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 21:51:28.626: INFO: sre-dns-latency-exporter-jsn9x from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container main ready: true, restart count 1
Jan 19 21:51:28.626: INFO: multus-5zck8 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 21:51:28.626: INFO: multus-additional-cni-plugins-88bc2 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 21:51:28.626: INFO: network-metrics-daemon-zvctv from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.626: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 21:51:28.626: INFO: network-check-source-746dd6c885-bmf5k from openshift-network-diagnostics started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 19 21:51:28.626: INFO: network-check-target-g5qsk from openshift-network-diagnostics started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 21:51:28.626: INFO: obo-prometheus-operator-64989cfc68-hdxb9 from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 21:51:28.626: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-gqhlz from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 21:51:28.626: INFO: osd-metrics-exporter-registry-qdjtk from openshift-osd-metrics started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.626: INFO: ovnkube-node-ftv9n from openshift-ovn-kubernetes started at 2023-01-19 18:15:41 +0000 UTC (5 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 21:51:28.626: INFO: 	Container ovn-acl-logging ready: true, restart count 2
Jan 19 21:51:28.626: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 21:51:28.626: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 21:51:28.626: INFO: rbac-permissions-operator-registry-rc4js from openshift-rbac-permissions started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.626: INFO: splunkforwarder-ds-7rpx7 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 21:51:28.626: INFO: prometheus-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (6 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 21:51:28.626: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 21:51:28.626: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 21:51:28.626: INFO: thanos-ruler-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (3 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:51:28.626: INFO: 	Container thanos-ruler ready: true, restart count 0
Jan 19 21:51:28.626: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
Jan 19 21:51:28.626: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-chbvs from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.626: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:51:28.626: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 21:51:28.626: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-151-158.ec2.internal before test
Jan 19 21:51:28.666: INFO: addon-operator-manager-84ff88fc6-fmhnb from openshift-addon-operator started at 2023-01-19 18:46:50 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container manager ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container metrics-relay-server ready: true, restart count 0
Jan 19 21:51:28.666: INFO: addon-operator-webhooks-67d9d47489-4kbk2 from openshift-addon-operator started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container webhook ready: true, restart count 0
Jan 19 21:51:28.666: INFO: aws-ebs-csi-driver-node-c8m6c from openshift-cluster-csi-drivers started at 2023-01-19 18:32:48 +0000 UTC (3 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 21:51:28.666: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 21:51:28.666: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 21:51:28.666: INFO: tuned-5h69k from openshift-cluster-node-tuning-operator started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container tuned ready: true, restart count 1
Jan 19 21:51:28.666: INFO: custom-domains-operator-5764f4df49-8nbdk from openshift-custom-domains-operator started at 2023-01-19 19:15:08 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container custom-domains-operator ready: true, restart count 0
Jan 19 21:51:28.666: INFO: deployment-validation-operator-7b8fd479cb-wtzg7 from openshift-deployment-validation-operator started at 2023-01-19 18:46:50 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container deployment-validation-operator ready: true, restart count 0
Jan 19 21:51:28.666: INFO: dns-default-d9szl from openshift-dns started at 2023-01-19 21:45:22 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container dns ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:51:28.666: INFO: node-resolver-9cwpx from openshift-dns started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 21:51:28.666: INFO: image-pruner-27902580-lzd8r from openshift-image-registry started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 21:51:28.666: INFO: image-pruner-27902640-c6mlt from openshift-image-registry started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 21:51:28.666: INFO: image-pruner-27902700-cf7x6 from openshift-image-registry started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 21:51:28.666: INFO: image-registry-656cfd9bd9-9x67r from openshift-image-registry started at 2023-01-19 19:15:11 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container registry ready: true, restart count 0
Jan 19 21:51:28.666: INFO: node-ca-k89p7 from openshift-image-registry started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 21:51:28.666: INFO: ingress-canary-v2tjj from openshift-ingress-canary started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 21:51:28.666: INFO: router-default-6bf5ff7dcb-5nzvc from openshift-ingress started at 2023-01-19 18:47:44 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container router ready: true, restart count 0
Jan 19 21:51:28.666: INFO: machine-config-daemon-28fzw from openshift-machine-config-operator started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 21:51:28.666: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 21:51:28.666: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container alertmanager ready: true, restart count 1
Jan 19 21:51:28.666: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 21:51:28.666: INFO: configure-alertmanager-operator-7d94bbbcf-8h7xs from openshift-monitoring started at 2023-01-19 19:16:27 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Jan 19 21:51:28.666: INFO: kube-state-metrics-666f4cbf77-5w8rk from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 19 21:51:28.666: INFO: node-exporter-7t5b7 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.666: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 21:51:28.666: INFO: openshift-state-metrics-7d5c9d867c-97gzd from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 19 21:51:28.666: INFO: prometheus-adapter-5bc6487f7c-q2ng7 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 19 21:51:28.666: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 21:51:28.666: INFO: prometheus-operator-564b78d5ff-v4xnk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 21:51:28.666: INFO: prometheus-operator-admission-webhook-749df5cf4f-zj8xj from openshift-monitoring started at 2023-01-19 18:46:40 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 21:51:28.666: INFO: sre-dns-latency-exporter-2q899 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container main ready: true, restart count 1
Jan 19 21:51:28.666: INFO: sre-stuck-ebs-vols-1-vdlrh from openshift-monitoring started at 2023-01-19 19:17:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container main ready: true, restart count 0
Jan 19 21:51:28.666: INFO: telemeter-client-54bbbbf598-d9fdk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (3 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container reload ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 19 21:51:28.666: INFO: thanos-querier-fcd5b6c8d-ql8qb from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container thanos-query ready: true, restart count 0
Jan 19 21:51:28.666: INFO: multus-additional-cni-plugins-vslld from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 21:51:28.666: INFO: multus-zqhtq from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 21:51:28.666: INFO: network-metrics-daemon-677lg from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.666: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 21:51:28.666: INFO: must-gather-operator-56b466776c-qv8sh from openshift-must-gather-operator started at 2023-01-19 19:17:38 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container must-gather-operator ready: true, restart count 0
Jan 19 21:51:28.666: INFO: network-check-target-xrr4d from openshift-network-diagnostics started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 21:51:28.666: INFO: ocm-agent-ddfdc6544-4fxrp from openshift-ocm-agent-operator started at 2023-01-19 19:17:41 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container ocm-agent ready: true, restart count 0
Jan 19 21:51:28.666: INFO: ocm-agent-operator-6cdb45fd86-4clcj from openshift-ocm-agent-operator started at 2023-01-19 19:17:43 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container ocm-agent-operator ready: true, restart count 0
Jan 19 21:51:28.666: INFO: ovnkube-node-gvwrr from openshift-ovn-kubernetes started at 2023-01-19 18:32:48 +0000 UTC (5 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 21:51:28.666: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 21:51:28.666: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 21:51:28.666: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 21:51:28.666: INFO: rbac-permissions-operator-5fb57974d8-htnjj from openshift-rbac-permissions started at 2023-01-19 19:17:50 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Jan 19 21:51:28.666: INFO: splunkforwarder-ds-ztlpn from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 21:51:28.666: INFO: splunk-forwarder-operator-7d66568cf7-bksrn from openshift-splunk-forwarder-operator started at 2023-01-19 19:17:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Jan 19 21:51:28.666: INFO: managed-velero-operator-788754d4b6-n89fw from openshift-velero started at 2023-01-19 19:17:56 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container managed-velero-operator ready: true, restart count 0
Jan 19 21:51:28.666: INFO: velero-7c64598b8c-8blx4 from openshift-velero started at 2023-01-19 19:18:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container velero ready: true, restart count 0
Jan 19 21:51:28.666: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-8mc8n from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 21:51:28.666: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-171-213.ec2.internal before test
Jan 19 21:51:28.699: INFO: aws-ebs-csi-driver-node-svd2s from openshift-cluster-csi-drivers started at 2023-01-19 18:15:58 +0000 UTC (3 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 21:51:28.699: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 21:51:28.699: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 21:51:28.699: INFO: tuned-9p2db from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container tuned ready: true, restart count 1
Jan 19 21:51:28.699: INFO: dns-default-zbx44 from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container dns ready: true, restart count 1
Jan 19 21:51:28.699: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.699: INFO: node-resolver-5mb95 from openshift-dns started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 21:51:28.699: INFO: node-ca-7c5fq from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 21:51:28.699: INFO: ingress-canary-6sc9q from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 21:51:28.699: INFO: machine-config-daemon-xxm6j from openshift-machine-config-operator started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 21:51:28.699: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 21:51:28.699: INFO: node-exporter-h445b from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.699: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 21:51:28.699: INFO: sre-dns-latency-exporter-vhwwx from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container main ready: true, restart count 1
Jan 19 21:51:28.699: INFO: multus-additional-cni-plugins-nhwfz from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 21:51:28.699: INFO: multus-j4ngm from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 21:51:28.699: INFO: network-metrics-daemon-hp85h from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.699: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 21:51:28.699: INFO: network-check-target-mg7p4 from openshift-network-diagnostics started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 21:51:28.699: INFO: ovnkube-node-kk5t9 from openshift-ovn-kubernetes started at 2023-01-19 18:15:58 +0000 UTC (5 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.699: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 21:51:28.699: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 21:51:28.699: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 21:51:28.699: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 21:51:28.699: INFO: splunkforwarder-ds-66kjq from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 21:51:28.699: INFO: sonobuoy-e2e-job-4d801e056c4540b8 from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container e2e ready: true, restart count 0
Jan 19 21:51:28.699: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:51:28.699: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-m8vnk from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.699: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:51:28.699: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 21:51:28.699: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-172-44.ec2.internal before test
Jan 19 21:51:28.733: INFO: ephemeral-containers-target-pod from ephemeral-containers-test-4608 started at 2023-01-19 21:51:19 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container test-container-1 ready: true, restart count 0
Jan 19 21:51:28.733: INFO: aws-ebs-csi-driver-node-z6klb from openshift-cluster-csi-drivers started at 2023-01-19 18:15:33 +0000 UTC (3 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 21:51:28.733: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 21:51:28.733: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 21:51:28.733: INFO: tuned-gjnt5 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container tuned ready: true, restart count 1
Jan 19 21:51:28.733: INFO: dns-default-dfndw from openshift-dns started at 2023-01-19 21:22:44 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container dns ready: true, restart count 0
Jan 19 21:51:28.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:51:28.733: INFO: node-resolver-b85qs from openshift-dns started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 21:51:28.733: INFO: node-ca-dsxvs from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 21:51:28.733: INFO: ingress-canary-cs8pq from openshift-ingress-canary started at 2023-01-19 21:22:44 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 19 21:51:28.733: INFO: machine-config-daemon-cllj8 from openshift-machine-config-operator started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 21:51:28.733: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 21:51:28.733: INFO: node-exporter-2vp9v from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.733: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 21:51:28.733: INFO: sre-dns-latency-exporter-ctmgr from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container main ready: true, restart count 1
Jan 19 21:51:28.733: INFO: multus-additional-cni-plugins-p5pjp from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 21:51:28.733: INFO: multus-xzxwg from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 21:51:28.733: INFO: network-metrics-daemon-hz5pj from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.733: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 21:51:28.733: INFO: network-check-target-nq9ln from openshift-network-diagnostics started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 21:51:28.733: INFO: collect-profiles-27902730-zm5kk from openshift-operator-lifecycle-manager started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 21:51:28.733: INFO: collect-profiles-27902745-9gctc from openshift-operator-lifecycle-manager started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 21:51:28.733: INFO: ovnkube-node-zmvs8 from openshift-ovn-kubernetes started at 2023-01-19 18:15:33 +0000 UTC (5 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.733: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 21:51:28.733: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 21:51:28.733: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 21:51:28.733: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 21:51:28.733: INFO: splunkforwarder-ds-pj5q8 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 21:51:28.733: INFO: sonobuoy from sonobuoy started at 2023-01-19 20:29:23 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 19 21:51:28.733: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-cbfjg from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.733: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:51:28.733: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 21:51:28.733: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-188-71.ec2.internal before test
Jan 19 21:51:28.768: INFO: addon-operator-catalog-zxs8v from openshift-addon-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.768: INFO: cloud-ingress-operator-registry-pqwm7 from openshift-cloud-ingress-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.768: INFO: aws-ebs-csi-driver-node-wmvd4 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:13 +0000 UTC (3 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 21:51:28.768: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 21:51:28.768: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 21:51:28.768: INFO: tuned-7x9t2 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container tuned ready: true, restart count 1
Jan 19 21:51:28.768: INFO: downloads-8d695cd69-hltgd from openshift-console started at 2023-01-19 18:39:20 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container download-server ready: true, restart count 0
Jan 19 21:51:28.768: INFO: custom-domains-operator-registry-kdw7s from openshift-custom-domains-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.768: INFO: deployment-validation-operator-catalog-nq94w from openshift-deployment-validation-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.768: INFO: dns-default-tzrx6 from openshift-dns started at 2023-01-19 18:15:43 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container dns ready: true, restart count 1
Jan 19 21:51:28.768: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.768: INFO: node-resolver-l2fzw from openshift-dns started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 21:51:28.768: INFO: node-ca-kzsvk from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 21:51:28.768: INFO: ingress-canary-t6tqp from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 21:51:28.768: INFO: machine-config-daemon-zhllf from openshift-machine-config-operator started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 21:51:28.768: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 21:51:28.768: INFO: managed-node-metadata-operator-registry-m6dxz from openshift-managed-node-metadata-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.768: INFO: managed-upgrade-operator-catalog-thxts from openshift-managed-upgrade-operator started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.768: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.768: INFO: configure-alertmanager-operator-registry-sjcb5 from openshift-monitoring started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.769: INFO: node-exporter-dc59d from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.769: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 21:51:28.769: INFO: osd-cluster-ready-nb8xk from openshift-monitoring started at 2023-01-19 18:39:21 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container osd-cluster-ready ready: false, restart count 7
Jan 19 21:51:28.769: INFO: sre-dns-latency-exporter-5dz25 from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container main ready: true, restart count 1
Jan 19 21:51:28.769: INFO: multus-additional-cni-plugins-hvpd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 21:51:28.769: INFO: multus-mptd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 21:51:28.769: INFO: network-metrics-daemon-s2l94 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.769: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 21:51:28.769: INFO: must-gather-operator-registry-n7fvg from openshift-must-gather-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.769: INFO: network-check-target-d8gvt from openshift-network-diagnostics started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 21:51:28.769: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-b4r6s from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 21:51:28.769: INFO: observability-operator-6dd79df7dc-z7frh from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container operator ready: true, restart count 0
Jan 19 21:51:28.769: INFO: observability-operator-catalog-vrnkr from openshift-observability-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.769: INFO: ocm-agent-operator-registry-dwfc4 from openshift-ocm-agent-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.769: INFO: collect-profiles-27902715-lq9hm from openshift-operator-lifecycle-manager started at 2023-01-19 21:15:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 21:51:28.769: INFO: ovnkube-node-wcrc4 from openshift-ovn-kubernetes started at 2023-01-19 18:15:13 +0000 UTC (5 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 21:51:28.769: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 21:51:28.769: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 21:51:28.769: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 21:51:28.769: INFO: route-monitor-operator-registry-whpgh from openshift-route-monitor-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.769: INFO: splunkforwarder-ds-xzbdk from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 21:51:28.769: INFO: splunk-forwarder-operator-catalog-fqp2b from openshift-splunk-forwarder-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.769: INFO: prometheus-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:37:05 +0000 UTC (6 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 21:51:28.769: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 21:51:28.769: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 21:51:28.769: INFO: thanos-ruler-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:37:04 +0000 UTC (3 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:51:28.769: INFO: 	Container thanos-ruler ready: true, restart count 0
Jan 19 21:51:28.769: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
Jan 19 21:51:28.769: INFO: managed-velero-operator-registry-6lc6b from openshift-velero started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 21:51:28.769: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-q5j5c from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.769: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:51:28.769: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 21:51:28.769: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-207-77.ec2.internal before test
Jan 19 21:51:28.812: INFO: addon-operator-webhooks-67d9d47489-f6z86 from openshift-addon-operator started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container webhook ready: true, restart count 0
Jan 19 21:51:28.812: INFO: osd-delete-ownerrefs-serviceaccounts-27902677-st5fv from openshift-backplane-srep started at 2023-01-19 20:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 21:51:28.812: INFO: osd-delete-ownerrefs-serviceaccounts-27902707-pxrnh from openshift-backplane-srep started at 2023-01-19 21:07:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 21:51:28.812: INFO: osd-delete-ownerrefs-serviceaccounts-27902737-ww4vb from openshift-backplane-srep started at 2023-01-19 21:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 21:51:28.812: INFO: osd-delete-backplane-serviceaccounts-27902730-ph7cv from openshift-backplane started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 21:51:28.812: INFO: osd-delete-backplane-serviceaccounts-27902740-x6pck from openshift-backplane started at 2023-01-19 21:40:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 21:51:28.812: INFO: osd-delete-backplane-serviceaccounts-27902750-b5lmz from openshift-backplane started at 2023-01-19 21:50:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 21:51:28.812: INFO: sre-build-test-27902711-r8jct from openshift-build-test started at 2023-01-19 21:11:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container sre-build-test ready: false, restart count 0
Jan 19 21:51:28.812: INFO: cloud-ingress-operator-5b5df787b5-bdk4j from openshift-cloud-ingress-operator started at 2023-01-19 19:15:07 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Jan 19 21:51:28.812: INFO: aws-ebs-csi-driver-node-9sjjv from openshift-cluster-csi-drivers started at 2023-01-19 18:33:36 +0000 UTC (3 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 21:51:28.812: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 21:51:28.812: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 21:51:28.812: INFO: tuned-qkjmb from openshift-cluster-node-tuning-operator started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container tuned ready: true, restart count 1
Jan 19 21:51:28.812: INFO: dns-default-bcrdt from openshift-dns started at 2023-01-19 21:45:58 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container dns ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:51:28.812: INFO: node-resolver-m4bvp from openshift-dns started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 21:51:28.812: INFO: image-registry-656cfd9bd9-bj4wn from openshift-image-registry started at 2023-01-19 19:15:47 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container registry ready: true, restart count 0
Jan 19 21:51:28.812: INFO: node-ca-kwgfx from openshift-image-registry started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 21:51:28.812: INFO: ingress-canary-6xnfp from openshift-ingress-canary started at 2023-01-19 18:34:10 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 21:51:28.812: INFO: router-default-6bf5ff7dcb-6vrcg from openshift-ingress started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container router ready: true, restart count 0
Jan 19 21:51:28.812: INFO: machine-config-daemon-2bt5b from openshift-machine-config-operator started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 21:51:28.812: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 21:51:28.812: INFO: managed-node-metadata-operator-649f646d54-smsh7 from openshift-managed-node-metadata-operator started at 2023-01-19 19:16:23 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
Jan 19 21:51:28.812: INFO: osd-patch-subscription-source-27902580-hz9tz from openshift-marketplace started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 21:51:28.812: INFO: osd-patch-subscription-source-27902640-qrd58 from openshift-marketplace started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 21:51:28.812: INFO: osd-patch-subscription-source-27902700-smwhh from openshift-marketplace started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 21:51:28.812: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container alertmanager ready: true, restart count 1
Jan 19 21:51:28.812: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 21:51:28.812: INFO: node-exporter-s77nf from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.812: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 21:51:28.812: INFO: osd-rebalance-infra-nodes-27902715-hm9rx from openshift-monitoring started at 2023-01-19 21:15:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 21:51:28.812: INFO: osd-rebalance-infra-nodes-27902730-h7lrq from openshift-monitoring started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 21:51:28.812: INFO: osd-rebalance-infra-nodes-27902745-cwdn2 from openshift-monitoring started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 21:51:28.812: INFO: prometheus-adapter-5bc6487f7c-sv96c from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 19 21:51:28.812: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 21:51:28.812: INFO: prometheus-operator-admission-webhook-749df5cf4f-bmj4n from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 21:51:28.812: INFO: sre-dns-latency-exporter-4lksg from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container main ready: true, restart count 1
Jan 19 21:51:28.812: INFO: sre-ebs-iops-reporter-1-sg7b2 from openshift-monitoring started at 2023-01-19 19:16:28 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container main ready: true, restart count 0
Jan 19 21:51:28.812: INFO: thanos-querier-fcd5b6c8d-9jjkp from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 21:51:28.812: INFO: 	Container thanos-query ready: true, restart count 0
Jan 19 21:51:28.812: INFO: token-refresher-6f7c46d758-mld9t from openshift-monitoring started at 2023-01-19 19:17:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container token-refresher ready: true, restart count 0
Jan 19 21:51:28.812: INFO: multus-additional-cni-plugins-476gw from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 21:51:28.812: INFO: multus-mczpf from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 21:51:28.812: INFO: network-metrics-daemon-ddkb6 from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.812: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 21:51:28.812: INFO: network-check-target-d8k5s from openshift-network-diagnostics started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.812: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 21:51:28.812: INFO: osd-metrics-exporter-679d75d598-rsv49 from openshift-osd-metrics started at 2023-01-19 19:17:47 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.813: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
Jan 19 21:51:28.813: INFO: ovnkube-node-gq9vc from openshift-ovn-kubernetes started at 2023-01-19 18:33:36 +0000 UTC (5 container statuses recorded)
Jan 19 21:51:28.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 21:51:28.813: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 21:51:28.813: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 21:51:28.813: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 21:51:28.813: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 21:51:28.813: INFO: blackbox-exporter-5f7f7bf859-l8ptw from openshift-route-monitor-operator started at 2023-01-19 19:17:51 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.813: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 19 21:51:28.813: INFO: route-monitor-operator-controller-manager-bfbff575-pfqc4 from openshift-route-monitor-operator started at 2023-01-19 19:17:53 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.813: INFO: 	Container manager ready: true, restart count 0
Jan 19 21:51:28.813: INFO: splunkforwarder-ds-h56tr from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.813: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 21:51:28.813: INFO: builds-pruner-27902580-p9bnq from openshift-sre-pruning started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.813: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 21:51:28.813: INFO: builds-pruner-27902640-bj57l from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.813: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 21:51:28.813: INFO: builds-pruner-27902700-t7v8n from openshift-sre-pruning started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.813: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 21:51:28.813: INFO: deployments-pruner-27902580-qfgtd from openshift-sre-pruning started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.813: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 21:51:28.813: INFO: deployments-pruner-27902640-vhqsv from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.813: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 21:51:28.813: INFO: deployments-pruner-27902700-fvhm7 from openshift-sre-pruning started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 21:51:28.813: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 21:51:28.813: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-hj9fv from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 21:51:28.813: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 21:51:28.813: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/19/23 21:51:28.813
Jan 19 21:51:28.829: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4552" to be "running"
Jan 19 21:51:28.832: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.728273ms
Jan 19 21:51:30.836: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006848493s
Jan 19 21:51:30.836: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/19/23 21:51:30.839
STEP: Trying to apply a random label on the found node. 01/19/23 21:51:30.855
STEP: verifying the node has the label kubernetes.io/e2e-aaa484df-6968-4112-9c88-8bf7ea353098 95 01/19/23 21:51:30.869
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/19/23 21:51:30.892
Jan 19 21:51:30.919: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4552" to be "not pending"
Jan 19 21:51:30.929: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.308531ms
Jan 19 21:51:32.947: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.028716851s
Jan 19 21:51:32.947: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.171.213 on the node which pod4 resides and expect not scheduled 01/19/23 21:51:32.947
Jan 19 21:51:32.959: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4552" to be "not pending"
Jan 19 21:51:32.963: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.293325ms
Jan 19 21:51:34.991: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031734706s
Jan 19 21:51:36.968: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008262642s
Jan 19 21:51:38.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005749115s
Jan 19 21:51:40.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007358296s
Jan 19 21:51:42.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006767259s
Jan 19 21:51:44.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007528852s
Jan 19 21:51:46.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006218067s
Jan 19 21:51:48.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007569623s
Jan 19 21:51:50.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006697213s
Jan 19 21:51:52.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007096938s
Jan 19 21:51:54.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007662401s
Jan 19 21:51:56.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006575534s
Jan 19 21:51:58.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008120074s
Jan 19 21:52:00.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006781643s
Jan 19 21:52:02.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006924337s
Jan 19 21:52:04.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006880689s
Jan 19 21:52:06.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006675585s
Jan 19 21:52:08.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006244013s
Jan 19 21:52:10.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007376029s
Jan 19 21:52:12.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006402755s
Jan 19 21:52:14.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.007040162s
Jan 19 21:52:16.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006861753s
Jan 19 21:52:18.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007491902s
Jan 19 21:52:20.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006533116s
Jan 19 21:52:22.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007100188s
Jan 19 21:52:24.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007429753s
Jan 19 21:52:26.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006579225s
Jan 19 21:52:28.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006118101s
Jan 19 21:52:30.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.00672274s
Jan 19 21:52:32.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007280239s
Jan 19 21:52:34.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006707444s
Jan 19 21:52:36.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005723672s
Jan 19 21:52:38.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006080099s
Jan 19 21:52:40.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006658052s
Jan 19 21:52:42.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006026646s
Jan 19 21:52:44.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00804641s
Jan 19 21:52:46.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007046239s
Jan 19 21:52:48.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007461278s
Jan 19 21:52:50.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006627254s
Jan 19 21:52:52.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006924096s
Jan 19 21:52:54.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006645043s
Jan 19 21:52:56.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006670873s
Jan 19 21:52:58.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006516378s
Jan 19 21:53:00.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006717651s
Jan 19 21:53:02.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.006988259s
Jan 19 21:53:04.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006864835s
Jan 19 21:53:06.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006136877s
Jan 19 21:53:08.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007405397s
Jan 19 21:53:10.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006624983s
Jan 19 21:53:12.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006760407s
Jan 19 21:53:14.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007535383s
Jan 19 21:53:16.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006259771s
Jan 19 21:53:18.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008234155s
Jan 19 21:53:20.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00628514s
Jan 19 21:53:22.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007755721s
Jan 19 21:53:24.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.007450678s
Jan 19 21:53:26.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006934225s
Jan 19 21:53:28.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006238935s
Jan 19 21:53:30.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006866914s
Jan 19 21:53:32.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006897533s
Jan 19 21:53:34.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.006094399s
Jan 19 21:53:36.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006725022s
Jan 19 21:53:38.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006938461s
Jan 19 21:53:40.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.007002133s
Jan 19 21:53:42.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.007024603s
Jan 19 21:53:44.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.007547258s
Jan 19 21:53:46.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.006419534s
Jan 19 21:53:48.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.006884318s
Jan 19 21:53:50.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.00622338s
Jan 19 21:53:52.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008207849s
Jan 19 21:53:54.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007902056s
Jan 19 21:53:56.968: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.00841107s
Jan 19 21:53:58.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007434107s
Jan 19 21:54:00.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.005916525s
Jan 19 21:54:02.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007090547s
Jan 19 21:54:04.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.007660305s
Jan 19 21:54:06.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.006137081s
Jan 19 21:54:08.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.00698551s
Jan 19 21:54:10.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.006405479s
Jan 19 21:54:12.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.006509629s
Jan 19 21:54:14.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.006822783s
Jan 19 21:54:16.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.006324102s
Jan 19 21:54:18.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.02745248s
Jan 19 21:54:20.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.006491179s
Jan 19 21:54:22.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.00686865s
Jan 19 21:54:24.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007620225s
Jan 19 21:54:26.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.006159714s
Jan 19 21:54:28.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007424472s
Jan 19 21:54:30.989: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.030118893s
Jan 19 21:54:32.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.007817901s
Jan 19 21:54:34.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006888504s
Jan 19 21:54:36.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.006531581s
Jan 19 21:54:38.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007616782s
Jan 19 21:54:40.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.007436721s
Jan 19 21:54:42.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.00689993s
Jan 19 21:54:44.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007174142s
Jan 19 21:54:46.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.005951917s
Jan 19 21:54:48.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007408131s
Jan 19 21:54:50.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007083883s
Jan 19 21:54:52.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.007588095s
Jan 19 21:54:54.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.007102994s
Jan 19 21:54:56.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.00702965s
Jan 19 21:54:58.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.0068198s
Jan 19 21:55:00.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.006679993s
Jan 19 21:55:02.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007044511s
Jan 19 21:55:04.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.007197125s
Jan 19 21:55:06.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.007138494s
Jan 19 21:55:08.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.006879899s
Jan 19 21:55:10.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006757802s
Jan 19 21:55:12.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.007604008s
Jan 19 21:55:14.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.006002566s
Jan 19 21:55:16.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.006146303s
Jan 19 21:55:18.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.007364685s
Jan 19 21:55:20.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006030196s
Jan 19 21:55:22.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.007013637s
Jan 19 21:55:24.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007466758s
Jan 19 21:55:26.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.00624481s
Jan 19 21:55:28.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007105041s
Jan 19 21:55:30.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006116341s
Jan 19 21:55:32.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007739387s
Jan 19 21:55:34.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007277902s
Jan 19 21:55:36.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.007649357s
Jan 19 21:55:38.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007688411s
Jan 19 21:55:40.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.007361896s
Jan 19 21:55:42.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.005915519s
Jan 19 21:55:44.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.007595782s
Jan 19 21:55:46.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.006226979s
Jan 19 21:55:48.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007830039s
Jan 19 21:55:50.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.00660483s
Jan 19 21:55:52.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00718625s
Jan 19 21:55:54.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007757524s
Jan 19 21:55:56.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.00658081s
Jan 19 21:55:58.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.008193617s
Jan 19 21:56:00.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.006568591s
Jan 19 21:56:02.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007070561s
Jan 19 21:56:04.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.007409222s
Jan 19 21:56:06.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.006407745s
Jan 19 21:56:08.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007250978s
Jan 19 21:56:10.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.006742057s
Jan 19 21:56:12.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007136449s
Jan 19 21:56:14.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.007551327s
Jan 19 21:56:16.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006200201s
Jan 19 21:56:18.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.007616104s
Jan 19 21:56:20.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006186422s
Jan 19 21:56:22.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.007750143s
Jan 19 21:56:24.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007676663s
Jan 19 21:56:26.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.005993565s
Jan 19 21:56:28.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007165184s
Jan 19 21:56:30.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.006066879s
Jan 19 21:56:32.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007523829s
Jan 19 21:56:32.969: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009653508s
STEP: removing the label kubernetes.io/e2e-aaa484df-6968-4112-9c88-8bf7ea353098 off the node ip-10-0-171-213.ec2.internal 01/19/23 21:56:32.969
STEP: verifying the node doesn't have the label kubernetes.io/e2e-aaa484df-6968-4112-9c88-8bf7ea353098 01/19/23 21:56:32.984
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:56:32.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4552" for this suite. 01/19/23 21:56:32.998
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":312,"skipped":5822,"failed":0}
------------------------------
• [SLOW TEST] [304.486 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:51:28.519
    Jan 19 21:51:28.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename sched-pred 01/19/23 21:51:28.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:51:28.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:51:28.544
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 19 21:51:28.546: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 19 21:51:28.576: INFO: Waiting for terminating namespaces to be deleted...
    Jan 19 21:51:28.588: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-146-42.ec2.internal before test
    Jan 19 21:51:28.626: INFO: aws-ebs-csi-driver-node-vxgq2 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:41 +0000 UTC (3 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: tuned-nbzt4 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: downloads-8d695cd69-lcsd9 from openshift-console started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container download-server ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: dns-default-c5qwq from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container dns ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: node-resolver-g5jk6 from openshift-dns started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: node-ca-crm7t from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: ingress-canary-nfwhl from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: migrator-5c54d8d69d-l5lss from openshift-kube-storage-version-migrator started at 2023-01-19 18:42:53 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container migrator ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: machine-config-daemon-cc29b from openshift-machine-config-operator started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: node-exporter-9zc87 from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: sre-dns-latency-exporter-jsn9x from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container main ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: multus-5zck8 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: multus-additional-cni-plugins-88bc2 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: network-metrics-daemon-zvctv from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: network-check-source-746dd6c885-bmf5k from openshift-network-diagnostics started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container check-endpoints ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: network-check-target-g5qsk from openshift-network-diagnostics started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: obo-prometheus-operator-64989cfc68-hdxb9 from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container prometheus-operator ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-gqhlz from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: osd-metrics-exporter-registry-qdjtk from openshift-osd-metrics started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: ovnkube-node-ftv9n from openshift-ovn-kubernetes started at 2023-01-19 18:15:41 +0000 UTC (5 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: 	Container ovn-acl-logging ready: true, restart count 2
    Jan 19 21:51:28.626: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: rbac-permissions-operator-registry-rc4js from openshift-rbac-permissions started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: splunkforwarder-ds-7rpx7 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 21:51:28.626: INFO: prometheus-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (6 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: thanos-ruler-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (3 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: 	Container thanos-ruler ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-chbvs from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.626: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 21:51:28.626: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-151-158.ec2.internal before test
    Jan 19 21:51:28.666: INFO: addon-operator-manager-84ff88fc6-fmhnb from openshift-addon-operator started at 2023-01-19 18:46:50 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container manager ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container metrics-relay-server ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: addon-operator-webhooks-67d9d47489-4kbk2 from openshift-addon-operator started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container webhook ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: aws-ebs-csi-driver-node-c8m6c from openshift-cluster-csi-drivers started at 2023-01-19 18:32:48 +0000 UTC (3 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: tuned-5h69k from openshift-cluster-node-tuning-operator started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: custom-domains-operator-5764f4df49-8nbdk from openshift-custom-domains-operator started at 2023-01-19 19:15:08 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container custom-domains-operator ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: deployment-validation-operator-7b8fd479cb-wtzg7 from openshift-deployment-validation-operator started at 2023-01-19 18:46:50 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container deployment-validation-operator ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: dns-default-d9szl from openshift-dns started at 2023-01-19 21:45:22 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container dns ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: node-resolver-9cwpx from openshift-dns started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: image-pruner-27902580-lzd8r from openshift-image-registry started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container image-pruner ready: false, restart count 0
    Jan 19 21:51:28.666: INFO: image-pruner-27902640-c6mlt from openshift-image-registry started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container image-pruner ready: false, restart count 0
    Jan 19 21:51:28.666: INFO: image-pruner-27902700-cf7x6 from openshift-image-registry started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container image-pruner ready: false, restart count 0
    Jan 19 21:51:28.666: INFO: image-registry-656cfd9bd9-9x67r from openshift-image-registry started at 2023-01-19 19:15:11 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container registry ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: node-ca-k89p7 from openshift-image-registry started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: ingress-canary-v2tjj from openshift-ingress-canary started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: router-default-6bf5ff7dcb-5nzvc from openshift-ingress started at 2023-01-19 18:47:44 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container router ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: machine-config-daemon-28fzw from openshift-machine-config-operator started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: configure-alertmanager-operator-7d94bbbcf-8h7xs from openshift-monitoring started at 2023-01-19 19:16:27 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: kube-state-metrics-666f4cbf77-5w8rk from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: node-exporter-7t5b7 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: openshift-state-metrics-7d5c9d867c-97gzd from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: prometheus-adapter-5bc6487f7c-q2ng7 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: prometheus-operator-564b78d5ff-v4xnk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container prometheus-operator ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: prometheus-operator-admission-webhook-749df5cf4f-zj8xj from openshift-monitoring started at 2023-01-19 18:46:40 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: sre-dns-latency-exporter-2q899 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container main ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: sre-stuck-ebs-vols-1-vdlrh from openshift-monitoring started at 2023-01-19 19:17:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container main ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: telemeter-client-54bbbbf598-d9fdk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (3 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container reload ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container telemeter-client ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: thanos-querier-fcd5b6c8d-ql8qb from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: multus-additional-cni-plugins-vslld from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: multus-zqhtq from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: network-metrics-daemon-677lg from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: must-gather-operator-56b466776c-qv8sh from openshift-must-gather-operator started at 2023-01-19 19:17:38 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container must-gather-operator ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: network-check-target-xrr4d from openshift-network-diagnostics started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: ocm-agent-ddfdc6544-4fxrp from openshift-ocm-agent-operator started at 2023-01-19 19:17:41 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container ocm-agent ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: ocm-agent-operator-6cdb45fd86-4clcj from openshift-ocm-agent-operator started at 2023-01-19 19:17:43 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container ocm-agent-operator ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: ovnkube-node-gvwrr from openshift-ovn-kubernetes started at 2023-01-19 18:32:48 +0000 UTC (5 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: rbac-permissions-operator-5fb57974d8-htnjj from openshift-rbac-permissions started at 2023-01-19 19:17:50 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: splunkforwarder-ds-ztlpn from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 21:51:28.666: INFO: splunk-forwarder-operator-7d66568cf7-bksrn from openshift-splunk-forwarder-operator started at 2023-01-19 19:17:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: managed-velero-operator-788754d4b6-n89fw from openshift-velero started at 2023-01-19 19:17:56 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container managed-velero-operator ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: velero-7c64598b8c-8blx4 from openshift-velero started at 2023-01-19 19:18:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container velero ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-8mc8n from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.666: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 21:51:28.666: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-171-213.ec2.internal before test
    Jan 19 21:51:28.699: INFO: aws-ebs-csi-driver-node-svd2s from openshift-cluster-csi-drivers started at 2023-01-19 18:15:58 +0000 UTC (3 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: tuned-9p2db from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: dns-default-zbx44 from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container dns ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: node-resolver-5mb95 from openshift-dns started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: node-ca-7c5fq from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: ingress-canary-6sc9q from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: machine-config-daemon-xxm6j from openshift-machine-config-operator started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: node-exporter-h445b from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: sre-dns-latency-exporter-vhwwx from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container main ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: multus-additional-cni-plugins-nhwfz from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: multus-j4ngm from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: network-metrics-daemon-hp85h from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: network-check-target-mg7p4 from openshift-network-diagnostics started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: ovnkube-node-kk5t9 from openshift-ovn-kubernetes started at 2023-01-19 18:15:58 +0000 UTC (5 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: splunkforwarder-ds-66kjq from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 21:51:28.699: INFO: sonobuoy-e2e-job-4d801e056c4540b8 from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container e2e ready: true, restart count 0
    Jan 19 21:51:28.699: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:51:28.699: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-m8vnk from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.699: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:51:28.699: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 21:51:28.699: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-172-44.ec2.internal before test
    Jan 19 21:51:28.733: INFO: ephemeral-containers-target-pod from ephemeral-containers-test-4608 started at 2023-01-19 21:51:19 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container test-container-1 ready: true, restart count 0
    Jan 19 21:51:28.733: INFO: aws-ebs-csi-driver-node-z6klb from openshift-cluster-csi-drivers started at 2023-01-19 18:15:33 +0000 UTC (3 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: tuned-gjnt5 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: dns-default-dfndw from openshift-dns started at 2023-01-19 21:22:44 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container dns ready: true, restart count 0
    Jan 19 21:51:28.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:51:28.733: INFO: node-resolver-b85qs from openshift-dns started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: node-ca-dsxvs from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: ingress-canary-cs8pq from openshift-ingress-canary started at 2023-01-19 21:22:44 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 19 21:51:28.733: INFO: machine-config-daemon-cllj8 from openshift-machine-config-operator started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: node-exporter-2vp9v from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: sre-dns-latency-exporter-ctmgr from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container main ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: multus-additional-cni-plugins-p5pjp from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: multus-xzxwg from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: network-metrics-daemon-hz5pj from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: network-check-target-nq9ln from openshift-network-diagnostics started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: collect-profiles-27902730-zm5kk from openshift-operator-lifecycle-manager started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 19 21:51:28.733: INFO: collect-profiles-27902745-9gctc from openshift-operator-lifecycle-manager started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 19 21:51:28.733: INFO: ovnkube-node-zmvs8 from openshift-ovn-kubernetes started at 2023-01-19 18:15:33 +0000 UTC (5 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: splunkforwarder-ds-pj5q8 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 21:51:28.733: INFO: sonobuoy from sonobuoy started at 2023-01-19 20:29:23 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 19 21:51:28.733: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-cbfjg from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.733: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:51:28.733: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 21:51:28.733: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-188-71.ec2.internal before test
    Jan 19 21:51:28.768: INFO: addon-operator-catalog-zxs8v from openshift-addon-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.768: INFO: cloud-ingress-operator-registry-pqwm7 from openshift-cloud-ingress-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.768: INFO: aws-ebs-csi-driver-node-wmvd4 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:13 +0000 UTC (3 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 21:51:28.768: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 21:51:28.768: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 21:51:28.768: INFO: tuned-7x9t2 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 21:51:28.768: INFO: downloads-8d695cd69-hltgd from openshift-console started at 2023-01-19 18:39:20 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container download-server ready: true, restart count 0
    Jan 19 21:51:28.768: INFO: custom-domains-operator-registry-kdw7s from openshift-custom-domains-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.768: INFO: deployment-validation-operator-catalog-nq94w from openshift-deployment-validation-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.768: INFO: dns-default-tzrx6 from openshift-dns started at 2023-01-19 18:15:43 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container dns ready: true, restart count 1
    Jan 19 21:51:28.768: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.768: INFO: node-resolver-l2fzw from openshift-dns started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 21:51:28.768: INFO: node-ca-kzsvk from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 21:51:28.768: INFO: ingress-canary-t6tqp from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 21:51:28.768: INFO: machine-config-daemon-zhllf from openshift-machine-config-operator started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 21:51:28.768: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 21:51:28.768: INFO: managed-node-metadata-operator-registry-m6dxz from openshift-managed-node-metadata-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.768: INFO: managed-upgrade-operator-catalog-thxts from openshift-managed-upgrade-operator started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.768: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.768: INFO: configure-alertmanager-operator-registry-sjcb5 from openshift-monitoring started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: node-exporter-dc59d from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: osd-cluster-ready-nb8xk from openshift-monitoring started at 2023-01-19 18:39:21 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container osd-cluster-ready ready: false, restart count 7
    Jan 19 21:51:28.769: INFO: sre-dns-latency-exporter-5dz25 from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container main ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: multus-additional-cni-plugins-hvpd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: multus-mptd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: network-metrics-daemon-s2l94 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: must-gather-operator-registry-n7fvg from openshift-must-gather-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: network-check-target-d8gvt from openshift-network-diagnostics started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-b4r6s from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: observability-operator-6dd79df7dc-z7frh from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container operator ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: observability-operator-catalog-vrnkr from openshift-observability-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: ocm-agent-operator-registry-dwfc4 from openshift-ocm-agent-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: collect-profiles-27902715-lq9hm from openshift-operator-lifecycle-manager started at 2023-01-19 21:15:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 19 21:51:28.769: INFO: ovnkube-node-wcrc4 from openshift-ovn-kubernetes started at 2023-01-19 18:15:13 +0000 UTC (5 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: route-monitor-operator-registry-whpgh from openshift-route-monitor-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: splunkforwarder-ds-xzbdk from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 21:51:28.769: INFO: splunk-forwarder-operator-catalog-fqp2b from openshift-splunk-forwarder-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: prometheus-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:37:05 +0000 UTC (6 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: thanos-ruler-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:37:04 +0000 UTC (3 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: 	Container thanos-ruler ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: managed-velero-operator-registry-6lc6b from openshift-velero started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-q5j5c from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.769: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 21:51:28.769: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-207-77.ec2.internal before test
    Jan 19 21:51:28.812: INFO: addon-operator-webhooks-67d9d47489-f6z86 from openshift-addon-operator started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container webhook ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: osd-delete-ownerrefs-serviceaccounts-27902677-st5fv from openshift-backplane-srep started at 2023-01-19 20:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: osd-delete-ownerrefs-serviceaccounts-27902707-pxrnh from openshift-backplane-srep started at 2023-01-19 21:07:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: osd-delete-ownerrefs-serviceaccounts-27902737-ww4vb from openshift-backplane-srep started at 2023-01-19 21:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: osd-delete-backplane-serviceaccounts-27902730-ph7cv from openshift-backplane started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: osd-delete-backplane-serviceaccounts-27902740-x6pck from openshift-backplane started at 2023-01-19 21:40:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: osd-delete-backplane-serviceaccounts-27902750-b5lmz from openshift-backplane started at 2023-01-19 21:50:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: sre-build-test-27902711-r8jct from openshift-build-test started at 2023-01-19 21:11:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container sre-build-test ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: cloud-ingress-operator-5b5df787b5-bdk4j from openshift-cloud-ingress-operator started at 2023-01-19 19:15:07 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: aws-ebs-csi-driver-node-9sjjv from openshift-cluster-csi-drivers started at 2023-01-19 18:33:36 +0000 UTC (3 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: tuned-qkjmb from openshift-cluster-node-tuning-operator started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: dns-default-bcrdt from openshift-dns started at 2023-01-19 21:45:58 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container dns ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: node-resolver-m4bvp from openshift-dns started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: image-registry-656cfd9bd9-bj4wn from openshift-image-registry started at 2023-01-19 19:15:47 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container registry ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: node-ca-kwgfx from openshift-image-registry started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: ingress-canary-6xnfp from openshift-ingress-canary started at 2023-01-19 18:34:10 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: router-default-6bf5ff7dcb-6vrcg from openshift-ingress started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container router ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: machine-config-daemon-2bt5b from openshift-machine-config-operator started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: managed-node-metadata-operator-649f646d54-smsh7 from openshift-managed-node-metadata-operator started at 2023-01-19 19:16:23 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: osd-patch-subscription-source-27902580-hz9tz from openshift-marketplace started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: osd-patch-subscription-source-27902640-qrd58 from openshift-marketplace started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: osd-patch-subscription-source-27902700-smwhh from openshift-marketplace started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: node-exporter-s77nf from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: osd-rebalance-infra-nodes-27902715-hm9rx from openshift-monitoring started at 2023-01-19 21:15:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: osd-rebalance-infra-nodes-27902730-h7lrq from openshift-monitoring started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: osd-rebalance-infra-nodes-27902745-cwdn2 from openshift-monitoring started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
    Jan 19 21:51:28.812: INFO: prometheus-adapter-5bc6487f7c-sv96c from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: prometheus-operator-admission-webhook-749df5cf4f-bmj4n from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: sre-dns-latency-exporter-4lksg from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container main ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: sre-ebs-iops-reporter-1-sg7b2 from openshift-monitoring started at 2023-01-19 19:16:28 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container main ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: thanos-querier-fcd5b6c8d-9jjkp from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: token-refresher-6f7c46d758-mld9t from openshift-monitoring started at 2023-01-19 19:17:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container token-refresher ready: true, restart count 0
    Jan 19 21:51:28.812: INFO: multus-additional-cni-plugins-476gw from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: multus-mczpf from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: network-metrics-daemon-ddkb6 from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: network-check-target-d8k5s from openshift-network-diagnostics started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.812: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 21:51:28.812: INFO: osd-metrics-exporter-679d75d598-rsv49 from openshift-osd-metrics started at 2023-01-19 19:17:47 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.813: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
    Jan 19 21:51:28.813: INFO: ovnkube-node-gq9vc from openshift-ovn-kubernetes started at 2023-01-19 18:33:36 +0000 UTC (5 container statuses recorded)
    Jan 19 21:51:28.813: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 21:51:28.813: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 21:51:28.813: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 21:51:28.813: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 21:51:28.813: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 21:51:28.813: INFO: blackbox-exporter-5f7f7bf859-l8ptw from openshift-route-monitor-operator started at 2023-01-19 19:17:51 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.813: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Jan 19 21:51:28.813: INFO: route-monitor-operator-controller-manager-bfbff575-pfqc4 from openshift-route-monitor-operator started at 2023-01-19 19:17:53 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.813: INFO: 	Container manager ready: true, restart count 0
    Jan 19 21:51:28.813: INFO: splunkforwarder-ds-h56tr from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.813: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 21:51:28.813: INFO: builds-pruner-27902580-p9bnq from openshift-sre-pruning started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.813: INFO: 	Container builds-pruner ready: false, restart count 0
    Jan 19 21:51:28.813: INFO: builds-pruner-27902640-bj57l from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.813: INFO: 	Container builds-pruner ready: false, restart count 0
    Jan 19 21:51:28.813: INFO: builds-pruner-27902700-t7v8n from openshift-sre-pruning started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.813: INFO: 	Container builds-pruner ready: false, restart count 0
    Jan 19 21:51:28.813: INFO: deployments-pruner-27902580-qfgtd from openshift-sre-pruning started at 2023-01-19 19:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.813: INFO: 	Container deployments-pruner ready: false, restart count 0
    Jan 19 21:51:28.813: INFO: deployments-pruner-27902640-vhqsv from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.813: INFO: 	Container deployments-pruner ready: false, restart count 0
    Jan 19 21:51:28.813: INFO: deployments-pruner-27902700-fvhm7 from openshift-sre-pruning started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 21:51:28.813: INFO: 	Container deployments-pruner ready: false, restart count 0
    Jan 19 21:51:28.813: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-hj9fv from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 21:51:28.813: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 21:51:28.813: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/19/23 21:51:28.813
    Jan 19 21:51:28.829: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4552" to be "running"
    Jan 19 21:51:28.832: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.728273ms
    Jan 19 21:51:30.836: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006848493s
    Jan 19 21:51:30.836: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/19/23 21:51:30.839
    STEP: Trying to apply a random label on the found node. 01/19/23 21:51:30.855
    STEP: verifying the node has the label kubernetes.io/e2e-aaa484df-6968-4112-9c88-8bf7ea353098 95 01/19/23 21:51:30.869
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/19/23 21:51:30.892
    Jan 19 21:51:30.919: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4552" to be "not pending"
    Jan 19 21:51:30.929: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.308531ms
    Jan 19 21:51:32.947: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.028716851s
    Jan 19 21:51:32.947: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.171.213 on the node which pod4 resides and expect not scheduled 01/19/23 21:51:32.947
    Jan 19 21:51:32.959: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4552" to be "not pending"
    Jan 19 21:51:32.963: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.293325ms
    Jan 19 21:51:34.991: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031734706s
    Jan 19 21:51:36.968: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008262642s
    Jan 19 21:51:38.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005749115s
    Jan 19 21:51:40.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007358296s
    Jan 19 21:51:42.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.006767259s
    Jan 19 21:51:44.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007528852s
    Jan 19 21:51:46.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006218067s
    Jan 19 21:51:48.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.007569623s
    Jan 19 21:51:50.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006697213s
    Jan 19 21:51:52.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007096938s
    Jan 19 21:51:54.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007662401s
    Jan 19 21:51:56.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006575534s
    Jan 19 21:51:58.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008120074s
    Jan 19 21:52:00.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006781643s
    Jan 19 21:52:02.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.006924337s
    Jan 19 21:52:04.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.006880689s
    Jan 19 21:52:06.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006675585s
    Jan 19 21:52:08.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006244013s
    Jan 19 21:52:10.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007376029s
    Jan 19 21:52:12.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006402755s
    Jan 19 21:52:14.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.007040162s
    Jan 19 21:52:16.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.006861753s
    Jan 19 21:52:18.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007491902s
    Jan 19 21:52:20.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.006533116s
    Jan 19 21:52:22.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007100188s
    Jan 19 21:52:24.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.007429753s
    Jan 19 21:52:26.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006579225s
    Jan 19 21:52:28.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006118101s
    Jan 19 21:52:30.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.00672274s
    Jan 19 21:52:32.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007280239s
    Jan 19 21:52:34.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006707444s
    Jan 19 21:52:36.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.005723672s
    Jan 19 21:52:38.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006080099s
    Jan 19 21:52:40.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.006658052s
    Jan 19 21:52:42.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006026646s
    Jan 19 21:52:44.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00804641s
    Jan 19 21:52:46.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007046239s
    Jan 19 21:52:48.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007461278s
    Jan 19 21:52:50.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.006627254s
    Jan 19 21:52:52.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006924096s
    Jan 19 21:52:54.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006645043s
    Jan 19 21:52:56.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.006670873s
    Jan 19 21:52:58.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006516378s
    Jan 19 21:53:00.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006717651s
    Jan 19 21:53:02.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.006988259s
    Jan 19 21:53:04.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.006864835s
    Jan 19 21:53:06.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006136877s
    Jan 19 21:53:08.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007405397s
    Jan 19 21:53:10.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.006624983s
    Jan 19 21:53:12.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.006760407s
    Jan 19 21:53:14.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.007535383s
    Jan 19 21:53:16.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006259771s
    Jan 19 21:53:18.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008234155s
    Jan 19 21:53:20.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00628514s
    Jan 19 21:53:22.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007755721s
    Jan 19 21:53:24.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.007450678s
    Jan 19 21:53:26.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006934225s
    Jan 19 21:53:28.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.006238935s
    Jan 19 21:53:30.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006866914s
    Jan 19 21:53:32.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006897533s
    Jan 19 21:53:34.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.006094399s
    Jan 19 21:53:36.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006725022s
    Jan 19 21:53:38.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.006938461s
    Jan 19 21:53:40.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.007002133s
    Jan 19 21:53:42.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.007024603s
    Jan 19 21:53:44.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.007547258s
    Jan 19 21:53:46.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.006419534s
    Jan 19 21:53:48.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.006884318s
    Jan 19 21:53:50.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.00622338s
    Jan 19 21:53:52.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008207849s
    Jan 19 21:53:54.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007902056s
    Jan 19 21:53:56.968: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.00841107s
    Jan 19 21:53:58.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007434107s
    Jan 19 21:54:00.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.005916525s
    Jan 19 21:54:02.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007090547s
    Jan 19 21:54:04.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.007660305s
    Jan 19 21:54:06.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.006137081s
    Jan 19 21:54:08.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.00698551s
    Jan 19 21:54:10.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.006405479s
    Jan 19 21:54:12.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.006509629s
    Jan 19 21:54:14.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.006822783s
    Jan 19 21:54:16.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.006324102s
    Jan 19 21:54:18.987: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.02745248s
    Jan 19 21:54:20.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.006491179s
    Jan 19 21:54:22.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.00686865s
    Jan 19 21:54:24.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007620225s
    Jan 19 21:54:26.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.006159714s
    Jan 19 21:54:28.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007424472s
    Jan 19 21:54:30.989: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.030118893s
    Jan 19 21:54:32.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.007817901s
    Jan 19 21:54:34.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.006888504s
    Jan 19 21:54:36.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.006531581s
    Jan 19 21:54:38.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007616782s
    Jan 19 21:54:40.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.007436721s
    Jan 19 21:54:42.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.00689993s
    Jan 19 21:54:44.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007174142s
    Jan 19 21:54:46.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.005951917s
    Jan 19 21:54:48.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007408131s
    Jan 19 21:54:50.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007083883s
    Jan 19 21:54:52.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.007588095s
    Jan 19 21:54:54.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.007102994s
    Jan 19 21:54:56.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.00702965s
    Jan 19 21:54:58.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.0068198s
    Jan 19 21:55:00.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.006679993s
    Jan 19 21:55:02.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007044511s
    Jan 19 21:55:04.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.007197125s
    Jan 19 21:55:06.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.007138494s
    Jan 19 21:55:08.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.006879899s
    Jan 19 21:55:10.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006757802s
    Jan 19 21:55:12.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.007604008s
    Jan 19 21:55:14.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.006002566s
    Jan 19 21:55:16.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.006146303s
    Jan 19 21:55:18.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.007364685s
    Jan 19 21:55:20.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.006030196s
    Jan 19 21:55:22.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.007013637s
    Jan 19 21:55:24.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007466758s
    Jan 19 21:55:26.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.00624481s
    Jan 19 21:55:28.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.007105041s
    Jan 19 21:55:30.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.006116341s
    Jan 19 21:55:32.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.007739387s
    Jan 19 21:55:34.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.007277902s
    Jan 19 21:55:36.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.007649357s
    Jan 19 21:55:38.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007688411s
    Jan 19 21:55:40.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.007361896s
    Jan 19 21:55:42.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.005915519s
    Jan 19 21:55:44.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.007595782s
    Jan 19 21:55:46.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.006226979s
    Jan 19 21:55:48.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007830039s
    Jan 19 21:55:50.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.00660483s
    Jan 19 21:55:52.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00718625s
    Jan 19 21:55:54.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007757524s
    Jan 19 21:55:56.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.00658081s
    Jan 19 21:55:58.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.008193617s
    Jan 19 21:56:00.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.006568591s
    Jan 19 21:56:02.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007070561s
    Jan 19 21:56:04.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.007409222s
    Jan 19 21:56:06.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.006407745s
    Jan 19 21:56:08.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007250978s
    Jan 19 21:56:10.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.006742057s
    Jan 19 21:56:12.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007136449s
    Jan 19 21:56:14.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.007551327s
    Jan 19 21:56:16.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.006200201s
    Jan 19 21:56:18.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.007616104s
    Jan 19 21:56:20.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.006186422s
    Jan 19 21:56:22.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.007750143s
    Jan 19 21:56:24.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007676663s
    Jan 19 21:56:26.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.005993565s
    Jan 19 21:56:28.966: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.007165184s
    Jan 19 21:56:30.965: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.006066879s
    Jan 19 21:56:32.967: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007523829s
    Jan 19 21:56:32.969: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009653508s
    STEP: removing the label kubernetes.io/e2e-aaa484df-6968-4112-9c88-8bf7ea353098 off the node ip-10-0-171-213.ec2.internal 01/19/23 21:56:32.969
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-aaa484df-6968-4112-9c88-8bf7ea353098 01/19/23 21:56:32.984
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:56:32.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4552" for this suite. 01/19/23 21:56:32.998
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:56:33.006
Jan 19 21:56:33.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename daemonsets 01/19/23 21:56:33.007
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:56:33.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:56:33.023
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jan 19 21:56:33.112: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 21:56:33.122
Jan 19 21:56:33.127: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:33.127: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:33.127: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:33.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:56:33.131: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:56:34.136: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:34.136: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:34.136: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:34.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 21:56:34.140: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:56:35.136: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:35.136: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:35.136: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:35.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 19 21:56:35.140: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
STEP: Update daemon pods image. 01/19/23 21:56:35.154
STEP: Check that daemon pods images are updated. 01/19/23 21:56:35.164
Jan 19 21:56:35.176: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:35.176: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:35.176: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:35.176: INFO: Wrong image for pod: daemon-set-d94x4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:35.176: INFO: Wrong image for pod: daemon-set-pstht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:35.176: INFO: Wrong image for pod: daemon-set-tbfls. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:35.189: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:35.189: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:35.189: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:36.195: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:36.195: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:36.195: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:36.195: INFO: Wrong image for pod: daemon-set-d94x4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:36.195: INFO: Wrong image for pod: daemon-set-pstht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:36.200: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:36.200: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:36.200: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:37.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:37.194: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:37.194: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:37.194: INFO: Wrong image for pod: daemon-set-d94x4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:37.194: INFO: Wrong image for pod: daemon-set-pstht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:37.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:37.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:37.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:38.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:38.193: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:38.193: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:38.193: INFO: Wrong image for pod: daemon-set-d94x4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:38.193: INFO: Pod daemon-set-lsnmd is not available
Jan 19 21:56:38.193: INFO: Wrong image for pod: daemon-set-pstht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:38.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:38.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:38.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:39.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:39.193: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:39.193: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:39.193: INFO: Wrong image for pod: daemon-set-pstht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:40.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:40.193: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:40.193: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:40.193: INFO: Pod daemon-set-pp9mr is not available
Jan 19 21:56:40.193: INFO: Wrong image for pod: daemon-set-pstht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:40.197: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:40.197: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:40.197: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:41.197: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:41.197: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:41.197: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:41.202: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:41.202: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:41.202: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:42.214: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:42.214: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:42.214: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:42.229: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:42.229: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:42.229: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:43.194: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:43.194: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:43.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:43.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:43.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:44.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:44.193: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:44.193: INFO: Pod daemon-set-lnw85 is not available
Jan 19 21:56:44.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:44.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:44.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:45.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:45.197: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:45.197: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:45.197: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:46.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:46.193: INFO: Pod daemon-set-jnvdm is not available
Jan 19 21:56:46.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:46.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:46.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:47.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jan 19 21:56:47.193: INFO: Pod daemon-set-jnvdm is not available
Jan 19 21:56:47.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:47.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:47.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:48.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:48.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:48.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:49.193: INFO: Pod daemon-set-v2sz9 is not available
Jan 19 21:56:49.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:49.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:49.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 01/19/23 21:56:49.198
Jan 19 21:56:49.203: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:49.203: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:49.203: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:49.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Jan 19 21:56:49.206: INFO: Node ip-10-0-207-77.ec2.internal is running 0 daemon pod, expected 1
Jan 19 21:56:50.212: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:50.212: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:50.212: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jan 19 21:56:50.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
Jan 19 21:56:50.215: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/19/23 21:56:50.226
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5065, will wait for the garbage collector to delete the pods 01/19/23 21:56:50.226
Jan 19 21:56:50.284: INFO: Deleting DaemonSet.extensions daemon-set took: 5.031716ms
Jan 19 21:56:50.385: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.888842ms
Jan 19 21:56:53.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 21:56:53.188: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 21:56:53.190: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"229622"},"items":null}

Jan 19 21:56:53.194: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"229622"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 21:56:53.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5065" for this suite. 01/19/23 21:56:53.224
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":313,"skipped":5824,"failed":0}
------------------------------
• [SLOW TEST] [20.224 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:56:33.006
    Jan 19 21:56:33.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename daemonsets 01/19/23 21:56:33.007
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:56:33.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:56:33.023
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jan 19 21:56:33.112: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/19/23 21:56:33.122
    Jan 19 21:56:33.127: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:33.127: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:33.127: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:33.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:56:33.131: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:56:34.136: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:34.136: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:34.136: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:34.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 21:56:34.140: INFO: Node ip-10-0-146-42.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:56:35.136: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:35.136: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:35.136: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:35.140: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
    Jan 19 21:56:35.140: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
    STEP: Update daemon pods image. 01/19/23 21:56:35.154
    STEP: Check that daemon pods images are updated. 01/19/23 21:56:35.164
    Jan 19 21:56:35.176: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:35.176: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:35.176: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:35.176: INFO: Wrong image for pod: daemon-set-d94x4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:35.176: INFO: Wrong image for pod: daemon-set-pstht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:35.176: INFO: Wrong image for pod: daemon-set-tbfls. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:35.189: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:35.189: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:35.189: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:36.195: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:36.195: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:36.195: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:36.195: INFO: Wrong image for pod: daemon-set-d94x4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:36.195: INFO: Wrong image for pod: daemon-set-pstht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:36.200: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:36.200: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:36.200: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:37.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:37.194: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:37.194: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:37.194: INFO: Wrong image for pod: daemon-set-d94x4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:37.194: INFO: Wrong image for pod: daemon-set-pstht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:37.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:37.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:37.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:38.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:38.193: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:38.193: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:38.193: INFO: Wrong image for pod: daemon-set-d94x4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:38.193: INFO: Pod daemon-set-lsnmd is not available
    Jan 19 21:56:38.193: INFO: Wrong image for pod: daemon-set-pstht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:38.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:38.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:38.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:39.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:39.193: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:39.193: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:39.193: INFO: Wrong image for pod: daemon-set-pstht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:39.197: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:40.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:40.193: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:40.193: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:40.193: INFO: Pod daemon-set-pp9mr is not available
    Jan 19 21:56:40.193: INFO: Wrong image for pod: daemon-set-pstht. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:40.197: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:40.197: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:40.197: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:41.197: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:41.197: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:41.197: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:41.202: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:41.202: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:41.202: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:42.214: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:42.214: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:42.214: INFO: Wrong image for pod: daemon-set-8dhgf. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:42.229: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:42.229: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:42.229: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:43.194: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:43.194: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:43.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:43.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:43.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:44.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:44.193: INFO: Wrong image for pod: daemon-set-5hcnr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:44.193: INFO: Pod daemon-set-lnw85 is not available
    Jan 19 21:56:44.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:44.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:44.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:45.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:45.197: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:45.197: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:45.197: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:46.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:46.193: INFO: Pod daemon-set-jnvdm is not available
    Jan 19 21:56:46.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:46.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:46.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:47.193: INFO: Wrong image for pod: daemon-set-47qwz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jan 19 21:56:47.193: INFO: Pod daemon-set-jnvdm is not available
    Jan 19 21:56:47.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:47.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:47.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:48.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:48.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:48.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:49.193: INFO: Pod daemon-set-v2sz9 is not available
    Jan 19 21:56:49.198: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:49.198: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:49.198: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 01/19/23 21:56:49.198
    Jan 19 21:56:49.203: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:49.203: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:49.203: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:49.206: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Jan 19 21:56:49.206: INFO: Node ip-10-0-207-77.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 21:56:50.212: INFO: DaemonSet pods can't tolerate node ip-10-0-145-33.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:50.212: INFO: DaemonSet pods can't tolerate node ip-10-0-170-255.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:50.212: INFO: DaemonSet pods can't tolerate node ip-10-0-194-246.ec2.internal with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Jan 19 21:56:50.215: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 6
    Jan 19 21:56:50.215: INFO: Number of running nodes: 6, number of available pods: 6 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/19/23 21:56:50.226
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5065, will wait for the garbage collector to delete the pods 01/19/23 21:56:50.226
    Jan 19 21:56:50.284: INFO: Deleting DaemonSet.extensions daemon-set took: 5.031716ms
    Jan 19 21:56:50.385: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.888842ms
    Jan 19 21:56:53.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 21:56:53.188: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 19 21:56:53.190: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"229622"},"items":null}

    Jan 19 21:56:53.194: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"229622"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 21:56:53.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5065" for this suite. 01/19/23 21:56:53.224
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:56:53.23
Jan 19 21:56:53.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename proxy 01/19/23 21:56:53.231
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:56:53.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:56:53.249
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan 19 21:56:53.251: INFO: Creating pod...
Jan 19 21:56:53.298: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-963" to be "running"
Jan 19 21:56:53.302: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.884227ms
Jan 19 21:56:55.305: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007233162s
Jan 19 21:56:55.305: INFO: Pod "agnhost" satisfied condition "running"
Jan 19 21:56:55.305: INFO: Creating service...
Jan 19 21:56:55.314: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/DELETE
Jan 19 21:56:55.321: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 19 21:56:55.321: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/GET
Jan 19 21:56:55.325: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 19 21:56:55.325: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/HEAD
Jan 19 21:56:55.328: INFO: http.Client request:HEAD | StatusCode:200
Jan 19 21:56:55.328: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/OPTIONS
Jan 19 21:56:55.332: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 19 21:56:55.332: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/PATCH
Jan 19 21:56:55.334: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 19 21:56:55.335: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/POST
Jan 19 21:56:55.338: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 19 21:56:55.338: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/PUT
Jan 19 21:56:55.340: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan 19 21:56:55.340: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/DELETE
Jan 19 21:56:55.349: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan 19 21:56:55.349: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/GET
Jan 19 21:56:55.353: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan 19 21:56:55.353: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/HEAD
Jan 19 21:56:55.357: INFO: http.Client request:HEAD | StatusCode:200
Jan 19 21:56:55.357: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/OPTIONS
Jan 19 21:56:55.361: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan 19 21:56:55.361: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/PATCH
Jan 19 21:56:55.366: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan 19 21:56:55.366: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/POST
Jan 19 21:56:55.370: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan 19 21:56:55.370: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/PUT
Jan 19 21:56:55.374: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jan 19 21:56:55.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-963" for this suite. 01/19/23 21:56:55.378
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":314,"skipped":5828,"failed":0}
------------------------------
• [2.153 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:56:53.23
    Jan 19 21:56:53.230: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename proxy 01/19/23 21:56:53.231
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:56:53.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:56:53.249
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan 19 21:56:53.251: INFO: Creating pod...
    Jan 19 21:56:53.298: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-963" to be "running"
    Jan 19 21:56:53.302: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.884227ms
    Jan 19 21:56:55.305: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007233162s
    Jan 19 21:56:55.305: INFO: Pod "agnhost" satisfied condition "running"
    Jan 19 21:56:55.305: INFO: Creating service...
    Jan 19 21:56:55.314: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/DELETE
    Jan 19 21:56:55.321: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 19 21:56:55.321: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/GET
    Jan 19 21:56:55.325: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 19 21:56:55.325: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/HEAD
    Jan 19 21:56:55.328: INFO: http.Client request:HEAD | StatusCode:200
    Jan 19 21:56:55.328: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan 19 21:56:55.332: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 19 21:56:55.332: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/PATCH
    Jan 19 21:56:55.334: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 19 21:56:55.335: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/POST
    Jan 19 21:56:55.338: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 19 21:56:55.338: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/pods/agnhost/proxy/some/path/with/PUT
    Jan 19 21:56:55.340: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan 19 21:56:55.340: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/DELETE
    Jan 19 21:56:55.349: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan 19 21:56:55.349: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/GET
    Jan 19 21:56:55.353: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan 19 21:56:55.353: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/HEAD
    Jan 19 21:56:55.357: INFO: http.Client request:HEAD | StatusCode:200
    Jan 19 21:56:55.357: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/OPTIONS
    Jan 19 21:56:55.361: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan 19 21:56:55.361: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/PATCH
    Jan 19 21:56:55.366: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan 19 21:56:55.366: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/POST
    Jan 19 21:56:55.370: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan 19 21:56:55.370: INFO: Starting http.Client for https://172.30.0.1:443/api/v1/namespaces/proxy-963/services/test-service/proxy/some/path/with/PUT
    Jan 19 21:56:55.374: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jan 19 21:56:55.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-963" for this suite. 01/19/23 21:56:55.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:56:55.384
Jan 19 21:56:55.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename cronjob 01/19/23 21:56:55.384
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:56:55.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:56:55.41
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/19/23 21:56:55.412
STEP: creating 01/19/23 21:56:55.412
W0119 21:56:55.424583      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: getting 01/19/23 21:56:55.424
STEP: listing 01/19/23 21:56:55.431
STEP: watching 01/19/23 21:56:55.436
Jan 19 21:56:55.436: INFO: starting watch
STEP: cluster-wide listing 01/19/23 21:56:55.437
STEP: cluster-wide watching 01/19/23 21:56:55.443
Jan 19 21:56:55.443: INFO: starting watch
STEP: patching 01/19/23 21:56:55.444
STEP: updating 01/19/23 21:56:55.45
Jan 19 21:56:55.466: INFO: waiting for watch events with expected annotations
Jan 19 21:56:55.466: INFO: saw patched and updated annotations
STEP: patching /status 01/19/23 21:56:55.466
STEP: updating /status 01/19/23 21:56:55.485
STEP: get /status 01/19/23 21:56:55.496
STEP: deleting 01/19/23 21:56:55.5
STEP: deleting a collection 01/19/23 21:56:55.529
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jan 19 21:56:55.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2060" for this suite. 01/19/23 21:56:55.549
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":315,"skipped":5833,"failed":0}
------------------------------
• [0.173 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:56:55.384
    Jan 19 21:56:55.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename cronjob 01/19/23 21:56:55.384
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:56:55.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:56:55.41
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/19/23 21:56:55.412
    STEP: creating 01/19/23 21:56:55.412
    W0119 21:56:55.424583      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: getting 01/19/23 21:56:55.424
    STEP: listing 01/19/23 21:56:55.431
    STEP: watching 01/19/23 21:56:55.436
    Jan 19 21:56:55.436: INFO: starting watch
    STEP: cluster-wide listing 01/19/23 21:56:55.437
    STEP: cluster-wide watching 01/19/23 21:56:55.443
    Jan 19 21:56:55.443: INFO: starting watch
    STEP: patching 01/19/23 21:56:55.444
    STEP: updating 01/19/23 21:56:55.45
    Jan 19 21:56:55.466: INFO: waiting for watch events with expected annotations
    Jan 19 21:56:55.466: INFO: saw patched and updated annotations
    STEP: patching /status 01/19/23 21:56:55.466
    STEP: updating /status 01/19/23 21:56:55.485
    STEP: get /status 01/19/23 21:56:55.496
    STEP: deleting 01/19/23 21:56:55.5
    STEP: deleting a collection 01/19/23 21:56:55.529
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jan 19 21:56:55.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2060" for this suite. 01/19/23 21:56:55.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:56:55.558
Jan 19 21:56:55.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:56:55.558
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:56:55.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:56:55.632
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 01/19/23 21:56:55.639
Jan 19 21:56:55.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-8803 create -f -'
Jan 19 21:56:58.121: INFO: stderr: ""
Jan 19 21:56:58.121: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/19/23 21:56:58.121
Jan 19 21:56:59.124: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 21:56:59.124: INFO: Found 0 / 1
Jan 19 21:57:00.125: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 21:57:00.125: INFO: Found 1 / 1
Jan 19 21:57:00.125: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/19/23 21:57:00.125
Jan 19 21:57:00.127: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 21:57:00.127: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan 19 21:57:00.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-8803 patch pod agnhost-primary-r6p9v -p {"metadata":{"annotations":{"x":"y"}}}'
Jan 19 21:57:00.197: INFO: stderr: ""
Jan 19 21:57:00.197: INFO: stdout: "pod/agnhost-primary-r6p9v patched\n"
STEP: checking annotations 01/19/23 21:57:00.197
Jan 19 21:57:00.199: INFO: Selector matched 1 pods for map[app:agnhost]
Jan 19 21:57:00.199: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:57:00.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8803" for this suite. 01/19/23 21:57:00.204
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":316,"skipped":5853,"failed":0}
------------------------------
• [4.652 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:56:55.558
    Jan 19 21:56:55.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:56:55.558
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:56:55.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:56:55.632
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 01/19/23 21:56:55.639
    Jan 19 21:56:55.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-8803 create -f -'
    Jan 19 21:56:58.121: INFO: stderr: ""
    Jan 19 21:56:58.121: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/19/23 21:56:58.121
    Jan 19 21:56:59.124: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 21:56:59.124: INFO: Found 0 / 1
    Jan 19 21:57:00.125: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 21:57:00.125: INFO: Found 1 / 1
    Jan 19 21:57:00.125: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/19/23 21:57:00.125
    Jan 19 21:57:00.127: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 21:57:00.127: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan 19 21:57:00.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-8803 patch pod agnhost-primary-r6p9v -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan 19 21:57:00.197: INFO: stderr: ""
    Jan 19 21:57:00.197: INFO: stdout: "pod/agnhost-primary-r6p9v patched\n"
    STEP: checking annotations 01/19/23 21:57:00.197
    Jan 19 21:57:00.199: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan 19 21:57:00.199: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:57:00.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8803" for this suite. 01/19/23 21:57:00.204
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:57:00.211
Jan 19 21:57:00.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename pod-network-test 01/19/23 21:57:00.212
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:57:00.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:57:00.234
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-8644 01/19/23 21:57:00.238
STEP: creating a selector 01/19/23 21:57:00.238
STEP: Creating the service pods in kubernetes 01/19/23 21:57:00.238
Jan 19 21:57:00.238: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan 19 21:57:00.409: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8644" to be "running and ready"
Jan 19 21:57:00.423: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.830449ms
Jan 19 21:57:00.423: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 21:57:02.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017804296s
Jan 19 21:57:02.427: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:57:04.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017252103s
Jan 19 21:57:04.426: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:57:06.425: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016278827s
Jan 19 21:57:06.425: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:57:08.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016873556s
Jan 19 21:57:08.426: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:57:10.425: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016425806s
Jan 19 21:57:10.425: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:57:12.425: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.016752799s
Jan 19 21:57:12.425: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:57:14.425: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.016371061s
Jan 19 21:57:14.425: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:57:16.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016994323s
Jan 19 21:57:16.426: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:57:18.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.017338707s
Jan 19 21:57:18.426: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:57:20.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016985106s
Jan 19 21:57:20.426: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan 19 21:57:22.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.017468893s
Jan 19 21:57:22.426: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan 19 21:57:22.426: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan 19 21:57:22.429: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8644" to be "running and ready"
Jan 19 21:57:22.431: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.386607ms
Jan 19 21:57:22.431: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan 19 21:57:22.431: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan 19 21:57:22.433: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8644" to be "running and ready"
Jan 19 21:57:22.435: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.162976ms
Jan 19 21:57:22.435: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan 19 21:57:22.435: INFO: Pod "netserver-2" satisfied condition "running and ready"
Jan 19 21:57:22.437: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8644" to be "running and ready"
Jan 19 21:57:22.440: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.415351ms
Jan 19 21:57:22.440: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Jan 19 21:57:22.440: INFO: Pod "netserver-3" satisfied condition "running and ready"
Jan 19 21:57:22.442: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-8644" to be "running and ready"
Jan 19 21:57:22.444: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 2.192806ms
Jan 19 21:57:22.444: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Jan 19 21:57:22.444: INFO: Pod "netserver-4" satisfied condition "running and ready"
Jan 19 21:57:22.446: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-8644" to be "running and ready"
Jan 19 21:57:22.448: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 2.186953ms
Jan 19 21:57:22.448: INFO: The phase of Pod netserver-5 is Running (Ready = true)
Jan 19 21:57:22.449: INFO: Pod "netserver-5" satisfied condition "running and ready"
STEP: Creating test pods 01/19/23 21:57:22.451
Jan 19 21:57:22.462: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8644" to be "running"
Jan 19 21:57:22.464: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243144ms
Jan 19 21:57:24.467: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00515758s
Jan 19 21:57:24.467: INFO: Pod "test-container-pod" satisfied condition "running"
Jan 19 21:57:24.469: INFO: Setting MaxTries for pod polling to 66 for networking test based on endpoint count 6
Jan 19 21:57:24.469: INFO: Breadth first check of 10.128.10.131 on host 10.0.146.42...
Jan 19 21:57:24.472: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.9.59:9080/dial?request=hostname&protocol=udp&host=10.128.10.131&port=8081&tries=1'] Namespace:pod-network-test-8644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:57:24.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:57:24.472: INFO: ExecWithOptions: Clientset creation
Jan 19 21:57:24.472: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.9.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.10.131%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 21:57:24.548: INFO: Waiting for responses: map[]
Jan 19 21:57:24.549: INFO: reached 10.128.10.131 after 0/1 tries
Jan 19 21:57:24.549: INFO: Breadth first check of 10.128.14.107 on host 10.0.151.158...
Jan 19 21:57:24.551: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.9.59:9080/dial?request=hostname&protocol=udp&host=10.128.14.107&port=8081&tries=1'] Namespace:pod-network-test-8644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:57:24.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:57:24.551: INFO: ExecWithOptions: Clientset creation
Jan 19 21:57:24.551: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.9.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.14.107%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 21:57:24.627: INFO: Waiting for responses: map[]
Jan 19 21:57:24.627: INFO: reached 10.128.14.107 after 0/1 tries
Jan 19 21:57:24.627: INFO: Breadth first check of 10.128.12.177 on host 10.0.171.213...
Jan 19 21:57:24.629: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.9.59:9080/dial?request=hostname&protocol=udp&host=10.128.12.177&port=8081&tries=1'] Namespace:pod-network-test-8644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:57:24.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:57:24.630: INFO: ExecWithOptions: Clientset creation
Jan 19 21:57:24.630: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.9.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.12.177%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 21:57:24.685: INFO: Waiting for responses: map[]
Jan 19 21:57:24.685: INFO: reached 10.128.12.177 after 0/1 tries
Jan 19 21:57:24.685: INFO: Breadth first check of 10.128.9.58 on host 10.0.172.44...
Jan 19 21:57:24.688: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.9.59:9080/dial?request=hostname&protocol=udp&host=10.128.9.58&port=8081&tries=1'] Namespace:pod-network-test-8644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:57:24.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:57:24.688: INFO: ExecWithOptions: Clientset creation
Jan 19 21:57:24.688: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.9.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.9.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 21:57:24.744: INFO: Waiting for responses: map[]
Jan 19 21:57:24.744: INFO: reached 10.128.9.58 after 0/1 tries
Jan 19 21:57:24.744: INFO: Breadth first check of 10.128.6.106 on host 10.0.188.71...
Jan 19 21:57:24.746: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.9.59:9080/dial?request=hostname&protocol=udp&host=10.128.6.106&port=8081&tries=1'] Namespace:pod-network-test-8644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:57:24.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:57:24.747: INFO: ExecWithOptions: Clientset creation
Jan 19 21:57:24.747: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.9.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.6.106%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 21:57:24.966: INFO: Waiting for responses: map[]
Jan 19 21:57:24.966: INFO: reached 10.128.6.106 after 0/1 tries
Jan 19 21:57:24.966: INFO: Breadth first check of 10.128.16.159 on host 10.0.207.77...
Jan 19 21:57:24.968: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.9.59:9080/dial?request=hostname&protocol=udp&host=10.128.16.159&port=8081&tries=1'] Namespace:pod-network-test-8644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan 19 21:57:24.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
Jan 19 21:57:24.969: INFO: ExecWithOptions: Clientset creation
Jan 19 21:57:24.969: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.9.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.16.159%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan 19 21:57:25.027: INFO: Waiting for responses: map[]
Jan 19 21:57:25.027: INFO: reached 10.128.16.159 after 0/1 tries
Jan 19 21:57:25.027: INFO: Going to retry 0 out of 6 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jan 19 21:57:25.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8644" for this suite. 01/19/23 21:57:25.032
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":317,"skipped":5891,"failed":0}
------------------------------
• [SLOW TEST] [24.825 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:57:00.211
    Jan 19 21:57:00.211: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename pod-network-test 01/19/23 21:57:00.212
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:57:00.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:57:00.234
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-8644 01/19/23 21:57:00.238
    STEP: creating a selector 01/19/23 21:57:00.238
    STEP: Creating the service pods in kubernetes 01/19/23 21:57:00.238
    Jan 19 21:57:00.238: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan 19 21:57:00.409: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8644" to be "running and ready"
    Jan 19 21:57:00.423: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.830449ms
    Jan 19 21:57:00.423: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 21:57:02.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.017804296s
    Jan 19 21:57:02.427: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:57:04.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017252103s
    Jan 19 21:57:04.426: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:57:06.425: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.016278827s
    Jan 19 21:57:06.425: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:57:08.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016873556s
    Jan 19 21:57:08.426: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:57:10.425: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016425806s
    Jan 19 21:57:10.425: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:57:12.425: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.016752799s
    Jan 19 21:57:12.425: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:57:14.425: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.016371061s
    Jan 19 21:57:14.425: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:57:16.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.016994323s
    Jan 19 21:57:16.426: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:57:18.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.017338707s
    Jan 19 21:57:18.426: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:57:20.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.016985106s
    Jan 19 21:57:20.426: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan 19 21:57:22.426: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.017468893s
    Jan 19 21:57:22.426: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan 19 21:57:22.426: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan 19 21:57:22.429: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8644" to be "running and ready"
    Jan 19 21:57:22.431: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.386607ms
    Jan 19 21:57:22.431: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan 19 21:57:22.431: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan 19 21:57:22.433: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8644" to be "running and ready"
    Jan 19 21:57:22.435: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.162976ms
    Jan 19 21:57:22.435: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan 19 21:57:22.435: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Jan 19 21:57:22.437: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-8644" to be "running and ready"
    Jan 19 21:57:22.440: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.415351ms
    Jan 19 21:57:22.440: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Jan 19 21:57:22.440: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Jan 19 21:57:22.442: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-8644" to be "running and ready"
    Jan 19 21:57:22.444: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 2.192806ms
    Jan 19 21:57:22.444: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Jan 19 21:57:22.444: INFO: Pod "netserver-4" satisfied condition "running and ready"
    Jan 19 21:57:22.446: INFO: Waiting up to 5m0s for pod "netserver-5" in namespace "pod-network-test-8644" to be "running and ready"
    Jan 19 21:57:22.448: INFO: Pod "netserver-5": Phase="Running", Reason="", readiness=true. Elapsed: 2.186953ms
    Jan 19 21:57:22.448: INFO: The phase of Pod netserver-5 is Running (Ready = true)
    Jan 19 21:57:22.449: INFO: Pod "netserver-5" satisfied condition "running and ready"
    STEP: Creating test pods 01/19/23 21:57:22.451
    Jan 19 21:57:22.462: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8644" to be "running"
    Jan 19 21:57:22.464: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243144ms
    Jan 19 21:57:24.467: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00515758s
    Jan 19 21:57:24.467: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan 19 21:57:24.469: INFO: Setting MaxTries for pod polling to 66 for networking test based on endpoint count 6
    Jan 19 21:57:24.469: INFO: Breadth first check of 10.128.10.131 on host 10.0.146.42...
    Jan 19 21:57:24.472: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.9.59:9080/dial?request=hostname&protocol=udp&host=10.128.10.131&port=8081&tries=1'] Namespace:pod-network-test-8644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:57:24.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:57:24.472: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:57:24.472: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.9.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.10.131%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 21:57:24.548: INFO: Waiting for responses: map[]
    Jan 19 21:57:24.549: INFO: reached 10.128.10.131 after 0/1 tries
    Jan 19 21:57:24.549: INFO: Breadth first check of 10.128.14.107 on host 10.0.151.158...
    Jan 19 21:57:24.551: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.9.59:9080/dial?request=hostname&protocol=udp&host=10.128.14.107&port=8081&tries=1'] Namespace:pod-network-test-8644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:57:24.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:57:24.551: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:57:24.551: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.9.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.14.107%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 21:57:24.627: INFO: Waiting for responses: map[]
    Jan 19 21:57:24.627: INFO: reached 10.128.14.107 after 0/1 tries
    Jan 19 21:57:24.627: INFO: Breadth first check of 10.128.12.177 on host 10.0.171.213...
    Jan 19 21:57:24.629: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.9.59:9080/dial?request=hostname&protocol=udp&host=10.128.12.177&port=8081&tries=1'] Namespace:pod-network-test-8644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:57:24.629: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:57:24.630: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:57:24.630: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.9.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.12.177%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 21:57:24.685: INFO: Waiting for responses: map[]
    Jan 19 21:57:24.685: INFO: reached 10.128.12.177 after 0/1 tries
    Jan 19 21:57:24.685: INFO: Breadth first check of 10.128.9.58 on host 10.0.172.44...
    Jan 19 21:57:24.688: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.9.59:9080/dial?request=hostname&protocol=udp&host=10.128.9.58&port=8081&tries=1'] Namespace:pod-network-test-8644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:57:24.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:57:24.688: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:57:24.688: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.9.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.9.58%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 21:57:24.744: INFO: Waiting for responses: map[]
    Jan 19 21:57:24.744: INFO: reached 10.128.9.58 after 0/1 tries
    Jan 19 21:57:24.744: INFO: Breadth first check of 10.128.6.106 on host 10.0.188.71...
    Jan 19 21:57:24.746: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.9.59:9080/dial?request=hostname&protocol=udp&host=10.128.6.106&port=8081&tries=1'] Namespace:pod-network-test-8644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:57:24.746: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:57:24.747: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:57:24.747: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.9.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.6.106%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 21:57:24.966: INFO: Waiting for responses: map[]
    Jan 19 21:57:24.966: INFO: reached 10.128.6.106 after 0/1 tries
    Jan 19 21:57:24.966: INFO: Breadth first check of 10.128.16.159 on host 10.0.207.77...
    Jan 19 21:57:24.968: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.128.9.59:9080/dial?request=hostname&protocol=udp&host=10.128.16.159&port=8081&tries=1'] Namespace:pod-network-test-8644 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan 19 21:57:24.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    Jan 19 21:57:24.969: INFO: ExecWithOptions: Clientset creation
    Jan 19 21:57:24.969: INFO: ExecWithOptions: execute(POST https://172.30.0.1:443/api/v1/namespaces/pod-network-test-8644/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.128.9.59%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.128.16.159%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan 19 21:57:25.027: INFO: Waiting for responses: map[]
    Jan 19 21:57:25.027: INFO: reached 10.128.16.159 after 0/1 tries
    Jan 19 21:57:25.027: INFO: Going to retry 0 out of 6 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jan 19 21:57:25.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8644" for this suite. 01/19/23 21:57:25.032
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:57:25.038
Jan 19 21:57:25.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 21:57:25.039
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:57:25.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:57:25.055
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-9181/configmap-test-b854b54e-7f4c-4ae1-b3e4-d33692c01787 01/19/23 21:57:25.058
STEP: Creating a pod to test consume configMaps 01/19/23 21:57:25.071
Jan 19 21:57:25.094: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c" in namespace "configmap-9181" to be "Succeeded or Failed"
Jan 19 21:57:25.097: INFO: Pod "pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.665391ms
Jan 19 21:57:27.101: INFO: Pod "pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00651863s
Jan 19 21:57:29.101: INFO: Pod "pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006717292s
STEP: Saw pod success 01/19/23 21:57:29.101
Jan 19 21:57:29.101: INFO: Pod "pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c" satisfied condition "Succeeded or Failed"
Jan 19 21:57:29.104: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c container env-test: <nil>
STEP: delete the pod 01/19/23 21:57:29.12
Jan 19 21:57:29.129: INFO: Waiting for pod pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c to disappear
Jan 19 21:57:29.132: INFO: Pod pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 21:57:29.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9181" for this suite. 01/19/23 21:57:29.137
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":318,"skipped":5932,"failed":0}
------------------------------
• [4.105 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:57:25.038
    Jan 19 21:57:25.038: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 21:57:25.039
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:57:25.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:57:25.055
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-9181/configmap-test-b854b54e-7f4c-4ae1-b3e4-d33692c01787 01/19/23 21:57:25.058
    STEP: Creating a pod to test consume configMaps 01/19/23 21:57:25.071
    Jan 19 21:57:25.094: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c" in namespace "configmap-9181" to be "Succeeded or Failed"
    Jan 19 21:57:25.097: INFO: Pod "pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.665391ms
    Jan 19 21:57:27.101: INFO: Pod "pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00651863s
    Jan 19 21:57:29.101: INFO: Pod "pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006717292s
    STEP: Saw pod success 01/19/23 21:57:29.101
    Jan 19 21:57:29.101: INFO: Pod "pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c" satisfied condition "Succeeded or Failed"
    Jan 19 21:57:29.104: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c container env-test: <nil>
    STEP: delete the pod 01/19/23 21:57:29.12
    Jan 19 21:57:29.129: INFO: Waiting for pod pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c to disappear
    Jan 19 21:57:29.132: INFO: Pod pod-configmaps-6ad6d561-5ec4-41e1-be19-aea50c84194c no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 21:57:29.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9181" for this suite. 01/19/23 21:57:29.137
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:57:29.143
Jan 19 21:57:29.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:57:29.144
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:57:29.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:57:29.157
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:57:29.202
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:57:29.52
STEP: Deploying the webhook pod 01/19/23 21:57:29.528
STEP: Wait for the deployment to be ready 01/19/23 21:57:29.54
Jan 19 21:57:29.547: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jan 19 21:57:31.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 57, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 57, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 57, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 57, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 01/19/23 21:57:33.559
STEP: Verifying the service has paired with the endpoint 01/19/23 21:57:33.569
Jan 19 21:57:34.569: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/19/23 21:57:34.572
STEP: create a pod that should be updated by the webhook 01/19/23 21:57:34.585
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:57:34.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8892" for this suite. 01/19/23 21:57:34.627
STEP: Destroying namespace "webhook-8892-markers" for this suite. 01/19/23 21:57:34.632
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":319,"skipped":5946,"failed":0}
------------------------------
• [SLOW TEST] [5.551 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:57:29.143
    Jan 19 21:57:29.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:57:29.144
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:57:29.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:57:29.157
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:57:29.202
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:57:29.52
    STEP: Deploying the webhook pod 01/19/23 21:57:29.528
    STEP: Wait for the deployment to be ready 01/19/23 21:57:29.54
    Jan 19 21:57:29.547: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jan 19 21:57:31.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 19, 21, 57, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 57, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 21, 57, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 21, 57, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 01/19/23 21:57:33.559
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:57:33.569
    Jan 19 21:57:34.569: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/19/23 21:57:34.572
    STEP: create a pod that should be updated by the webhook 01/19/23 21:57:34.585
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:57:34.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8892" for this suite. 01/19/23 21:57:34.627
    STEP: Destroying namespace "webhook-8892-markers" for this suite. 01/19/23 21:57:34.632
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:57:34.695
Jan 19 21:57:34.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename dns 01/19/23 21:57:34.695
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:57:34.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:57:34.711
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/19/23 21:57:34.713
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1401.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1401.svc.cluster.local; sleep 1; done
 01/19/23 21:57:34.72
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1401.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local; sleep 1; done
 01/19/23 21:57:34.72
STEP: creating a pod to probe DNS 01/19/23 21:57:34.721
STEP: submitting the pod to kubernetes 01/19/23 21:57:34.721
Jan 19 21:57:34.788: INFO: Waiting up to 15m0s for pod "dns-test-74300974-732a-4248-a587-d64519f576b8" in namespace "dns-1401" to be "running"
Jan 19 21:57:34.802: INFO: Pod "dns-test-74300974-732a-4248-a587-d64519f576b8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.332356ms
Jan 19 21:57:36.804: INFO: Pod "dns-test-74300974-732a-4248-a587-d64519f576b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.015931047s
Jan 19 21:57:36.804: INFO: Pod "dns-test-74300974-732a-4248-a587-d64519f576b8" satisfied condition "running"
STEP: retrieving the pod 01/19/23 21:57:36.804
STEP: looking for the results for each expected name from probers 01/19/23 21:57:36.807
Jan 19 21:57:36.814: INFO: DNS probes using dns-test-74300974-732a-4248-a587-d64519f576b8 succeeded

STEP: deleting the pod 01/19/23 21:57:36.814
STEP: changing the externalName to bar.example.com 01/19/23 21:57:36.823
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1401.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1401.svc.cluster.local; sleep 1; done
 01/19/23 21:57:36.829
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1401.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local; sleep 1; done
 01/19/23 21:57:36.83
STEP: creating a second pod to probe DNS 01/19/23 21:57:36.83
STEP: submitting the pod to kubernetes 01/19/23 21:57:36.83
Jan 19 21:57:36.842: INFO: Waiting up to 15m0s for pod "dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e" in namespace "dns-1401" to be "running"
Jan 19 21:57:36.845: INFO: Pod "dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.339413ms
Jan 19 21:57:38.848: INFO: Pod "dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005318291s
Jan 19 21:57:38.848: INFO: Pod "dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e" satisfied condition "running"
STEP: retrieving the pod 01/19/23 21:57:38.848
STEP: looking for the results for each expected name from probers 01/19/23 21:57:38.851
Jan 19 21:57:38.856: INFO: File wheezy_udp@dns-test-service-3.dns-1401.svc.cluster.local from pod  dns-1401/dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 19 21:57:38.859: INFO: File jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local from pod  dns-1401/dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e contains 'foo.example.com.
' instead of 'bar.example.com.'
Jan 19 21:57:38.859: INFO: Lookups using dns-1401/dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e failed for: [wheezy_udp@dns-test-service-3.dns-1401.svc.cluster.local jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local]

Jan 19 21:57:43.869: INFO: DNS probes using dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e succeeded

STEP: deleting the pod 01/19/23 21:57:43.869
STEP: changing the service to type=ClusterIP 01/19/23 21:57:43.9
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1401.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1401.svc.cluster.local; sleep 1; done
 01/19/23 21:57:43.952
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1401.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local; sleep 1; done
 01/19/23 21:57:43.952
STEP: creating a third pod to probe DNS 01/19/23 21:57:43.952
STEP: submitting the pod to kubernetes 01/19/23 21:57:43.955
Jan 19 21:57:44.013: INFO: Waiting up to 15m0s for pod "dns-test-0bd12353-c299-4b1a-89ad-b3c705998988" in namespace "dns-1401" to be "running"
Jan 19 21:57:44.023: INFO: Pod "dns-test-0bd12353-c299-4b1a-89ad-b3c705998988": Phase="Pending", Reason="", readiness=false. Elapsed: 10.567875ms
Jan 19 21:57:46.026: INFO: Pod "dns-test-0bd12353-c299-4b1a-89ad-b3c705998988": Phase="Running", Reason="", readiness=true. Elapsed: 2.013152775s
Jan 19 21:57:46.026: INFO: Pod "dns-test-0bd12353-c299-4b1a-89ad-b3c705998988" satisfied condition "running"
STEP: retrieving the pod 01/19/23 21:57:46.026
STEP: looking for the results for each expected name from probers 01/19/23 21:57:46.028
Jan 19 21:57:46.036: INFO: File jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local from pod  dns-1401/dns-test-0bd12353-c299-4b1a-89ad-b3c705998988 contains '' instead of '172.30.98.116'
Jan 19 21:57:46.036: INFO: Lookups using dns-1401/dns-test-0bd12353-c299-4b1a-89ad-b3c705998988 failed for: [jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local]

Jan 19 21:57:51.043: INFO: DNS probes using dns-test-0bd12353-c299-4b1a-89ad-b3c705998988 succeeded

STEP: deleting the pod 01/19/23 21:57:51.043
STEP: deleting the test externalName service 01/19/23 21:57:51.054
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 21:57:51.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1401" for this suite. 01/19/23 21:57:51.075
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":320,"skipped":5963,"failed":0}
------------------------------
• [SLOW TEST] [16.389 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:57:34.695
    Jan 19 21:57:34.695: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename dns 01/19/23 21:57:34.695
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:57:34.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:57:34.711
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/19/23 21:57:34.713
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1401.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1401.svc.cluster.local; sleep 1; done
     01/19/23 21:57:34.72
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1401.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local; sleep 1; done
     01/19/23 21:57:34.72
    STEP: creating a pod to probe DNS 01/19/23 21:57:34.721
    STEP: submitting the pod to kubernetes 01/19/23 21:57:34.721
    Jan 19 21:57:34.788: INFO: Waiting up to 15m0s for pod "dns-test-74300974-732a-4248-a587-d64519f576b8" in namespace "dns-1401" to be "running"
    Jan 19 21:57:34.802: INFO: Pod "dns-test-74300974-732a-4248-a587-d64519f576b8": Phase="Pending", Reason="", readiness=false. Elapsed: 13.332356ms
    Jan 19 21:57:36.804: INFO: Pod "dns-test-74300974-732a-4248-a587-d64519f576b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.015931047s
    Jan 19 21:57:36.804: INFO: Pod "dns-test-74300974-732a-4248-a587-d64519f576b8" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 21:57:36.804
    STEP: looking for the results for each expected name from probers 01/19/23 21:57:36.807
    Jan 19 21:57:36.814: INFO: DNS probes using dns-test-74300974-732a-4248-a587-d64519f576b8 succeeded

    STEP: deleting the pod 01/19/23 21:57:36.814
    STEP: changing the externalName to bar.example.com 01/19/23 21:57:36.823
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1401.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1401.svc.cluster.local; sleep 1; done
     01/19/23 21:57:36.829
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1401.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local; sleep 1; done
     01/19/23 21:57:36.83
    STEP: creating a second pod to probe DNS 01/19/23 21:57:36.83
    STEP: submitting the pod to kubernetes 01/19/23 21:57:36.83
    Jan 19 21:57:36.842: INFO: Waiting up to 15m0s for pod "dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e" in namespace "dns-1401" to be "running"
    Jan 19 21:57:36.845: INFO: Pod "dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.339413ms
    Jan 19 21:57:38.848: INFO: Pod "dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.005318291s
    Jan 19 21:57:38.848: INFO: Pod "dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 21:57:38.848
    STEP: looking for the results for each expected name from probers 01/19/23 21:57:38.851
    Jan 19 21:57:38.856: INFO: File wheezy_udp@dns-test-service-3.dns-1401.svc.cluster.local from pod  dns-1401/dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 19 21:57:38.859: INFO: File jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local from pod  dns-1401/dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Jan 19 21:57:38.859: INFO: Lookups using dns-1401/dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e failed for: [wheezy_udp@dns-test-service-3.dns-1401.svc.cluster.local jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local]

    Jan 19 21:57:43.869: INFO: DNS probes using dns-test-2716d52f-0d57-4e23-a434-5d13f59fcc4e succeeded

    STEP: deleting the pod 01/19/23 21:57:43.869
    STEP: changing the service to type=ClusterIP 01/19/23 21:57:43.9
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1401.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1401.svc.cluster.local; sleep 1; done
     01/19/23 21:57:43.952
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1401.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local; sleep 1; done
     01/19/23 21:57:43.952
    STEP: creating a third pod to probe DNS 01/19/23 21:57:43.952
    STEP: submitting the pod to kubernetes 01/19/23 21:57:43.955
    Jan 19 21:57:44.013: INFO: Waiting up to 15m0s for pod "dns-test-0bd12353-c299-4b1a-89ad-b3c705998988" in namespace "dns-1401" to be "running"
    Jan 19 21:57:44.023: INFO: Pod "dns-test-0bd12353-c299-4b1a-89ad-b3c705998988": Phase="Pending", Reason="", readiness=false. Elapsed: 10.567875ms
    Jan 19 21:57:46.026: INFO: Pod "dns-test-0bd12353-c299-4b1a-89ad-b3c705998988": Phase="Running", Reason="", readiness=true. Elapsed: 2.013152775s
    Jan 19 21:57:46.026: INFO: Pod "dns-test-0bd12353-c299-4b1a-89ad-b3c705998988" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 21:57:46.026
    STEP: looking for the results for each expected name from probers 01/19/23 21:57:46.028
    Jan 19 21:57:46.036: INFO: File jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local from pod  dns-1401/dns-test-0bd12353-c299-4b1a-89ad-b3c705998988 contains '' instead of '172.30.98.116'
    Jan 19 21:57:46.036: INFO: Lookups using dns-1401/dns-test-0bd12353-c299-4b1a-89ad-b3c705998988 failed for: [jessie_udp@dns-test-service-3.dns-1401.svc.cluster.local]

    Jan 19 21:57:51.043: INFO: DNS probes using dns-test-0bd12353-c299-4b1a-89ad-b3c705998988 succeeded

    STEP: deleting the pod 01/19/23 21:57:51.043
    STEP: deleting the test externalName service 01/19/23 21:57:51.054
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 21:57:51.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1401" for this suite. 01/19/23 21:57:51.075
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:57:51.085
Jan 19 21:57:51.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 21:57:51.085
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:57:51.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:57:51.111
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204
STEP: creating service in namespace services-7368 01/19/23 21:57:51.113
STEP: creating service affinity-nodeport in namespace services-7368 01/19/23 21:57:51.113
STEP: creating replication controller affinity-nodeport in namespace services-7368 01/19/23 21:57:51.149
W0119 21:57:51.166089      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "affinity-nodeport" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-nodeport" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-nodeport" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-nodeport" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
I0119 21:57:51.166220      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7368, replica count: 3
I0119 21:57:54.217388      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 21:57:54.225: INFO: Creating new exec pod
Jan 19 21:57:54.235: INFO: Waiting up to 5m0s for pod "execpod-affinitytqmmq" in namespace "services-7368" to be "running"
Jan 19 21:57:54.237: INFO: Pod "execpod-affinitytqmmq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155047ms
Jan 19 21:57:56.241: INFO: Pod "execpod-affinitytqmmq": Phase="Running", Reason="", readiness=true. Elapsed: 2.005979918s
Jan 19 21:57:56.241: INFO: Pod "execpod-affinitytqmmq" satisfied condition "running"
Jan 19 21:57:57.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7368 exec execpod-affinitytqmmq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jan 19 21:57:58.444: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan 19 21:57:58.444: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:57:58.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7368 exec execpod-affinitytqmmq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.197.28 80'
Jan 19 21:57:58.579: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.197.28 80\nConnection to 172.30.197.28 80 port [tcp/http] succeeded!\n"
Jan 19 21:57:58.579: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:57:58.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7368 exec execpod-affinitytqmmq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.207.77 30925'
Jan 19 21:57:59.727: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.207.77 30925\nConnection to 10.0.207.77 30925 port [tcp/*] succeeded!\n"
Jan 19 21:57:59.727: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:57:59.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7368 exec execpod-affinitytqmmq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.172.44 30925'
Jan 19 21:58:00.878: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.172.44 30925\nConnection to 10.0.172.44 30925 port [tcp/*] succeeded!\n"
Jan 19 21:58:00.878: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 21:58:00.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7368 exec execpod-affinitytqmmq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.146.42:30925/ ; done'
Jan 19 21:58:02.090: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n"
Jan 19 21:58:02.090: INFO: stdout: "\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88"
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
Jan 19 21:58:02.090: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7368, will wait for the garbage collector to delete the pods 01/19/23 21:58:02.106
Jan 19 21:58:02.165: INFO: Deleting ReplicationController affinity-nodeport took: 6.439632ms
Jan 19 21:58:02.266: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.217443ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 21:58:04.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7368" for this suite. 01/19/23 21:58:04.402
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":321,"skipped":5996,"failed":0}
------------------------------
• [SLOW TEST] [13.323 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:57:51.085
    Jan 19 21:57:51.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 21:57:51.085
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:57:51.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:57:51.111
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2204
    STEP: creating service in namespace services-7368 01/19/23 21:57:51.113
    STEP: creating service affinity-nodeport in namespace services-7368 01/19/23 21:57:51.113
    STEP: creating replication controller affinity-nodeport in namespace services-7368 01/19/23 21:57:51.149
    W0119 21:57:51.166089      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "affinity-nodeport" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "affinity-nodeport" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "affinity-nodeport" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "affinity-nodeport" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    I0119 21:57:51.166220      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7368, replica count: 3
    I0119 21:57:54.217388      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 21:57:54.225: INFO: Creating new exec pod
    Jan 19 21:57:54.235: INFO: Waiting up to 5m0s for pod "execpod-affinitytqmmq" in namespace "services-7368" to be "running"
    Jan 19 21:57:54.237: INFO: Pod "execpod-affinitytqmmq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.155047ms
    Jan 19 21:57:56.241: INFO: Pod "execpod-affinitytqmmq": Phase="Running", Reason="", readiness=true. Elapsed: 2.005979918s
    Jan 19 21:57:56.241: INFO: Pod "execpod-affinitytqmmq" satisfied condition "running"
    Jan 19 21:57:57.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7368 exec execpod-affinitytqmmq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jan 19 21:57:58.444: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan 19 21:57:58.444: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:57:58.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7368 exec execpod-affinitytqmmq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.197.28 80'
    Jan 19 21:57:58.579: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.197.28 80\nConnection to 172.30.197.28 80 port [tcp/http] succeeded!\n"
    Jan 19 21:57:58.579: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:57:58.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7368 exec execpod-affinitytqmmq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.207.77 30925'
    Jan 19 21:57:59.727: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.207.77 30925\nConnection to 10.0.207.77 30925 port [tcp/*] succeeded!\n"
    Jan 19 21:57:59.727: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:57:59.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7368 exec execpod-affinitytqmmq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.172.44 30925'
    Jan 19 21:58:00.878: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.172.44 30925\nConnection to 10.0.172.44 30925 port [tcp/*] succeeded!\n"
    Jan 19 21:58:00.878: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 21:58:00.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-7368 exec execpod-affinitytqmmq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.146.42:30925/ ; done'
    Jan 19 21:58:02.090: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30925/\n"
    Jan 19 21:58:02.090: INFO: stdout: "\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88\naffinity-nodeport-jhk88"
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Received response from host: affinity-nodeport-jhk88
    Jan 19 21:58:02.090: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-7368, will wait for the garbage collector to delete the pods 01/19/23 21:58:02.106
    Jan 19 21:58:02.165: INFO: Deleting ReplicationController affinity-nodeport took: 6.439632ms
    Jan 19 21:58:02.266: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.217443ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 21:58:04.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7368" for this suite. 01/19/23 21:58:04.402
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:58:04.409
Jan 19 21:58:04.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:58:04.41
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:04.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:04.43
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:58:04.432
Jan 19 21:58:04.493: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7" in namespace "projected-1925" to be "Succeeded or Failed"
Jan 19 21:58:04.511: INFO: Pod "downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.998317ms
Jan 19 21:58:06.515: INFO: Pod "downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021331911s
Jan 19 21:58:08.515: INFO: Pod "downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021904849s
STEP: Saw pod success 01/19/23 21:58:08.515
Jan 19 21:58:08.516: INFO: Pod "downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7" satisfied condition "Succeeded or Failed"
Jan 19 21:58:08.518: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7 container client-container: <nil>
STEP: delete the pod 01/19/23 21:58:08.523
Jan 19 21:58:08.532: INFO: Waiting for pod downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7 to disappear
Jan 19 21:58:08.534: INFO: Pod downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 21:58:08.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1925" for this suite. 01/19/23 21:58:08.538
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":322,"skipped":6014,"failed":0}
------------------------------
• [4.134 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:58:04.409
    Jan 19 21:58:04.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:58:04.41
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:04.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:04.43
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:58:04.432
    Jan 19 21:58:04.493: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7" in namespace "projected-1925" to be "Succeeded or Failed"
    Jan 19 21:58:04.511: INFO: Pod "downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.998317ms
    Jan 19 21:58:06.515: INFO: Pod "downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021331911s
    Jan 19 21:58:08.515: INFO: Pod "downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021904849s
    STEP: Saw pod success 01/19/23 21:58:08.515
    Jan 19 21:58:08.516: INFO: Pod "downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7" satisfied condition "Succeeded or Failed"
    Jan 19 21:58:08.518: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7 container client-container: <nil>
    STEP: delete the pod 01/19/23 21:58:08.523
    Jan 19 21:58:08.532: INFO: Waiting for pod downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7 to disappear
    Jan 19 21:58:08.534: INFO: Pod downwardapi-volume-cfe990cd-32f8-45de-a4a1-f87556c9bba7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 21:58:08.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1925" for this suite. 01/19/23 21:58:08.538
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:58:08.544
Jan 19 21:58:08.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 21:58:08.545
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:08.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:08.559
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 01/19/23 21:58:08.563
Jan 19 21:58:08.637: INFO: Waiting up to 5m0s for pod "pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c" in namespace "emptydir-2756" to be "Succeeded or Failed"
Jan 19 21:58:08.657: INFO: Pod "pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.228012ms
Jan 19 21:58:10.661: INFO: Pod "pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02362876s
Jan 19 21:58:12.660: INFO: Pod "pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023119235s
STEP: Saw pod success 01/19/23 21:58:12.66
Jan 19 21:58:12.660: INFO: Pod "pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c" satisfied condition "Succeeded or Failed"
Jan 19 21:58:12.663: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c container test-container: <nil>
STEP: delete the pod 01/19/23 21:58:12.677
Jan 19 21:58:12.685: INFO: Waiting for pod pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c to disappear
Jan 19 21:58:12.687: INFO: Pod pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 21:58:12.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2756" for this suite. 01/19/23 21:58:12.692
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":323,"skipped":6044,"failed":0}
------------------------------
• [4.154 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:58:08.544
    Jan 19 21:58:08.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 21:58:08.545
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:08.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:08.559
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/19/23 21:58:08.563
    Jan 19 21:58:08.637: INFO: Waiting up to 5m0s for pod "pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c" in namespace "emptydir-2756" to be "Succeeded or Failed"
    Jan 19 21:58:08.657: INFO: Pod "pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c": Phase="Pending", Reason="", readiness=false. Elapsed: 20.228012ms
    Jan 19 21:58:10.661: INFO: Pod "pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02362876s
    Jan 19 21:58:12.660: INFO: Pod "pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023119235s
    STEP: Saw pod success 01/19/23 21:58:12.66
    Jan 19 21:58:12.660: INFO: Pod "pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c" satisfied condition "Succeeded or Failed"
    Jan 19 21:58:12.663: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c container test-container: <nil>
    STEP: delete the pod 01/19/23 21:58:12.677
    Jan 19 21:58:12.685: INFO: Waiting for pod pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c to disappear
    Jan 19 21:58:12.687: INFO: Pod pod-e4ae36ad-42d1-40b2-ba01-6f7f6ee5928c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 21:58:12.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2756" for this suite. 01/19/23 21:58:12.692
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:58:12.698
Jan 19 21:58:12.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 21:58:12.699
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:12.714
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:12.716
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231
STEP: creating an Endpoint 01/19/23 21:58:12.724
STEP: waiting for available Endpoint 01/19/23 21:58:12.733
STEP: listing all Endpoints 01/19/23 21:58:12.735
STEP: updating the Endpoint 01/19/23 21:58:12.743
STEP: fetching the Endpoint 01/19/23 21:58:12.75
STEP: patching the Endpoint 01/19/23 21:58:12.753
STEP: fetching the Endpoint 01/19/23 21:58:12.764
STEP: deleting the Endpoint by Collection 01/19/23 21:58:12.773
STEP: waiting for Endpoint deletion 01/19/23 21:58:12.785
STEP: fetching the Endpoint 01/19/23 21:58:12.786
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 21:58:12.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5010" for this suite. 01/19/23 21:58:12.795
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":324,"skipped":6056,"failed":0}
------------------------------
• [0.102 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:58:12.698
    Jan 19 21:58:12.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 21:58:12.699
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:12.714
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:12.716
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3231
    STEP: creating an Endpoint 01/19/23 21:58:12.724
    STEP: waiting for available Endpoint 01/19/23 21:58:12.733
    STEP: listing all Endpoints 01/19/23 21:58:12.735
    STEP: updating the Endpoint 01/19/23 21:58:12.743
    STEP: fetching the Endpoint 01/19/23 21:58:12.75
    STEP: patching the Endpoint 01/19/23 21:58:12.753
    STEP: fetching the Endpoint 01/19/23 21:58:12.764
    STEP: deleting the Endpoint by Collection 01/19/23 21:58:12.773
    STEP: waiting for Endpoint deletion 01/19/23 21:58:12.785
    STEP: fetching the Endpoint 01/19/23 21:58:12.786
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 21:58:12.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5010" for this suite. 01/19/23 21:58:12.795
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:58:12.801
Jan 19 21:58:12.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:58:12.802
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:12.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:12.827
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 01/19/23 21:58:12.829
Jan 19 21:58:12.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 create -f -'
Jan 19 21:58:15.162: INFO: stderr: ""
Jan 19 21:58:15.162: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 21:58:15.162
Jan 19 21:58:15.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 21:58:15.220: INFO: stderr: ""
Jan 19 21:58:15.220: INFO: stdout: "update-demo-nautilus-fslmj update-demo-nautilus-lm965 "
Jan 19 21:58:15.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 21:58:15.280: INFO: stderr: ""
Jan 19 21:58:15.280: INFO: stdout: ""
Jan 19 21:58:15.280: INFO: update-demo-nautilus-fslmj is created but not running
Jan 19 21:58:20.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 21:58:20.339: INFO: stderr: ""
Jan 19 21:58:20.339: INFO: stdout: "update-demo-nautilus-fslmj update-demo-nautilus-lm965 "
Jan 19 21:58:20.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 21:58:20.399: INFO: stderr: ""
Jan 19 21:58:20.399: INFO: stdout: "true"
Jan 19 21:58:20.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 21:58:20.456: INFO: stderr: ""
Jan 19 21:58:20.456: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 21:58:20.456: INFO: validating pod update-demo-nautilus-fslmj
Jan 19 21:58:20.461: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 21:58:20.461: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 21:58:20.461: INFO: update-demo-nautilus-fslmj is verified up and running
Jan 19 21:58:20.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-lm965 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 21:58:20.517: INFO: stderr: ""
Jan 19 21:58:20.517: INFO: stdout: "true"
Jan 19 21:58:20.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-lm965 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 21:58:20.572: INFO: stderr: ""
Jan 19 21:58:20.572: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 21:58:20.572: INFO: validating pod update-demo-nautilus-lm965
Jan 19 21:58:20.578: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 21:58:20.578: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 21:58:20.578: INFO: update-demo-nautilus-lm965 is verified up and running
STEP: scaling down the replication controller 01/19/23 21:58:20.578
Jan 19 21:58:20.582: INFO: scanned /root for discovery docs: <nil>
Jan 19 21:58:20.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan 19 21:58:21.657: INFO: stderr: ""
Jan 19 21:58:21.657: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 21:58:21.657
Jan 19 21:58:21.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 21:58:21.715: INFO: stderr: ""
Jan 19 21:58:21.715: INFO: stdout: "update-demo-nautilus-fslmj update-demo-nautilus-lm965 "
STEP: Replicas for name=update-demo: expected=1 actual=2 01/19/23 21:58:21.715
Jan 19 21:58:26.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 21:58:26.771: INFO: stderr: ""
Jan 19 21:58:26.771: INFO: stdout: "update-demo-nautilus-fslmj "
Jan 19 21:58:26.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 21:58:26.828: INFO: stderr: ""
Jan 19 21:58:26.828: INFO: stdout: "true"
Jan 19 21:58:26.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 21:58:26.883: INFO: stderr: ""
Jan 19 21:58:26.883: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 21:58:26.883: INFO: validating pod update-demo-nautilus-fslmj
Jan 19 21:58:26.886: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 21:58:26.886: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 21:58:26.886: INFO: update-demo-nautilus-fslmj is verified up and running
STEP: scaling up the replication controller 01/19/23 21:58:26.886
Jan 19 21:58:26.889: INFO: scanned /root for discovery docs: <nil>
Jan 19 21:58:26.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan 19 21:58:27.960: INFO: stderr: ""
Jan 19 21:58:27.960: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 21:58:27.96
Jan 19 21:58:27.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 21:58:28.020: INFO: stderr: ""
Jan 19 21:58:28.020: INFO: stdout: "update-demo-nautilus-9ljjq update-demo-nautilus-fslmj "
Jan 19 21:58:28.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-9ljjq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 21:58:28.076: INFO: stderr: ""
Jan 19 21:58:28.076: INFO: stdout: ""
Jan 19 21:58:28.076: INFO: update-demo-nautilus-9ljjq is created but not running
Jan 19 21:58:33.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 21:58:33.137: INFO: stderr: ""
Jan 19 21:58:33.137: INFO: stdout: "update-demo-nautilus-9ljjq update-demo-nautilus-fslmj "
Jan 19 21:58:33.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-9ljjq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 21:58:33.192: INFO: stderr: ""
Jan 19 21:58:33.192: INFO: stdout: "true"
Jan 19 21:58:33.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-9ljjq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 21:58:33.247: INFO: stderr: ""
Jan 19 21:58:33.247: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 21:58:33.247: INFO: validating pod update-demo-nautilus-9ljjq
Jan 19 21:58:33.252: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 21:58:33.252: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 21:58:33.252: INFO: update-demo-nautilus-9ljjq is verified up and running
Jan 19 21:58:33.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 21:58:33.309: INFO: stderr: ""
Jan 19 21:58:33.309: INFO: stdout: "true"
Jan 19 21:58:33.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 21:58:33.368: INFO: stderr: ""
Jan 19 21:58:33.368: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 21:58:33.368: INFO: validating pod update-demo-nautilus-fslmj
Jan 19 21:58:33.372: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 21:58:33.372: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 21:58:33.372: INFO: update-demo-nautilus-fslmj is verified up and running
STEP: using delete to clean up resources 01/19/23 21:58:33.372
Jan 19 21:58:33.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 delete --grace-period=0 --force -f -'
Jan 19 21:58:33.431: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 21:58:33.431: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 19 21:58:33.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get rc,svc -l name=update-demo --no-headers'
Jan 19 21:58:33.505: INFO: stderr: "No resources found in kubectl-9943 namespace.\n"
Jan 19 21:58:33.505: INFO: stdout: ""
Jan 19 21:58:33.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 19 21:58:33.569: INFO: stderr: ""
Jan 19 21:58:33.569: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:58:33.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9943" for this suite. 01/19/23 21:58:33.573
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":325,"skipped":6065,"failed":0}
------------------------------
• [SLOW TEST] [20.778 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:58:12.801
    Jan 19 21:58:12.801: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:58:12.802
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:12.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:12.827
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 01/19/23 21:58:12.829
    Jan 19 21:58:12.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 create -f -'
    Jan 19 21:58:15.162: INFO: stderr: ""
    Jan 19 21:58:15.162: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 21:58:15.162
    Jan 19 21:58:15.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 21:58:15.220: INFO: stderr: ""
    Jan 19 21:58:15.220: INFO: stdout: "update-demo-nautilus-fslmj update-demo-nautilus-lm965 "
    Jan 19 21:58:15.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 21:58:15.280: INFO: stderr: ""
    Jan 19 21:58:15.280: INFO: stdout: ""
    Jan 19 21:58:15.280: INFO: update-demo-nautilus-fslmj is created but not running
    Jan 19 21:58:20.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 21:58:20.339: INFO: stderr: ""
    Jan 19 21:58:20.339: INFO: stdout: "update-demo-nautilus-fslmj update-demo-nautilus-lm965 "
    Jan 19 21:58:20.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 21:58:20.399: INFO: stderr: ""
    Jan 19 21:58:20.399: INFO: stdout: "true"
    Jan 19 21:58:20.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 21:58:20.456: INFO: stderr: ""
    Jan 19 21:58:20.456: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 21:58:20.456: INFO: validating pod update-demo-nautilus-fslmj
    Jan 19 21:58:20.461: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 21:58:20.461: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 21:58:20.461: INFO: update-demo-nautilus-fslmj is verified up and running
    Jan 19 21:58:20.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-lm965 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 21:58:20.517: INFO: stderr: ""
    Jan 19 21:58:20.517: INFO: stdout: "true"
    Jan 19 21:58:20.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-lm965 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 21:58:20.572: INFO: stderr: ""
    Jan 19 21:58:20.572: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 21:58:20.572: INFO: validating pod update-demo-nautilus-lm965
    Jan 19 21:58:20.578: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 21:58:20.578: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 21:58:20.578: INFO: update-demo-nautilus-lm965 is verified up and running
    STEP: scaling down the replication controller 01/19/23 21:58:20.578
    Jan 19 21:58:20.582: INFO: scanned /root for discovery docs: <nil>
    Jan 19 21:58:20.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan 19 21:58:21.657: INFO: stderr: ""
    Jan 19 21:58:21.657: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 21:58:21.657
    Jan 19 21:58:21.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 21:58:21.715: INFO: stderr: ""
    Jan 19 21:58:21.715: INFO: stdout: "update-demo-nautilus-fslmj update-demo-nautilus-lm965 "
    STEP: Replicas for name=update-demo: expected=1 actual=2 01/19/23 21:58:21.715
    Jan 19 21:58:26.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 21:58:26.771: INFO: stderr: ""
    Jan 19 21:58:26.771: INFO: stdout: "update-demo-nautilus-fslmj "
    Jan 19 21:58:26.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 21:58:26.828: INFO: stderr: ""
    Jan 19 21:58:26.828: INFO: stdout: "true"
    Jan 19 21:58:26.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 21:58:26.883: INFO: stderr: ""
    Jan 19 21:58:26.883: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 21:58:26.883: INFO: validating pod update-demo-nautilus-fslmj
    Jan 19 21:58:26.886: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 21:58:26.886: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 21:58:26.886: INFO: update-demo-nautilus-fslmj is verified up and running
    STEP: scaling up the replication controller 01/19/23 21:58:26.886
    Jan 19 21:58:26.889: INFO: scanned /root for discovery docs: <nil>
    Jan 19 21:58:26.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan 19 21:58:27.960: INFO: stderr: ""
    Jan 19 21:58:27.960: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 21:58:27.96
    Jan 19 21:58:27.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 21:58:28.020: INFO: stderr: ""
    Jan 19 21:58:28.020: INFO: stdout: "update-demo-nautilus-9ljjq update-demo-nautilus-fslmj "
    Jan 19 21:58:28.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-9ljjq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 21:58:28.076: INFO: stderr: ""
    Jan 19 21:58:28.076: INFO: stdout: ""
    Jan 19 21:58:28.076: INFO: update-demo-nautilus-9ljjq is created but not running
    Jan 19 21:58:33.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 21:58:33.137: INFO: stderr: ""
    Jan 19 21:58:33.137: INFO: stdout: "update-demo-nautilus-9ljjq update-demo-nautilus-fslmj "
    Jan 19 21:58:33.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-9ljjq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 21:58:33.192: INFO: stderr: ""
    Jan 19 21:58:33.192: INFO: stdout: "true"
    Jan 19 21:58:33.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-9ljjq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 21:58:33.247: INFO: stderr: ""
    Jan 19 21:58:33.247: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 21:58:33.247: INFO: validating pod update-demo-nautilus-9ljjq
    Jan 19 21:58:33.252: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 21:58:33.252: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 21:58:33.252: INFO: update-demo-nautilus-9ljjq is verified up and running
    Jan 19 21:58:33.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 21:58:33.309: INFO: stderr: ""
    Jan 19 21:58:33.309: INFO: stdout: "true"
    Jan 19 21:58:33.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods update-demo-nautilus-fslmj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 21:58:33.368: INFO: stderr: ""
    Jan 19 21:58:33.368: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 21:58:33.368: INFO: validating pod update-demo-nautilus-fslmj
    Jan 19 21:58:33.372: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 21:58:33.372: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 21:58:33.372: INFO: update-demo-nautilus-fslmj is verified up and running
    STEP: using delete to clean up resources 01/19/23 21:58:33.372
    Jan 19 21:58:33.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 delete --grace-period=0 --force -f -'
    Jan 19 21:58:33.431: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 21:58:33.431: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 19 21:58:33.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get rc,svc -l name=update-demo --no-headers'
    Jan 19 21:58:33.505: INFO: stderr: "No resources found in kubectl-9943 namespace.\n"
    Jan 19 21:58:33.505: INFO: stdout: ""
    Jan 19 21:58:33.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-9943 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 19 21:58:33.569: INFO: stderr: ""
    Jan 19 21:58:33.569: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:58:33.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9943" for this suite. 01/19/23 21:58:33.573
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:58:33.58
Jan 19 21:58:33.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 21:58:33.58
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:33.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:33.593
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 21:58:33.636
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:58:33.976
STEP: Deploying the webhook pod 01/19/23 21:58:33.984
STEP: Wait for the deployment to be ready 01/19/23 21:58:33.996
Jan 19 21:58:34.003: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/19/23 21:58:36.01
STEP: Verifying the service has paired with the endpoint 01/19/23 21:58:36.019
Jan 19 21:58:37.020: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 01/19/23 21:58:37.023
STEP: create a pod 01/19/23 21:58:37.036
Jan 19 21:58:37.047: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3097" to be "running"
Jan 19 21:58:37.049: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346454ms
Jan 19 21:58:39.053: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005583979s
Jan 19 21:58:39.053: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/19/23 21:58:39.053
Jan 19 21:58:39.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=webhook-3097 attach --namespace=webhook-3097 to-be-attached-pod -i -c=container1'
Jan 19 21:58:39.123: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 21:58:39.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3097" for this suite. 01/19/23 21:58:39.132
STEP: Destroying namespace "webhook-3097-markers" for this suite. 01/19/23 21:58:39.137
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":326,"skipped":6083,"failed":0}
------------------------------
• [SLOW TEST] [5.611 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:58:33.58
    Jan 19 21:58:33.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 21:58:33.58
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:33.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:33.593
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 21:58:33.636
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 21:58:33.976
    STEP: Deploying the webhook pod 01/19/23 21:58:33.984
    STEP: Wait for the deployment to be ready 01/19/23 21:58:33.996
    Jan 19 21:58:34.003: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/19/23 21:58:36.01
    STEP: Verifying the service has paired with the endpoint 01/19/23 21:58:36.019
    Jan 19 21:58:37.020: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 01/19/23 21:58:37.023
    STEP: create a pod 01/19/23 21:58:37.036
    Jan 19 21:58:37.047: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3097" to be "running"
    Jan 19 21:58:37.049: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.346454ms
    Jan 19 21:58:39.053: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005583979s
    Jan 19 21:58:39.053: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/19/23 21:58:39.053
    Jan 19 21:58:39.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=webhook-3097 attach --namespace=webhook-3097 to-be-attached-pod -i -c=container1'
    Jan 19 21:58:39.123: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 21:58:39.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3097" for this suite. 01/19/23 21:58:39.132
    STEP: Destroying namespace "webhook-3097-markers" for this suite. 01/19/23 21:58:39.137
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:58:39.192
Jan 19 21:58:39.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename statefulset 01/19/23 21:58:39.192
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:39.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:39.211
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8207 01/19/23 21:58:39.216
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
W0119 21:58:39.236407      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:58:39.254: INFO: Found 0 stateful pods, waiting for 1
Jan 19 21:58:49.258: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/19/23 21:58:49.262
W0119 21:58:49.267510      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan 19 21:58:49.273: INFO: Found 1 stateful pods, waiting for 2
Jan 19 21:58:59.278: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan 19 21:58:59.278: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/19/23 21:58:59.283
STEP: Delete all of the StatefulSets 01/19/23 21:58:59.286
STEP: Verify that StatefulSets have been deleted 01/19/23 21:58:59.293
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jan 19 21:58:59.297: INFO: Deleting all statefulset in ns statefulset-8207
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jan 19 21:58:59.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8207" for this suite. 01/19/23 21:58:59.31
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":327,"skipped":6124,"failed":0}
------------------------------
• [SLOW TEST] [20.123 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:58:39.192
    Jan 19 21:58:39.192: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename statefulset 01/19/23 21:58:39.192
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:39.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:39.211
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8207 01/19/23 21:58:39.216
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    W0119 21:58:39.236407      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:58:39.254: INFO: Found 0 stateful pods, waiting for 1
    Jan 19 21:58:49.258: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/19/23 21:58:49.262
    W0119 21:58:49.267510      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan 19 21:58:49.273: INFO: Found 1 stateful pods, waiting for 2
    Jan 19 21:58:59.278: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan 19 21:58:59.278: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/19/23 21:58:59.283
    STEP: Delete all of the StatefulSets 01/19/23 21:58:59.286
    STEP: Verify that StatefulSets have been deleted 01/19/23 21:58:59.293
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jan 19 21:58:59.297: INFO: Deleting all statefulset in ns statefulset-8207
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jan 19 21:58:59.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8207" for this suite. 01/19/23 21:58:59.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:58:59.315
Jan 19 21:58:59.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename job 01/19/23 21:58:59.316
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:59.329
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:59.331
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 01/19/23 21:58:59.335
W0119 21:58:59.347548      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 01/19/23 21:58:59.347
STEP: Ensuring pods with index for job exist 01/19/23 21:59:09.351
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 19 21:59:09.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7705" for this suite. 01/19/23 21:59:09.358
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":328,"skipped":6137,"failed":0}
------------------------------
• [SLOW TEST] [10.048 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:58:59.315
    Jan 19 21:58:59.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename job 01/19/23 21:58:59.316
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:58:59.329
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:58:59.331
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 01/19/23 21:58:59.335
    W0119 21:58:59.347548      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 01/19/23 21:58:59.347
    STEP: Ensuring pods with index for job exist 01/19/23 21:59:09.351
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 19 21:59:09.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7705" for this suite. 01/19/23 21:59:09.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:59:09.364
Jan 19 21:59:09.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:59:09.365
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:09.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:09.385
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 01/19/23 21:59:09.387
Jan 19 21:59:09.387: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan 19 21:59:09.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 create -f -'
Jan 19 21:59:11.773: INFO: stderr: ""
Jan 19 21:59:11.773: INFO: stdout: "service/agnhost-replica created\n"
Jan 19 21:59:11.773: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan 19 21:59:11.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 create -f -'
Jan 19 21:59:12.148: INFO: stderr: ""
Jan 19 21:59:12.148: INFO: stdout: "service/agnhost-primary created\n"
Jan 19 21:59:12.148: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan 19 21:59:12.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 create -f -'
Jan 19 21:59:12.513: INFO: stderr: ""
Jan 19 21:59:12.513: INFO: stdout: "service/frontend created\n"
Jan 19 21:59:12.513: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan 19 21:59:12.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 create -f -'
Jan 19 21:59:12.884: INFO: stderr: ""
Jan 19 21:59:12.884: INFO: stdout: "deployment.apps/frontend created\n"
Jan 19 21:59:12.884: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 19 21:59:12.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 create -f -'
Jan 19 21:59:13.249: INFO: stderr: ""
Jan 19 21:59:13.249: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan 19 21:59:13.249: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan 19 21:59:13.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 create -f -'
Jan 19 21:59:13.739: INFO: stderr: ""
Jan 19 21:59:13.739: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/19/23 21:59:13.739
Jan 19 21:59:13.740: INFO: Waiting for all frontend pods to be Running.
Jan 19 21:59:18.790: INFO: Waiting for frontend to serve content.
Jan 19 21:59:18.801: INFO: Trying to add a new entry to the guestbook.
Jan 19 21:59:18.811: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 01/19/23 21:59:18.82
Jan 19 21:59:18.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 delete --grace-period=0 --force -f -'
Jan 19 21:59:18.890: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 21:59:18.890: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/19/23 21:59:18.89
Jan 19 21:59:18.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 delete --grace-period=0 --force -f -'
Jan 19 21:59:18.966: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 21:59:18.966: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/19/23 21:59:18.966
Jan 19 21:59:18.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 delete --grace-period=0 --force -f -'
Jan 19 21:59:19.037: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 21:59:19.037: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/19/23 21:59:19.037
Jan 19 21:59:19.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 delete --grace-period=0 --force -f -'
Jan 19 21:59:19.098: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 21:59:19.098: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/19/23 21:59:19.099
Jan 19 21:59:19.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 delete --grace-period=0 --force -f -'
Jan 19 21:59:19.157: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 21:59:19.157: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/19/23 21:59:19.157
Jan 19 21:59:19.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 delete --grace-period=0 --force -f -'
Jan 19 21:59:19.212: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 21:59:19.212: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:59:19.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4734" for this suite. 01/19/23 21:59:19.217
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":329,"skipped":6163,"failed":0}
------------------------------
• [SLOW TEST] [9.858 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:59:09.364
    Jan 19 21:59:09.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:59:09.365
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:09.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:09.385
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 01/19/23 21:59:09.387
    Jan 19 21:59:09.387: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan 19 21:59:09.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 create -f -'
    Jan 19 21:59:11.773: INFO: stderr: ""
    Jan 19 21:59:11.773: INFO: stdout: "service/agnhost-replica created\n"
    Jan 19 21:59:11.773: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan 19 21:59:11.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 create -f -'
    Jan 19 21:59:12.148: INFO: stderr: ""
    Jan 19 21:59:12.148: INFO: stdout: "service/agnhost-primary created\n"
    Jan 19 21:59:12.148: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan 19 21:59:12.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 create -f -'
    Jan 19 21:59:12.513: INFO: stderr: ""
    Jan 19 21:59:12.513: INFO: stdout: "service/frontend created\n"
    Jan 19 21:59:12.513: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan 19 21:59:12.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 create -f -'
    Jan 19 21:59:12.884: INFO: stderr: ""
    Jan 19 21:59:12.884: INFO: stdout: "deployment.apps/frontend created\n"
    Jan 19 21:59:12.884: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 19 21:59:12.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 create -f -'
    Jan 19 21:59:13.249: INFO: stderr: ""
    Jan 19 21:59:13.249: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan 19 21:59:13.249: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan 19 21:59:13.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 create -f -'
    Jan 19 21:59:13.739: INFO: stderr: ""
    Jan 19 21:59:13.739: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/19/23 21:59:13.739
    Jan 19 21:59:13.740: INFO: Waiting for all frontend pods to be Running.
    Jan 19 21:59:18.790: INFO: Waiting for frontend to serve content.
    Jan 19 21:59:18.801: INFO: Trying to add a new entry to the guestbook.
    Jan 19 21:59:18.811: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 01/19/23 21:59:18.82
    Jan 19 21:59:18.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 delete --grace-period=0 --force -f -'
    Jan 19 21:59:18.890: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 21:59:18.890: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/19/23 21:59:18.89
    Jan 19 21:59:18.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 delete --grace-period=0 --force -f -'
    Jan 19 21:59:18.966: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 21:59:18.966: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/19/23 21:59:18.966
    Jan 19 21:59:18.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 delete --grace-period=0 --force -f -'
    Jan 19 21:59:19.037: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 21:59:19.037: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/19/23 21:59:19.037
    Jan 19 21:59:19.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 delete --grace-period=0 --force -f -'
    Jan 19 21:59:19.098: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 21:59:19.098: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/19/23 21:59:19.099
    Jan 19 21:59:19.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 delete --grace-period=0 --force -f -'
    Jan 19 21:59:19.157: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 21:59:19.157: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/19/23 21:59:19.157
    Jan 19 21:59:19.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-4734 delete --grace-period=0 --force -f -'
    Jan 19 21:59:19.212: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 21:59:19.212: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:59:19.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4734" for this suite. 01/19/23 21:59:19.217
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:59:19.223
Jan 19 21:59:19.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename var-expansion 01/19/23 21:59:19.224
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:19.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:19.244
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
W0119 21:59:19.275722      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:59:19.275: INFO: Waiting up to 2m0s for pod "var-expansion-967cecde-8f40-417d-ad43-1d4a995bdcbc" in namespace "var-expansion-4179" to be "container 0 failed with reason CreateContainerConfigError"
Jan 19 21:59:19.281: INFO: Pod "var-expansion-967cecde-8f40-417d-ad43-1d4a995bdcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.245053ms
Jan 19 21:59:21.285: INFO: Pod "var-expansion-967cecde-8f40-417d-ad43-1d4a995bdcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009821406s
Jan 19 21:59:21.285: INFO: Pod "var-expansion-967cecde-8f40-417d-ad43-1d4a995bdcbc" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan 19 21:59:21.285: INFO: Deleting pod "var-expansion-967cecde-8f40-417d-ad43-1d4a995bdcbc" in namespace "var-expansion-4179"
Jan 19 21:59:21.293: INFO: Wait up to 5m0s for pod "var-expansion-967cecde-8f40-417d-ad43-1d4a995bdcbc" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jan 19 21:59:25.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4179" for this suite. 01/19/23 21:59:25.301
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":330,"skipped":6165,"failed":0}
------------------------------
• [SLOW TEST] [6.084 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:59:19.223
    Jan 19 21:59:19.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename var-expansion 01/19/23 21:59:19.224
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:19.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:19.244
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    W0119 21:59:19.275722      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "dapi-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "dapi-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "dapi-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "dapi-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:59:19.275: INFO: Waiting up to 2m0s for pod "var-expansion-967cecde-8f40-417d-ad43-1d4a995bdcbc" in namespace "var-expansion-4179" to be "container 0 failed with reason CreateContainerConfigError"
    Jan 19 21:59:19.281: INFO: Pod "var-expansion-967cecde-8f40-417d-ad43-1d4a995bdcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.245053ms
    Jan 19 21:59:21.285: INFO: Pod "var-expansion-967cecde-8f40-417d-ad43-1d4a995bdcbc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009821406s
    Jan 19 21:59:21.285: INFO: Pod "var-expansion-967cecde-8f40-417d-ad43-1d4a995bdcbc" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan 19 21:59:21.285: INFO: Deleting pod "var-expansion-967cecde-8f40-417d-ad43-1d4a995bdcbc" in namespace "var-expansion-4179"
    Jan 19 21:59:21.293: INFO: Wait up to 5m0s for pod "var-expansion-967cecde-8f40-417d-ad43-1d4a995bdcbc" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jan 19 21:59:25.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4179" for this suite. 01/19/23 21:59:25.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:59:25.308
Jan 19 21:59:25.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:59:25.308
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:25.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:25.327
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-97a39aaa-03cf-46a0-b708-2682ca7fc58c 01/19/23 21:59:25.329
STEP: Creating a pod to test consume configMaps 01/19/23 21:59:25.338
Jan 19 21:59:25.365: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991" in namespace "projected-7730" to be "Succeeded or Failed"
Jan 19 21:59:25.367: INFO: Pod "pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991": Phase="Pending", Reason="", readiness=false. Elapsed: 2.226741ms
Jan 19 21:59:27.370: INFO: Pod "pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005114611s
Jan 19 21:59:29.370: INFO: Pod "pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005607579s
STEP: Saw pod success 01/19/23 21:59:29.37
Jan 19 21:59:29.371: INFO: Pod "pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991" satisfied condition "Succeeded or Failed"
Jan 19 21:59:29.373: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991 container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/19/23 21:59:29.378
Jan 19 21:59:29.387: INFO: Waiting for pod pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991 to disappear
Jan 19 21:59:29.390: INFO: Pod pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jan 19 21:59:29.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7730" for this suite. 01/19/23 21:59:29.394
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":331,"skipped":6175,"failed":0}
------------------------------
• [4.091 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:59:25.308
    Jan 19 21:59:25.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:59:25.308
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:25.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:25.327
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-97a39aaa-03cf-46a0-b708-2682ca7fc58c 01/19/23 21:59:25.329
    STEP: Creating a pod to test consume configMaps 01/19/23 21:59:25.338
    Jan 19 21:59:25.365: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991" in namespace "projected-7730" to be "Succeeded or Failed"
    Jan 19 21:59:25.367: INFO: Pod "pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991": Phase="Pending", Reason="", readiness=false. Elapsed: 2.226741ms
    Jan 19 21:59:27.370: INFO: Pod "pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005114611s
    Jan 19 21:59:29.370: INFO: Pod "pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005607579s
    STEP: Saw pod success 01/19/23 21:59:29.37
    Jan 19 21:59:29.371: INFO: Pod "pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991" satisfied condition "Succeeded or Failed"
    Jan 19 21:59:29.373: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/19/23 21:59:29.378
    Jan 19 21:59:29.387: INFO: Waiting for pod pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991 to disappear
    Jan 19 21:59:29.390: INFO: Pod pod-projected-configmaps-d6cf1270-1092-41ff-be9d-648da1beb991 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jan 19 21:59:29.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7730" for this suite. 01/19/23 21:59:29.394
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:59:29.399
Jan 19 21:59:29.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename resourcequota 01/19/23 21:59:29.4
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:29.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:29.412
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 01/19/23 21:59:29.417
STEP: Ensuring ResourceQuota status is calculated 01/19/23 21:59:29.425
STEP: Creating a ResourceQuota with not best effort scope 01/19/23 21:59:31.432
STEP: Ensuring ResourceQuota status is calculated 01/19/23 21:59:31.436
STEP: Creating a best-effort pod 01/19/23 21:59:33.439
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/19/23 21:59:33.46
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/19/23 21:59:35.463
STEP: Deleting the pod 01/19/23 21:59:37.467
STEP: Ensuring resource quota status released the pod usage 01/19/23 21:59:37.475
STEP: Creating a not best-effort pod 01/19/23 21:59:39.486
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/19/23 21:59:39.507
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/19/23 21:59:41.511
STEP: Deleting the pod 01/19/23 21:59:43.514
STEP: Ensuring resource quota status released the pod usage 01/19/23 21:59:43.527
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jan 19 21:59:45.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3667" for this suite. 01/19/23 21:59:45.534
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":332,"skipped":6175,"failed":0}
------------------------------
• [SLOW TEST] [16.141 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:59:29.399
    Jan 19 21:59:29.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename resourcequota 01/19/23 21:59:29.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:29.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:29.412
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 01/19/23 21:59:29.417
    STEP: Ensuring ResourceQuota status is calculated 01/19/23 21:59:29.425
    STEP: Creating a ResourceQuota with not best effort scope 01/19/23 21:59:31.432
    STEP: Ensuring ResourceQuota status is calculated 01/19/23 21:59:31.436
    STEP: Creating a best-effort pod 01/19/23 21:59:33.439
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/19/23 21:59:33.46
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/19/23 21:59:35.463
    STEP: Deleting the pod 01/19/23 21:59:37.467
    STEP: Ensuring resource quota status released the pod usage 01/19/23 21:59:37.475
    STEP: Creating a not best-effort pod 01/19/23 21:59:39.486
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/19/23 21:59:39.507
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/19/23 21:59:41.511
    STEP: Deleting the pod 01/19/23 21:59:43.514
    STEP: Ensuring resource quota status released the pod usage 01/19/23 21:59:43.527
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jan 19 21:59:45.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3667" for this suite. 01/19/23 21:59:45.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:59:45.541
Jan 19 21:59:45.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 21:59:45.541
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:45.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:45.559
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 01/19/23 21:59:45.561
Jan 19 21:59:45.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 create -f -'
Jan 19 21:59:45.925: INFO: stderr: ""
Jan 19 21:59:45.925: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 21:59:45.925
Jan 19 21:59:45.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 21:59:45.998: INFO: stderr: ""
Jan 19 21:59:45.998: INFO: stdout: "update-demo-nautilus-hksdt update-demo-nautilus-kspgd "
Jan 19 21:59:45.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods update-demo-nautilus-hksdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 21:59:46.058: INFO: stderr: ""
Jan 19 21:59:46.058: INFO: stdout: ""
Jan 19 21:59:46.058: INFO: update-demo-nautilus-hksdt is created but not running
Jan 19 21:59:51.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan 19 21:59:51.118: INFO: stderr: ""
Jan 19 21:59:51.118: INFO: stdout: "update-demo-nautilus-hksdt update-demo-nautilus-kspgd "
Jan 19 21:59:51.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods update-demo-nautilus-hksdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 21:59:51.177: INFO: stderr: ""
Jan 19 21:59:51.177: INFO: stdout: "true"
Jan 19 21:59:51.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods update-demo-nautilus-hksdt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 21:59:51.232: INFO: stderr: ""
Jan 19 21:59:51.232: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 21:59:51.232: INFO: validating pod update-demo-nautilus-hksdt
Jan 19 21:59:51.238: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 21:59:51.238: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 21:59:51.238: INFO: update-demo-nautilus-hksdt is verified up and running
Jan 19 21:59:51.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods update-demo-nautilus-kspgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan 19 21:59:51.294: INFO: stderr: ""
Jan 19 21:59:51.294: INFO: stdout: "true"
Jan 19 21:59:51.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods update-demo-nautilus-kspgd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan 19 21:59:51.349: INFO: stderr: ""
Jan 19 21:59:51.349: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jan 19 21:59:51.349: INFO: validating pod update-demo-nautilus-kspgd
Jan 19 21:59:51.354: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan 19 21:59:51.354: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan 19 21:59:51.354: INFO: update-demo-nautilus-kspgd is verified up and running
STEP: using delete to clean up resources 01/19/23 21:59:51.354
Jan 19 21:59:51.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 delete --grace-period=0 --force -f -'
Jan 19 21:59:51.407: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan 19 21:59:51.407: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan 19 21:59:51.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get rc,svc -l name=update-demo --no-headers'
Jan 19 21:59:51.476: INFO: stderr: "No resources found in kubectl-5136 namespace.\n"
Jan 19 21:59:51.476: INFO: stdout: ""
Jan 19 21:59:51.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan 19 21:59:51.536: INFO: stderr: ""
Jan 19 21:59:51.536: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 21:59:51.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5136" for this suite. 01/19/23 21:59:51.541
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":333,"skipped":6184,"failed":0}
------------------------------
• [SLOW TEST] [6.006 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:59:45.541
    Jan 19 21:59:45.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 21:59:45.541
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:45.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:45.559
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 01/19/23 21:59:45.561
    Jan 19 21:59:45.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 create -f -'
    Jan 19 21:59:45.925: INFO: stderr: ""
    Jan 19 21:59:45.925: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/19/23 21:59:45.925
    Jan 19 21:59:45.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 21:59:45.998: INFO: stderr: ""
    Jan 19 21:59:45.998: INFO: stdout: "update-demo-nautilus-hksdt update-demo-nautilus-kspgd "
    Jan 19 21:59:45.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods update-demo-nautilus-hksdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 21:59:46.058: INFO: stderr: ""
    Jan 19 21:59:46.058: INFO: stdout: ""
    Jan 19 21:59:46.058: INFO: update-demo-nautilus-hksdt is created but not running
    Jan 19 21:59:51.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan 19 21:59:51.118: INFO: stderr: ""
    Jan 19 21:59:51.118: INFO: stdout: "update-demo-nautilus-hksdt update-demo-nautilus-kspgd "
    Jan 19 21:59:51.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods update-demo-nautilus-hksdt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 21:59:51.177: INFO: stderr: ""
    Jan 19 21:59:51.177: INFO: stdout: "true"
    Jan 19 21:59:51.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods update-demo-nautilus-hksdt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 21:59:51.232: INFO: stderr: ""
    Jan 19 21:59:51.232: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 21:59:51.232: INFO: validating pod update-demo-nautilus-hksdt
    Jan 19 21:59:51.238: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 21:59:51.238: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 21:59:51.238: INFO: update-demo-nautilus-hksdt is verified up and running
    Jan 19 21:59:51.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods update-demo-nautilus-kspgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan 19 21:59:51.294: INFO: stderr: ""
    Jan 19 21:59:51.294: INFO: stdout: "true"
    Jan 19 21:59:51.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods update-demo-nautilus-kspgd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan 19 21:59:51.349: INFO: stderr: ""
    Jan 19 21:59:51.349: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jan 19 21:59:51.349: INFO: validating pod update-demo-nautilus-kspgd
    Jan 19 21:59:51.354: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan 19 21:59:51.354: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan 19 21:59:51.354: INFO: update-demo-nautilus-kspgd is verified up and running
    STEP: using delete to clean up resources 01/19/23 21:59:51.354
    Jan 19 21:59:51.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 delete --grace-period=0 --force -f -'
    Jan 19 21:59:51.407: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan 19 21:59:51.407: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan 19 21:59:51.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get rc,svc -l name=update-demo --no-headers'
    Jan 19 21:59:51.476: INFO: stderr: "No resources found in kubectl-5136 namespace.\n"
    Jan 19 21:59:51.476: INFO: stdout: ""
    Jan 19 21:59:51.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-5136 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan 19 21:59:51.536: INFO: stderr: ""
    Jan 19 21:59:51.536: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 21:59:51.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5136" for this suite. 01/19/23 21:59:51.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:59:51.547
Jan 19 21:59:51.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 21:59:51.548
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:51.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:51.566
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 01/19/23 21:59:51.57
W0119 21:59:51.596031      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 21:59:51.596: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a" in namespace "projected-3214" to be "Succeeded or Failed"
Jan 19 21:59:51.607: INFO: Pod "downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.007317ms
Jan 19 21:59:53.610: INFO: Pod "downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014702707s
Jan 19 21:59:55.609: INFO: Pod "downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013454885s
STEP: Saw pod success 01/19/23 21:59:55.609
Jan 19 21:59:55.609: INFO: Pod "downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a" satisfied condition "Succeeded or Failed"
Jan 19 21:59:55.612: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a container client-container: <nil>
STEP: delete the pod 01/19/23 21:59:55.616
Jan 19 21:59:55.625: INFO: Waiting for pod downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a to disappear
Jan 19 21:59:55.627: INFO: Pod downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 21:59:55.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3214" for this suite. 01/19/23 21:59:55.632
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":334,"skipped":6195,"failed":0}
------------------------------
• [4.091 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:59:51.547
    Jan 19 21:59:51.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 21:59:51.548
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:51.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:51.566
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 01/19/23 21:59:51.57
    W0119 21:59:51.596031      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "client-container" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "client-container" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "client-container" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "client-container" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 21:59:51.596: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a" in namespace "projected-3214" to be "Succeeded or Failed"
    Jan 19 21:59:51.607: INFO: Pod "downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.007317ms
    Jan 19 21:59:53.610: INFO: Pod "downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014702707s
    Jan 19 21:59:55.609: INFO: Pod "downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013454885s
    STEP: Saw pod success 01/19/23 21:59:55.609
    Jan 19 21:59:55.609: INFO: Pod "downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a" satisfied condition "Succeeded or Failed"
    Jan 19 21:59:55.612: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a container client-container: <nil>
    STEP: delete the pod 01/19/23 21:59:55.616
    Jan 19 21:59:55.625: INFO: Waiting for pod downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a to disappear
    Jan 19 21:59:55.627: INFO: Pod downwardapi-volume-e97fde1b-ffd5-4f1b-b1d2-2d63ab468f8a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 21:59:55.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3214" for this suite. 01/19/23 21:59:55.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 21:59:55.639
Jan 19 21:59:55.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-probe 01/19/23 21:59:55.641
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:55.656
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:55.658
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed in namespace container-probe-4640 01/19/23 21:59:55.663
Jan 19 21:59:55.694: INFO: Waiting up to 5m0s for pod "liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed" in namespace "container-probe-4640" to be "not pending"
Jan 19 21:59:55.696: INFO: Pod "liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.598337ms
Jan 19 21:59:57.699: INFO: Pod "liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed": Phase="Running", Reason="", readiness=true. Elapsed: 2.005330655s
Jan 19 21:59:57.699: INFO: Pod "liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed" satisfied condition "not pending"
Jan 19 21:59:57.699: INFO: Started pod liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed in namespace container-probe-4640
STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 21:59:57.699
Jan 19 21:59:57.701: INFO: Initial restart count of pod liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed is 0
Jan 19 22:00:17.745: INFO: Restart count of pod container-probe-4640/liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed is now 1 (20.043982453s elapsed)
STEP: deleting the pod 01/19/23 22:00:17.745
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 22:00:17.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4640" for this suite. 01/19/23 22:00:17.759
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":335,"skipped":6233,"failed":0}
------------------------------
• [SLOW TEST] [22.124 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 21:59:55.639
    Jan 19 21:59:55.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-probe 01/19/23 21:59:55.641
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 21:59:55.656
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 21:59:55.658
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed in namespace container-probe-4640 01/19/23 21:59:55.663
    Jan 19 21:59:55.694: INFO: Waiting up to 5m0s for pod "liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed" in namespace "container-probe-4640" to be "not pending"
    Jan 19 21:59:55.696: INFO: Pod "liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.598337ms
    Jan 19 21:59:57.699: INFO: Pod "liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed": Phase="Running", Reason="", readiness=true. Elapsed: 2.005330655s
    Jan 19 21:59:57.699: INFO: Pod "liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed" satisfied condition "not pending"
    Jan 19 21:59:57.699: INFO: Started pod liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed in namespace container-probe-4640
    STEP: checking the pod's current state and verifying that restartCount is present 01/19/23 21:59:57.699
    Jan 19 21:59:57.701: INFO: Initial restart count of pod liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed is 0
    Jan 19 22:00:17.745: INFO: Restart count of pod container-probe-4640/liveness-1e5463a1-502d-44c3-a6d4-ef0ddeeb15ed is now 1 (20.043982453s elapsed)
    STEP: deleting the pod 01/19/23 22:00:17.745
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 22:00:17.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4640" for this suite. 01/19/23 22:00:17.759
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:00:17.764
Jan 19 22:00:17.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename downward-api 01/19/23 22:00:17.765
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:00:17.785
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:00:17.787
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 01/19/23 22:00:17.789
Jan 19 22:00:17.840: INFO: Waiting up to 5m0s for pod "downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59" in namespace "downward-api-8701" to be "Succeeded or Failed"
Jan 19 22:00:17.847: INFO: Pod "downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59": Phase="Pending", Reason="", readiness=false. Elapsed: 6.826967ms
Jan 19 22:00:19.851: INFO: Pod "downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010272131s
Jan 19 22:00:21.851: INFO: Pod "downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010903951s
STEP: Saw pod success 01/19/23 22:00:21.851
Jan 19 22:00:21.851: INFO: Pod "downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59" satisfied condition "Succeeded or Failed"
Jan 19 22:00:21.855: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59 container client-container: <nil>
STEP: delete the pod 01/19/23 22:00:21.86
Jan 19 22:00:21.869: INFO: Waiting for pod downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59 to disappear
Jan 19 22:00:21.871: INFO: Pod downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jan 19 22:00:21.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8701" for this suite. 01/19/23 22:00:21.875
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":336,"skipped":6256,"failed":0}
------------------------------
• [4.116 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:00:17.764
    Jan 19 22:00:17.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename downward-api 01/19/23 22:00:17.765
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:00:17.785
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:00:17.787
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 01/19/23 22:00:17.789
    Jan 19 22:00:17.840: INFO: Waiting up to 5m0s for pod "downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59" in namespace "downward-api-8701" to be "Succeeded or Failed"
    Jan 19 22:00:17.847: INFO: Pod "downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59": Phase="Pending", Reason="", readiness=false. Elapsed: 6.826967ms
    Jan 19 22:00:19.851: INFO: Pod "downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010272131s
    Jan 19 22:00:21.851: INFO: Pod "downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010903951s
    STEP: Saw pod success 01/19/23 22:00:21.851
    Jan 19 22:00:21.851: INFO: Pod "downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59" satisfied condition "Succeeded or Failed"
    Jan 19 22:00:21.855: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59 container client-container: <nil>
    STEP: delete the pod 01/19/23 22:00:21.86
    Jan 19 22:00:21.869: INFO: Waiting for pod downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59 to disappear
    Jan 19 22:00:21.871: INFO: Pod downwardapi-volume-354a3d4c-a642-4730-a3d9-84cfe7402d59 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jan 19 22:00:21.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8701" for this suite. 01/19/23 22:00:21.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:00:21.881
Jan 19 22:00:21.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename job 01/19/23 22:00:21.882
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:00:21.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:00:21.901
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 01/19/23 22:00:21.903
W0119 22:00:21.912328      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 01/19/23 22:00:21.912
STEP: delete a job 01/19/23 22:00:23.915
STEP: deleting Job.batch foo in namespace job-5510, will wait for the garbage collector to delete the pods 01/19/23 22:00:23.915
Jan 19 22:00:23.972: INFO: Deleting Job.batch foo took: 4.523723ms
Jan 19 22:00:24.073: INFO: Terminating Job.batch foo pods took: 100.977154ms
STEP: Ensuring job was deleted 01/19/23 22:00:56.774
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 19 22:00:56.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5510" for this suite. 01/19/23 22:00:56.782
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":337,"skipped":6261,"failed":0}
------------------------------
• [SLOW TEST] [34.907 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:00:21.881
    Jan 19 22:00:21.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename job 01/19/23 22:00:21.882
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:00:21.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:00:21.901
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 01/19/23 22:00:21.903
    W0119 22:00:21.912328      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 01/19/23 22:00:21.912
    STEP: delete a job 01/19/23 22:00:23.915
    STEP: deleting Job.batch foo in namespace job-5510, will wait for the garbage collector to delete the pods 01/19/23 22:00:23.915
    Jan 19 22:00:23.972: INFO: Deleting Job.batch foo took: 4.523723ms
    Jan 19 22:00:24.073: INFO: Terminating Job.batch foo pods took: 100.977154ms
    STEP: Ensuring job was deleted 01/19/23 22:00:56.774
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 19 22:00:56.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5510" for this suite. 01/19/23 22:00:56.782
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:00:56.788
Jan 19 22:00:56.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename watch 01/19/23 22:00:56.789
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:00:56.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:00:56.813
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/19/23 22:00:56.815
STEP: creating a new configmap 01/19/23 22:00:56.817
STEP: modifying the configmap once 01/19/23 22:00:56.833
STEP: changing the label value of the configmap 01/19/23 22:00:56.85
STEP: Expecting to observe a delete notification for the watched object 01/19/23 22:00:56.891
Jan 19 22:00:56.892: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3924  ed94aebe-4155-402d-9004-92a3bda42818 236034 0 2023-01-19 22:00:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 22:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 22:00:56.892: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3924  ed94aebe-4155-402d-9004-92a3bda42818 236043 0 2023-01-19 22:00:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 22:00:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 22:00:56.898: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3924  ed94aebe-4155-402d-9004-92a3bda42818 236053 0 2023-01-19 22:00:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 22:00:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/19/23 22:00:56.898
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/19/23 22:00:56.924
STEP: changing the label value of the configmap back 01/19/23 22:01:06.924
STEP: modifying the configmap a third time 01/19/23 22:01:06.932
STEP: deleting the configmap 01/19/23 22:01:06.939
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/19/23 22:01:06.944
Jan 19 22:01:06.944: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3924  ed94aebe-4155-402d-9004-92a3bda42818 236260 0 2023-01-19 22:00:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 22:01:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 22:01:06.944: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3924  ed94aebe-4155-402d-9004-92a3bda42818 236261 0 2023-01-19 22:00:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 22:01:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan 19 22:01:06.945: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3924  ed94aebe-4155-402d-9004-92a3bda42818 236262 0 2023-01-19 22:00:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 22:01:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jan 19 22:01:06.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3924" for this suite. 01/19/23 22:01:06.949
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":338,"skipped":6273,"failed":0}
------------------------------
• [SLOW TEST] [10.168 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:00:56.788
    Jan 19 22:00:56.788: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename watch 01/19/23 22:00:56.789
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:00:56.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:00:56.813
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/19/23 22:00:56.815
    STEP: creating a new configmap 01/19/23 22:00:56.817
    STEP: modifying the configmap once 01/19/23 22:00:56.833
    STEP: changing the label value of the configmap 01/19/23 22:00:56.85
    STEP: Expecting to observe a delete notification for the watched object 01/19/23 22:00:56.891
    Jan 19 22:00:56.892: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3924  ed94aebe-4155-402d-9004-92a3bda42818 236034 0 2023-01-19 22:00:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 22:00:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 22:00:56.892: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3924  ed94aebe-4155-402d-9004-92a3bda42818 236043 0 2023-01-19 22:00:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 22:00:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 22:00:56.898: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3924  ed94aebe-4155-402d-9004-92a3bda42818 236053 0 2023-01-19 22:00:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 22:00:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/19/23 22:00:56.898
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/19/23 22:00:56.924
    STEP: changing the label value of the configmap back 01/19/23 22:01:06.924
    STEP: modifying the configmap a third time 01/19/23 22:01:06.932
    STEP: deleting the configmap 01/19/23 22:01:06.939
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/19/23 22:01:06.944
    Jan 19 22:01:06.944: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3924  ed94aebe-4155-402d-9004-92a3bda42818 236260 0 2023-01-19 22:00:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 22:01:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 22:01:06.944: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3924  ed94aebe-4155-402d-9004-92a3bda42818 236261 0 2023-01-19 22:00:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 22:01:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan 19 22:01:06.945: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3924  ed94aebe-4155-402d-9004-92a3bda42818 236262 0 2023-01-19 22:00:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-19 22:01:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jan 19 22:01:06.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3924" for this suite. 01/19/23 22:01:06.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:01:06.958
Jan 19 22:01:06.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 22:01:06.958
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:06.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:06.974
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 01/19/23 22:01:06.976
Jan 19 22:01:06.976: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-821 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/19/23 22:01:07.012
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 22:01:07.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-821" for this suite. 01/19/23 22:01:07.029
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":339,"skipped":6309,"failed":0}
------------------------------
• [0.082 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:01:06.958
    Jan 19 22:01:06.958: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 22:01:06.958
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:06.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:06.974
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 01/19/23 22:01:06.976
    Jan 19 22:01:06.976: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-821 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/19/23 22:01:07.012
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 22:01:07.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-821" for this suite. 01/19/23 22:01:07.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:01:07.04
Jan 19 22:01:07.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename deployment 01/19/23 22:01:07.041
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:07.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:07.073
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
W0119 22:01:07.080654      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 22:01:07.088: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan 19 22:01:12.093: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/19/23 22:01:12.093
Jan 19 22:01:12.093: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan 19 22:01:14.097: INFO: Creating deployment "test-rollover-deployment"
Jan 19 22:01:14.104: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan 19 22:01:16.111: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan 19 22:01:16.116: INFO: Ensure that both replica sets have 1 created replica
Jan 19 22:01:16.122: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan 19 22:01:16.129: INFO: Updating deployment test-rollover-deployment
Jan 19 22:01:16.129: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan 19 22:01:18.135: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan 19 22:01:18.139: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan 19 22:01:18.145: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 22:01:18.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 22:01:20.150: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 22:01:20.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 22:01:22.151: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 22:01:22.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 22:01:24.153: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 22:01:24.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 22:01:26.151: INFO: all replica sets need to contain the pod-template-hash label
Jan 19 22:01:26.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan 19 22:01:28.151: INFO: 
Jan 19 22:01:28.151: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan 19 22:01:28.158: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9199  fbd10885-9cf1-47e2-99c2-40a8accf5044 236738 2 2023-01-19 22:01:14 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 22:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004232de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 22:01:14 +0000 UTC,LastTransitionTime:2023-01-19 22:01:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-19 22:01:27 +0000 UTC,LastTransitionTime:2023-01-19 22:01:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan 19 22:01:28.160: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-9199  cfabb8d8-6cfb-4fde-98dc-af1f161da147 236728 2 2023-01-19 22:01:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment fbd10885-9cf1-47e2-99c2-40a8accf5044 0xc00c33bb97 0xc00c33bb98}] [] [{kube-controller-manager Update apps/v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fbd10885-9cf1-47e2-99c2-40a8accf5044\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 22:01:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c33bc48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan 19 22:01:28.160: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan 19 22:01:28.160: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9199  c4d54b4f-5f5a-463f-83cf-863a178d2907 236737 2 2023-01-19 22:01:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment fbd10885-9cf1-47e2-99c2-40a8accf5044 0xc00c33b947 0xc00c33b948}] [] [{e2e.test Update apps/v1 2023-01-19 22:01:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 22:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fbd10885-9cf1-47e2-99c2-40a8accf5044\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-19 22:01:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00c33ba08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 22:01:28.160: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-9199  5e807a63-7058-4dcf-b356-95d03e9f59bb 236599 2 2023-01-19 22:01:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment fbd10885-9cf1-47e2-99c2-40a8accf5044 0xc00c33ba77 0xc00c33ba78}] [] [{kube-controller-manager Update apps/v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fbd10885-9cf1-47e2-99c2-40a8accf5044\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c33bb28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan 19 22:01:28.163: INFO: Pod "test-rollover-deployment-6d45fd857b-4wvj8" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-4wvj8 test-rollover-deployment-6d45fd857b- deployment-9199  278d4ea5-cb2f-4986-8977-71af34ef1c16 236627 0 2023-01-19 22:01:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.184/23"],"mac_address":"0a:58:0a:80:0c:b8","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.184/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.184"
    ],
    "mac": "0a:58:0a:80:0c:b8",
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "ovn-kubernetes",
    "interface": "eth0",
    "ips": [
        "10.128.12.184"
    ],
    "mac": "0a:58:0a:80:0c:b8",
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b cfabb8d8-6cfb-4fde-98dc-af1f161da147 0xc00bb081a7 0xc00bb081a8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfabb8d8-6cfb-4fde-98dc-af1f161da147\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 22:01:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vtzlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vtzlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-171-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c69,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vwtsl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 22:01:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 22:01:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 22:01:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 22:01:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.171.213,PodIP:10.128.12.184,StartTime:2023-01-19 22:01:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 22:01:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://0870a3a0f9b56f15c42d669a8bedf50db85f04a0e2e22cd64637d49fe8f61524,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.12.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jan 19 22:01:28.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9199" for this suite. 01/19/23 22:01:28.167
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":340,"skipped":6318,"failed":0}
------------------------------
• [SLOW TEST] [21.134 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:01:07.04
    Jan 19 22:01:07.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename deployment 01/19/23 22:01:07.041
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:07.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:07.073
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    W0119 22:01:07.080654      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 22:01:07.088: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan 19 22:01:12.093: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/19/23 22:01:12.093
    Jan 19 22:01:12.093: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan 19 22:01:14.097: INFO: Creating deployment "test-rollover-deployment"
    Jan 19 22:01:14.104: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan 19 22:01:16.111: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan 19 22:01:16.116: INFO: Ensure that both replica sets have 1 created replica
    Jan 19 22:01:16.122: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan 19 22:01:16.129: INFO: Updating deployment test-rollover-deployment
    Jan 19 22:01:16.129: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan 19 22:01:18.135: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan 19 22:01:18.139: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan 19 22:01:18.145: INFO: all replica sets need to contain the pod-template-hash label
    Jan 19 22:01:18.145: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 22:01:20.150: INFO: all replica sets need to contain the pod-template-hash label
    Jan 19 22:01:20.150: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 22:01:22.151: INFO: all replica sets need to contain the pod-template-hash label
    Jan 19 22:01:22.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 22:01:24.153: INFO: all replica sets need to contain the pod-template-hash label
    Jan 19 22:01:24.153: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 22:01:26.151: INFO: all replica sets need to contain the pod-template-hash label
    Jan 19 22:01:26.151: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 19, 22, 1, 17, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 19, 22, 1, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan 19 22:01:28.151: INFO: 
    Jan 19 22:01:28.151: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan 19 22:01:28.158: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-9199  fbd10885-9cf1-47e2-99c2-40a8accf5044 236738 2 2023-01-19 22:01:14 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 22:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004232de8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-19 22:01:14 +0000 UTC,LastTransitionTime:2023-01-19 22:01:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-01-19 22:01:27 +0000 UTC,LastTransitionTime:2023-01-19 22:01:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan 19 22:01:28.160: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-9199  cfabb8d8-6cfb-4fde-98dc-af1f161da147 236728 2 2023-01-19 22:01:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment fbd10885-9cf1-47e2-99c2-40a8accf5044 0xc00c33bb97 0xc00c33bb98}] [] [{kube-controller-manager Update apps/v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fbd10885-9cf1-47e2-99c2-40a8accf5044\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 22:01:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c33bc48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 22:01:28.160: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan 19 22:01:28.160: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9199  c4d54b4f-5f5a-463f-83cf-863a178d2907 236737 2 2023-01-19 22:01:07 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment fbd10885-9cf1-47e2-99c2-40a8accf5044 0xc00c33b947 0xc00c33b948}] [] [{e2e.test Update apps/v1 2023-01-19 22:01:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 22:01:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fbd10885-9cf1-47e2-99c2-40a8accf5044\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-19 22:01:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00c33ba08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 22:01:28.160: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-9199  5e807a63-7058-4dcf-b356-95d03e9f59bb 236599 2 2023-01-19 22:01:14 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment fbd10885-9cf1-47e2-99c2-40a8accf5044 0xc00c33ba77 0xc00c33ba78}] [] [{kube-controller-manager Update apps/v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fbd10885-9cf1-47e2-99c2-40a8accf5044\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c33bb28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan 19 22:01:28.163: INFO: Pod "test-rollover-deployment-6d45fd857b-4wvj8" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-4wvj8 test-rollover-deployment-6d45fd857b- deployment-9199  278d4ea5-cb2f-4986-8977-71af34ef1c16 236627 0 2023-01-19 22:01:16 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[k8s.ovn.org/pod-networks:{"default":{"ip_addresses":["10.128.12.184/23"],"mac_address":"0a:58:0a:80:0c:b8","gateway_ips":["10.128.12.1"],"ip_address":"10.128.12.184/23","gateway_ip":"10.128.12.1"}} k8s.v1.cni.cncf.io/network-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.12.184"
        ],
        "mac": "0a:58:0a:80:0c:b8",
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "ovn-kubernetes",
        "interface": "eth0",
        "ips": [
            "10.128.12.184"
        ],
        "mac": "0a:58:0a:80:0c:b8",
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b cfabb8d8-6cfb-4fde-98dc-af1f161da147 0xc00bb081a7 0xc00bb081a8}] [] [{ip-10-0-194-246 Update v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.ovn.org/pod-networks":{}}}} } {kube-controller-manager Update v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cfabb8d8-6cfb-4fde-98dc-af1f161da147\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-01-19 22:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-01-19 22:01:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.128.12.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vtzlw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vtzlw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-171-213.ec2.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c69,c9,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-vwtsl,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 22:01:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 22:01:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 22:01:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-19 22:01:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.171.213,PodIP:10.128.12.184,StartTime:2023-01-19 22:01:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-19 22:01:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://0870a3a0f9b56f15c42d669a8bedf50db85f04a0e2e22cd64637d49fe8f61524,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.128.12.184,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jan 19 22:01:28.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-9199" for this suite. 01/19/23 22:01:28.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:01:28.175
Jan 19 22:01:28.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 22:01:28.176
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:28.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:28.19
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 01/19/23 22:01:28.199
Jan 19 22:01:28.232: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1918" to be "running and ready"
Jan 19 22:01:28.235: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.730759ms
Jan 19 22:01:28.235: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:01:30.239: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007320513s
Jan 19 22:01:30.239: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan 19 22:01:30.239: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 01/19/23 22:01:30.241
Jan 19 22:01:30.257: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1918" to be "running and ready"
Jan 19 22:01:30.259: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.117287ms
Jan 19 22:01:30.259: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:01:32.263: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006082749s
Jan 19 22:01:32.263: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan 19 22:01:32.263: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/19/23 22:01:32.266
STEP: delete the pod with lifecycle hook 01/19/23 22:01:32.277
Jan 19 22:01:32.283: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 19 22:01:32.287: INFO: Pod pod-with-poststart-http-hook still exists
Jan 19 22:01:34.287: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan 19 22:01:34.290: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jan 19 22:01:34.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1918" for this suite. 01/19/23 22:01:34.294
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":341,"skipped":6331,"failed":0}
------------------------------
• [SLOW TEST] [6.130 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:01:28.175
    Jan 19 22:01:28.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/19/23 22:01:28.176
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:28.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:28.19
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 01/19/23 22:01:28.199
    Jan 19 22:01:28.232: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1918" to be "running and ready"
    Jan 19 22:01:28.235: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.730759ms
    Jan 19 22:01:28.235: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 22:01:30.239: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007320513s
    Jan 19 22:01:30.239: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan 19 22:01:30.239: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 01/19/23 22:01:30.241
    Jan 19 22:01:30.257: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-1918" to be "running and ready"
    Jan 19 22:01:30.259: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.117287ms
    Jan 19 22:01:30.259: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 22:01:32.263: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006082749s
    Jan 19 22:01:32.263: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan 19 22:01:32.263: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/19/23 22:01:32.266
    STEP: delete the pod with lifecycle hook 01/19/23 22:01:32.277
    Jan 19 22:01:32.283: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 19 22:01:32.287: INFO: Pod pod-with-poststart-http-hook still exists
    Jan 19 22:01:34.287: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan 19 22:01:34.290: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jan 19 22:01:34.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-1918" for this suite. 01/19/23 22:01:34.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:01:34.306
Jan 19 22:01:34.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename emptydir 01/19/23 22:01:34.306
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:34.324
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:34.326
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 01/19/23 22:01:34.328
Jan 19 22:01:34.377: INFO: Waiting up to 5m0s for pod "pod-52854465-7a8d-44ca-879d-b55b9ab924a8" in namespace "emptydir-8761" to be "Succeeded or Failed"
Jan 19 22:01:34.384: INFO: Pod "pod-52854465-7a8d-44ca-879d-b55b9ab924a8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.4831ms
Jan 19 22:01:36.390: INFO: Pod "pod-52854465-7a8d-44ca-879d-b55b9ab924a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01325591s
Jan 19 22:01:38.387: INFO: Pod "pod-52854465-7a8d-44ca-879d-b55b9ab924a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010289871s
STEP: Saw pod success 01/19/23 22:01:38.387
Jan 19 22:01:38.387: INFO: Pod "pod-52854465-7a8d-44ca-879d-b55b9ab924a8" satisfied condition "Succeeded or Failed"
Jan 19 22:01:38.389: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-52854465-7a8d-44ca-879d-b55b9ab924a8 container test-container: <nil>
STEP: delete the pod 01/19/23 22:01:38.394
Jan 19 22:01:38.407: INFO: Waiting for pod pod-52854465-7a8d-44ca-879d-b55b9ab924a8 to disappear
Jan 19 22:01:38.409: INFO: Pod pod-52854465-7a8d-44ca-879d-b55b9ab924a8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jan 19 22:01:38.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8761" for this suite. 01/19/23 22:01:38.414
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":342,"skipped":6372,"failed":0}
------------------------------
• [4.114 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:01:34.306
    Jan 19 22:01:34.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename emptydir 01/19/23 22:01:34.306
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:34.324
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:34.326
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/19/23 22:01:34.328
    Jan 19 22:01:34.377: INFO: Waiting up to 5m0s for pod "pod-52854465-7a8d-44ca-879d-b55b9ab924a8" in namespace "emptydir-8761" to be "Succeeded or Failed"
    Jan 19 22:01:34.384: INFO: Pod "pod-52854465-7a8d-44ca-879d-b55b9ab924a8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.4831ms
    Jan 19 22:01:36.390: INFO: Pod "pod-52854465-7a8d-44ca-879d-b55b9ab924a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01325591s
    Jan 19 22:01:38.387: INFO: Pod "pod-52854465-7a8d-44ca-879d-b55b9ab924a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010289871s
    STEP: Saw pod success 01/19/23 22:01:38.387
    Jan 19 22:01:38.387: INFO: Pod "pod-52854465-7a8d-44ca-879d-b55b9ab924a8" satisfied condition "Succeeded or Failed"
    Jan 19 22:01:38.389: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-52854465-7a8d-44ca-879d-b55b9ab924a8 container test-container: <nil>
    STEP: delete the pod 01/19/23 22:01:38.394
    Jan 19 22:01:38.407: INFO: Waiting for pod pod-52854465-7a8d-44ca-879d-b55b9ab924a8 to disappear
    Jan 19 22:01:38.409: INFO: Pod pod-52854465-7a8d-44ca-879d-b55b9ab924a8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jan 19 22:01:38.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8761" for this suite. 01/19/23 22:01:38.414
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:01:38.42
Jan 19 22:01:38.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename sched-pred 01/19/23 22:01:38.421
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:38.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:38.444
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jan 19 22:01:38.450: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan 19 22:01:38.469: INFO: Waiting for terminating namespaces to be deleted...
Jan 19 22:01:38.480: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-146-42.ec2.internal before test
Jan 19 22:01:38.517: INFO: aws-ebs-csi-driver-node-vxgq2 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:41 +0000 UTC (3 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 22:01:38.517: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 22:01:38.517: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 22:01:38.517: INFO: tuned-nbzt4 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container tuned ready: true, restart count 1
Jan 19 22:01:38.517: INFO: downloads-8d695cd69-lcsd9 from openshift-console started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container download-server ready: true, restart count 0
Jan 19 22:01:38.517: INFO: dns-default-c5qwq from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container dns ready: true, restart count 1
Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.517: INFO: node-resolver-g5jk6 from openshift-dns started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 22:01:38.517: INFO: node-ca-crm7t from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 22:01:38.517: INFO: ingress-canary-nfwhl from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 22:01:38.517: INFO: migrator-5c54d8d69d-l5lss from openshift-kube-storage-version-migrator started at 2023-01-19 18:42:53 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container migrator ready: true, restart count 0
Jan 19 22:01:38.517: INFO: machine-config-daemon-cc29b from openshift-machine-config-operator started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 22:01:38.517: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 22:01:38.517: INFO: node-exporter-9zc87 from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.517: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 22:01:38.517: INFO: sre-dns-latency-exporter-jsn9x from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container main ready: true, restart count 1
Jan 19 22:01:38.517: INFO: multus-5zck8 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 22:01:38.517: INFO: multus-additional-cni-plugins-88bc2 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 22:01:38.517: INFO: network-metrics-daemon-zvctv from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.517: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 22:01:38.517: INFO: network-check-source-746dd6c885-bmf5k from openshift-network-diagnostics started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container check-endpoints ready: true, restart count 0
Jan 19 22:01:38.517: INFO: network-check-target-g5qsk from openshift-network-diagnostics started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 22:01:38.517: INFO: obo-prometheus-operator-64989cfc68-hdxb9 from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 22:01:38.517: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-gqhlz from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 22:01:38.517: INFO: osd-metrics-exporter-registry-qdjtk from openshift-osd-metrics started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.517: INFO: ovnkube-node-ftv9n from openshift-ovn-kubernetes started at 2023-01-19 18:15:41 +0000 UTC (5 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 22:01:38.517: INFO: 	Container ovn-acl-logging ready: true, restart count 2
Jan 19 22:01:38.517: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 22:01:38.517: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 22:01:38.517: INFO: rbac-permissions-operator-registry-rc4js from openshift-rbac-permissions started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.517: INFO: splunkforwarder-ds-7rpx7 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 22:01:38.517: INFO: prometheus-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (6 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 22:01:38.517: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 22:01:38.517: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 22:01:38.517: INFO: thanos-ruler-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (3 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 22:01:38.517: INFO: 	Container thanos-ruler ready: true, restart count 0
Jan 19 22:01:38.517: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
Jan 19 22:01:38.517: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-chbvs from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.517: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 22:01:38.517: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 22:01:38.517: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-151-158.ec2.internal before test
Jan 19 22:01:38.552: INFO: addon-operator-manager-84ff88fc6-fmhnb from openshift-addon-operator started at 2023-01-19 18:46:50 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container manager ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container metrics-relay-server ready: true, restart count 0
Jan 19 22:01:38.553: INFO: addon-operator-webhooks-67d9d47489-4kbk2 from openshift-addon-operator started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container webhook ready: true, restart count 0
Jan 19 22:01:38.553: INFO: aws-ebs-csi-driver-node-c8m6c from openshift-cluster-csi-drivers started at 2023-01-19 18:32:48 +0000 UTC (3 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 22:01:38.553: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 22:01:38.553: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 22:01:38.553: INFO: tuned-5h69k from openshift-cluster-node-tuning-operator started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container tuned ready: true, restart count 1
Jan 19 22:01:38.553: INFO: custom-domains-operator-5764f4df49-8nbdk from openshift-custom-domains-operator started at 2023-01-19 19:15:08 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container custom-domains-operator ready: true, restart count 0
Jan 19 22:01:38.553: INFO: deployment-validation-operator-7b8fd479cb-wtzg7 from openshift-deployment-validation-operator started at 2023-01-19 18:46:50 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container deployment-validation-operator ready: true, restart count 0
Jan 19 22:01:38.553: INFO: dns-default-zd4bs from openshift-dns started at 2023-01-19 22:00:25 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container dns ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:01:38.553: INFO: node-resolver-9cwpx from openshift-dns started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 22:01:38.553: INFO: image-pruner-27902640-c6mlt from openshift-image-registry started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 22:01:38.553: INFO: image-pruner-27902700-cf7x6 from openshift-image-registry started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 22:01:38.553: INFO: image-registry-656cfd9bd9-9x67r from openshift-image-registry started at 2023-01-19 19:15:11 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container registry ready: true, restart count 0
Jan 19 22:01:38.553: INFO: node-ca-k89p7 from openshift-image-registry started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 22:01:38.553: INFO: ingress-canary-v2tjj from openshift-ingress-canary started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 22:01:38.553: INFO: router-default-6bf5ff7dcb-5nzvc from openshift-ingress started at 2023-01-19 18:47:44 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container router ready: true, restart count 0
Jan 19 22:01:38.553: INFO: machine-config-daemon-28fzw from openshift-machine-config-operator started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 22:01:38.553: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 22:01:38.553: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container alertmanager ready: true, restart count 1
Jan 19 22:01:38.553: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 22:01:38.553: INFO: configure-alertmanager-operator-7d94bbbcf-8h7xs from openshift-monitoring started at 2023-01-19 19:16:27 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
Jan 19 22:01:38.553: INFO: kube-state-metrics-666f4cbf77-5w8rk from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jan 19 22:01:38.553: INFO: node-exporter-7t5b7 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.553: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 22:01:38.553: INFO: openshift-state-metrics-7d5c9d867c-97gzd from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jan 19 22:01:38.553: INFO: prometheus-adapter-5bc6487f7c-q2ng7 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 19 22:01:38.553: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 22:01:38.553: INFO: prometheus-operator-564b78d5ff-v4xnk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container prometheus-operator ready: true, restart count 0
Jan 19 22:01:38.553: INFO: prometheus-operator-admission-webhook-749df5cf4f-zj8xj from openshift-monitoring started at 2023-01-19 18:46:40 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 22:01:38.553: INFO: sre-dns-latency-exporter-2q899 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container main ready: true, restart count 1
Jan 19 22:01:38.553: INFO: sre-stuck-ebs-vols-1-vdlrh from openshift-monitoring started at 2023-01-19 19:17:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container main ready: true, restart count 0
Jan 19 22:01:38.553: INFO: telemeter-client-54bbbbf598-d9fdk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (3 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container reload ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container telemeter-client ready: true, restart count 0
Jan 19 22:01:38.553: INFO: thanos-querier-fcd5b6c8d-ql8qb from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container thanos-query ready: true, restart count 0
Jan 19 22:01:38.553: INFO: multus-additional-cni-plugins-vslld from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 22:01:38.553: INFO: multus-zqhtq from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 22:01:38.553: INFO: network-metrics-daemon-677lg from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.553: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 22:01:38.553: INFO: must-gather-operator-56b466776c-qv8sh from openshift-must-gather-operator started at 2023-01-19 19:17:38 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container must-gather-operator ready: true, restart count 0
Jan 19 22:01:38.553: INFO: network-check-target-xrr4d from openshift-network-diagnostics started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 22:01:38.553: INFO: ocm-agent-ddfdc6544-4fxrp from openshift-ocm-agent-operator started at 2023-01-19 19:17:41 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container ocm-agent ready: true, restart count 0
Jan 19 22:01:38.553: INFO: ocm-agent-operator-6cdb45fd86-4clcj from openshift-ocm-agent-operator started at 2023-01-19 19:17:43 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container ocm-agent-operator ready: true, restart count 0
Jan 19 22:01:38.553: INFO: ovnkube-node-gvwrr from openshift-ovn-kubernetes started at 2023-01-19 18:32:48 +0000 UTC (5 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 22:01:38.553: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 22:01:38.553: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 22:01:38.553: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 22:01:38.553: INFO: rbac-permissions-operator-5fb57974d8-htnjj from openshift-rbac-permissions started at 2023-01-19 19:17:50 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
Jan 19 22:01:38.553: INFO: splunkforwarder-ds-ztlpn from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 22:01:38.553: INFO: splunk-forwarder-operator-7d66568cf7-bksrn from openshift-splunk-forwarder-operator started at 2023-01-19 19:17:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
Jan 19 22:01:38.553: INFO: managed-velero-operator-788754d4b6-n89fw from openshift-velero started at 2023-01-19 19:17:56 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container managed-velero-operator ready: true, restart count 0
Jan 19 22:01:38.553: INFO: velero-7c64598b8c-8blx4 from openshift-velero started at 2023-01-19 19:18:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container velero ready: true, restart count 0
Jan 19 22:01:38.553: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-8mc8n from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.553: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 22:01:38.553: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-171-213.ec2.internal before test
Jan 19 22:01:38.600: INFO: aws-ebs-csi-driver-node-svd2s from openshift-cluster-csi-drivers started at 2023-01-19 18:15:58 +0000 UTC (3 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 22:01:38.600: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 22:01:38.600: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 22:01:38.600: INFO: tuned-9p2db from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container tuned ready: true, restart count 1
Jan 19 22:01:38.600: INFO: dns-default-zbx44 from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container dns ready: true, restart count 1
Jan 19 22:01:38.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.600: INFO: node-resolver-5mb95 from openshift-dns started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 22:01:38.600: INFO: node-ca-7c5fq from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 22:01:38.600: INFO: ingress-canary-6sc9q from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 22:01:38.600: INFO: machine-config-daemon-xxm6j from openshift-machine-config-operator started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 22:01:38.600: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 22:01:38.600: INFO: node-exporter-h445b from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.600: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 22:01:38.600: INFO: sre-dns-latency-exporter-vhwwx from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container main ready: true, restart count 1
Jan 19 22:01:38.600: INFO: multus-additional-cni-plugins-nhwfz from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 22:01:38.600: INFO: multus-j4ngm from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 22:01:38.600: INFO: network-metrics-daemon-hp85h from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.600: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 22:01:38.600: INFO: network-check-target-mg7p4 from openshift-network-diagnostics started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 22:01:38.600: INFO: ovnkube-node-kk5t9 from openshift-ovn-kubernetes started at 2023-01-19 18:15:58 +0000 UTC (5 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.600: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 22:01:38.600: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 22:01:38.600: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 22:01:38.600: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 22:01:38.600: INFO: splunkforwarder-ds-66kjq from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 22:01:38.600: INFO: sonobuoy-e2e-job-4d801e056c4540b8 from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container e2e ready: true, restart count 0
Jan 19 22:01:38.600: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 22:01:38.600: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-m8vnk from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.600: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 22:01:38.600: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 22:01:38.600: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-172-44.ec2.internal before test
Jan 19 22:01:38.634: INFO: aws-ebs-csi-driver-node-z6klb from openshift-cluster-csi-drivers started at 2023-01-19 18:15:33 +0000 UTC (3 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 22:01:38.634: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 22:01:38.634: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 22:01:38.634: INFO: tuned-gjnt5 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container tuned ready: true, restart count 1
Jan 19 22:01:38.634: INFO: dns-default-dfndw from openshift-dns started at 2023-01-19 21:22:44 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container dns ready: true, restart count 0
Jan 19 22:01:38.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:01:38.634: INFO: node-resolver-b85qs from openshift-dns started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 22:01:38.634: INFO: node-ca-dsxvs from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 22:01:38.634: INFO: ingress-canary-cs8pq from openshift-ingress-canary started at 2023-01-19 21:22:44 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jan 19 22:01:38.634: INFO: machine-config-daemon-cllj8 from openshift-machine-config-operator started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 22:01:38.634: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 22:01:38.634: INFO: node-exporter-2vp9v from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.634: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 22:01:38.634: INFO: sre-dns-latency-exporter-ctmgr from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container main ready: true, restart count 1
Jan 19 22:01:38.634: INFO: multus-additional-cni-plugins-p5pjp from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 22:01:38.634: INFO: multus-xzxwg from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 22:01:38.634: INFO: network-metrics-daemon-hz5pj from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.634: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 22:01:38.634: INFO: network-check-target-nq9ln from openshift-network-diagnostics started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 22:01:38.634: INFO: collect-profiles-27902730-zm5kk from openshift-operator-lifecycle-manager started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 22:01:38.634: INFO: collect-profiles-27902745-9gctc from openshift-operator-lifecycle-manager started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 22:01:38.634: INFO: collect-profiles-27902760-nlwrf from openshift-operator-lifecycle-manager started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container collect-profiles ready: false, restart count 0
Jan 19 22:01:38.634: INFO: ovnkube-node-zmvs8 from openshift-ovn-kubernetes started at 2023-01-19 18:15:33 +0000 UTC (5 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.634: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 22:01:38.634: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 22:01:38.634: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 22:01:38.634: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 22:01:38.634: INFO: splunkforwarder-ds-pj5q8 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 22:01:38.634: INFO: sonobuoy from sonobuoy started at 2023-01-19 20:29:23 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan 19 22:01:38.634: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-cbfjg from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.634: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 22:01:38.634: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 22:01:38.634: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-188-71.ec2.internal before test
Jan 19 22:01:38.667: INFO: addon-operator-catalog-zxs8v from openshift-addon-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: cloud-ingress-operator-registry-pqwm7 from openshift-cloud-ingress-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: aws-ebs-csi-driver-node-wmvd4 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:13 +0000 UTC (3 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 22:01:38.667: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 22:01:38.667: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 22:01:38.667: INFO: tuned-7x9t2 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container tuned ready: true, restart count 1
Jan 19 22:01:38.667: INFO: downloads-8d695cd69-hltgd from openshift-console started at 2023-01-19 18:39:20 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container download-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: custom-domains-operator-registry-kdw7s from openshift-custom-domains-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: deployment-validation-operator-catalog-nq94w from openshift-deployment-validation-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: dns-default-tzrx6 from openshift-dns started at 2023-01-19 18:15:43 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container dns ready: true, restart count 1
Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.667: INFO: node-resolver-l2fzw from openshift-dns started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 22:01:38.667: INFO: node-ca-kzsvk from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 22:01:38.667: INFO: ingress-canary-t6tqp from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 22:01:38.667: INFO: machine-config-daemon-zhllf from openshift-machine-config-operator started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 22:01:38.667: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 22:01:38.667: INFO: managed-node-metadata-operator-registry-m6dxz from openshift-managed-node-metadata-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: managed-upgrade-operator-catalog-thxts from openshift-managed-upgrade-operator started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: configure-alertmanager-operator-registry-sjcb5 from openshift-monitoring started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: node-exporter-dc59d from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.667: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 22:01:38.667: INFO: osd-cluster-ready-nb8xk from openshift-monitoring started at 2023-01-19 18:39:21 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container osd-cluster-ready ready: false, restart count 7
Jan 19 22:01:38.667: INFO: sre-dns-latency-exporter-5dz25 from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container main ready: true, restart count 1
Jan 19 22:01:38.667: INFO: multus-additional-cni-plugins-hvpd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 22:01:38.667: INFO: multus-mptd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 22:01:38.667: INFO: network-metrics-daemon-s2l94 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.667: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 22:01:38.667: INFO: must-gather-operator-registry-n7fvg from openshift-must-gather-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: network-check-target-d8gvt from openshift-network-diagnostics started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 22:01:38.667: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-b4r6s from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 22:01:38.667: INFO: observability-operator-6dd79df7dc-z7frh from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container operator ready: true, restart count 0
Jan 19 22:01:38.667: INFO: observability-operator-catalog-vrnkr from openshift-observability-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: ocm-agent-operator-registry-dwfc4 from openshift-ocm-agent-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: ovnkube-node-wcrc4 from openshift-ovn-kubernetes started at 2023-01-19 18:15:13 +0000 UTC (5 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 22:01:38.667: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 22:01:38.667: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 22:01:38.667: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 22:01:38.667: INFO: route-monitor-operator-registry-whpgh from openshift-route-monitor-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: splunkforwarder-ds-xzbdk from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 22:01:38.667: INFO: splunk-forwarder-operator-catalog-fqp2b from openshift-splunk-forwarder-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: prometheus-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:37:05 +0000 UTC (6 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 22:01:38.667: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 22:01:38.667: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 22:01:38.667: INFO: thanos-ruler-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:37:04 +0000 UTC (3 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 22:01:38.667: INFO: 	Container thanos-ruler ready: true, restart count 0
Jan 19 22:01:38.667: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
Jan 19 22:01:38.667: INFO: managed-velero-operator-registry-6lc6b from openshift-velero started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
Jan 19 22:01:38.667: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-q5j5c from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.667: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 22:01:38.667: INFO: 	Container systemd-logs ready: true, restart count 0
Jan 19 22:01:38.667: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-207-77.ec2.internal before test
Jan 19 22:01:38.711: INFO: pod-handle-http-request from container-lifecycle-hook-1918 started at 2023-01-19 22:01:28 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container agnhost-container ready: true, restart count 0
Jan 19 22:01:38.711: INFO: addon-operator-webhooks-67d9d47489-f6z86 from openshift-addon-operator started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container webhook ready: true, restart count 0
Jan 19 22:01:38.711: INFO: osd-delete-ownerrefs-serviceaccounts-27902677-st5fv from openshift-backplane-srep started at 2023-01-19 20:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 22:01:38.711: INFO: osd-delete-ownerrefs-serviceaccounts-27902707-pxrnh from openshift-backplane-srep started at 2023-01-19 21:07:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 22:01:38.711: INFO: osd-delete-ownerrefs-serviceaccounts-27902737-ww4vb from openshift-backplane-srep started at 2023-01-19 21:37:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
Jan 19 22:01:38.711: INFO: osd-delete-backplane-serviceaccounts-27902740-x6pck from openshift-backplane started at 2023-01-19 21:40:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 22:01:38.711: INFO: osd-delete-backplane-serviceaccounts-27902750-b5lmz from openshift-backplane started at 2023-01-19 21:50:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 22:01:38.711: INFO: osd-delete-backplane-serviceaccounts-27902760-92hgz from openshift-backplane started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
Jan 19 22:01:38.711: INFO: sre-build-test-27902711-r8jct from openshift-build-test started at 2023-01-19 21:11:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container sre-build-test ready: false, restart count 0
Jan 19 22:01:38.711: INFO: cloud-ingress-operator-5b5df787b5-bdk4j from openshift-cloud-ingress-operator started at 2023-01-19 19:15:07 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
Jan 19 22:01:38.711: INFO: aws-ebs-csi-driver-node-9sjjv from openshift-cluster-csi-drivers started at 2023-01-19 18:33:36 +0000 UTC (3 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container csi-driver ready: true, restart count 1
Jan 19 22:01:38.711: INFO: 	Container csi-liveness-probe ready: true, restart count 1
Jan 19 22:01:38.711: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
Jan 19 22:01:38.711: INFO: tuned-qkjmb from openshift-cluster-node-tuning-operator started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container tuned ready: true, restart count 1
Jan 19 22:01:38.711: INFO: dns-default-mzpq5 from openshift-dns started at 2023-01-19 22:01:09 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container dns ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:01:38.711: INFO: node-resolver-m4bvp from openshift-dns started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container dns-node-resolver ready: true, restart count 1
Jan 19 22:01:38.711: INFO: image-pruner-27902760-clz8n from openshift-image-registry started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container image-pruner ready: false, restart count 0
Jan 19 22:01:38.711: INFO: image-registry-656cfd9bd9-bj4wn from openshift-image-registry started at 2023-01-19 19:15:47 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container registry ready: true, restart count 0
Jan 19 22:01:38.711: INFO: node-ca-kwgfx from openshift-image-registry started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container node-ca ready: true, restart count 1
Jan 19 22:01:38.711: INFO: ingress-canary-6xnfp from openshift-ingress-canary started at 2023-01-19 18:34:10 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
Jan 19 22:01:38.711: INFO: router-default-6bf5ff7dcb-6vrcg from openshift-ingress started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container router ready: true, restart count 0
Jan 19 22:01:38.711: INFO: osd-disable-cpms-27902760-pftcr from openshift-machine-api started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-disable-cpms ready: false, restart count 0
Jan 19 22:01:38.711: INFO: machine-config-daemon-2bt5b from openshift-machine-config-operator started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container machine-config-daemon ready: true, restart count 1
Jan 19 22:01:38.711: INFO: 	Container oauth-proxy ready: true, restart count 1
Jan 19 22:01:38.711: INFO: managed-node-metadata-operator-649f646d54-smsh7 from openshift-managed-node-metadata-operator started at 2023-01-19 19:16:23 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
Jan 19 22:01:38.711: INFO: osd-patch-subscription-source-27902640-qrd58 from openshift-marketplace started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 22:01:38.711: INFO: osd-patch-subscription-source-27902700-smwhh from openshift-marketplace started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 22:01:38.711: INFO: osd-patch-subscription-source-27902760-pt59p from openshift-marketplace started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
Jan 19 22:01:38.711: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container alertmanager ready: true, restart count 1
Jan 19 22:01:38.711: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 22:01:38.711: INFO: node-exporter-s77nf from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.711: INFO: 	Container node-exporter ready: true, restart count 1
Jan 19 22:01:38.711: INFO: osd-rebalance-infra-nodes-27902730-h7lrq from openshift-monitoring started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 22:01:38.711: INFO: osd-rebalance-infra-nodes-27902745-cwdn2 from openshift-monitoring started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 22:01:38.711: INFO: osd-rebalance-infra-nodes-27902760-7n5vq from openshift-monitoring started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
Jan 19 22:01:38.711: INFO: prometheus-adapter-5bc6487f7c-sv96c from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jan 19 22:01:38.711: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container config-reloader ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container prometheus ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jan 19 22:01:38.711: INFO: prometheus-operator-admission-webhook-749df5cf4f-bmj4n from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jan 19 22:01:38.711: INFO: sre-dns-latency-exporter-4lksg from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container main ready: true, restart count 1
Jan 19 22:01:38.711: INFO: sre-ebs-iops-reporter-1-sg7b2 from openshift-monitoring started at 2023-01-19 19:16:28 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container main ready: true, restart count 0
Jan 19 22:01:38.711: INFO: thanos-querier-fcd5b6c8d-9jjkp from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container oauth-proxy ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container thanos-query ready: true, restart count 0
Jan 19 22:01:38.711: INFO: token-refresher-6f7c46d758-mld9t from openshift-monitoring started at 2023-01-19 19:17:36 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container token-refresher ready: true, restart count 0
Jan 19 22:01:38.711: INFO: multus-additional-cni-plugins-476gw from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
Jan 19 22:01:38.711: INFO: multus-mczpf from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container kube-multus ready: true, restart count 1
Jan 19 22:01:38.711: INFO: network-metrics-daemon-ddkb6 from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.711: INFO: 	Container network-metrics-daemon ready: true, restart count 1
Jan 19 22:01:38.711: INFO: network-check-target-d8k5s from openshift-network-diagnostics started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container network-check-target-container ready: true, restart count 1
Jan 19 22:01:38.711: INFO: osd-metrics-exporter-679d75d598-rsv49 from openshift-osd-metrics started at 2023-01-19 19:17:47 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
Jan 19 22:01:38.711: INFO: ovnkube-node-gq9vc from openshift-ovn-kubernetes started at 2023-01-19 18:33:36 +0000 UTC (5 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
Jan 19 22:01:38.711: INFO: 	Container ovn-acl-logging ready: true, restart count 1
Jan 19 22:01:38.711: INFO: 	Container ovn-controller ready: true, restart count 1
Jan 19 22:01:38.711: INFO: 	Container ovnkube-node ready: true, restart count 1
Jan 19 22:01:38.711: INFO: blackbox-exporter-5f7f7bf859-l8ptw from openshift-route-monitor-operator started at 2023-01-19 19:17:51 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container blackbox-exporter ready: true, restart count 0
Jan 19 22:01:38.711: INFO: route-monitor-operator-controller-manager-bfbff575-pfqc4 from openshift-route-monitor-operator started at 2023-01-19 19:17:53 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container manager ready: true, restart count 0
Jan 19 22:01:38.711: INFO: splunkforwarder-ds-h56tr from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container splunk-uf ready: true, restart count 1
Jan 19 22:01:38.711: INFO: builds-pruner-27902640-bj57l from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 22:01:38.711: INFO: builds-pruner-27902700-t7v8n from openshift-sre-pruning started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 22:01:38.711: INFO: builds-pruner-27902760-7b5d6 from openshift-sre-pruning started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container builds-pruner ready: false, restart count 0
Jan 19 22:01:38.711: INFO: deployments-pruner-27902640-vhqsv from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 22:01:38.711: INFO: deployments-pruner-27902700-fvhm7 from openshift-sre-pruning started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 22:01:38.711: INFO: deployments-pruner-27902760-tmvqf from openshift-sre-pruning started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container deployments-pruner ready: false, restart count 0
Jan 19 22:01:38.711: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-hj9fv from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
Jan 19 22:01:38.711: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan 19 22:01:38.711: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/19/23 22:01:38.711
Jan 19 22:01:38.724: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-241" to be "running"
Jan 19 22:01:38.726: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.165438ms
Jan 19 22:01:40.731: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006659498s
Jan 19 22:01:40.731: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/19/23 22:01:40.733
STEP: Trying to apply a random label on the found node. 01/19/23 22:01:40.758
STEP: verifying the node has the label kubernetes.io/e2e-a6bb2c82-5bbc-4cab-a003-4b26aaf0435a 42 01/19/23 22:01:40.782
STEP: Trying to relaunch the pod, now with labels. 01/19/23 22:01:40.806
Jan 19 22:01:40.864: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-241" to be "not pending"
Jan 19 22:01:40.879: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 15.211844ms
Jan 19 22:01:42.884: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.01984657s
Jan 19 22:01:42.884: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-a6bb2c82-5bbc-4cab-a003-4b26aaf0435a off the node ip-10-0-172-44.ec2.internal 01/19/23 22:01:42.886
STEP: verifying the node doesn't have the label kubernetes.io/e2e-a6bb2c82-5bbc-4cab-a003-4b26aaf0435a 01/19/23 22:01:42.901
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jan 19 22:01:42.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-241" for this suite. 01/19/23 22:01:42.915
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":343,"skipped":6375,"failed":0}
------------------------------
• [4.503 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:01:38.42
    Jan 19 22:01:38.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename sched-pred 01/19/23 22:01:38.421
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:38.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:38.444
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jan 19 22:01:38.450: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan 19 22:01:38.469: INFO: Waiting for terminating namespaces to be deleted...
    Jan 19 22:01:38.480: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-146-42.ec2.internal before test
    Jan 19 22:01:38.517: INFO: aws-ebs-csi-driver-node-vxgq2 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:41 +0000 UTC (3 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: tuned-nbzt4 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: downloads-8d695cd69-lcsd9 from openshift-console started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container download-server ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: dns-default-c5qwq from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container dns ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: node-resolver-g5jk6 from openshift-dns started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: node-ca-crm7t from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: ingress-canary-nfwhl from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: migrator-5c54d8d69d-l5lss from openshift-kube-storage-version-migrator started at 2023-01-19 18:42:53 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container migrator ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: machine-config-daemon-cc29b from openshift-machine-config-operator started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: node-exporter-9zc87 from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: sre-dns-latency-exporter-jsn9x from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container main ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: multus-5zck8 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: multus-additional-cni-plugins-88bc2 from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: network-metrics-daemon-zvctv from openshift-multus started at 2023-01-19 18:15:41 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: network-check-source-746dd6c885-bmf5k from openshift-network-diagnostics started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container check-endpoints ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: network-check-target-g5qsk from openshift-network-diagnostics started at 2023-01-19 18:15:41 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: obo-prometheus-operator-64989cfc68-hdxb9 from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container prometheus-operator ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-gqhlz from openshift-observability-operator started at 2023-01-19 18:41:39 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: osd-metrics-exporter-registry-qdjtk from openshift-osd-metrics started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: ovnkube-node-ftv9n from openshift-ovn-kubernetes started at 2023-01-19 18:15:41 +0000 UTC (5 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: 	Container ovn-acl-logging ready: true, restart count 2
    Jan 19 22:01:38.517: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: rbac-permissions-operator-registry-rc4js from openshift-rbac-permissions started at 2023-01-19 18:41:43 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: splunkforwarder-ds-7rpx7 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 22:01:38.517: INFO: prometheus-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (6 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: thanos-ruler-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:41:43 +0000 UTC (3 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: 	Container thanos-ruler ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-chbvs from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.517: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 22:01:38.517: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-151-158.ec2.internal before test
    Jan 19 22:01:38.552: INFO: addon-operator-manager-84ff88fc6-fmhnb from openshift-addon-operator started at 2023-01-19 18:46:50 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container manager ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container metrics-relay-server ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: addon-operator-webhooks-67d9d47489-4kbk2 from openshift-addon-operator started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container webhook ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: aws-ebs-csi-driver-node-c8m6c from openshift-cluster-csi-drivers started at 2023-01-19 18:32:48 +0000 UTC (3 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: tuned-5h69k from openshift-cluster-node-tuning-operator started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: custom-domains-operator-5764f4df49-8nbdk from openshift-custom-domains-operator started at 2023-01-19 19:15:08 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container custom-domains-operator ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: deployment-validation-operator-7b8fd479cb-wtzg7 from openshift-deployment-validation-operator started at 2023-01-19 18:46:50 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container deployment-validation-operator ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: dns-default-zd4bs from openshift-dns started at 2023-01-19 22:00:25 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container dns ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: node-resolver-9cwpx from openshift-dns started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: image-pruner-27902640-c6mlt from openshift-image-registry started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container image-pruner ready: false, restart count 0
    Jan 19 22:01:38.553: INFO: image-pruner-27902700-cf7x6 from openshift-image-registry started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container image-pruner ready: false, restart count 0
    Jan 19 22:01:38.553: INFO: image-registry-656cfd9bd9-9x67r from openshift-image-registry started at 2023-01-19 19:15:11 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container registry ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: node-ca-k89p7 from openshift-image-registry started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: ingress-canary-v2tjj from openshift-ingress-canary started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: router-default-6bf5ff7dcb-5nzvc from openshift-ingress started at 2023-01-19 18:47:44 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container router ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: machine-config-daemon-28fzw from openshift-machine-config-operator started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: configure-alertmanager-operator-7d94bbbcf-8h7xs from openshift-monitoring started at 2023-01-19 19:16:27 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container configure-alertmanager-operator ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: kube-state-metrics-666f4cbf77-5w8rk from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: node-exporter-7t5b7 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: openshift-state-metrics-7d5c9d867c-97gzd from openshift-monitoring started at 2023-01-19 18:46:50 +0000 UTC (3 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: prometheus-adapter-5bc6487f7c-q2ng7 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: prometheus-operator-564b78d5ff-v4xnk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container prometheus-operator ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: prometheus-operator-admission-webhook-749df5cf4f-zj8xj from openshift-monitoring started at 2023-01-19 18:46:40 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: sre-dns-latency-exporter-2q899 from openshift-monitoring started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container main ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: sre-stuck-ebs-vols-1-vdlrh from openshift-monitoring started at 2023-01-19 19:17:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container main ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: telemeter-client-54bbbbf598-d9fdk from openshift-monitoring started at 2023-01-19 18:46:51 +0000 UTC (3 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container reload ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container telemeter-client ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: thanos-querier-fcd5b6c8d-ql8qb from openshift-monitoring started at 2023-01-19 18:46:42 +0000 UTC (6 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: multus-additional-cni-plugins-vslld from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: multus-zqhtq from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: network-metrics-daemon-677lg from openshift-multus started at 2023-01-19 18:32:48 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: must-gather-operator-56b466776c-qv8sh from openshift-must-gather-operator started at 2023-01-19 19:17:38 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container must-gather-operator ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: network-check-target-xrr4d from openshift-network-diagnostics started at 2023-01-19 18:32:48 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: ocm-agent-ddfdc6544-4fxrp from openshift-ocm-agent-operator started at 2023-01-19 19:17:41 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container ocm-agent ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: ocm-agent-operator-6cdb45fd86-4clcj from openshift-ocm-agent-operator started at 2023-01-19 19:17:43 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container ocm-agent-operator ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: ovnkube-node-gvwrr from openshift-ovn-kubernetes started at 2023-01-19 18:32:48 +0000 UTC (5 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: rbac-permissions-operator-5fb57974d8-htnjj from openshift-rbac-permissions started at 2023-01-19 19:17:50 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container rbac-permissions-operator ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: splunkforwarder-ds-ztlpn from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 22:01:38.553: INFO: splunk-forwarder-operator-7d66568cf7-bksrn from openshift-splunk-forwarder-operator started at 2023-01-19 19:17:55 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container splunk-forwarder-operator ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: managed-velero-operator-788754d4b6-n89fw from openshift-velero started at 2023-01-19 19:17:56 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container managed-velero-operator ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: velero-7c64598b8c-8blx4 from openshift-velero started at 2023-01-19 19:18:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container velero ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-8mc8n from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.553: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 22:01:38.553: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-171-213.ec2.internal before test
    Jan 19 22:01:38.600: INFO: aws-ebs-csi-driver-node-svd2s from openshift-cluster-csi-drivers started at 2023-01-19 18:15:58 +0000 UTC (3 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: tuned-9p2db from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: dns-default-zbx44 from openshift-dns started at 2023-01-19 18:18:13 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container dns ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: node-resolver-5mb95 from openshift-dns started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: node-ca-7c5fq from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: ingress-canary-6sc9q from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: machine-config-daemon-xxm6j from openshift-machine-config-operator started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: node-exporter-h445b from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: sre-dns-latency-exporter-vhwwx from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container main ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: multus-additional-cni-plugins-nhwfz from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: multus-j4ngm from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: network-metrics-daemon-hp85h from openshift-multus started at 2023-01-19 18:15:58 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: network-check-target-mg7p4 from openshift-network-diagnostics started at 2023-01-19 18:15:58 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: ovnkube-node-kk5t9 from openshift-ovn-kubernetes started at 2023-01-19 18:15:58 +0000 UTC (5 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: splunkforwarder-ds-66kjq from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 22:01:38.600: INFO: sonobuoy-e2e-job-4d801e056c4540b8 from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container e2e ready: true, restart count 0
    Jan 19 22:01:38.600: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 22:01:38.600: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-m8vnk from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.600: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 22:01:38.600: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 22:01:38.600: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-172-44.ec2.internal before test
    Jan 19 22:01:38.634: INFO: aws-ebs-csi-driver-node-z6klb from openshift-cluster-csi-drivers started at 2023-01-19 18:15:33 +0000 UTC (3 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: tuned-gjnt5 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: dns-default-dfndw from openshift-dns started at 2023-01-19 21:22:44 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container dns ready: true, restart count 0
    Jan 19 22:01:38.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 22:01:38.634: INFO: node-resolver-b85qs from openshift-dns started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: node-ca-dsxvs from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: ingress-canary-cs8pq from openshift-ingress-canary started at 2023-01-19 21:22:44 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jan 19 22:01:38.634: INFO: machine-config-daemon-cllj8 from openshift-machine-config-operator started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: node-exporter-2vp9v from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: sre-dns-latency-exporter-ctmgr from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container main ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: multus-additional-cni-plugins-p5pjp from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: multus-xzxwg from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: network-metrics-daemon-hz5pj from openshift-multus started at 2023-01-19 18:15:33 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: network-check-target-nq9ln from openshift-network-diagnostics started at 2023-01-19 18:15:33 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: collect-profiles-27902730-zm5kk from openshift-operator-lifecycle-manager started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 19 22:01:38.634: INFO: collect-profiles-27902745-9gctc from openshift-operator-lifecycle-manager started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 19 22:01:38.634: INFO: collect-profiles-27902760-nlwrf from openshift-operator-lifecycle-manager started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container collect-profiles ready: false, restart count 0
    Jan 19 22:01:38.634: INFO: ovnkube-node-zmvs8 from openshift-ovn-kubernetes started at 2023-01-19 18:15:33 +0000 UTC (5 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: splunkforwarder-ds-pj5q8 from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 22:01:38.634: INFO: sonobuoy from sonobuoy started at 2023-01-19 20:29:23 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan 19 22:01:38.634: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-cbfjg from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.634: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 22:01:38.634: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 22:01:38.634: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-188-71.ec2.internal before test
    Jan 19 22:01:38.667: INFO: addon-operator-catalog-zxs8v from openshift-addon-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: cloud-ingress-operator-registry-pqwm7 from openshift-cloud-ingress-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: aws-ebs-csi-driver-node-wmvd4 from openshift-cluster-csi-drivers started at 2023-01-19 18:15:13 +0000 UTC (3 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: tuned-7x9t2 from openshift-cluster-node-tuning-operator started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: downloads-8d695cd69-hltgd from openshift-console started at 2023-01-19 18:39:20 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container download-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: custom-domains-operator-registry-kdw7s from openshift-custom-domains-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: deployment-validation-operator-catalog-nq94w from openshift-deployment-validation-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: dns-default-tzrx6 from openshift-dns started at 2023-01-19 18:15:43 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container dns ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: node-resolver-l2fzw from openshift-dns started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: node-ca-kzsvk from openshift-image-registry started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: ingress-canary-t6tqp from openshift-ingress-canary started at 2023-01-19 18:18:13 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: machine-config-daemon-zhllf from openshift-machine-config-operator started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: managed-node-metadata-operator-registry-m6dxz from openshift-managed-node-metadata-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: managed-upgrade-operator-catalog-thxts from openshift-managed-upgrade-operator started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: configure-alertmanager-operator-registry-sjcb5 from openshift-monitoring started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: node-exporter-dc59d from openshift-monitoring started at 2023-01-19 18:18:50 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: osd-cluster-ready-nb8xk from openshift-monitoring started at 2023-01-19 18:39:21 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container osd-cluster-ready ready: false, restart count 7
    Jan 19 22:01:38.667: INFO: sre-dns-latency-exporter-5dz25 from openshift-monitoring started at 2023-01-19 18:29:55 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container main ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: multus-additional-cni-plugins-hvpd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: multus-mptd7 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: network-metrics-daemon-s2l94 from openshift-multus started at 2023-01-19 18:15:13 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: must-gather-operator-registry-n7fvg from openshift-must-gather-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: network-check-target-d8gvt from openshift-network-diagnostics started at 2023-01-19 18:15:13 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: obo-prometheus-operator-admission-webhook-558d9c7d6-b4r6s from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: observability-operator-6dd79df7dc-z7frh from openshift-observability-operator started at 2023-01-19 18:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container operator ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: observability-operator-catalog-vrnkr from openshift-observability-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: ocm-agent-operator-registry-dwfc4 from openshift-ocm-agent-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: ovnkube-node-wcrc4 from openshift-ovn-kubernetes started at 2023-01-19 18:15:13 +0000 UTC (5 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: route-monitor-operator-registry-whpgh from openshift-route-monitor-operator started at 2023-01-19 18:39:29 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: splunkforwarder-ds-xzbdk from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 22:01:38.667: INFO: splunk-forwarder-operator-catalog-fqp2b from openshift-splunk-forwarder-operator started at 2023-01-19 18:37:05 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: prometheus-user-workload-0 from openshift-user-workload-monitoring started at 2023-01-19 18:37:05 +0000 UTC (6 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy-federate ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: thanos-ruler-user-workload-1 from openshift-user-workload-monitoring started at 2023-01-19 18:37:04 +0000 UTC (3 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: 	Container thanos-ruler ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: 	Container thanos-ruler-proxy ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: managed-velero-operator-registry-6lc6b from openshift-velero started at 2023-01-19 18:37:06 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container registry-server ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-q5j5c from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.667: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan 19 22:01:38.667: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-207-77.ec2.internal before test
    Jan 19 22:01:38.711: INFO: pod-handle-http-request from container-lifecycle-hook-1918 started at 2023-01-19 22:01:28 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container agnhost-container ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: addon-operator-webhooks-67d9d47489-f6z86 from openshift-addon-operator started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container webhook ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: osd-delete-ownerrefs-serviceaccounts-27902677-st5fv from openshift-backplane-srep started at 2023-01-19 20:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: osd-delete-ownerrefs-serviceaccounts-27902707-pxrnh from openshift-backplane-srep started at 2023-01-19 21:07:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: osd-delete-ownerrefs-serviceaccounts-27902737-ww4vb from openshift-backplane-srep started at 2023-01-19 21:37:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-delete-ownerrefs-serviceaccounts ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: osd-delete-backplane-serviceaccounts-27902740-x6pck from openshift-backplane started at 2023-01-19 21:40:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: osd-delete-backplane-serviceaccounts-27902750-b5lmz from openshift-backplane started at 2023-01-19 21:50:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: osd-delete-backplane-serviceaccounts-27902760-92hgz from openshift-backplane started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-delete-backplane-serviceaccounts ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: sre-build-test-27902711-r8jct from openshift-build-test started at 2023-01-19 21:11:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container sre-build-test ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: cloud-ingress-operator-5b5df787b5-bdk4j from openshift-cloud-ingress-operator started at 2023-01-19 19:15:07 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container cloud-ingress-operator ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: aws-ebs-csi-driver-node-9sjjv from openshift-cluster-csi-drivers started at 2023-01-19 18:33:36 +0000 UTC (3 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container csi-driver ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: 	Container csi-liveness-probe ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: 	Container csi-node-driver-registrar ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: tuned-qkjmb from openshift-cluster-node-tuning-operator started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container tuned ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: dns-default-mzpq5 from openshift-dns started at 2023-01-19 22:01:09 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container dns ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: node-resolver-m4bvp from openshift-dns started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container dns-node-resolver ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: image-pruner-27902760-clz8n from openshift-image-registry started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container image-pruner ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: image-registry-656cfd9bd9-bj4wn from openshift-image-registry started at 2023-01-19 19:15:47 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container registry ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: node-ca-kwgfx from openshift-image-registry started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container node-ca ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: ingress-canary-6xnfp from openshift-ingress-canary started at 2023-01-19 18:34:10 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container serve-healthcheck-canary ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: router-default-6bf5ff7dcb-6vrcg from openshift-ingress started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container router ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: osd-disable-cpms-27902760-pftcr from openshift-machine-api started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-disable-cpms ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: machine-config-daemon-2bt5b from openshift-machine-config-operator started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container machine-config-daemon ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: 	Container oauth-proxy ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: managed-node-metadata-operator-649f646d54-smsh7 from openshift-managed-node-metadata-operator started at 2023-01-19 19:16:23 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container managed-node-metadata-operator ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: osd-patch-subscription-source-27902640-qrd58 from openshift-marketplace started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: osd-patch-subscription-source-27902700-smwhh from openshift-marketplace started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: osd-patch-subscription-source-27902760-pt59p from openshift-marketplace started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-patch-subscription-source ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container alertmanager ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: node-exporter-s77nf from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: 	Container node-exporter ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: osd-rebalance-infra-nodes-27902730-h7lrq from openshift-monitoring started at 2023-01-19 21:30:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: osd-rebalance-infra-nodes-27902745-cwdn2 from openshift-monitoring started at 2023-01-19 21:45:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: osd-rebalance-infra-nodes-27902760-7n5vq from openshift-monitoring started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-rebalance-infra-nodes ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: prometheus-adapter-5bc6487f7c-sv96c from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container config-reloader ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container prometheus ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: prometheus-operator-admission-webhook-749df5cf4f-bmj4n from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: sre-dns-latency-exporter-4lksg from openshift-monitoring started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container main ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: sre-ebs-iops-reporter-1-sg7b2 from openshift-monitoring started at 2023-01-19 19:16:28 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container main ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: thanos-querier-fcd5b6c8d-9jjkp from openshift-monitoring started at 2023-01-19 18:49:32 +0000 UTC (6 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container thanos-query ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: token-refresher-6f7c46d758-mld9t from openshift-monitoring started at 2023-01-19 19:17:36 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container token-refresher ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: multus-additional-cni-plugins-476gw from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: multus-mczpf from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container kube-multus ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: network-metrics-daemon-ddkb6 from openshift-multus started at 2023-01-19 18:33:36 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: 	Container network-metrics-daemon ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: network-check-target-d8k5s from openshift-network-diagnostics started at 2023-01-19 18:33:36 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container network-check-target-container ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: osd-metrics-exporter-679d75d598-rsv49 from openshift-osd-metrics started at 2023-01-19 19:17:47 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container osd-metrics-exporter ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: ovnkube-node-gq9vc from openshift-ovn-kubernetes started at 2023-01-19 18:33:36 +0000 UTC (5 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: 	Container kube-rbac-proxy-ovn-metrics ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: 	Container ovn-acl-logging ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: 	Container ovn-controller ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: 	Container ovnkube-node ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: blackbox-exporter-5f7f7bf859-l8ptw from openshift-route-monitor-operator started at 2023-01-19 19:17:51 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: route-monitor-operator-controller-manager-bfbff575-pfqc4 from openshift-route-monitor-operator started at 2023-01-19 19:17:53 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container manager ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: splunkforwarder-ds-h56tr from openshift-security started at 2023-01-19 18:33:55 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container splunk-uf ready: true, restart count 1
    Jan 19 22:01:38.711: INFO: builds-pruner-27902640-bj57l from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container builds-pruner ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: builds-pruner-27902700-t7v8n from openshift-sre-pruning started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container builds-pruner ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: builds-pruner-27902760-7b5d6 from openshift-sre-pruning started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container builds-pruner ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: deployments-pruner-27902640-vhqsv from openshift-sre-pruning started at 2023-01-19 20:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container deployments-pruner ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: deployments-pruner-27902700-fvhm7 from openshift-sre-pruning started at 2023-01-19 21:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container deployments-pruner ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: deployments-pruner-27902760-tmvqf from openshift-sre-pruning started at 2023-01-19 22:00:00 +0000 UTC (1 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container deployments-pruner ready: false, restart count 0
    Jan 19 22:01:38.711: INFO: sonobuoy-systemd-logs-daemon-set-21c708abcb7d4028-hj9fv from sonobuoy started at 2023-01-19 20:29:26 +0000 UTC (2 container statuses recorded)
    Jan 19 22:01:38.711: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan 19 22:01:38.711: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/19/23 22:01:38.711
    Jan 19 22:01:38.724: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-241" to be "running"
    Jan 19 22:01:38.726: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.165438ms
    Jan 19 22:01:40.731: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.006659498s
    Jan 19 22:01:40.731: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/19/23 22:01:40.733
    STEP: Trying to apply a random label on the found node. 01/19/23 22:01:40.758
    STEP: verifying the node has the label kubernetes.io/e2e-a6bb2c82-5bbc-4cab-a003-4b26aaf0435a 42 01/19/23 22:01:40.782
    STEP: Trying to relaunch the pod, now with labels. 01/19/23 22:01:40.806
    Jan 19 22:01:40.864: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-241" to be "not pending"
    Jan 19 22:01:40.879: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 15.211844ms
    Jan 19 22:01:42.884: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.01984657s
    Jan 19 22:01:42.884: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-a6bb2c82-5bbc-4cab-a003-4b26aaf0435a off the node ip-10-0-172-44.ec2.internal 01/19/23 22:01:42.886
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-a6bb2c82-5bbc-4cab-a003-4b26aaf0435a 01/19/23 22:01:42.901
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 22:01:42.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-241" for this suite. 01/19/23 22:01:42.915
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:01:42.924
Jan 19 22:01:42.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename security-context-test 01/19/23 22:01:42.925
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:42.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:42.956
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jan 19 22:01:43.989: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-774a4847-c83a-4f3e-a295-963944a26413" in namespace "security-context-test-1271" to be "Succeeded or Failed"
Jan 19 22:01:43.991: INFO: Pod "alpine-nnp-false-774a4847-c83a-4f3e-a295-963944a26413": Phase="Pending", Reason="", readiness=false. Elapsed: 2.351462ms
Jan 19 22:01:45.995: INFO: Pod "alpine-nnp-false-774a4847-c83a-4f3e-a295-963944a26413": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006204796s
Jan 19 22:01:47.995: INFO: Pod "alpine-nnp-false-774a4847-c83a-4f3e-a295-963944a26413": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0062142s
Jan 19 22:01:49.994: INFO: Pod "alpine-nnp-false-774a4847-c83a-4f3e-a295-963944a26413": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005214385s
Jan 19 22:01:49.994: INFO: Pod "alpine-nnp-false-774a4847-c83a-4f3e-a295-963944a26413" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jan 19 22:01:50.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1271" for this suite. 01/19/23 22:01:50.009
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":344,"skipped":6396,"failed":0}
------------------------------
• [SLOW TEST] [7.091 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:01:42.924
    Jan 19 22:01:42.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename security-context-test 01/19/23 22:01:42.925
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:42.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:42.956
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jan 19 22:01:43.989: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-774a4847-c83a-4f3e-a295-963944a26413" in namespace "security-context-test-1271" to be "Succeeded or Failed"
    Jan 19 22:01:43.991: INFO: Pod "alpine-nnp-false-774a4847-c83a-4f3e-a295-963944a26413": Phase="Pending", Reason="", readiness=false. Elapsed: 2.351462ms
    Jan 19 22:01:45.995: INFO: Pod "alpine-nnp-false-774a4847-c83a-4f3e-a295-963944a26413": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006204796s
    Jan 19 22:01:47.995: INFO: Pod "alpine-nnp-false-774a4847-c83a-4f3e-a295-963944a26413": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0062142s
    Jan 19 22:01:49.994: INFO: Pod "alpine-nnp-false-774a4847-c83a-4f3e-a295-963944a26413": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005214385s
    Jan 19 22:01:49.994: INFO: Pod "alpine-nnp-false-774a4847-c83a-4f3e-a295-963944a26413" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jan 19 22:01:50.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-1271" for this suite. 01/19/23 22:01:50.009
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:01:50.015
Jan 19 22:01:50.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename daemonsets 01/19/23 22:01:50.016
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:50.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:50.031
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jan 19 22:01:50.088: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/19/23 22:01:50.096
Jan 19 22:01:50.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:01:50.101: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/19/23 22:01:50.101
Jan 19 22:01:50.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:01:50.138: INFO: Node ip-10-0-151-158.ec2.internal is running 0 daemon pod, expected 1
Jan 19 22:01:51.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:01:51.141: INFO: Node ip-10-0-151-158.ec2.internal is running 0 daemon pod, expected 1
Jan 19 22:01:52.143: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 22:01:52.143: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/19/23 22:01:52.146
Jan 19 22:01:52.195: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 22:01:52.195: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan 19 22:01:53.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:01:53.198: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/19/23 22:01:53.198
Jan 19 22:01:53.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:01:53.204: INFO: Node ip-10-0-151-158.ec2.internal is running 0 daemon pod, expected 1
Jan 19 22:01:54.207: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:01:54.207: INFO: Node ip-10-0-151-158.ec2.internal is running 0 daemon pod, expected 1
Jan 19 22:01:55.207: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:01:55.207: INFO: Node ip-10-0-151-158.ec2.internal is running 0 daemon pod, expected 1
Jan 19 22:01:56.208: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan 19 22:01:56.208: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 01/19/23 22:01:56.213
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5125, will wait for the garbage collector to delete the pods 01/19/23 22:01:56.213
Jan 19 22:01:56.271: INFO: Deleting DaemonSet.extensions daemon-set took: 5.239587ms
Jan 19 22:01:56.372: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.805542ms
Jan 19 22:01:58.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan 19 22:01:58.675: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan 19 22:01:58.677: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"237730"},"items":null}

Jan 19 22:01:58.679: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"237730"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jan 19 22:01:58.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5125" for this suite. 01/19/23 22:01:58.728
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":345,"skipped":6399,"failed":0}
------------------------------
• [SLOW TEST] [8.720 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:01:50.015
    Jan 19 22:01:50.015: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename daemonsets 01/19/23 22:01:50.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:50.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:50.031
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jan 19 22:01:50.088: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/19/23 22:01:50.096
    Jan 19 22:01:50.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 22:01:50.101: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/19/23 22:01:50.101
    Jan 19 22:01:50.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 22:01:50.138: INFO: Node ip-10-0-151-158.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 22:01:51.141: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 22:01:51.141: INFO: Node ip-10-0-151-158.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 22:01:52.143: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 22:01:52.143: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/19/23 22:01:52.146
    Jan 19 22:01:52.195: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 22:01:52.195: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan 19 22:01:53.197: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 22:01:53.198: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/19/23 22:01:53.198
    Jan 19 22:01:53.204: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 22:01:53.204: INFO: Node ip-10-0-151-158.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 22:01:54.207: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 22:01:54.207: INFO: Node ip-10-0-151-158.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 22:01:55.207: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 22:01:55.207: INFO: Node ip-10-0-151-158.ec2.internal is running 0 daemon pod, expected 1
    Jan 19 22:01:56.208: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan 19 22:01:56.208: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 01/19/23 22:01:56.213
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5125, will wait for the garbage collector to delete the pods 01/19/23 22:01:56.213
    Jan 19 22:01:56.271: INFO: Deleting DaemonSet.extensions daemon-set took: 5.239587ms
    Jan 19 22:01:56.372: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.805542ms
    Jan 19 22:01:58.675: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan 19 22:01:58.675: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan 19 22:01:58.677: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"237730"},"items":null}

    Jan 19 22:01:58.679: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"237730"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 22:01:58.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5125" for this suite. 01/19/23 22:01:58.728
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:01:58.736
Jan 19 22:01:58.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename container-probe 01/19/23 22:01:58.736
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:58.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:58.751
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
W0119 22:01:58.802591      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
Jan 19 22:01:58.802: INFO: Waiting up to 5m0s for pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0" in namespace "container-probe-581" to be "running and ready"
Jan 19 22:01:58.808: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.213508ms
Jan 19 22:01:58.808: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:02:00.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00919103s
Jan 19 22:02:00.811: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
Jan 19 22:02:02.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008981462s
Jan 19 22:02:02.811: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
Jan 19 22:02:04.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008860936s
Jan 19 22:02:04.811: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
Jan 19 22:02:06.812: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009614822s
Jan 19 22:02:06.812: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
Jan 19 22:02:08.812: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009668566s
Jan 19 22:02:08.812: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
Jan 19 22:02:10.812: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010080384s
Jan 19 22:02:10.812: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
Jan 19 22:02:12.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 14.009073168s
Jan 19 22:02:12.811: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
Jan 19 22:02:14.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008836388s
Jan 19 22:02:14.811: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
Jan 19 22:02:16.812: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009523194s
Jan 19 22:02:16.812: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
Jan 19 22:02:18.812: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010276424s
Jan 19 22:02:18.813: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
Jan 19 22:02:20.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009107194s
Jan 19 22:02:20.811: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = true)
Jan 19 22:02:20.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0" satisfied condition "running and ready"
Jan 19 22:02:20.814: INFO: Container started at 2023-01-19 22:01:59 +0000 UTC, pod became ready at 2023-01-19 22:02:19 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jan 19 22:02:20.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-581" for this suite. 01/19/23 22:02:20.818
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":346,"skipped":6401,"failed":0}
------------------------------
• [SLOW TEST] [22.087 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:01:58.736
    Jan 19 22:01:58.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename container-probe 01/19/23 22:01:58.736
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:01:58.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:01:58.751
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    W0119 22:01:58.802591      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "test-webserver" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "test-webserver" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "test-webserver" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "test-webserver" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    Jan 19 22:01:58.802: INFO: Waiting up to 5m0s for pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0" in namespace "container-probe-581" to be "running and ready"
    Jan 19 22:01:58.808: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.213508ms
    Jan 19 22:01:58.808: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 22:02:00.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00919103s
    Jan 19 22:02:00.811: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
    Jan 19 22:02:02.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008981462s
    Jan 19 22:02:02.811: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
    Jan 19 22:02:04.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008860936s
    Jan 19 22:02:04.811: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
    Jan 19 22:02:06.812: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009614822s
    Jan 19 22:02:06.812: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
    Jan 19 22:02:08.812: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009668566s
    Jan 19 22:02:08.812: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
    Jan 19 22:02:10.812: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010080384s
    Jan 19 22:02:10.812: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
    Jan 19 22:02:12.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 14.009073168s
    Jan 19 22:02:12.811: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
    Jan 19 22:02:14.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 16.008836388s
    Jan 19 22:02:14.811: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
    Jan 19 22:02:16.812: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 18.009523194s
    Jan 19 22:02:16.812: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
    Jan 19 22:02:18.812: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010276424s
    Jan 19 22:02:18.813: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = false)
    Jan 19 22:02:20.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0": Phase="Running", Reason="", readiness=true. Elapsed: 22.009107194s
    Jan 19 22:02:20.811: INFO: The phase of Pod test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0 is Running (Ready = true)
    Jan 19 22:02:20.811: INFO: Pod "test-webserver-1ccef51e-4d9a-4f87-a112-94f8519a5bf0" satisfied condition "running and ready"
    Jan 19 22:02:20.814: INFO: Container started at 2023-01-19 22:01:59 +0000 UTC, pod became ready at 2023-01-19 22:02:19 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jan 19 22:02:20.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-581" for this suite. 01/19/23 22:02:20.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:02:20.823
Jan 19 22:02:20.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 22:02:20.824
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:02:20.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:02:20.846
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 01/19/23 22:02:20.851
Jan 19 22:02:20.929: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de" in namespace "projected-706" to be "Succeeded or Failed"
Jan 19 22:02:20.931: INFO: Pod "downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260861ms
Jan 19 22:02:22.935: INFO: Pod "downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005863621s
Jan 19 22:02:24.935: INFO: Pod "downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006527033s
STEP: Saw pod success 01/19/23 22:02:24.935
Jan 19 22:02:24.935: INFO: Pod "downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de" satisfied condition "Succeeded or Failed"
Jan 19 22:02:24.938: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de container client-container: <nil>
STEP: delete the pod 01/19/23 22:02:24.942
Jan 19 22:02:24.955: INFO: Waiting for pod downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de to disappear
Jan 19 22:02:24.958: INFO: Pod downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 22:02:24.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-706" for this suite. 01/19/23 22:02:24.961
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":347,"skipped":6414,"failed":0}
------------------------------
• [4.145 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:02:20.823
    Jan 19 22:02:20.823: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 22:02:20.824
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:02:20.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:02:20.846
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 01/19/23 22:02:20.851
    Jan 19 22:02:20.929: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de" in namespace "projected-706" to be "Succeeded or Failed"
    Jan 19 22:02:20.931: INFO: Pod "downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260861ms
    Jan 19 22:02:22.935: INFO: Pod "downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005863621s
    Jan 19 22:02:24.935: INFO: Pod "downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006527033s
    STEP: Saw pod success 01/19/23 22:02:24.935
    Jan 19 22:02:24.935: INFO: Pod "downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de" satisfied condition "Succeeded or Failed"
    Jan 19 22:02:24.938: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de container client-container: <nil>
    STEP: delete the pod 01/19/23 22:02:24.942
    Jan 19 22:02:24.955: INFO: Waiting for pod downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de to disappear
    Jan 19 22:02:24.958: INFO: Pod downwardapi-volume-a051776b-7bfb-4324-b070-91c9af8344de no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 22:02:24.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-706" for this suite. 01/19/23 22:02:24.961
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:02:24.968
Jan 19 22:02:24.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename sched-preemption 01/19/23 22:02:24.969
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:02:24.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:02:24.984
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jan 19 22:02:25.018: INFO: Waiting up to 1m0s for all nodes to be ready
Jan 19 22:03:25.214: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:03:25.222
Jan 19 22:03:25.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename sched-preemption-path 01/19/23 22:03:25.224
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:03:25.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:03:25.238
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jan 19 22:03:25.282: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan 19 22:03:25.286: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jan 19 22:03:25.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-9874" for this suite. 01/19/23 22:03:25.321
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jan 19 22:03:25.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-1896" for this suite. 01/19/23 22:03:25.365
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":348,"skipped":6418,"failed":0}
------------------------------
• [SLOW TEST] [60.524 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:02:24.968
    Jan 19 22:02:24.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename sched-preemption 01/19/23 22:02:24.969
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:02:24.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:02:24.984
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jan 19 22:02:25.018: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan 19 22:03:25.214: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:03:25.222
    Jan 19 22:03:25.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename sched-preemption-path 01/19/23 22:03:25.224
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:03:25.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:03:25.238
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jan 19 22:03:25.282: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan 19 22:03:25.286: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jan 19 22:03:25.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-9874" for this suite. 01/19/23 22:03:25.321
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jan 19 22:03:25.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-1896" for this suite. 01/19/23 22:03:25.365
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:03:25.495
Jan 19 22:03:25.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename kubectl 01/19/23 22:03:25.496
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:03:25.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:03:25.521
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 01/19/23 22:03:25.529
Jan 19 22:03:25.529: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-2461 proxy --unix-socket=/tmp/kubectl-proxy-unix501757941/test'
STEP: retrieving proxy /api/ output 01/19/23 22:03:25.565
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jan 19 22:03:25.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2461" for this suite. 01/19/23 22:03:25.579
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":349,"skipped":6507,"failed":0}
------------------------------
• [0.118 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:03:25.495
    Jan 19 22:03:25.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename kubectl 01/19/23 22:03:25.496
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:03:25.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:03:25.521
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 01/19/23 22:03:25.529
    Jan 19 22:03:25.529: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=kubectl-2461 proxy --unix-socket=/tmp/kubectl-proxy-unix501757941/test'
    STEP: retrieving proxy /api/ output 01/19/23 22:03:25.565
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jan 19 22:03:25.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2461" for this suite. 01/19/23 22:03:25.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:03:25.614
Jan 19 22:03:25.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename projected 01/19/23 22:03:25.615
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:03:25.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:03:25.636
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 01/19/23 22:03:25.638
Jan 19 22:03:25.686: INFO: Waiting up to 5m0s for pod "labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8" in namespace "projected-2419" to be "running and ready"
Jan 19 22:03:25.697: INFO: Pod "labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.778399ms
Jan 19 22:03:25.697: INFO: The phase of Pod labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8 is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:03:27.700: INFO: Pod "labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8": Phase="Running", Reason="", readiness=true. Elapsed: 2.01416724s
Jan 19 22:03:27.700: INFO: The phase of Pod labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8 is Running (Ready = true)
Jan 19 22:03:27.700: INFO: Pod "labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8" satisfied condition "running and ready"
Jan 19 22:03:28.227: INFO: Successfully updated pod "labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jan 19 22:03:32.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2419" for this suite. 01/19/23 22:03:32.257
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":350,"skipped":6524,"failed":0}
------------------------------
• [SLOW TEST] [6.649 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:03:25.614
    Jan 19 22:03:25.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename projected 01/19/23 22:03:25.615
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:03:25.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:03:25.636
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 01/19/23 22:03:25.638
    Jan 19 22:03:25.686: INFO: Waiting up to 5m0s for pod "labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8" in namespace "projected-2419" to be "running and ready"
    Jan 19 22:03:25.697: INFO: Pod "labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.778399ms
    Jan 19 22:03:25.697: INFO: The phase of Pod labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8 is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 22:03:27.700: INFO: Pod "labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8": Phase="Running", Reason="", readiness=true. Elapsed: 2.01416724s
    Jan 19 22:03:27.700: INFO: The phase of Pod labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8 is Running (Ready = true)
    Jan 19 22:03:27.700: INFO: Pod "labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8" satisfied condition "running and ready"
    Jan 19 22:03:28.227: INFO: Successfully updated pod "labelsupdatef826db84-1007-430c-ade5-94acc2a4daf8"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jan 19 22:03:32.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-2419" for this suite. 01/19/23 22:03:32.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:03:32.263
Jan 19 22:03:32.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 22:03:32.264
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:03:32.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:03:32.282
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237
STEP: creating service in namespace services-1513 01/19/23 22:03:32.297
STEP: creating service affinity-nodeport-transition in namespace services-1513 01/19/23 22:03:32.297
STEP: creating replication controller affinity-nodeport-transition in namespace services-1513 01/19/23 22:03:32.342
I0119 22:03:32.354805      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1513, replica count: 3
I0119 22:03:35.405752      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan 19 22:03:35.415: INFO: Creating new exec pod
Jan 19 22:03:35.429: INFO: Waiting up to 5m0s for pod "execpod-affinityrc8mx" in namespace "services-1513" to be "running"
Jan 19 22:03:35.431: INFO: Pod "execpod-affinityrc8mx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536382ms
Jan 19 22:03:37.435: INFO: Pod "execpod-affinityrc8mx": Phase="Running", Reason="", readiness=true. Elapsed: 2.006032563s
Jan 19 22:03:37.435: INFO: Pod "execpod-affinityrc8mx" satisfied condition "running"
Jan 19 22:03:38.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1513 exec execpod-affinityrc8mx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jan 19 22:03:39.602: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan 19 22:03:39.602: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:03:39.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1513 exec execpod-affinityrc8mx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.225.198 80'
Jan 19 22:03:39.734: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.225.198 80\nConnection to 172.30.225.198 80 port [tcp/http] succeeded!\n"
Jan 19 22:03:39.734: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:03:39.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1513 exec execpod-affinityrc8mx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.207.77 30041'
Jan 19 22:03:40.885: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.207.77 30041\nConnection to 10.0.207.77 30041 port [tcp/*] succeeded!\n"
Jan 19 22:03:40.885: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:03:40.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1513 exec execpod-affinityrc8mx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.172.44 30041'
Jan 19 22:03:42.042: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.172.44 30041\nConnection to 10.0.172.44 30041 port [tcp/*] succeeded!\n"
Jan 19 22:03:42.042: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jan 19 22:03:42.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1513 exec execpod-affinityrc8mx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.146.42:30041/ ; done'
Jan 19 22:03:42.241: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n"
Jan 19 22:03:42.241: INFO: stdout: "\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-bpxrn\naffinity-nodeport-transition-bpxrn\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-wftx5\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-wftx5\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-wftx5\naffinity-nodeport-transition-bpxrn\naffinity-nodeport-transition-wftx5\naffinity-nodeport-transition-7sbdf"
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-bpxrn
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-bpxrn
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-wftx5
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-wftx5
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-wftx5
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-bpxrn
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-wftx5
Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1513 exec execpod-affinityrc8mx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.146.42:30041/ ; done'
Jan 19 22:03:42.459: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n"
Jan 19 22:03:42.459: INFO: stdout: "\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf"
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
Jan 19 22:03:42.459: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1513, will wait for the garbage collector to delete the pods 01/19/23 22:03:42.471
Jan 19 22:03:42.529: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.39749ms
Jan 19 22:03:42.630: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.314091ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 22:03:45.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1513" for this suite. 01/19/23 22:03:45.156
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":351,"skipped":6530,"failed":0}
------------------------------
• [SLOW TEST] [12.898 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2237

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:03:32.263
    Jan 19 22:03:32.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 22:03:32.264
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:03:32.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:03:32.282
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2237
    STEP: creating service in namespace services-1513 01/19/23 22:03:32.297
    STEP: creating service affinity-nodeport-transition in namespace services-1513 01/19/23 22:03:32.297
    STEP: creating replication controller affinity-nodeport-transition in namespace services-1513 01/19/23 22:03:32.342
    I0119 22:03:32.354805      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-1513, replica count: 3
    I0119 22:03:35.405752      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan 19 22:03:35.415: INFO: Creating new exec pod
    Jan 19 22:03:35.429: INFO: Waiting up to 5m0s for pod "execpod-affinityrc8mx" in namespace "services-1513" to be "running"
    Jan 19 22:03:35.431: INFO: Pod "execpod-affinityrc8mx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.536382ms
    Jan 19 22:03:37.435: INFO: Pod "execpod-affinityrc8mx": Phase="Running", Reason="", readiness=true. Elapsed: 2.006032563s
    Jan 19 22:03:37.435: INFO: Pod "execpod-affinityrc8mx" satisfied condition "running"
    Jan 19 22:03:38.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1513 exec execpod-affinityrc8mx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jan 19 22:03:39.602: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan 19 22:03:39.602: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 22:03:39.602: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1513 exec execpod-affinityrc8mx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.30.225.198 80'
    Jan 19 22:03:39.734: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.30.225.198 80\nConnection to 172.30.225.198 80 port [tcp/http] succeeded!\n"
    Jan 19 22:03:39.734: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 22:03:39.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1513 exec execpod-affinityrc8mx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.207.77 30041'
    Jan 19 22:03:40.885: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.207.77 30041\nConnection to 10.0.207.77 30041 port [tcp/*] succeeded!\n"
    Jan 19 22:03:40.885: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 22:03:40.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1513 exec execpod-affinityrc8mx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.0.172.44 30041'
    Jan 19 22:03:42.042: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.0.172.44 30041\nConnection to 10.0.172.44 30041 port [tcp/*] succeeded!\n"
    Jan 19 22:03:42.042: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jan 19 22:03:42.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1513 exec execpod-affinityrc8mx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.146.42:30041/ ; done'
    Jan 19 22:03:42.241: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n"
    Jan 19 22:03:42.241: INFO: stdout: "\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-bpxrn\naffinity-nodeport-transition-bpxrn\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-wftx5\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-wftx5\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-wftx5\naffinity-nodeport-transition-bpxrn\naffinity-nodeport-transition-wftx5\naffinity-nodeport-transition-7sbdf"
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-bpxrn
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-bpxrn
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-wftx5
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-wftx5
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-wftx5
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-bpxrn
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-wftx5
    Jan 19 22:03:42.241: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3983998075 --namespace=services-1513 exec execpod-affinityrc8mx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.146.42:30041/ ; done'
    Jan 19 22:03:42.459: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.146.42:30041/\n"
    Jan 19 22:03:42.459: INFO: stdout: "\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf\naffinity-nodeport-transition-7sbdf"
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Received response from host: affinity-nodeport-transition-7sbdf
    Jan 19 22:03:42.459: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-1513, will wait for the garbage collector to delete the pods 01/19/23 22:03:42.471
    Jan 19 22:03:42.529: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.39749ms
    Jan 19 22:03:42.630: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.314091ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 22:03:45.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1513" for this suite. 01/19/23 22:03:45.156
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:03:45.162
Jan 19 22:03:45.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 22:03:45.163
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:03:45.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:03:45.185
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
Jan 19 22:03:45.192: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-2f5c48ed-7023-44e6-9f12-204acaf8d7bc 01/19/23 22:03:45.192
STEP: Creating secret with name s-test-opt-upd-1012c76d-1ee1-465a-9abc-c100256233c1 01/19/23 22:03:45.199
STEP: Creating the pod 01/19/23 22:03:45.24
Jan 19 22:03:45.293: INFO: Waiting up to 5m0s for pod "pod-secrets-f9b593e7-310d-4fa4-a287-6ad2bb0eaf3a" in namespace "secrets-3044" to be "running and ready"
Jan 19 22:03:45.302: INFO: Pod "pod-secrets-f9b593e7-310d-4fa4-a287-6ad2bb0eaf3a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.468857ms
Jan 19 22:03:45.302: INFO: The phase of Pod pod-secrets-f9b593e7-310d-4fa4-a287-6ad2bb0eaf3a is Pending, waiting for it to be Running (with Ready = true)
Jan 19 22:03:47.307: INFO: Pod "pod-secrets-f9b593e7-310d-4fa4-a287-6ad2bb0eaf3a": Phase="Running", Reason="", readiness=true. Elapsed: 2.013754909s
Jan 19 22:03:47.307: INFO: The phase of Pod pod-secrets-f9b593e7-310d-4fa4-a287-6ad2bb0eaf3a is Running (Ready = true)
Jan 19 22:03:47.307: INFO: Pod "pod-secrets-f9b593e7-310d-4fa4-a287-6ad2bb0eaf3a" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-2f5c48ed-7023-44e6-9f12-204acaf8d7bc 01/19/23 22:03:47.359
STEP: Updating secret s-test-opt-upd-1012c76d-1ee1-465a-9abc-c100256233c1 01/19/23 22:03:47.398
STEP: Creating secret with name s-test-opt-create-7c23a02f-24fd-4165-9f80-3a3efb0fe450 01/19/23 22:03:47.405
STEP: waiting to observe update in volume 01/19/23 22:03:47.41
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 22:03:51.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3044" for this suite. 01/19/23 22:03:51.49
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":352,"skipped":6530,"failed":0}
------------------------------
• [SLOW TEST] [6.334 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:03:45.162
    Jan 19 22:03:45.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 22:03:45.163
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:03:45.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:03:45.185
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    Jan 19 22:03:45.192: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-2f5c48ed-7023-44e6-9f12-204acaf8d7bc 01/19/23 22:03:45.192
    STEP: Creating secret with name s-test-opt-upd-1012c76d-1ee1-465a-9abc-c100256233c1 01/19/23 22:03:45.199
    STEP: Creating the pod 01/19/23 22:03:45.24
    Jan 19 22:03:45.293: INFO: Waiting up to 5m0s for pod "pod-secrets-f9b593e7-310d-4fa4-a287-6ad2bb0eaf3a" in namespace "secrets-3044" to be "running and ready"
    Jan 19 22:03:45.302: INFO: Pod "pod-secrets-f9b593e7-310d-4fa4-a287-6ad2bb0eaf3a": Phase="Pending", Reason="", readiness=false. Elapsed: 9.468857ms
    Jan 19 22:03:45.302: INFO: The phase of Pod pod-secrets-f9b593e7-310d-4fa4-a287-6ad2bb0eaf3a is Pending, waiting for it to be Running (with Ready = true)
    Jan 19 22:03:47.307: INFO: Pod "pod-secrets-f9b593e7-310d-4fa4-a287-6ad2bb0eaf3a": Phase="Running", Reason="", readiness=true. Elapsed: 2.013754909s
    Jan 19 22:03:47.307: INFO: The phase of Pod pod-secrets-f9b593e7-310d-4fa4-a287-6ad2bb0eaf3a is Running (Ready = true)
    Jan 19 22:03:47.307: INFO: Pod "pod-secrets-f9b593e7-310d-4fa4-a287-6ad2bb0eaf3a" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-2f5c48ed-7023-44e6-9f12-204acaf8d7bc 01/19/23 22:03:47.359
    STEP: Updating secret s-test-opt-upd-1012c76d-1ee1-465a-9abc-c100256233c1 01/19/23 22:03:47.398
    STEP: Creating secret with name s-test-opt-create-7c23a02f-24fd-4165-9f80-3a3efb0fe450 01/19/23 22:03:47.405
    STEP: waiting to observe update in volume 01/19/23 22:03:47.41
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 22:03:51.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3044" for this suite. 01/19/23 22:03:51.49
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:03:51.496
Jan 19 22:03:51.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename job 01/19/23 22:03:51.497
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:03:51.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:03:51.516
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 01/19/23 22:03:51.519
W0119 22:03:51.527699      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 01/19/23 22:03:51.527
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jan 19 22:04:03.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8419" for this suite. 01/19/23 22:04:03.535
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":353,"skipped":6531,"failed":0}
------------------------------
• [SLOW TEST] [12.047 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:03:51.496
    Jan 19 22:03:51.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename job 01/19/23 22:03:51.497
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:03:51.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:03:51.516
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 01/19/23 22:03:51.519
    W0119 22:03:51.527699      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 01/19/23 22:03:51.527
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jan 19 22:04:03.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8419" for this suite. 01/19/23 22:04:03.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:04:03.543
Jan 19 22:04:03.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename webhook 01/19/23 22:04:03.544
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:03.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:03.565
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 01/19/23 22:04:03.608
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 22:04:04.364
STEP: Deploying the webhook pod 01/19/23 22:04:04.372
STEP: Wait for the deployment to be ready 01/19/23 22:04:04.382
Jan 19 22:04:04.388: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/19/23 22:04:06.395
STEP: Verifying the service has paired with the endpoint 01/19/23 22:04:06.404
Jan 19 22:04:07.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 01/19/23 22:04:07.407
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/19/23 22:04:07.423
STEP: Creating a configMap that should not be mutated 01/19/23 22:04:07.428
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/19/23 22:04:07.439
STEP: Creating a configMap that should be mutated 01/19/23 22:04:07.445
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jan 19 22:04:07.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5417" for this suite. 01/19/23 22:04:07.468
STEP: Destroying namespace "webhook-5417-markers" for this suite. 01/19/23 22:04:07.473
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":354,"skipped":6554,"failed":0}
------------------------------
• [3.984 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:04:03.543
    Jan 19 22:04:03.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename webhook 01/19/23 22:04:03.544
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:03.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:03.565
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 01/19/23 22:04:03.608
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/19/23 22:04:04.364
    STEP: Deploying the webhook pod 01/19/23 22:04:04.372
    STEP: Wait for the deployment to be ready 01/19/23 22:04:04.382
    Jan 19 22:04:04.388: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/19/23 22:04:06.395
    STEP: Verifying the service has paired with the endpoint 01/19/23 22:04:06.404
    Jan 19 22:04:07.405: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 01/19/23 22:04:07.407
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/19/23 22:04:07.423
    STEP: Creating a configMap that should not be mutated 01/19/23 22:04:07.428
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/19/23 22:04:07.439
    STEP: Creating a configMap that should be mutated 01/19/23 22:04:07.445
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jan 19 22:04:07.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5417" for this suite. 01/19/23 22:04:07.468
    STEP: Destroying namespace "webhook-5417-markers" for this suite. 01/19/23 22:04:07.473
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:04:07.528
Jan 19 22:04:07.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 22:04:07.529
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:07.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:07.557
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-cc8db7b5-c70c-4b04-b4a9-8242aa4a371f 01/19/23 22:04:07.64
STEP: Creating a pod to test consume secrets 01/19/23 22:04:07.652
Jan 19 22:04:07.685: INFO: Waiting up to 5m0s for pod "pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e" in namespace "secrets-6162" to be "Succeeded or Failed"
Jan 19 22:04:07.691: INFO: Pod "pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.08486ms
Jan 19 22:04:09.694: INFO: Pod "pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009798199s
Jan 19 22:04:11.694: INFO: Pod "pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009575782s
STEP: Saw pod success 01/19/23 22:04:11.694
Jan 19 22:04:11.694: INFO: Pod "pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e" satisfied condition "Succeeded or Failed"
Jan 19 22:04:11.697: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e container secret-volume-test: <nil>
STEP: delete the pod 01/19/23 22:04:11.702
Jan 19 22:04:11.710: INFO: Waiting for pod pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e to disappear
Jan 19 22:04:11.713: INFO: Pod pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jan 19 22:04:11.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6162" for this suite. 01/19/23 22:04:11.717
STEP: Destroying namespace "secret-namespace-5804" for this suite. 01/19/23 22:04:11.735
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":355,"skipped":6572,"failed":0}
------------------------------
• [4.214 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:04:07.528
    Jan 19 22:04:07.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 22:04:07.529
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:07.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:07.557
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-cc8db7b5-c70c-4b04-b4a9-8242aa4a371f 01/19/23 22:04:07.64
    STEP: Creating a pod to test consume secrets 01/19/23 22:04:07.652
    Jan 19 22:04:07.685: INFO: Waiting up to 5m0s for pod "pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e" in namespace "secrets-6162" to be "Succeeded or Failed"
    Jan 19 22:04:07.691: INFO: Pod "pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.08486ms
    Jan 19 22:04:09.694: INFO: Pod "pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009798199s
    Jan 19 22:04:11.694: INFO: Pod "pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009575782s
    STEP: Saw pod success 01/19/23 22:04:11.694
    Jan 19 22:04:11.694: INFO: Pod "pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e" satisfied condition "Succeeded or Failed"
    Jan 19 22:04:11.697: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e container secret-volume-test: <nil>
    STEP: delete the pod 01/19/23 22:04:11.702
    Jan 19 22:04:11.710: INFO: Waiting for pod pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e to disappear
    Jan 19 22:04:11.713: INFO: Pod pod-secrets-ce257963-ddec-4484-8bb6-9dd3545c4d4e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 22:04:11.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6162" for this suite. 01/19/23 22:04:11.717
    STEP: Destroying namespace "secret-namespace-5804" for this suite. 01/19/23 22:04:11.735
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:04:11.742
Jan 19 22:04:11.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename dns 01/19/23 22:04:11.743
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:11.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:11.758
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/19/23 22:04:11.762
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local;sleep 1; done
 01/19/23 22:04:11.773
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local;sleep 1; done
 01/19/23 22:04:11.773
STEP: creating a pod to probe DNS 01/19/23 22:04:11.774
STEP: submitting the pod to kubernetes 01/19/23 22:04:11.774
Jan 19 22:04:11.810: INFO: Waiting up to 15m0s for pod "dns-test-759f784c-0858-40b6-8c00-560fd154ef02" in namespace "dns-8260" to be "running"
Jan 19 22:04:11.815: INFO: Pod "dns-test-759f784c-0858-40b6-8c00-560fd154ef02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.431918ms
Jan 19 22:04:13.819: INFO: Pod "dns-test-759f784c-0858-40b6-8c00-560fd154ef02": Phase="Running", Reason="", readiness=true. Elapsed: 2.008365487s
Jan 19 22:04:13.819: INFO: Pod "dns-test-759f784c-0858-40b6-8c00-560fd154ef02" satisfied condition "running"
STEP: retrieving the pod 01/19/23 22:04:13.819
STEP: looking for the results for each expected name from probers 01/19/23 22:04:13.821
Jan 19 22:04:13.826: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
Jan 19 22:04:13.829: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
Jan 19 22:04:13.831: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
Jan 19 22:04:13.835: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
Jan 19 22:04:13.838: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
Jan 19 22:04:13.840: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
Jan 19 22:04:13.844: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
Jan 19 22:04:13.847: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
Jan 19 22:04:13.847: INFO: Lookups using dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

Jan 19 22:04:18.874: INFO: DNS probes using dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02 succeeded

STEP: deleting the pod 01/19/23 22:04:18.874
STEP: deleting the test headless service 01/19/23 22:04:18.889
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jan 19 22:04:18.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8260" for this suite. 01/19/23 22:04:18.907
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":356,"skipped":6573,"failed":0}
------------------------------
• [SLOW TEST] [7.169 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:04:11.742
    Jan 19 22:04:11.742: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename dns 01/19/23 22:04:11.743
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:11.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:11.758
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/19/23 22:04:11.762
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local;sleep 1; done
     01/19/23 22:04:11.773
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local;sleep 1; done
     01/19/23 22:04:11.773
    STEP: creating a pod to probe DNS 01/19/23 22:04:11.774
    STEP: submitting the pod to kubernetes 01/19/23 22:04:11.774
    Jan 19 22:04:11.810: INFO: Waiting up to 15m0s for pod "dns-test-759f784c-0858-40b6-8c00-560fd154ef02" in namespace "dns-8260" to be "running"
    Jan 19 22:04:11.815: INFO: Pod "dns-test-759f784c-0858-40b6-8c00-560fd154ef02": Phase="Pending", Reason="", readiness=false. Elapsed: 4.431918ms
    Jan 19 22:04:13.819: INFO: Pod "dns-test-759f784c-0858-40b6-8c00-560fd154ef02": Phase="Running", Reason="", readiness=true. Elapsed: 2.008365487s
    Jan 19 22:04:13.819: INFO: Pod "dns-test-759f784c-0858-40b6-8c00-560fd154ef02" satisfied condition "running"
    STEP: retrieving the pod 01/19/23 22:04:13.819
    STEP: looking for the results for each expected name from probers 01/19/23 22:04:13.821
    Jan 19 22:04:13.826: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
    Jan 19 22:04:13.829: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
    Jan 19 22:04:13.831: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
    Jan 19 22:04:13.835: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
    Jan 19 22:04:13.838: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
    Jan 19 22:04:13.840: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
    Jan 19 22:04:13.844: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
    Jan 19 22:04:13.847: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local from pod dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02: the server could not find the requested resource (get pods dns-test-759f784c-0858-40b6-8c00-560fd154ef02)
    Jan 19 22:04:13.847: INFO: Lookups using dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8260.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8260.svc.cluster.local jessie_udp@dns-test-service-2.dns-8260.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8260.svc.cluster.local]

    Jan 19 22:04:18.874: INFO: DNS probes using dns-8260/dns-test-759f784c-0858-40b6-8c00-560fd154ef02 succeeded

    STEP: deleting the pod 01/19/23 22:04:18.874
    STEP: deleting the test headless service 01/19/23 22:04:18.889
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jan 19 22:04:18.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8260" for this suite. 01/19/23 22:04:18.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:04:18.913
Jan 19 22:04:18.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename secrets 01/19/23 22:04:18.914
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:18.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:18.933
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-cb4f5328-d8b0-41f9-93e2-c9e9c4e1ade6 01/19/23 22:04:18.936
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jan 19 22:04:18.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4140" for this suite. 01/19/23 22:04:18.946
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":357,"skipped":6623,"failed":0}
------------------------------
• [0.058 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:04:18.913
    Jan 19 22:04:18.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename secrets 01/19/23 22:04:18.914
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:18.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:18.933
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-cb4f5328-d8b0-41f9-93e2-c9e9c4e1ade6 01/19/23 22:04:18.936
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jan 19 22:04:18.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4140" for this suite. 01/19/23 22:04:18.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:04:18.973
Jan 19 22:04:18.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename replicaset 01/19/23 22:04:18.973
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:18.989
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:18.991
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/19/23 22:04:18.993
W0119 22:04:19.001434      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Verify that the required pods have come up 01/19/23 22:04:19.001
Jan 19 22:04:19.005: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan 19 22:04:24.010: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/19/23 22:04:24.01
Jan 19 22:04:24.013: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/19/23 22:04:24.013
STEP: DeleteCollection of the ReplicaSets 01/19/23 22:04:24.023
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/19/23 22:04:24.03
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jan 19 22:04:24.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3589" for this suite. 01/19/23 22:04:24.046
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":358,"skipped":6658,"failed":0}
------------------------------
• [SLOW TEST] [5.079 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:04:18.973
    Jan 19 22:04:18.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename replicaset 01/19/23 22:04:18.973
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:18.989
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:18.991
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/19/23 22:04:18.993
    W0119 22:04:19.001434      22 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "httpd" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "httpd" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "httpd" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "httpd" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Verify that the required pods have come up 01/19/23 22:04:19.001
    Jan 19 22:04:19.005: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan 19 22:04:24.010: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/19/23 22:04:24.01
    Jan 19 22:04:24.013: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/19/23 22:04:24.013
    STEP: DeleteCollection of the ReplicaSets 01/19/23 22:04:24.023
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/19/23 22:04:24.03
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jan 19 22:04:24.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3589" for this suite. 01/19/23 22:04:24.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:04:24.052
Jan 19 22:04:24.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 22:04:24.053
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:24.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:24.073
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206
STEP: fetching services 01/19/23 22:04:24.075
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 22:04:24.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7998" for this suite. 01/19/23 22:04:24.097
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":359,"skipped":6678,"failed":0}
------------------------------
• [0.061 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:04:24.052
    Jan 19 22:04:24.053: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 22:04:24.053
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:24.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:24.073
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3206
    STEP: fetching services 01/19/23 22:04:24.075
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 22:04:24.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7998" for this suite. 01/19/23 22:04:24.097
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:04:24.114
Jan 19 22:04:24.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename services 01/19/23 22:04:24.115
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:24.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:24.143
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641
STEP: creating a collection of services 01/19/23 22:04:24.145
Jan 19 22:04:24.145: INFO: Creating e2e-svc-a-jnprr
Jan 19 22:04:24.165: INFO: Creating e2e-svc-b-sz5gt
Jan 19 22:04:24.191: INFO: Creating e2e-svc-c-x7d2d
STEP: deleting service collection 01/19/23 22:04:24.209
Jan 19 22:04:24.272: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jan 19 22:04:24.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-529" for this suite. 01/19/23 22:04:24.279
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":360,"skipped":6688,"failed":0}
------------------------------
• [0.176 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3641

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:04:24.114
    Jan 19 22:04:24.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename services 01/19/23 22:04:24.115
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:24.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:24.143
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3641
    STEP: creating a collection of services 01/19/23 22:04:24.145
    Jan 19 22:04:24.145: INFO: Creating e2e-svc-a-jnprr
    Jan 19 22:04:24.165: INFO: Creating e2e-svc-b-sz5gt
    Jan 19 22:04:24.191: INFO: Creating e2e-svc-c-x7d2d
    STEP: deleting service collection 01/19/23 22:04:24.209
    Jan 19 22:04:24.272: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jan 19 22:04:24.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-529" for this suite. 01/19/23 22:04:24.279
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:04:24.29
Jan 19 22:04:24.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename configmap 01/19/23 22:04:24.291
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:24.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:24.321
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-36594756-a40d-4538-ae91-6e3e6997f22b 01/19/23 22:04:24.323
STEP: Creating a pod to test consume configMaps 01/19/23 22:04:24.338
Jan 19 22:04:24.366: INFO: Waiting up to 5m0s for pod "pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc" in namespace "configmap-3389" to be "Succeeded or Failed"
Jan 19 22:04:24.368: INFO: Pod "pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552364ms
Jan 19 22:04:26.373: INFO: Pod "pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007461366s
Jan 19 22:04:28.371: INFO: Pod "pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005747303s
STEP: Saw pod success 01/19/23 22:04:28.371
Jan 19 22:04:28.372: INFO: Pod "pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc" satisfied condition "Succeeded or Failed"
Jan 19 22:04:28.374: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc container agnhost-container: <nil>
STEP: delete the pod 01/19/23 22:04:28.379
Jan 19 22:04:28.388: INFO: Waiting for pod pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc to disappear
Jan 19 22:04:28.390: INFO: Pod pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jan 19 22:04:28.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3389" for this suite. 01/19/23 22:04:28.394
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":361,"skipped":6698,"failed":0}
------------------------------
• [4.109 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:04:24.29
    Jan 19 22:04:24.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename configmap 01/19/23 22:04:24.291
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:24.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:24.321
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-36594756-a40d-4538-ae91-6e3e6997f22b 01/19/23 22:04:24.323
    STEP: Creating a pod to test consume configMaps 01/19/23 22:04:24.338
    Jan 19 22:04:24.366: INFO: Waiting up to 5m0s for pod "pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc" in namespace "configmap-3389" to be "Succeeded or Failed"
    Jan 19 22:04:24.368: INFO: Pod "pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552364ms
    Jan 19 22:04:26.373: INFO: Pod "pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007461366s
    Jan 19 22:04:28.371: INFO: Pod "pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005747303s
    STEP: Saw pod success 01/19/23 22:04:28.371
    Jan 19 22:04:28.372: INFO: Pod "pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc" satisfied condition "Succeeded or Failed"
    Jan 19 22:04:28.374: INFO: Trying to get logs from node ip-10-0-172-44.ec2.internal pod pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc container agnhost-container: <nil>
    STEP: delete the pod 01/19/23 22:04:28.379
    Jan 19 22:04:28.388: INFO: Waiting for pod pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc to disappear
    Jan 19 22:04:28.390: INFO: Pod pod-configmaps-4acbdbf3-4a04-43fc-bb92-ceca0df552cc no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jan 19 22:04:28.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3389" for this suite. 01/19/23 22:04:28.394
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 01/19/23 22:04:28.399
Jan 19 22:04:28.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
STEP: Building a namespace api object, basename endpointslice 01/19/23 22:04:28.4
STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:28.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:28.422
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jan 19 22:04:30.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2102" for this suite. 01/19/23 22:04:30.502
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":362,"skipped":6699,"failed":0}
------------------------------
• [2.112 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 01/19/23 22:04:28.399
    Jan 19 22:04:28.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3983998075
    STEP: Building a namespace api object, basename endpointslice 01/19/23 22:04:28.4
    STEP: Waiting for a default service account to be provisioned in namespace 01/19/23 22:04:28.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/19/23 22:04:28.422
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jan 19 22:04:30.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2102" for this suite. 01/19/23 22:04:30.502
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":362,"skipped":6705,"failed":0}
Jan 19 22:04:30.512: INFO: Running AfterSuite actions on all nodes
Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jan 19 22:04:30.512: INFO: Running AfterSuite actions on node 1
Jan 19 22:04:30.512: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 19 22:04:30.512: INFO: Running AfterSuite actions on all nodes
    Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jan 19 22:04:30.512: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jan 19 22:04:30.512: INFO: Running AfterSuite actions on node 1
    Jan 19 22:04:30.512: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.056 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 362 of 7067 Specs in 5694.695 seconds
SUCCESS! -- 362 Passed | 0 Failed | 0 Pending | 6705 Skipped
PASS

Ginkgo ran 1 suite in 1h34m54.913220329s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m

